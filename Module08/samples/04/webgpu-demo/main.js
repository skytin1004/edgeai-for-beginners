const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');
const metricsEl = document.getElementById('metrics');
const loadTimeEl = document.getElementById('load-time');
const inferenceTimeEl = document.getElementById('inference-time');
const predictionEl = document.getElementById('prediction');

function log(msg) {
    const timestamp = new Date().toLocaleTimeString();
    outputEl.textContent += `[${timestamp}] ${msg}\n`;
    outputEl.scrollTop = outputEl.scrollHeight;
    console.log(msg);
}

function updateStatus(message, type = 'info') {
    statusEl.textContent = message;
    statusEl.className = `status ${type}`;
}

function updateMetric(elementId, value) {
    document.getElementById(elementId).textContent = value;
}

(async () => {
    let startTime, endTime;
    
    try {
        // Step 1: WebGPU Detection
        log('ğŸ” Checking WebGPU availability...');
        
        if (!('gpu' in navigator)) {
            updateStatus('âŒ WebGPU not available in this browser', 'error');
            log('âŒ WebGPU not supported. Please use Chrome/Edge 113+ with WebGPU enabled.');
            log('ğŸ’¡ Check chrome://gpu to verify WebGPU status');
            log('ğŸ”„ Falling back to CPU execution...');
            
            // Continue with CPU fallback
            await runWithCPU();
            return;
        }
        
        log('âœ… WebGPU API detected');
        updateStatus('ğŸ” WebGPU detected. Initializing ONNX Runtime...', 'success');
        
        // Step 2: ONNX Runtime Initialization
        log('ğŸ“¦ Initializing ONNX Runtime Web...');
        
        // Configure ONNX Runtime for WebGPU
        ort.env.wasm.wasmPaths = 'https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/';
        
        log('ğŸŒ Loading MNIST classification model...');
        updateStatus('ğŸ“¥ Loading AI model (MNIST classifier)...', 'info');
        
        startTime = performance.now();
        
        // Use a small, publicly available ONNX model
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        // Create inference session with WebGPU
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu', 'wasm'], // WebGPU first, fallback to WASM
            graphOptimizationLevel: 'all'
        });
        
        endTime = performance.now();
        const loadTime = Math.round(endTime - startTime);
        
        log('âœ… ONNX Runtime session created successfully');
        log(`âš¡ Model loaded in ${loadTime}ms`);
        log(`ğŸ“Š Execution provider: ${session.executionProviders.join(', ')}`);
        log(`ğŸ”§ Input names: [${session.inputNames.join(', ')}]`);
        log(`ğŸ”§ Output names: [${session.outputNames.join(', ')}]`);
        
        updateMetric('load-time', loadTime);
        updateStatus('ğŸš€ Running inference on sample data...', 'info');
        
        // Step 3: Inference
        log('ğŸ¯ Creating sample input data (28x28 handwritten digit)...');
        
        // Create a sample input that resembles a handwritten digit
        const inputData = new Float32Array(1 * 1 * 28 * 28);
        
        // Create a simple pattern (like a "1")
        for (let i = 0; i < 28; i++) {
            for (let j = 0; j < 28; j++) {
                const idx = i * 28 + j;
                if (j >= 12 && j <= 15) { // Vertical line for "1"
                    inputData[idx] = Math.random() * 0.5 + 0.5; // Some noise
                } else {
                    inputData[idx] = Math.random() * 0.1; // Background noise
                }
            }
        }
        
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        log('ğŸ”¥ Running WebGPU inference...');
        startTime = performance.now();
        
        // Prepare feeds
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        // Run inference
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        endTime = performance.now();
        const inferenceTime = Math.round(endTime - startTime);
        
        // Step 4: Process Results
        log(`âš¡ Inference completed in ${inferenceTime}ms`);
        
        // Find prediction (argmax)
        let maxIdx = 0;
        let maxValue = output.data[0];
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > maxValue) {
                maxValue = output.data[i];
                maxIdx = i;
            }
        }
        
        const confidence = (maxValue * 100).toFixed(1);
        
        log(`ğŸ¯ Predicted digit: ${maxIdx} (confidence: ${confidence}%)`);
        log(`ğŸ“ˆ Raw predictions: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
        // Update metrics display
        updateMetric('inference-time', inferenceTime);
        updateMetric('prediction', `${maxIdx} (${confidence}%)`);
        metricsEl.style.display = 'grid';
        
        updateStatus('âœ… WebGPU inference completed successfully!', 'success');
        
        // Step 5: Performance Analysis
        log('\nğŸ“Š Performance Summary:');
        log(`   â€¢ Model Load Time: ${loadTime}ms`);
        log(`   â€¢ Inference Time: ${inferenceTime}ms`);
        log(`   â€¢ Execution Provider: ${session.executionProviders[0]}`);
        log(`   â€¢ Total Time: ${loadTime + inferenceTime}ms`);
        
        log('\nğŸ‰ Demo completed successfully!');
        log('ğŸ’¡ This demonstrates running AI models directly in your browser');
        log('ğŸ”’ All processing happened locally - no data left your device');
        
    } catch (error) {
        updateStatus(`âŒ Error: ${error.message}`, 'error');
        log(`âŒ Error occurred: ${error.message}`);
        log(`ğŸ“ Full error: ${error.stack || error}`);
        
        log('\nğŸ”§ Troubleshooting suggestions:');
        log('   â€¢ Ensure you\'re using Chrome/Edge 113+ with WebGPU enabled');
        log('   â€¢ Check chrome://gpu for WebGPU status');
        log('   â€¢ Try refreshing the page');
        log('   â€¢ Check browser console for additional details');
        
        console.error('WebGPU Demo Error:', error);
    }
})();

// CPU Fallback Implementation
async function runWithCPU() {
    try {
        log('ğŸ”„ Initializing CPU fallback mode...');
        updateStatus('ğŸ”„ Running with CPU execution (WebGPU unavailable)', 'warning');
        
        const startTime = performance.now();
        
        // Use WASM execution provider
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['wasm']
        });
        
        const loadTime = Math.round(performance.now() - startTime);
        log(`âœ… Model loaded with CPU in ${loadTime}ms`);
        
        // Run same inference logic
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const inferenceStart = performance.now();
        const results = await session.run(feeds);
        const inferenceTime = Math.round(performance.now() - inferenceStart);
        
        const output = results[session.outputNames[0]];
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        updateMetric('load-time', loadTime);
        updateMetric('inference-time', inferenceTime);
        updateMetric('prediction', maxIdx);
        metricsEl.style.display = 'grid';
        
        updateStatus('âœ… CPU inference completed (WebGPU would be faster)', 'success');
        log(`ğŸ¯ CPU prediction: ${maxIdx}`);
        log('ğŸ’¡ For better performance, use a WebGPU-compatible browser');
        
    } catch (error) {
        updateStatus(`âŒ CPU fallback failed: ${error.message}`, 'error');
        log(`âŒ CPU execution failed: ${error.message}`);
    }
}