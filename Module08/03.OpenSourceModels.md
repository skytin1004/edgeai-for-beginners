# Session 3: Open-Source Models with Foundry Local

## Overview

This session explores how to bring open-source models to Foundry Local: selecting community models, integrating Hugging Face content, and adopting “bring your own model” (BYOM) strategies. You’ll also discover the Model Mondays series for continuous learning and model discovery.

References:
- Foundry Local docs: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Compile Hugging Face models: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Learning Objectives
- Discover and evaluate open-source models for local inference
- Compile and run select Hugging Face models within Foundry Local
- Apply model selection strategies for accuracy, latency, and resource needs
- Manage models locally with cache and versioning

## Part 1: Model Discovery and Selection (Step-by-step)

Step 1) List available models in the local catalog
```cmd
foundry model list
```

Step 2) Quick try of two candidates (auto-downloads on first run)
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```

Step 3) Note basic metrics
- Observe latency (subjective) and quality for a fixed prompt
- Watch memory usage via Task Manager while each model runs

## Part 2: Running Catalog Models via CLI (Step-by-step)

Step 1) Start a model
```cmd
foundry model run llama-3.2
```

Step 2) Send a test prompt via the OpenAI-compatible endpoint
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```

## Part 3: BYOM – Compile Hugging Face Models (Step-by-step)

Follow the official how-to for compiling models. High-level flow below—see the Microsoft Learn article for exact commands and supported configurations.

Step 1) Prepare a working directory
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```

Step 2) Compile a supported HF model
- Use the steps from the Learn doc to convert and place the compiled ONNX model in your `models` directory
- Confirm with:
```cmd
foundry cache ls
```
You should see your compiled model name (for example, `llama-3.2`).

Step 3) Run the compiled model
```cmd
foundry model run llama-3.2 --verbose
```

Notes:
- Ensure sufficient disk and RAM for compile and run
- Start with smaller models to validate flow, then scale up

## Part 4: Practical Model Curation (Step-by-step)

Step 1) Create a `models.json` registry
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```

Step 2) Small selector script
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```

## Part 5: Hands-On Benchmarks (Step-by-step)

Step 1) Simple latency benchmark
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```

Step 2) Quality spot-check
- Use a fixed prompt set, capture outputs to a CSV/JSON
- Manually rate fluency, relevance, and correctness (1–5)

## Part 6: Next Steps
- Subscribe to Model Mondays for new models and tips: https://aka.ms/model-mondays
- Contribute findings to your team’s `models.json`
- Prepare for Session 4: comparing LLMs vs SLMs, local vs cloud inference, and hands-on demos
