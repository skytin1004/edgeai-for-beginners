#############################################
# Workshop Environment Variables (.env)
#
# This file centralizes configuration for the Workshop samples.
# All Python scripts and notebooks pick these up via os.getenv().
#
# SDK Reference:
#   https://github.com/microsoft/Foundry-Local
#   https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local
#
# Prerequisites:
#   1. Install Foundry Local: Follow installation guide
#   2. Start service: foundry service start
#   3. Load model: foundry model run phi-4-mini
#   4. Verify: foundry service status
#
# Usage:
#   - Python scripts: Automatically loaded by samples
#   - Notebooks: May need to restart kernel after changes
#   - Terminal: Source this file or set manually
#
# Adjust values as needed per machine and deployment.
#############################################

# Python search path so helper utilities (workshop_utils) & samples are importable
# Append Workshop/samples for utilities; keep existing Module08 if needed.
PYTHONPATH=${workspaceFolder}/Workshop/samples;${workspaceFolder}/Module08

# Core Model Aliases
# Default model for most samples
FOUNDRY_LOCAL_ALIAS=phi-4-mini

# Model comparison aliases (Session 04)
SLM_ALIAS=phi-4-mini
LLM_ALIAS=qwen2.5-7b

# Endpoint override (leave blank to allow FoundryLocalManager to auto-detect)
# Only set this if you need to override the default endpoint
# Example for remote Windows host when developing on macOS/Linux:
# FOUNDRY_LOCAL_ENDPOINT=http://192.168.1.50:5273/v1
# Example for local custom port:
# FOUNDRY_LOCAL_ENDPOINT=http://localhost:8000
FOUNDRY_LOCAL_ENDPOINT=

# Benchmark Configuration (Session 03)
# Comma-separated list of model aliases to benchmark
BENCH_MODELS=phi-4-mini,qwen2.5-0.5b,gemma-2-2b
BENCH_ROUNDS=3
BENCH_PROMPT=Explain retrieval augmented generation briefly.
BENCH_STREAM=0            # Set to 1 to measure first-token latency (streaming)
COMPARE_RETRIES=2         # Number of retry attempts for model comparison

# RAG Configuration (Session 02)
EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
RAG_QUESTION=Why use RAG with local inference?

# Multi-Agent Configuration (Session 05)
AGENT_QUESTION=Explain why edge AI matters for compliance.
AGENT_MODEL_PRIMARY=phi-4-mini
AGENT_MODEL_EDITOR=phi-4-mini

# Model Comparison (Session 04)
COMPARE_PROMPT=List 5 benefits of local AI inference.

# Reliability / Telemetry
SHOW_USAGE=1              # Print token usage per completion
RETRY_ON_FAIL=1           # Retry transient errors once
RETRY_BACKOFF=1.0         # Seconds between retry attempts

# (Optional) Deterministic settings applied inside scripts; override if experimenting
# TEMPERATURE=0.0
# TOP_P=1.0

# Azure OpenAI Configuration (Optional - for hybrid scenarios)
# Add any cloud API secrets below (avoid committing real keys to version control)
# AZURE_OPENAI_ENDPOINT=
# AZURE_OPENAI_API_KEY=
# AZURE_OPENAI_API_VERSION=2024-08-01-preview

#############################################
# Recommended Model Configurations
#############################################
#
# Development & Testing:
#   FOUNDRY_LOCAL_ALIAS=phi-4-mini          # Balanced quality & speed
#   SLM_ALIAS=phi-4-mini                     # Fast responses
#   LLM_ALIAS=qwen2.5-7b                     # Higher quality
#
# Production Scenarios:
#   General purpose: phi-4-mini
#   Code generation: deepseek-coder-1.3b
#   Fast classification: qwen2.5-0.5b
#   High quality: qwen2.5-7b
#
# Benchmark Testing:
#   BENCH_MODELS=phi-4-mini,qwen2.5-0.5b,gemma-2-2b
#
# Multi-Agent (different models per role):
#   AGENT_MODEL_PRIMARY=phi-4-mini          # Fast for research
#   AGENT_MODEL_EDITOR=qwen2.5-7b           # Quality for editing
#
#############################################
# Troubleshooting
#############################################
#
# Service not responding:
#   1. Check: foundry service status
#   2. Start: foundry service start
#   3. Load model: foundry model run phi-4-mini
#
# Model not found:
#   1. List available: foundry model list
#   2. Update FOUNDRY_LOCAL_ALIAS to match available model
#
# Connection errors:
#   1. Verify endpoint: foundry service status (shows port)
#   2. Set FOUNDRY_LOCAL_ENDPOINT if needed
#   3. Check firewall settings
#
# Import errors:
#   1. Activate venv: .venv\Scripts\activate (Windows)
#   2. Install deps: pip install -r requirements.txt
#   3. Check PYTHONPATH includes Workshop/samples
#
#############################################

# End of .env