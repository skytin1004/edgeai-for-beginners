# EdgeAI for Beginners: Learning Paths and Study Schedule

### Concentrated Learning Path (1 week)

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 1 | Module 1: EdgeAI Fundamentals | 3 hours |
| Day 2 | Module 2: SLM Foundations | 3 hours |
| Day 3 | Module 3: SLM Deployment | 2 hours |
| Day 4-5 | Module 4: Model Optimization (6 frameworks) | 4 hours |
| Day 6 | Module 5: SLMOps | 3 hours |
| Day 7 | Module 6-7: AI Agents & Development Tools | 5 hours |

### Concentrated Learning Path (2 weeks)

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 1-2 | Module 1: EdgeAI Fundamentals | 3 hours |
| Day 3-4 | Module 2: SLM Foundations | 3 hours |
| Day 5-6 | Module 3: SLM Deployment | 2 hours |
| Day 7-8 | Module 4: Model Optimization | 4 hours |
| Day 9-10 | Module 5: SLMOps | 3 hours |
| Day 11-12 | Module 6: AI Agents | 2 hours |
| Day 13-14 | Module 7: Development Tools | 3 hours |

### Part-time Study (4 weeks)

| Week | Focus | Estimated Hours |
|------|-------|------------------|
| Week 1 | Module 1-2: Fundamentals & SLM Foundations | 6 hours |
| Week 2 | Module 3-4: Deployment & Optimization | 6 hours |
| Week 3 | Module 5-6: SLMOps & AI Agents | 5 hours |
| Week 4 | Module 7: Development Tools & Integration | 3 hours |

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 1-2 | Module 1: EdgeAI Fundamentals | 3 hours |
| Day 3-4 | Module 2: SLM Foundations | 3 hours |
| Day 5-6 | Module 3: SLM Deployment | 2 hours |
| Day 7-8 | Module 4: Model Optimization | 4 hours |
| Day 9-10 | Module 5: SLMOps | 3 hours |
| Day 11-12 | Module 6: SLM Agentic Systems | 2 hours |
| Day 13-14 | Module 7 : EdgeAI Implementation Samples | 2 hours |

| Module | Completion Date | Hours Spent | Key Takeaways |
|--------|----------------|-------------|--------------|
| Module 1: EdgeAI Fundamentals | | | |
| Module 2: SLM Foundations | | | |
| Module 3: SLM Deployment | | | |
| Module 4: Model Optimization (6 frameworks) | | | |
| Module 5: SLMOps | | | |
| Module 6: SLM Agentic Systems | | | |
| Module 7: EdgeAI Implementation Samples | | | |
| Hands-on Exercises | | | |
| Mini-Project | | | |

### Part-time Study (4 weeks)

| Week | Focus | Estimated Hours |
|------|-------|------------------|
| Week 1 | Module 1-2: Fundamentals & SLM Foundations | 6 hours |
| Week 2 | Module 3-4: Deployment & Optimization | 6 hours |
| Week 3 | Module 5-6: SLMOps & AI Agents | 5 hours |
| Week 4 | Module 7: Development Tools & Integration | 3 hours |udy Guide

## Introduction

Welcome to the EdgeAI for Beginners study guide! This document is designed to help you navigate the course materials effectively and maximize your learning experience. It provides structured learning paths, suggested study schedules, key concept summaries, and supplementary resources to deepen your understanding of Edge AI technologies.

This is a concise 20-hour course that delivers essential knowledge about EdgeAI in a time-efficient format, making it perfect for busy professionals and students who want to quickly gain practical skills in this emerging field.

## Course Overview

This course is organized into seven comprehensive modules:

1. **EdgeAI Fundamentals and Transformation** - Understanding the core concepts and technology shift
2. **Small Language Model Foundations** - Exploring various SLM families and their architectures
3. **Small Language Model Deployment** - Implementing practical deployment strategies
4. **Model Format Conversion and Quantization** - Advanced optimization with 6 frameworks including OpenVINO
5. **SLMOps - Small Language Model Operations** - Production lifecycle management and deployment
6. **SLM Agentic Systems** - AI agents, function calling, and Model Context Protocol
7. **EdgeAI Implementation Samples** - AI Toolkit, Windows development, and platform-specific implementations

## How to Use This Study Guide

- **Progressive Learning**: Follow the modules in order for the most coherent learning experience
- **Knowledge Checkpoints**: Use the self-assessment questions after each section
- **Hands-on Practice**: Complete the suggested exercises to reinforce theoretical concepts
- **Supplementary Resources**: Explore additional materials for topics that interest you most

## Study Schedule Recommendations

### Concentrated Learning Path (1 week)

| Day | Focus | Estimated Hours |
|------|-------|-----------------|
| Day 1-2 | Module 1: EdgeAI Fundamentals | 6 hours |
| Day 3-4 | Module 2: SLM Foundations | 8 hours |
| Day 5-6 | Module 3: SLM Deployment | 6 hours |

### Part-time Study (3 weeks)

| Week | Focus | Estimated Hours |
|------|-------|-----------------|
| Week 1 | Module 1: EdgeAI Fundamentals | 6-7 hours |
| Week 2 | Module 2: SLM Foundations | 7-8 hours |
| Week 3 | Module 3: SLM Deployment | 5-6 hours |

## Module 1: EdgeAI Fundamentals and Transformation

### Key Learning Objectives

- Understand the differences between cloud-based and edge-based AI
- Master core optimization techniques for resource-constrained environments
- Analyze real-world applications of EdgeAI technologies
- Set up a development environment for EdgeAI projects

### Study Focus Areas

#### Section 1: EdgeAI Fundamentals
- **Priority Concepts**: 
  - Edge vs. Cloud computing paradigms
  - Model quantization techniques
  - Hardware acceleration options (NPUs, GPUs, CPUs)
  - Privacy and security advantages

- **Supplementary Materials**:
  - [TensorFlow Lite Documentation](https://www.tensorflow.org/lite)
  - [ONNX Runtime GitHub](https://github.com/microsoft/onnxruntime)
  - [Edge Impulse Documentation](https://docs.edgeimpulse.com)

#### Section 2: Real-World Case Studies
- **Priority Concepts**: 
  - Microsoft Phi & Mu model ecosystem
  - Practical implementations across industries
  - Deployment considerations

#### Section 3: Practical Implementation Guide
- **Priority Concepts**: 
  - Development environment setup
  - Quantization and optimization tools
  - Assessment methods for EdgeAI implementations

#### Section 4: Edge Deployment Hardware
- **Priority Concepts**: 
  - Hardware platform comparisons
  - Optimization strategies for specific hardware
  - Deployment considerations

### Self-Assessment Questions

1. Compare and contrast cloud-based AI with edge-based AI implementations.
2. Explain three key techniques for optimizing models for edge deployment.
3. What are the primary advantages of running AI models at the edge?
4. Describe the process of quantizing a model and how it affects performance.
5. Explain how different hardware accelerators (NPUs, GPUs, CPUs) influence EdgeAI deployment.

### Hands-on Exercises

1. **Quick Environment Setup**: Configure a minimal development environment with the essential packages (30 minutes)
2. **Model Exploration**: Download and examine a pre-trained small language model (1 hour)
3. **Basic Quantization**: Try simple quantization on a small model (1 hour)

## Module 2: Small Language Model Foundations

### Key Learning Objectives

- Understand the architectural principles of different SLM families
- Compare model capabilities across different parameter scales
- Evaluate models based on efficiency, capability, and deployment requirements
- Recognize appropriate use cases for different model families

### Study Focus Areas

#### Section 1: Microsoft Phi Model Family
- **Priority Concepts**: 
  - Design philosophy evolution
  - Efficiency-first architecture
  - Specialized capabilities

#### Section 2: Qwen Family
- **Priority Concepts**: 
  - Open source contributions
  - Scalable deployment options
  - Advanced reasoning architecture

#### Section 3: Gemma Family
- **Priority Concepts**: 
  - Research-driven innovation
  - Multimodal capabilities
  - Mobile optimization

#### Section 4: BitNET Family
- **Priority Concepts**: 
  - 1-bit quantization technology
  - Inference optimization framework
  - Sustainability considerations

#### Section 5: Microsoft Mu Model
- **Priority Concepts**: 
  - Device-first architecture
  - System integration with Windows
  - Privacy-preserving operation

#### Section 6: Phi-Silica
- **Priority Concepts**: 
  - NPU-optimized architecture
  - Performance metrics
  - Developer integration

### Self-Assessment Questions

1. Compare the architectural approaches of the Phi and Qwen model families.
2. Explain how BitNET's quantization technology differs from traditional quantization.
3. What are the unique advantages of the Mu model for Windows integration?
4. Describe how Phi-Silica leverages NPU hardware for performance optimization.
5. For a mobile application with limited connectivity, which model family would be most appropriate and why?

### Hands-on Exercises

1. **Model Comparison**: Quick benchmark of two different SLM models (1 hour)
2. **Simple Text Generation**: Basic implementation of text generation with a small model (1 hour)
3. **Fast Optimization**: Apply one optimization technique to improve inference speed (1 hour)

## Module 3: Small Language Model Deployment

### Key Learning Objectives

- Select appropriate models based on deployment constraints
- Master optimization techniques for various deployment scenarios
- Implement SLMs in both local and cloud environments
- Design production-ready configurations for EdgeAI applications

### Study Focus Areas

#### Section 1: SLM Advanced Learning
- **Priority Concepts**: 
  - Parameter classification framework
  - Advanced optimization techniques
  - Model acquisition strategies

#### Section 2: Local Environment Deployment
- **Priority Concepts**: 
  - Ollama platform deployment
  - Microsoft Foundry local solutions
  - Framework comparative analysis

#### Section 3: Containerized Cloud Deployment
- **Priority Concepts**: 
  - vLLM high-performance inference
  - Container orchestration
  - ONNX Runtime implementation

### Self-Assessment Questions

1. What factors should be considered when selecting between local deployment and cloud deployment?
2. Compare Ollama and Microsoft Foundry Local as deployment options.
3. Explain the benefits of containerization for SLM deployment.
4. What are the key performance metrics to monitor for an edge-deployed SLM?
5. Describe a complete deployment workflow from model selection to production implementation.

### Hands-on Exercises

1. **Basic Local Deployment**: Deploy a simple SLM using Ollama (1 hour)
2. **Performance Check**: Run a quick benchmark on your deployed model (30 minutes)
3. **Simple Integration**: Create a minimal application that uses your deployed model (1 hour)

## Module 4: Model Format Conversion and Quantization

### Key Learning Objectives

- Master advanced quantization techniques from 1-bit to 8-bit precision
- Understand format conversion strategies (GGUF, ONNX)
- Implement optimization across six frameworks (Llama.cpp, Olive, OpenVINO, MLX, workflow synthesis)
- Deploy optimized models for production edge environments across Intel, Apple, and cross-platform hardware

### Study Focus Areas

#### Section 1: Quantization Foundations
- **Priority Concepts**: 
  - Precision classification framework
  - Performance vs. accuracy trade-offs
  - Memory footprint optimization

#### Section 2: Llama.cpp Implementation
- **Priority Concepts**: 
  - Cross-platform deployment
  - GGUF format optimization
  - Hardware acceleration techniques

#### Section 3: Microsoft Olive Suite
- **Priority Concepts**: 
  - Hardware-aware optimization
  - Enterprise-grade deployment
  - Automated optimization workflows

#### Section 4: OpenVINO Toolkit
- **Priority Concepts**: 
  - Intel hardware optimization
  - Neural Network Compression Framework (NNCF)
  - Cross-platform inference deployment
  - OpenVINO GenAI for LLM deployment

#### Section 5: Apple MLX Framework
- **Priority Concepts**: 
  - Apple Silicon optimization
  - Unified memory architecture
  - LoRA fine-tuning capabilities

#### Section 6: Edge AI Development Workflow Synthesis
- **Priority Concepts**: 
  - Unified workflow architecture
  - Framework selection decision trees
  - Production readiness validation
  - Future-proofing strategies

### Self-Assessment Questions

1. Compare quantization strategies across different precision levels (1-bit to 8-bit).
2. Explain the advantages of GGUF format for edge deployment.
3. How does hardware-aware optimization in Microsoft Olive improve deployment efficiency?
4. What are the key benefits of OpenVINO's NNCF for model compression?
5. Describe how Apple MLX leverages unified memory architecture for optimization.
6. How does workflow synthesis help in selecting optimal optimization frameworks?

### Hands-on Exercises

1. **Model Quantization**: Apply different quantization levels to a model and compare results (1 hour)
2. **OpenVINO Optimization**: Use NNCF to compress a model for Intel hardware (1 hour)
3. **Framework Comparison**: Test the same model across three different optimization frameworks (1 hour)
4. **Performance Benchmarking**: Measure optimization impact on inference speed and memory usage (1 hour)

## Module 5: SLMOps - Small Language Model Operations

### Key Learning Objectives

- Understand SLMOps lifecycle management principles
- Master distillation and fine-tuning techniques for edge deployment
- Implement production deployment strategies with monitoring
- Build enterprise-grade SLM operations and maintenance workflows

### Study Focus Areas

#### Section 1: Introduction to SLMOps
- **Priority Concepts**: 
  - SLMOps paradigm shift in AI operations
  - Cost efficiency and privacy-first architecture
  - Strategic business impact and competitive advantages

#### Section 2: Model Distillation
- **Priority Concepts**: 
  - Knowledge transfer techniques
  - Two-stage distillation process implementation
  - Azure ML distillation workflows

#### Section 3: Fine-tuning Strategies
- **Priority Concepts**: 
  - Parameter-efficient fine-tuning (PEFT)
  - LoRA and QLoRA advanced methods
  - Multi-adapter training and hyperparameter optimization

#### Section 4: Production Deployment
- **Priority Concepts**: 
  - Model conversion and quantization for production
  - Foundry Local deployment configuration
  - Performance benchmarking and quality validation

### Self-Assessment Questions

1. How does SLMOps differ from traditional MLOps?
2. Explain the benefits of model distillation for edge deployment.
3. What are the key considerations for fine-tuning SLMs in resource-constrained environments?
4. Describe a complete production deployment pipeline for edge AI applications.

### Hands-on Exercises

1. **Basic Distillation**: Create a smaller model from a larger teacher model (1 hour)
2. **Fine-tuning Experiment**: Fine-tune a model for a specific domain (1 hour)
3. **Deployment Pipeline**: Set up a basic CI/CD pipeline for model deployment (1 hour)

## Module 6: SLM Agentic Systems - AI Agents and Function Calling

### Key Learning Objectives

- Build intelligent AI agents for edge environments using Small Language Models
- Implement function calling capabilities with systematic workflows
- Master Model Context Protocol (MCP) integration for standardized tool interaction
- Create sophisticated agentic systems with minimal human intervention

### Study Focus Areas

#### Section 1: AI Agents and SLM Foundations
- **Priority Concepts**: 
  - Agent classification framework (reflex, model-based, goal-based, learning agents)
  - SLM vs LLM trade-offs analysis
  - Edge-specific agent design patterns
  - Resource optimization for agents

#### Section 2: Function Calling in Small Language Models
- **Priority Concepts**: 
  - Systematic workflow implementation (intent detection, JSON output, external execution)
  - Platform-specific implementations (Phi-4-mini, selected Qwen models, Microsoft Foundry Local)
  - Advanced examples (multi-agent collaboration, dynamic tool selection)
  - Production considerations (rate limiting, audit logging, security measures)

#### Section 3: Model Context Protocol (MCP) Integration
- **Priority Concepts**: 
  - Protocol architecture and layered system design
  - Multi-backend support (Ollama for development, vLLM for production)
  - Connection protocols (STDIO and SSE modes)
  - Real-world applications (web automation, data processing, API integration)

### Self-Assessment Questions

1. What are the key architectural considerations for edge AI agents?
2. How does function calling enhance agent capabilities?
3. Explain the role of Model Context Protocol in agent communication.

### Hands-on Exercises

1. **Simple Agent**: Build a basic AI agent with function calling (1 hour)
2. **MCP Integration**: Implement MCP in an agent application (30 minutes)

## Module 7: EdgeAI Implementation Samples

### Key Learning Objectives

- Master AI Toolkit for Visual Studio Code for comprehensive EdgeAI development workflows
- Gain expertise in Windows AI Foundry platform and NPU optimization strategies
- Implement EdgeAI across multiple hardware platforms and deployment scenarios
- Build production-ready EdgeAI applications with platform-specific optimizations

### Study Focus Areas

#### Section 1: AI Toolkit for Visual Studio Code
- **Priority Concepts**: 
  - Comprehensive Edge AI development environment within VS Code
  - Model catalog and discovery for edge deployment
  - Local testing, optimization, and agent development workflows
  - Performance monitoring and evaluation for edge scenarios

#### Section 2: Windows EdgeAI Development Guide
- **Priority Concepts**: 
  - Windows AI Foundry platform comprehensive overview
  - Phi Silica API for efficient NPU inference
  - Computer Vision APIs for image processing and OCR
  - Foundry Local CLI for local development and testing

#### Section 3: Platform-Specific Implementations
- **Priority Concepts**: 
  - NVIDIA Jetson Orin Nano deployment (67 TOPS AI performance)
  - Mobile applications with .NET MAUI and ONNX Runtime GenAI
  - Azure EdgeAI solutions with cloud-edge hybrid architecture
  - Windows ML optimization with universal hardware support
  - Foundry Local applications with privacy-focused RAG implementation

### Self-Assessment Questions

1. How does AI Toolkit streamline the EdgeAI development workflow?
2. Compare deployment strategies across different hardware platforms.
3. What are the advantages of Windows AI Foundry for edge development?
4. Explain the role of NPU optimization in modern edge AI applications.
5. How does the Phi Silica API leverage NPU hardware for performance optimization?
6. Compare the benefits of local vs. cloud deployment for privacy-sensitive applications.

### Hands-on Exercises

1. **AI Toolkit Setup**: Configure AI Toolkit and optimize a model (1 hour)
2. **Windows AI Foundry**: Build a simple Windows AI application using Phi Silica API (1 hour)
3. **Cross-Platform Deployment**: Deploy the same model on two different platforms (1 hour)
4. **NPU Optimization**: Test NPU performance with Windows AI Foundry tools (30 minutes)

## Time Allocation Guide

To help you make the most of the 20-hour course timeline, here's a suggested breakdown of how to allocate your time:

| Activity | Time Allocation | Description |
|----------|----------------|-------------|
| Reading Core Materials | 9 hours | Focusing on the essential concepts in each module |
| Hands-on Exercises | 6 hours | Practical implementation of key techniques |
| Self-Assessment | 2 hours | Testing your understanding through questions and reflection |
| Mini-Project | 3 hours | Applying knowledge to a small practical implementation |

### Key Focus Areas by Time Constraint

**If you only have 10 hours:**
- Complete Modules 1, 2, and 3 (core EdgeAI concepts)
- Do at least one hands-on exercise per module
- Focus on understanding the core concepts rather than implementation details

**If you can dedicate the full 20 hours:**
- Complete all seven modules
- Perform key hands-on exercises from each module
- Complete one mini-project from Module 7
- Explore at least 2-3 supplementary resources

**If you have more than 20 hours:**
- Complete all modules with detailed exercises
- Build multiple mini-projects
- Explore advanced optimization techniques in Module 4
- Implement production deployment from Module 5

## Essential Resources

These carefully selected resources provide the most value for your limited study time:

### Must-Read Documentation
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - The most efficient model optimization tool
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Fastest way to deploy SLMs locally
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Reference for a leading edge-optimized model
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Intel's comprehensive optimization toolkit
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Integrated EdgeAI development environment
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Windows-specific EdgeAI development platform

### Time-Saving Tools
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Quick model access and deployment
- [Gradio](https://www.gradio.app/docs/interface) - Rapid UI development for AI demos
- [Microsoft Olive](https://github.com/microsoft/Olive) - Simplified model optimization
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Efficient CPU inference
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Neural network compression framework
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Large language model deployment toolkit

## Progress Tracking Template

Use this simplified template to track your learning progress through the 20-hour course:

| Module | Completion Date | Hours Spent | Key Takeaways |
|--------|----------------|-------------|---------------|
| Module 1: EdgeAI Fundamentals | | | |
| Module 2: SLM Foundations | | | |
| Module 3: SLM Deployment | | | |
| Module 4: Model Optimization | | | |
| Module 5: SLMOps | | | |
| Module 6: AI Agents | | | |
| Module 7: Development Tools | | | |
| Hands-on Exercises | | | |
| Mini-Project | | | |

## Mini Project Ideas

Consider completing one of these projects to practice EdgeAI concepts (each designed to take 2-4 hours):

### Beginner Projects (2-3 hours each)
1. **Edge Text Assistant**: Create a simple offline text completion tool using a small language model
2. **Model Comparison Dashboard**: Build a basic visualization of performance metrics across different SLMs
3. **Optimization Experiment**: Measure the impact of different quantization levels on the same base model

### Intermediate Projects (3-4 hours each)
4. **AI Toolkit Workflow**: Use VS Code AI Toolkit to optimize and deploy a model from start to finish
5. **Windows AI Foundry Application**: Create a Windows app using Phi Silica API and NPU optimization
6. **Cross-Platform Deployment**: Deploy the same optimized model on Windows (OpenVINO) and mobile (.NET MAUI)
7. **Function Calling Agent**: Build an AI agent with function calling capabilities for edge scenarios

### Advanced Integration Projects (4-5 hours each)
8. **OpenVINO Optimization Pipeline**: Implement complete model optimization using NNCF and GenAI toolkit
9. **SLMOps Pipeline**: Implement a complete model lifecycle from training to edge deployment
10. **Multi-Model Edge System**: Deploy multiple specialized models working together on edge hardware
11. **MCP Integration System**: Build an agentic system using Model Context Protocol for tool interaction

## Learning Community

Join the discussion and connect with fellow learners:
- GitHub Discussions on the [EdgeAI for Beginners repository](https://github.com/microsoft/edgeai-for-beginners/discussions)
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)

## Conclusion

EdgeAI represents the frontier of artificial intelligence implementation, bringing powerful capabilities directly to devices while addressing critical concerns about privacy, latency, and connectivity. This 20-hour course provides you with the essential knowledge and practical skills to begin working with EdgeAI technologies immediately.

The course is deliberately concise and focused on the most important concepts, allowing you to quickly gain valuable expertise without an overwhelming time commitment. Remember that hands-on practice, even with simple examples, is the key to reinforcing what you've learned.

Happy learning!
