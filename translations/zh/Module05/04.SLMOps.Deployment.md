<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-07-22T04:12:43+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "zh"
}
-->
# ç¬¬å››éƒ¨åˆ†ï¼šéƒ¨ç½² - ç”Ÿäº§çº§æ¨¡å‹å®æ–½

## æ¦‚è¿°

æœ¬æ•™ç¨‹å°†å…¨é¢æŒ‡å¯¼æ‚¨ä½¿ç”¨ Foundry Local éƒ¨ç½²å¾®è°ƒé‡åŒ–æ¨¡å‹çš„å®Œæ•´æµç¨‹ã€‚æˆ‘ä»¬å°†ä»å¤´åˆ°å°¾æ¶µç›–æ¨¡å‹è½¬æ¢ã€é‡åŒ–ä¼˜åŒ–ä»¥åŠéƒ¨ç½²é…ç½®ã€‚

## å‰ææ¡ä»¶

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å…·å¤‡ä»¥ä¸‹æ¡ä»¶ï¼š

- âœ… ä¸€ä¸ªå·²å¾®è°ƒçš„ ONNX æ¨¡å‹ï¼Œå‡†å¤‡éƒ¨ç½²
- âœ… Windows æˆ– Mac ç”µè„‘
- âœ… Python 3.10 æˆ–æ›´é«˜ç‰ˆæœ¬
- âœ… è‡³å°‘ 8GB å¯ç”¨å†…å­˜
- âœ… å·²åœ¨ç³»ç»Ÿä¸Šå®‰è£… Foundry Local

## ç¬¬ä¸€éƒ¨åˆ†ï¼šç¯å¢ƒè®¾ç½®

### å®‰è£…æ‰€éœ€å·¥å…·

æ‰“å¼€ç»ˆç«¯ï¼ˆWindows ä¸Šçš„å‘½ä»¤æç¤ºç¬¦ï¼ŒMac ä¸Šçš„ç»ˆç«¯ï¼‰ï¼ŒæŒ‰é¡ºåºè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

âš ï¸ **é‡è¦æç¤º**ï¼šæ‚¨è¿˜éœ€è¦ CMake 3.31 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œå¯ä» [cmake.org](https://cmake.org/download/) ä¸‹è½½ã€‚

## ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹è½¬æ¢ä¸é‡åŒ–

### é€‰æ‹©åˆé€‚çš„æ ¼å¼

å¯¹äºå¾®è°ƒçš„å°å‹è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ **ONNX æ ¼å¼**ï¼Œå› ä¸ºå®ƒå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

- ğŸš€ æ›´å¥½çš„æ€§èƒ½ä¼˜åŒ–
- ğŸ”§ ç¡¬ä»¶æ— å…³çš„éƒ¨ç½²
- ğŸ­ ç”Ÿäº§çº§èƒ½åŠ›
- ğŸ“± è·¨å¹³å°å…¼å®¹æ€§

### æ–¹æ³•ä¸€ï¼šä¸€é”®è½¬æ¢ï¼ˆæ¨èï¼‰

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç›´æ¥è½¬æ¢æ‚¨çš„å¾®è°ƒæ¨¡å‹ï¼š

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**å‚æ•°è¯´æ˜ï¼š**
- `--model_name_or_path`ï¼šå¾®è°ƒæ¨¡å‹çš„è·¯å¾„
- `--device cpu`ï¼šä½¿ç”¨ CPU è¿›è¡Œä¼˜åŒ–
- `--precision int4`ï¼šä½¿ç”¨ INT4 é‡åŒ–ï¼ˆçº¦ 75% çš„å¤§å°ç¼©å‡ï¼‰
- `--output_path`ï¼šè½¬æ¢åæ¨¡å‹çš„è¾“å‡ºè·¯å¾„

### æ–¹æ³•äºŒï¼šé…ç½®æ–‡ä»¶æ–¹å¼ï¼ˆé«˜çº§ç”¨æˆ·ï¼‰

åˆ›å»ºä¸€ä¸ªåä¸º `finetuned_conversion_config.json` çš„é…ç½®æ–‡ä»¶ï¼š

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

ç„¶åè¿è¡Œï¼š

```bash
olive run --config ./finetuned_conversion_config.json
```

### é‡åŒ–é€‰é¡¹å¯¹æ¯”

| ç²¾åº¦      | æ–‡ä»¶å¤§å°       | æ¨ç†é€Ÿåº¦       | æ¨¡å‹è´¨é‡       | æ¨èç”¨é€”         |
|-----------|---------------|---------------|---------------|-----------------|
| FP16      | åŸºçº¿ Ã— 0.5    | å¿«é€Ÿ          | æœ€ä½³          | é«˜ç«¯ç¡¬ä»¶         |
| INT8      | åŸºçº¿ Ã— 0.25   | éå¸¸å¿«        | è‰¯å¥½          | å¹³è¡¡é€‰æ‹©         |
| INT4      | åŸºçº¿ Ã— 0.125  | æœ€å¿«          | å¯æ¥å—        | èµ„æºæœ‰é™         |

ğŸ’¡ **æ¨è**ï¼šé¦–æ¬¡éƒ¨ç½²å»ºè®®ä½¿ç”¨ INT4 é‡åŒ–ã€‚å¦‚æœè´¨é‡ä¸æ»¡æ„ï¼Œå¯å°è¯• INT8 æˆ– FP16ã€‚

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šFoundry Local éƒ¨ç½²é…ç½®

### åˆ›å»ºæ¨¡å‹é…ç½®

å¯¼èˆªåˆ° Foundry Local çš„æ¨¡å‹ç›®å½•ï¼š

```bash
foundry cache cd ./models/
```

åˆ›å»ºæ¨¡å‹ç›®å½•ç»“æ„ï¼š

```bash
mkdir -p ./models/custom/your-finetuned-model
```

åœ¨æ¨¡å‹ç›®å½•ä¸­åˆ›å»º `inference_model.json` é…ç½®æ–‡ä»¶ï¼š

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### ç‰¹å®šæ¨¡å‹æ¨¡æ¿é…ç½®

#### é’ˆå¯¹ Qwen ç³»åˆ—æ¨¡å‹ï¼š

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹æµ‹è¯•ä¸ä¼˜åŒ–

### éªŒè¯æ¨¡å‹å®‰è£…

æ£€æŸ¥ Foundry Local æ˜¯å¦èƒ½è¯†åˆ«æ‚¨çš„æ¨¡å‹ï¼š

```bash
foundry cache ls
```

æ‚¨åº”è¯¥åœ¨åˆ—è¡¨ä¸­çœ‹åˆ° `your-finetuned-model-int4`ã€‚

### å¼€å§‹æ¨¡å‹æµ‹è¯•

```bash
foundry model run your-finetuned-model-int4
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

åœ¨æµ‹è¯•æœŸé—´ç›‘æ§ä»¥ä¸‹å…³é”®æŒ‡æ ‡ï¼š

1. **å“åº”æ—¶é—´**ï¼šæµ‹é‡æ¯æ¬¡å“åº”çš„å¹³å‡æ—¶é—´
2. **å†…å­˜ä½¿ç”¨**ï¼šç›‘æ§ RAM æ¶ˆè€—
3. **CPU åˆ©ç”¨ç‡**ï¼šæ£€æŸ¥å¤„ç†å™¨è´Ÿè½½
4. **è¾“å‡ºè´¨é‡**ï¼šè¯„ä¼°å“åº”çš„ç›¸å…³æ€§å’Œè¿è´¯æ€§

### è´¨é‡éªŒè¯æ£€æŸ¥è¡¨

- âœ… æ¨¡å‹å¯¹å¾®è°ƒé¢†åŸŸçš„æŸ¥è¯¢å“åº”é€‚å½“
- âœ… å“åº”æ ¼å¼ç¬¦åˆé¢„æœŸçš„è¾“å‡ºç»“æ„
- âœ… é•¿æ—¶é—´ä½¿ç”¨æœŸé—´æ— å†…å­˜æ³„æ¼
- âœ… åœ¨ä¸åŒè¾“å…¥é•¿åº¦ä¸‹æ€§èƒ½ä¸€è‡´
- âœ… æ­£ç¡®å¤„ç†è¾¹ç•Œæƒ…å†µå’Œæ— æ•ˆè¾“å…¥

## æ€»ç»“

æ­å–œï¼æ‚¨å·²æˆåŠŸå®Œæˆä»¥ä¸‹å†…å®¹ï¼š

- âœ… å¾®è°ƒæ¨¡å‹æ ¼å¼è½¬æ¢
- âœ… æ¨¡å‹é‡åŒ–ä¼˜åŒ–
- âœ… Foundry Local éƒ¨ç½²é…ç½®
- âœ… æ€§èƒ½è°ƒä¼˜ä¸æ•…éšœæ’é™¤

**å…è´£å£°æ˜**ï¼š  
æœ¬æ–‡æ¡£ä½¿ç”¨AIç¿»è¯‘æœåŠ¡ [Co-op Translator](https://github.com/Azure/co-op-translator) è¿›è¡Œç¿»è¯‘ã€‚å°½ç®¡æˆ‘ä»¬åŠªåŠ›ç¡®ä¿ç¿»è¯‘çš„å‡†ç¡®æ€§ï¼Œä½†è¯·æ³¨æ„ï¼Œè‡ªåŠ¨ç¿»è¯‘å¯èƒ½åŒ…å«é”™è¯¯æˆ–ä¸å‡†ç¡®ä¹‹å¤„ã€‚åŸå§‹è¯­è¨€çš„æ–‡æ¡£åº”è¢«è§†ä¸ºæƒå¨æ¥æºã€‚å¯¹äºå…³é”®ä¿¡æ¯ï¼Œå»ºè®®ä½¿ç”¨ä¸“ä¸šäººå·¥ç¿»è¯‘ã€‚æˆ‘ä»¬å¯¹å› ä½¿ç”¨æ­¤ç¿»è¯‘è€Œäº§ç”Ÿçš„ä»»ä½•è¯¯è§£æˆ–è¯¯è¯»ä¸æ‰¿æ‹…è´£ä»»ã€‚