<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddaad917d0c16fc3d498a6b4eabc8088",
  "translation_date": "2025-10-08T16:43:21+00:00",
  "source_file": "Workshop/notebooks/quickstart.md",
  "language_code": "zh"
}
-->
# å·¥ä½œåŠç¬”è®°æœ¬ - å¿«é€Ÿå…¥é—¨æŒ‡å—

## ç›®å½•

- [å‰ç½®æ¡ä»¶](../../../../Workshop/notebooks)
- [åˆå§‹è®¾ç½®](../../../../Workshop/notebooks)
- [ç¬¬04èŠ‚ï¼šæ¨¡å‹æ¯”è¾ƒ](../../../../Workshop/notebooks)
- [ç¬¬05èŠ‚ï¼šå¤šä»£ç†åè°ƒå™¨](../../../../Workshop/notebooks)
- [ç¬¬06èŠ‚ï¼šåŸºäºæ„å›¾çš„æ¨¡å‹è·¯ç”±](../../../../Workshop/notebooks)
- [ç¯å¢ƒå˜é‡](../../../../Workshop/notebooks)
- [å¸¸ç”¨å‘½ä»¤](../../../../Workshop/notebooks)

---

## å‰ç½®æ¡ä»¶

### 1. å®‰è£… Foundry Local

**Windows:**
```bash
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**éªŒè¯å®‰è£…:**
```bash
foundry --version
```


### 2. å®‰è£… Python ä¾èµ–é¡¹

```bash
cd Workshop
pip install -r requirements.txt
```

æˆ–è€…å•ç‹¬å®‰è£…ï¼š
```bash
pip install foundry-local-sdk openai numpy requests
```


---

## åˆå§‹è®¾ç½®

### å¯åŠ¨ Foundry Local æœåŠ¡

**è¿è¡Œä»»ä½•ç¬”è®°æœ¬ä¹‹å‰å¿…é¡»å¯åŠ¨ï¼š**

```bash
# Start the service
foundry service start

# Verify it's running
foundry service status
```

é¢„æœŸè¾“å‡ºï¼š
```
âœ… Service started successfully
Endpoint: http://localhost:59959
```


### ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹

ç¬”è®°æœ¬é»˜è®¤ä½¿ç”¨ä»¥ä¸‹æ¨¡å‹ï¼š

```bash
# Download models (first time only - may take several minutes)
foundry model download phi-4-mini
foundry model download qwen2.5-3b
foundry model download phi-3.5-mini
foundry model download qwen2.5-0.5b

# Load models into memory
foundry model run phi-4-mini
foundry model run qwen2.5-3b
foundry model run phi-3.5-mini
```


### éªŒè¯è®¾ç½®

```bash
# List loaded models
foundry model ls

# Check service health
curl http://localhost:59959/v1/models
```


---

## ç¬¬04èŠ‚ï¼šæ¨¡å‹æ¯”è¾ƒ

### ç›®çš„
æ¯”è¾ƒå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚

### å¿«é€Ÿè®¾ç½®

```bash
# Start service (if not already running)
foundry service start

# Load required models
foundry model run phi-4-mini
foundry model run qwen2.5-3b
```


### è¿è¡Œç¬”è®°æœ¬

1. **æ‰“å¼€** `session04_model_compare.ipynb`ï¼ˆåœ¨ VS Code æˆ– Jupyter ä¸­ï¼‰
2. **é‡å¯å†…æ ¸**ï¼ˆå†…æ ¸ â†’ é‡å¯å†…æ ¸ï¼‰
3. **æŒ‰é¡ºåºè¿è¡Œæ‰€æœ‰å•å…ƒæ ¼**

### å…³é”®é…ç½®

**é»˜è®¤æ¨¡å‹ï¼š**
- **SLM:** `phi-4-mini`ï¼ˆçº¦4GB RAMï¼Œé€Ÿåº¦æ›´å¿«ï¼‰
- **LLM:** `qwen2.5-3b`ï¼ˆçº¦3GB RAMï¼Œå†…å­˜ä¼˜åŒ–ï¼‰

**ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰ï¼š**
```python
import os
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'
```


### é¢„æœŸè¾“å‡º

```
================================================================================
COMPARISON SUMMARY
================================================================================
Alias                Latency(s)      Tokens     Route               
--------------------------------------------------------------------------------
phi-4-mini           1.234           150        chat.completions    
qwen2.5-3b           2.456           180        chat.completions    
================================================================================

ğŸ’¡ SLM is 1.99x faster than LLM for this prompt
```


### è‡ªå®šä¹‰

**ä½¿ç”¨ä¸åŒæ¨¡å‹ï¼š**
```python
os.environ['SLM_ALIAS'] = 'phi-3.5-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-1.5b'
```

**è‡ªå®šä¹‰æç¤ºï¼š**
```python
os.environ['COMPARE_PROMPT'] = 'Explain quantum computing in simple terms'
```


### éªŒè¯æ¸…å•

- [ ] å•å…ƒæ ¼12æ˜¾ç¤ºæ­£ç¡®çš„æ¨¡å‹ï¼ˆphi-4-mini, qwen2.5-3bï¼‰
- [ ] å•å…ƒæ ¼12æ˜¾ç¤ºæ­£ç¡®çš„ç«¯ç‚¹ï¼ˆç«¯å£59959ï¼‰
- [ ] å•å…ƒæ ¼16è¯Šæ–­é€šè¿‡ï¼ˆâœ… æœåŠ¡æ­£åœ¨è¿è¡Œï¼‰
- [ ] å•å…ƒæ ¼20é¢„æ£€é€šè¿‡ï¼ˆä¸¤ä¸ªæ¨¡å‹æ­£å¸¸ï¼‰
- [ ] å•å…ƒæ ¼22æ¯”è¾ƒå®Œæˆå¹¶æ˜¾ç¤ºå»¶è¿Ÿå€¼
- [ ] å•å…ƒæ ¼24éªŒè¯æ˜¾ç¤º ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼

### æ—¶é—´ä¼°è®¡
- **é¦–æ¬¡è¿è¡Œï¼š** 5-10åˆ†é’Ÿï¼ˆåŒ…æ‹¬æ¨¡å‹ä¸‹è½½ï¼‰
- **åç»­è¿è¡Œï¼š** 1-2åˆ†é’Ÿ

---

## ç¬¬05èŠ‚ï¼šå¤šä»£ç†åè°ƒå™¨

### ç›®çš„
ä½¿ç”¨ Foundry Local SDK å±•ç¤ºå¤šä»£ç†åä½œï¼Œä»£ç†å…±åŒå·¥ä½œä»¥ç”Ÿæˆä¼˜åŒ–è¾“å‡ºã€‚

### å¿«é€Ÿè®¾ç½®

```bash
# Start service
foundry service start

# Load models
foundry model run phi-4-mini  # Primary model
foundry model run qwen2.5-7b  # Optional: higher quality editor
```


### è¿è¡Œç¬”è®°æœ¬

1. **æ‰“å¼€** `session05_agents_orchestrator.ipynb`
2. **é‡å¯å†…æ ¸**
3. **æŒ‰é¡ºåºè¿è¡Œæ‰€æœ‰å•å…ƒæ ¼**

### å…³é”®é…ç½®

**é»˜è®¤è®¾ç½®ï¼ˆä¸¤ä¸ªä»£ç†ä½¿ç”¨ç›¸åŒæ¨¡å‹ï¼‰ï¼š**
```python
PRIMARY_ALIAS = 'phi-4-mini'
EDITOR_ALIAS = 'phi-4-mini'  # Uses same model
```

**é«˜çº§è®¾ç½®ï¼ˆä¸åŒæ¨¡å‹ï¼‰ï¼š**
```python
import os
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'     # Fast for research
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'      # High quality for editing
```


### æ¶æ„

```
User Question
    â†“
Researcher Agent (phi-4-mini)
  â†’ Gathers bullet points
    â†“
Editor Agent (phi-4-mini or qwen2.5-7b)
  â†’ Refines into executive summary
    â†“
Final Output
```


### é¢„æœŸè¾“å‡º

```
================================================================================
[Pipeline] Question: Explain why edge AI matters for compliance.
================================================================================

[Stage 1: Research]
Output: â€¢ Edge AI processes data locally, reducing transmission...

[Stage 2: Editorial Refinement]
Output: Executive Summary: Edge AI enhances compliance by keeping data...

[FINAL OUTPUT]
Executive Summary: Edge AI enhances compliance by keeping sensitive data 
on-premises and reduces latency through local processing.

[METADATA]
Models used: {'researcher': 'phi-4-mini', 'editor': 'phi-4-mini'}
```


### æ‰©å±•

**æ·»åŠ æ›´å¤šä»£ç†ï¼š**
```python
critic = Agent(
    name='Critic',
    system='Review content for accuracy',
    client=client,
    model_id=model_id
)
```

**æ‰¹é‡æµ‹è¯•ï¼š**
```python
test_questions = [
    "What are benefits of local AI?",
    "How does RAG improve accuracy?",
]

for q in test_questions:
    result = pipeline(q, verbose=False)
    print(result['final'])
```


### æ—¶é—´ä¼°è®¡
- **é¦–æ¬¡è¿è¡Œï¼š** 3-5åˆ†é’Ÿ
- **åç»­è¿è¡Œï¼š** æ¯ä¸ªé—®é¢˜1-2åˆ†é’Ÿ

---

## ç¬¬06èŠ‚ï¼šåŸºäºæ„å›¾çš„æ¨¡å‹è·¯ç”±

### ç›®çš„
æ ¹æ®æ£€æµ‹åˆ°çš„æ„å›¾æ™ºèƒ½åœ°å°†æç¤ºè·¯ç”±åˆ°ä¸“ç”¨æ¨¡å‹ã€‚

### å¿«é€Ÿè®¾ç½®

```bash
# Start service
foundry service start

# Load all routing models (CPU variants recommended)
foundry model run phi-4-mini-cpu
foundry model run qwen2.5-0.5b-cpu
foundry model run phi-3.5-mini-cpu
```

**æ³¨æ„ï¼š** ç¬¬06èŠ‚é»˜è®¤ä½¿ç”¨CPUæ¨¡å‹ä»¥ç¡®ä¿æœ€å¤§å…¼å®¹æ€§ã€‚

### è¿è¡Œç¬”è®°æœ¬

1. **æ‰“å¼€** `session06_models_router.ipynb`
2. **é‡å¯å†…æ ¸**
3. **æŒ‰é¡ºåºè¿è¡Œæ‰€æœ‰å•å…ƒæ ¼**

### å…³é”®é…ç½®

**é»˜è®¤ç›®å½•ï¼ˆCPUæ¨¡å‹ï¼‰ï¼š**
```python
CATALOG = {
    'phi-4-mini-cpu': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b-cpu': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini-cpu': {'capabilities':['code','refactor'],'priority':3},
}
```

**æ›¿ä»£ç›®å½•ï¼ˆGPUæ¨¡å‹ï¼‰ï¼š**
```python
# Uncomment GPU catalog in Cell #6 if you have sufficient VRAM (8GB+)
CATALOG = {
    'phi-4-mini': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini': {'capabilities':['code','refactor'],'priority':3},
}
```


### æ„å›¾æ£€æµ‹

è·¯ç”±å™¨ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æ£€æµ‹æ„å›¾ï¼š

| æ„å›¾ | æ¨¡å¼ç¤ºä¾‹ | è·¯ç”±åˆ° |
|------|----------|--------|
| `code` | "é‡æ„", "å®ç°å‡½æ•°" | phi-3.5-mini-cpu |
| `classification` | "åˆ†ç±»", "å¯¹è¿™ä¸ªè¿›è¡Œåˆ†ç±»" | qwen2.5-0.5b-cpu |
| `summarize` | "æ€»ç»“", "tl;dr" | phi-4-mini-cpu |
| `general` | å…¶ä»–æ‰€æœ‰å†…å®¹ | phi-4-mini-cpu |

### é¢„æœŸè¾“å‡º

```
âœ“ Using CPU-optimized models (default configuration)
  Models: phi-4-mini-cpu, qwen2.5-0.5b-cpu, phi-3.5-mini-cpu

Routing prompts to specialized models...
============================================================

Prompt: Refactor this Python function for readability
  Intent: code           | Model: phi-3.5-mini-cpu
  Output: Here's a refactored version...
  Tokens: 156

Prompt: Categorize this email as urgent or normal
  Intent: classification | Model: qwen2.5-0.5b-cpu
  Output: Category: Normal
  Tokens: 45

âœ“ Success! All prompts routed correctly.
```


### è‡ªå®šä¹‰

**æ·»åŠ è‡ªå®šä¹‰æ„å›¾ï¼š**
```python
import re

# Add to RULES
RULES.append((re.compile('translate|ç¿»è¯‘', re.I), 'translation'))

# Add capability to catalog
CATALOG['phi-4-mini-cpu']['capabilities'].append('translation')
```

**å¯ç”¨ä»¤ç‰Œè·Ÿè¸ªï¼š**
```python
import os
os.environ['SHOW_USAGE'] = '1'
```


### åˆ‡æ¢åˆ°GPUæ¨¡å‹

å¦‚æœæ‚¨æœ‰8GBä»¥ä¸Šçš„æ˜¾å­˜ï¼š

1. åœ¨ **å•å…ƒæ ¼#6** ä¸­ï¼Œæ³¨é‡Šæ‰CPUç›®å½•
2. å–æ¶ˆæ³¨é‡ŠGPUç›®å½•
3. åŠ è½½GPUæ¨¡å‹ï¼š
   ```bash
   foundry model run phi-4-mini
   foundry model run qwen2.5-0.5b
   foundry model run phi-3.5-mini
   ```

4. é‡å¯å†…æ ¸å¹¶é‡æ–°è¿è¡Œç¬”è®°æœ¬

### æ—¶é—´ä¼°è®¡
- **é¦–æ¬¡è¿è¡Œï¼š** 5-10åˆ†é’Ÿï¼ˆåŠ è½½æ¨¡å‹ï¼‰
- **åç»­è¿è¡Œï¼š** æ¯æ¬¡æµ‹è¯•30-60ç§’

---

## ç¯å¢ƒå˜é‡

### å…¨å±€é…ç½®

åœ¨å¯åŠ¨ Jupyter/VS Code ä¹‹å‰è®¾ç½®ï¼š

**Windowsï¼ˆå‘½ä»¤æç¤ºç¬¦ï¼‰ï¼š**
```cmd
set FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
set SHOW_USAGE=1
set RETRY_ON_FAIL=1
```

**Windowsï¼ˆPowerShellï¼‰ï¼š**
```powershell
$env:FOUNDRY_LOCAL_ENDPOINT="http://localhost:59959/v1"
$env:SHOW_USAGE="1"
$env:RETRY_ON_FAIL="1"
```

**macOS/Linuxï¼š**
```bash
export FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
export SHOW_USAGE=1
export RETRY_ON_FAIL=1
```


### ç¬”è®°æœ¬å†…é…ç½®

åœ¨ä»»ä½•ç¬”è®°æœ¬å¼€å§‹æ—¶è®¾ç½®ï¼š

```python
import os

# Foundry Local configuration
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'

# Model selection
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'

# Agent models
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'

# Debugging
os.environ['SHOW_USAGE'] = '1'       # Show token usage
os.environ['RETRY_ON_FAIL'] = '1'    # Enable retries
os.environ['RETRY_BACKOFF'] = '2.0'  # Retry delay
```


---

## å¸¸ç”¨å‘½ä»¤

### æœåŠ¡ç®¡ç†

```bash
# Start service
foundry service start

# Check status
foundry service status

# Stop service
foundry service stop

# View logs
foundry service logs
```


### æ¨¡å‹ç®¡ç†

```bash
# List all available models in catalog
foundry model catalog

# List loaded models
foundry model ls

# Download a model
foundry model download phi-4-mini

# Load a model
foundry model run phi-4-mini

# Unload a model
foundry model unload phi-4-mini

# Remove a model
foundry model remove phi-4-mini

# Get model info
foundry model info phi-4-mini
```


### æµ‹è¯•ç«¯ç‚¹

```bash
# Check service health
curl http://localhost:59959/health

# List available models via API
curl http://localhost:59959/v1/models

# Test model completion
curl http://localhost:59959/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role":"user","content":"Hello"}],
    "max_tokens": 50
  }'
```


### è¯Šæ–­å‘½ä»¤

```bash
# Check everything
foundry --version
foundry service status
foundry model ls
foundry device info

# GPU status (NVIDIA)
nvidia-smi

# NPU status (Qualcomm)
foundry device info
```


---

## æœ€ä½³å®è·µ

### å¼€å§‹ä»»ä½•ç¬”è®°æœ¬ä¹‹å‰

1. **æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œï¼š**
   ```bash
   foundry service status
   ```

2. **éªŒè¯æ¨¡å‹æ˜¯å¦å·²åŠ è½½ï¼š**
   ```bash
   foundry model ls
   ```

3. **å¦‚æœé‡æ–°è¿è¡Œï¼Œé‡å¯ç¬”è®°æœ¬å†…æ ¸**

4. **æ¸…é™¤æ‰€æœ‰è¾“å‡º**ä»¥ç¡®ä¿å¹²å‡€è¿è¡Œ

### èµ„æºç®¡ç†

1. **é»˜è®¤ä½¿ç”¨CPUæ¨¡å‹**ä»¥ç¡®ä¿å…¼å®¹æ€§
2. **ä»…åœ¨æ˜¾å­˜8GBä»¥ä¸Šæ—¶åˆ‡æ¢åˆ°GPUæ¨¡å‹**
3. **è¿è¡Œå‰å…³é—­å…¶ä»–GPUåº”ç”¨ç¨‹åº**
4. **åœ¨ç¬”è®°æœ¬ä¼šè¯ä¹‹é—´ä¿æŒæœåŠ¡è¿è¡Œ**
5. **ä½¿ç”¨ä»»åŠ¡ç®¡ç†å™¨æˆ– nvidia-smi ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ**

### æ•…éšœæ’é™¤

1. **åœ¨è°ƒè¯•ä»£ç ä¹‹å‰å§‹ç»ˆæ£€æŸ¥æœåŠ¡çŠ¶æ€**
2. **å¦‚æœçœ‹åˆ°è¿‡æ—¶é…ç½®ï¼Œé‡å¯å†…æ ¸**
3. **åœ¨ä»»ä½•æ›´æ”¹åé‡æ–°è¿è¡Œè¯Šæ–­å•å…ƒæ ¼**
4. **æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦ä¸åŠ è½½çš„æ¨¡å‹åŒ¹é…**
5. **éªŒè¯ç«¯ç‚¹ç«¯å£æ˜¯å¦ä¸æœåŠ¡çŠ¶æ€åŒ¹é…**

---

## å¿«é€Ÿå‚è€ƒï¼šæ¨¡å‹åˆ«å

### å¸¸ç”¨æ¨¡å‹

| åˆ«å | å¤§å° | æœ€é€‚åˆ | RAM/VRAM | å˜ä½“ |
|------|------|--------|----------|------|
| `phi-4-mini` | ~4B | é€šç”¨èŠå¤©ã€æ€»ç»“ | 4-6GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `phi-3.5-mini` | ~3.5B | ä»£ç ç”Ÿæˆã€é‡æ„ | 3-5GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `qwen2.5-3b` | ~3B | é€šç”¨ä»»åŠ¡ã€é«˜æ•ˆ | 3-4GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-1.5b` | ~1.5B | å¿«é€Ÿã€ä½èµ„æº | 2-3GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-0.5b` | ~0.5B | åˆ†ç±»ã€èµ„æºæœ€å°‘ | 1-2GB | `-cpu`, `-cuda-gpu` |

### å˜ä½“å‘½å

- **åŸºç¡€åç§°**ï¼ˆä¾‹å¦‚ï¼Œ`phi-4-mini`ï¼‰ï¼šè‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆç¡¬ä»¶çš„å˜ä½“
- **`-cpu`**ï¼šCPUä¼˜åŒ–ï¼Œé€‚ç”¨äºæ‰€æœ‰ç¯å¢ƒ
- **`-cuda-gpu`**ï¼šNVIDIA GPUä¼˜åŒ–ï¼Œéœ€8GBä»¥ä¸Šæ˜¾å­˜
- **`-npu`**ï¼šQualcomm NPUä¼˜åŒ–ï¼Œéœ€å®‰è£…NPUé©±åŠ¨

**æ¨èï¼š** ä½¿ç”¨åŸºç¡€åç§°ï¼ˆä¸å¸¦åç¼€ï¼‰ï¼Œè®© Foundry Local è‡ªåŠ¨é€‰æ‹©æœ€ä½³å˜ä½“ã€‚

---

## æˆåŠŸæŒ‡æ ‡

å½“æ‚¨çœ‹åˆ°ä»¥ä¸‹å†…å®¹æ—¶ï¼Œè¯´æ˜å‡†å¤‡å°±ç»ªï¼š

âœ… `foundry service status` æ˜¾ç¤ºâ€œè¿è¡Œä¸­â€  
âœ… `foundry model ls` æ˜¾ç¤ºæ‰€éœ€æ¨¡å‹  
âœ… æœåŠ¡å¯åœ¨æ­£ç¡®çš„ç«¯ç‚¹è®¿é—®  
âœ… å¥åº·æ£€æŸ¥è¿”å›200 OK  
âœ… ç¬”è®°æœ¬è¯Šæ–­å•å…ƒæ ¼é€šè¿‡  
âœ… è¾“å‡ºä¸­æ— è¿æ¥é”™è¯¯  

---

## è·å–å¸®åŠ©

### æ–‡æ¡£
- **ä¸»ä»“åº“ï¼š** https://github.com/microsoft/Foundry-Local
- **Python SDKï¼š** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python
- **CLIå‚è€ƒï¼š** https://github.com/microsoft/Foundry-Local/blob/main/docs/reference/reference-cli.md
- **æ•…éšœæ’é™¤ï¼š** è¯·å‚é˜…æœ¬ç›®å½•ä¸­çš„ `troubleshooting.md`

### GitHubé—®é¢˜
- https://github.com/microsoft/Foundry-Local/issues
- https://github.com/microsoft/edgeai-for-beginners/issues

---

**æœ€åæ›´æ–°ï¼š** 2025å¹´10æœˆ8æ—¥  
**ç‰ˆæœ¬ï¼š** Workshop Notebooks 2.0

---

**å…è´£å£°æ˜**ï¼š  
æœ¬æ–‡æ¡£ä½¿ç”¨AIç¿»è¯‘æœåŠ¡ [Co-op Translator](https://github.com/Azure/co-op-translator) è¿›è¡Œç¿»è¯‘ã€‚å°½ç®¡æˆ‘ä»¬åŠªåŠ›ç¡®ä¿ç¿»è¯‘çš„å‡†ç¡®æ€§ï¼Œä½†è¯·æ³¨æ„ï¼Œè‡ªåŠ¨ç¿»è¯‘å¯èƒ½åŒ…å«é”™è¯¯æˆ–ä¸å‡†ç¡®ä¹‹å¤„ã€‚åŸå§‹è¯­è¨€çš„æ–‡æ¡£åº”è¢«è§†ä¸ºæƒå¨æ¥æºã€‚å¯¹äºå…³é”®ä¿¡æ¯ï¼Œå»ºè®®ä½¿ç”¨ä¸“ä¸šäººå·¥ç¿»è¯‘ã€‚æˆ‘ä»¬ä¸å¯¹å› ä½¿ç”¨æ­¤ç¿»è¯‘è€Œäº§ç”Ÿçš„ä»»ä½•è¯¯è§£æˆ–è¯¯è¯»æ‰¿æ‹…è´£ä»»ã€‚