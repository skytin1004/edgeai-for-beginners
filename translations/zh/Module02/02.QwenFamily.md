<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-01T20:28:57+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "zh"
}
-->
# ç¬¬2èŠ‚ï¼šQwenæ¨¡å‹å®¶æ—åŸºç¡€

Qwenæ¨¡å‹å®¶æ—ä»£è¡¨äº†é˜¿é‡Œäº‘åœ¨å¤§è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€AIé¢†åŸŸçš„å…¨é¢æ¢ç´¢ï¼Œå±•ç¤ºäº†å¼€æºæ¨¡å‹åœ¨å®ç°å“è¶Šæ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿé€‚åº”å¤šç§éƒ¨ç½²åœºæ™¯çš„å¯èƒ½æ€§ã€‚ç†è§£Qwenå®¶æ—å¦‚ä½•é€šè¿‡çµæ´»çš„éƒ¨ç½²é€‰é¡¹å®ç°å¼ºå¤§çš„AIèƒ½åŠ›ï¼ŒåŒæ—¶åœ¨å„ç§ä»»åŠ¡ä¸­ä¿æŒç«äº‰åŠ›ï¼Œæ˜¯éå¸¸é‡è¦çš„ã€‚

## å¼€å‘è€…èµ„æº

### Hugging Faceæ¨¡å‹åº“
éƒ¨åˆ†Qwenå®¶æ—æ¨¡å‹å¯é€šè¿‡ [Hugging Face](https://huggingface.co/models?search=qwen) è·å–ï¼Œæä¾›è¿™äº›æ¨¡å‹çš„éƒ¨åˆ†å˜ä½“ã€‚æ‚¨å¯ä»¥æ¢ç´¢å¯ç”¨çš„å˜ä½“ï¼Œæ ¹æ®å…·ä½“éœ€æ±‚è¿›è¡Œå¾®è°ƒï¼Œå¹¶é€šè¿‡å„ç§æ¡†æ¶è¿›è¡Œéƒ¨ç½²ã€‚

### æœ¬åœ°å¼€å‘å·¥å…·
å¯¹äºæœ¬åœ°å¼€å‘å’Œæµ‹è¯•ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) åœ¨å¼€å‘æœºå™¨ä¸Šè¿è¡ŒQwenæ¨¡å‹ï¼Œå¹¶è·å¾—ä¼˜åŒ–çš„æ€§èƒ½ã€‚

### æ–‡æ¡£èµ„æº
- [Qwenæ¨¡å‹æ–‡æ¡£](https://huggingface.co/docs/transformers/model_doc/qwen)
- [ä¼˜åŒ–Qwenæ¨¡å‹ä»¥é€‚åº”è¾¹ç¼˜éƒ¨ç½²](https://github.com/microsoft/olive)

## ç®€ä»‹

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨é˜¿é‡Œå·´å·´çš„Qwenæ¨¡å‹å®¶æ—åŠå…¶åŸºæœ¬æ¦‚å¿µã€‚æˆ‘ä»¬å°†æ¶µç›–Qwenå®¶æ—çš„æ¼”å˜ã€ä½¿Qwenæ¨¡å‹é«˜æ•ˆçš„åˆ›æ–°è®­ç»ƒæ–¹æ³•ã€å®¶æ—ä¸­çš„å…³é”®å˜ä½“ï¼Œä»¥åŠåœ¨ä¸åŒåœºæ™¯ä¸­çš„å®é™…åº”ç”¨ã€‚

## å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬æ•™ç¨‹ï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š

- ç†è§£é˜¿é‡Œå·´å·´Qwenæ¨¡å‹å®¶æ—çš„è®¾è®¡ç†å¿µå’Œæ¼”å˜è¿‡ç¨‹
- è¯†åˆ«ä½¿Qwenæ¨¡å‹åœ¨ä¸åŒå‚æ•°è§„æ¨¡ä¸­å®ç°é«˜æ€§èƒ½çš„å…³é”®åˆ›æ–°
- äº†è§£ä¸åŒQwenæ¨¡å‹å˜ä½“çš„ä¼˜åŠ¿å’Œå±€é™æ€§
- åº”ç”¨Qwenæ¨¡å‹çŸ¥è¯†ï¼Œé€‰æ‹©é€‚åˆå®é™…åœºæ™¯çš„å˜ä½“

## ç†è§£ç°ä»£AIæ¨¡å‹çš„æ ¼å±€

AIé¢†åŸŸå·²ç»å‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ï¼Œä¸åŒç»„ç»‡åœ¨è¯­è¨€æ¨¡å‹å¼€å‘æ–¹é¢é‡‡å–äº†ä¸åŒçš„ç­–ç•¥ã€‚ä¸€äº›ç»„ç»‡ä¸“æ³¨äºä¸“æœ‰çš„é—­æºæ¨¡å‹ï¼Œè€Œå¦ä¸€äº›åˆ™å¼ºè°ƒå¼€æºçš„å¯è®¿é—®æ€§å’Œé€æ˜æ€§ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸æ¶‰åŠåªèƒ½é€šè¿‡APIè®¿é—®çš„å¤§å‹ä¸“æœ‰æ¨¡å‹ï¼Œæˆ–èƒ½åŠ›å¯èƒ½ç¨æ˜¾è½åçš„å¼€æºæ¨¡å‹ã€‚

è¿™ç§èŒƒå¼ä¸ºå¸Œæœ›åœ¨ä¿æŒæ•°æ®æ§åˆ¶ã€æˆæœ¬ç®¡ç†å’Œéƒ¨ç½²çµæ´»æ€§çš„åŒæ—¶è·å¾—å¼ºå¤§AIèƒ½åŠ›çš„ç»„ç»‡å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦åœ¨å°–ç«¯æ€§èƒ½å’Œå®é™…éƒ¨ç½²è€ƒè™‘ä¹‹é—´åšå‡ºé€‰æ‹©ã€‚

## å¯è®¿é—®çš„AIå“è¶Šæ€§çš„æŒ‘æˆ˜

åœ¨å„ç§åœºæ™¯ä¸­ï¼Œå¯¹é«˜è´¨é‡ã€å¯è®¿é—®AIçš„éœ€æ±‚å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚è€ƒè™‘ä»¥ä¸‹åº”ç”¨åœºæ™¯ï¼šéœ€è¦çµæ´»éƒ¨ç½²é€‰é¡¹ä»¥æ»¡è¶³ä¸åŒç»„ç»‡éœ€æ±‚ã€æˆæœ¬æ•ˆç›Šçš„å®ç°ï¼ˆAPIæˆæœ¬å¯èƒ½ä¼šæ˜¾è‘—å¢åŠ ï¼‰ã€å…¨çƒåº”ç”¨ä¸­çš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œæˆ–åœ¨ç¼–ç å’Œæ•°å­¦ç­‰é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ã€‚

### å…³é”®éƒ¨ç½²éœ€æ±‚

ç°ä»£AIéƒ¨ç½²é¢ä¸´ä¸€äº›åŸºæœ¬éœ€æ±‚ï¼Œè¿™äº›éœ€æ±‚é™åˆ¶äº†å…¶å®é™…é€‚ç”¨æ€§ï¼š

- **å¯è®¿é—®æ€§**ï¼šå¼€æºå¯ç”¨æ€§ä»¥å®ç°é€æ˜æ€§å’Œå®šåˆ¶åŒ–
- **æˆæœ¬æ•ˆç›Š**ï¼šåˆç†çš„è®¡ç®—éœ€æ±‚ä»¥é€‚åº”ä¸åŒé¢„ç®—
- **çµæ´»æ€§**ï¼šå¤šç§æ¨¡å‹è§„æ¨¡ä»¥é€‚åº”ä¸åŒéƒ¨ç½²åœºæ™¯
- **å…¨çƒè¦†ç›–**ï¼šå¼ºå¤§çš„å¤šè¯­è¨€å’Œè·¨æ–‡åŒ–èƒ½åŠ›
- **ä¸“ä¸šåŒ–**ï¼šé’ˆå¯¹ç‰¹å®šç”¨ä¾‹çš„é¢†åŸŸä¸“å±å˜ä½“

## Qwenæ¨¡å‹çš„ç†å¿µ

Qwenæ¨¡å‹å®¶æ—ä»£è¡¨äº†ä¸€ç§å…¨é¢çš„AIæ¨¡å‹å¼€å‘æ–¹æ³•ï¼Œä¼˜å…ˆè€ƒè™‘å¼€æºå¯è®¿é—®æ€§ã€å¤šè¯­è¨€èƒ½åŠ›å’Œå®é™…éƒ¨ç½²ï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§èƒ½ç‰¹æ€§ã€‚Qwenæ¨¡å‹é€šè¿‡å¤šç§æ¨¡å‹è§„æ¨¡ã€é«˜è´¨é‡çš„è®­ç»ƒæ–¹æ³•ï¼Œä»¥åŠé’ˆå¯¹ä¸åŒé¢†åŸŸçš„ä¸“å±å˜ä½“å®ç°äº†è¿™ä¸€ç›®æ ‡ã€‚

Qwenå®¶æ—æ¶µç›–äº†å¤šç§æ–¹æ³•ï¼Œæ—¨åœ¨æä¾›æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´çš„é€‰æ‹©ï¼Œä»ç§»åŠ¨è®¾å¤‡åˆ°ä¼ä¸šæœåŠ¡å™¨çš„éƒ¨ç½²ï¼ŒåŒæ—¶æä¾›æœ‰æ„ä¹‰çš„AIèƒ½åŠ›ã€‚ç›®æ ‡æ˜¯è®©é«˜è´¨é‡AIçš„è®¿é—®æ°‘ä¸»åŒ–ï¼ŒåŒæ—¶æä¾›éƒ¨ç½²é€‰æ‹©çš„çµæ´»æ€§ã€‚

### Qwençš„æ ¸å¿ƒè®¾è®¡åŸåˆ™

Qwenæ¨¡å‹åŸºäºå‡ ä¸ªåŸºç¡€åŸåˆ™ï¼Œè¿™äº›åŸåˆ™ä½¿å…¶åŒºåˆ«äºå…¶ä»–è¯­è¨€æ¨¡å‹å®¶æ—ï¼š

- **å¼€æºä¼˜å…ˆ**ï¼šå®Œå…¨é€æ˜å’Œå¯è®¿é—®æ€§ï¼Œç”¨äºç ”ç©¶å’Œå•†ä¸šç”¨é€”
- **å…¨é¢è®­ç»ƒ**ï¼šåŸºäºè¦†ç›–å¤šç§è¯­è¨€å’Œé¢†åŸŸçš„å¤§è§„æ¨¡ã€å¤šæ ·åŒ–æ•°æ®é›†è¿›è¡Œè®­ç»ƒ
- **å¯æ‰©å±•æ¶æ„**ï¼šå¤šç§æ¨¡å‹è§„æ¨¡ä»¥åŒ¹é…ä¸åŒçš„è®¡ç®—éœ€æ±‚
- **ä¸“ä¸šåŒ–å“è¶Š**ï¼šé’ˆå¯¹ç‰¹å®šä»»åŠ¡ä¼˜åŒ–çš„é¢†åŸŸä¸“å±å˜ä½“

## æ”¯æ’‘Qwenå®¶æ—çš„å…³é”®æŠ€æœ¯

### å¤§è§„æ¨¡è®­ç»ƒ

Qwenå®¶æ—çš„ä¸€ä¸ªæ˜¾è‘—ç‰¹ç‚¹æ˜¯æ¨¡å‹å¼€å‘ä¸­æŠ•å…¥çš„å¤§è§„æ¨¡è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºã€‚Qwenæ¨¡å‹åˆ©ç”¨ç²¾å¿ƒç­–åˆ’çš„å¤šè¯­è¨€æ•°æ®é›†ï¼Œè¦†ç›–æ•°ä¸‡äº¿ä¸ªtokenï¼Œæ—¨åœ¨æä¾›å…¨é¢çš„ä¸–ç•ŒçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚

è¿™ç§æ–¹æ³•é€šè¿‡ç»“åˆé«˜è´¨é‡çš„ç½‘ç»œå†…å®¹ã€å­¦æœ¯æ–‡çŒ®ã€ä»£ç åº“å’Œå¤šè¯­è¨€èµ„æºæ¥å®ç°ã€‚è®­ç»ƒæ–¹æ³•å¼ºè°ƒçŸ¥è¯†çš„å¹¿åº¦å’Œå¯¹å„ç§é¢†åŸŸå’Œè¯­è¨€çš„æ·±åº¦ç†è§£ã€‚

### é«˜çº§æ¨ç†ä¸æ€è€ƒ

æœ€æ–°çš„Qwenæ¨¡å‹èå…¥äº†å¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å¾—å¤æ‚çš„å¤šæ­¥éª¤é—®é¢˜è§£å†³æˆä¸ºå¯èƒ½ï¼š

**æ€è€ƒæ¨¡å¼ï¼ˆQwen3ï¼‰**ï¼šæ¨¡å‹å¯ä»¥åœ¨ç»™å‡ºæœ€ç»ˆç­”æ¡ˆä¹‹å‰è¿›è¡Œè¯¦ç»†çš„é€æ­¥æ¨ç†ï¼Œç±»ä¼¼äºäººç±»è§£å†³é—®é¢˜çš„æ–¹å¼ã€‚

**åŒæ¨¡å¼æ“ä½œ**ï¼šèƒ½å¤Ÿåœ¨ç®€å•æŸ¥è¯¢çš„å¿«é€Ÿå“åº”æ¨¡å¼å’Œå¤æ‚é—®é¢˜çš„æ·±åº¦æ€è€ƒæ¨¡å¼ä¹‹é—´åˆ‡æ¢ã€‚

**æ€ç»´é“¾æ•´åˆ**ï¼šè‡ªç„¶åœ°æ•´åˆæ¨ç†æ­¥éª¤ï¼Œæé«˜å¤æ‚ä»»åŠ¡çš„é€æ˜åº¦å’Œå‡†ç¡®æ€§ã€‚

### æ¶æ„åˆ›æ–°

Qwenå®¶æ—èå…¥äº†å¤šç§æ¶æ„ä¼˜åŒ–ï¼Œæ—¨åœ¨åŒæ—¶å®ç°æ€§èƒ½å’Œæ•ˆç‡ï¼š

**å¯æ‰©å±•è®¾è®¡**ï¼šä¸€è‡´çš„æ¶æ„è·¨è¶Šä¸åŒæ¨¡å‹è§„æ¨¡ï¼Œä¾¿äºæ‰©å±•å’Œæ¯”è¾ƒã€‚

**å¤šæ¨¡æ€æ•´åˆ**ï¼šåœ¨ç»Ÿä¸€æ¶æ„ä¸­æ— ç¼æ•´åˆæ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘å¤„ç†èƒ½åŠ›ã€‚

**éƒ¨ç½²ä¼˜åŒ–**ï¼šé’ˆå¯¹ä¸åŒç¡¬ä»¶é…ç½®æä¾›å¤šç§é‡åŒ–é€‰é¡¹å’Œéƒ¨ç½²æ ¼å¼ã€‚

## æ¨¡å‹è§„æ¨¡ä¸éƒ¨ç½²é€‰é¡¹

ç°ä»£éƒ¨ç½²ç¯å¢ƒå—ç›ŠäºQwenæ¨¡å‹åœ¨å„ç§è®¡ç®—éœ€æ±‚ä¸­çš„çµæ´»æ€§ï¼š

### å°å‹æ¨¡å‹ï¼ˆ0.5B-3Bï¼‰

Qwenæä¾›é«˜æ•ˆçš„å°å‹æ¨¡å‹ï¼Œé€‚åˆè¾¹ç¼˜éƒ¨ç½²ã€ç§»åŠ¨åº”ç”¨å’Œèµ„æºå—é™ç¯å¢ƒï¼ŒåŒæ—¶ä¿æŒä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚

### ä¸­å‹æ¨¡å‹ï¼ˆ7B-32Bï¼‰

ä¸­å‹æ¨¡å‹ä¸ºä¸“ä¸šåº”ç”¨æä¾›å¢å¼ºèƒ½åŠ›ï¼Œåœ¨æ€§èƒ½å’Œè®¡ç®—éœ€æ±‚ä¹‹é—´å®ç°äº†è‰¯å¥½çš„å¹³è¡¡ã€‚

### å¤§å‹æ¨¡å‹ï¼ˆ72B+ï¼‰

å…¨è§„æ¨¡æ¨¡å‹ä¸ºéœ€è¦æœ€å¤§èƒ½åŠ›çš„é«˜è¦æ±‚åº”ç”¨ã€ç ”ç©¶å’Œä¼ä¸šéƒ¨ç½²æä¾›äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## Qwenæ¨¡å‹å®¶æ—çš„ä¼˜åŠ¿

### å¼€æºå¯è®¿é—®æ€§

Qwenæ¨¡å‹æä¾›å®Œå…¨çš„é€æ˜æ€§å’Œå®šåˆ¶èƒ½åŠ›ï¼Œä½¿ç»„ç»‡èƒ½å¤Ÿç†è§£ã€ä¿®æ”¹å’Œè°ƒæ•´æ¨¡å‹ä»¥æ»¡è¶³å…¶ç‰¹å®šéœ€æ±‚ï¼Œè€Œæ— éœ€ä¾èµ–ä¾›åº”å•†ã€‚

### éƒ¨ç½²çµæ´»æ€§

å¤šç§æ¨¡å‹è§„æ¨¡ä½¿å¾—ä»ç§»åŠ¨è®¾å¤‡åˆ°é«˜ç«¯æœåŠ¡å™¨çš„å¤šæ ·åŒ–ç¡¬ä»¶é…ç½®éƒ¨ç½²æˆä¸ºå¯èƒ½ï¼Œä¸ºç»„ç»‡çš„AIåŸºç¡€è®¾æ–½é€‰æ‹©æä¾›äº†çµæ´»æ€§ã€‚

### å¤šè¯­è¨€å“è¶Š

Qwenæ¨¡å‹åœ¨å¤šè¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ”¯æŒæ•°åç§è¯­è¨€ï¼Œå°¤å…¶åœ¨è‹±è¯­å’Œä¸­æ–‡æ–¹é¢è¡¨ç°çªå‡ºï¼Œéå¸¸é€‚åˆå…¨çƒåº”ç”¨ã€‚

### ç«äº‰æ€§èƒ½

Qwenæ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆè¡¨ç°å‡ºç«äº‰åŠ›ï¼ŒåŒæ—¶æä¾›å¼€æºå¯è®¿é—®æ€§ï¼Œè¯æ˜å¼€æºæ¨¡å‹å¯ä»¥ä¸ä¸“æœ‰æ¨¡å‹åª²ç¾ã€‚

### ä¸“ä¸šåŒ–èƒ½åŠ›

åƒQwen-Coderå’ŒQwen-Mathè¿™æ ·çš„é¢†åŸŸä¸“å±å˜ä½“åœ¨ä¿æŒä¸€èˆ¬è¯­è¨€ç†è§£èƒ½åŠ›çš„åŒæ—¶æä¾›ä¸“ä¸šçŸ¥è¯†ã€‚

## å®é™…ç¤ºä¾‹å’Œåº”ç”¨åœºæ™¯

åœ¨æ·±å…¥æŠ€æœ¯ç»†èŠ‚ä¹‹å‰ï¼Œè®©æˆ‘ä»¬æ¢ç´¢ä¸€äº›Qwenæ¨¡å‹å¯ä»¥å®ç°çš„å…·ä½“ç¤ºä¾‹ï¼š

### æ•°å­¦æ¨ç†ç¤ºä¾‹

Qwen-Mathæ“…é•¿é€æ­¥è§£å†³æ•°å­¦é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå½“è¢«è¦æ±‚è§£å†³å¤æ‚çš„å¾®ç§¯åˆ†é—®é¢˜æ—¶ï¼š

```
User: Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = xÂ³ + 2xÂ² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹

Breaking down each term:
- d/dx(xÂ³) = 3xÂ²
- d/dx(2xÂ²) = 2 Â· 2xÂ¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3xÂ² + 4x - 5
```

### å¤šè¯­è¨€æ”¯æŒç¤ºä¾‹

Qwenæ¨¡å‹åœ¨å¤šç§è¯­è¨€ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„å¤šè¯­è¨€èƒ½åŠ›ï¼š

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**ä¸­æ–‡:**
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡æœºå™¨ï¼ˆç‰¹åˆ«æ˜¯è®¡ç®—æœºç³»ç»Ÿï¼‰å¯¹äººç±»æ™ºèƒ½è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚è¿™äº›è¿‡ç¨‹åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€è§£å†³é—®é¢˜ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚äººå·¥æ™ºèƒ½ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»è®¤çŸ¥èƒ½åŠ›çš„ä»»åŠ¡ã€‚
```

### å¤šæ¨¡æ€èƒ½åŠ›ç¤ºä¾‹

Qwen-VLå¯ä»¥åŒæ—¶å¤„ç†æ–‡æœ¬å’Œå›¾åƒï¼š

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### ä»£ç ç”Ÿæˆç¤ºä¾‹

Qwen-Coderæ“…é•¿ç”Ÿæˆå’Œè§£é‡Šå¤šç§ç¼–ç¨‹è¯­è¨€çš„ä»£ç ï¼š

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

æ­¤å®ç°éµå¾ªæœ€ä½³å®è·µï¼Œå…·æœ‰æ¸…æ™°çš„å˜é‡å‘½åã€å…¨é¢çš„æ–‡æ¡£å’Œé«˜æ•ˆçš„é€»è¾‘ã€‚
```

### Edge Deployment Example

Qwen models can be deployed on various edge devices with optimized configurations:

```
# åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šä½¿ç”¨é‡åŒ–æ¨¡å‹è¿›è¡Œç¤ºä¾‹éƒ¨ç½²
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# åŠ è½½ç”¨äºç§»åŠ¨éƒ¨ç½²çš„é‡åŒ–æ¨¡å‹

```
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Qwenå®¶æ—çš„æ¼”å˜

### Qwen 1.0å’Œ1.5ï¼šåŸºç¡€æ¨¡å‹

æ—©æœŸçš„Qwenæ¨¡å‹ç¡®ç«‹äº†å…¨é¢è®­ç»ƒå’Œå¼€æºå¯è®¿é—®æ€§çš„åŸºç¡€åŸåˆ™ï¼š

- **Qwen-7Bï¼ˆ7Bå‚æ•°ï¼‰**ï¼šåˆå§‹ç‰ˆæœ¬ï¼Œä¸“æ³¨äºä¸­æ–‡å’Œè‹±æ–‡è¯­è¨€ç†è§£
- **Qwen-14Bï¼ˆ14Bå‚æ•°ï¼‰**ï¼šå¢å¼ºèƒ½åŠ›ï¼Œæ”¹è¿›æ¨ç†å’ŒçŸ¥è¯†
- **Qwen-72Bï¼ˆ72Bå‚æ•°ï¼‰**ï¼šå¤§è§„æ¨¡æ¨¡å‹ï¼Œæä¾›æœ€å…ˆè¿›çš„æ€§èƒ½
- **Qwen1.5ç³»åˆ—**ï¼šæ‰©å±•åˆ°å¤šç§è§„æ¨¡ï¼ˆ0.5Båˆ°110Bï¼‰ï¼Œæ”¹è¿›äº†é•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›

### Qwen2å®¶æ—ï¼šå¤šæ¨¡æ€æ‰©å±•

Qwen2ç³»åˆ—åœ¨è¯­è¨€å’Œå¤šæ¨¡æ€èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼š

- **Qwen2-0.5Båˆ°72B**ï¼šå…¨é¢çš„è¯­è¨€æ¨¡å‹èŒƒå›´ï¼Œæ»¡è¶³å„ç§éƒ¨ç½²éœ€æ±‚
- **Qwen2-57B-A14Bï¼ˆMoEï¼‰**ï¼šä¸“å®¶æ··åˆæ¶æ„ï¼Œæé«˜å‚æ•°ä½¿ç”¨æ•ˆç‡
- **Qwen2-VL**ï¼šé«˜çº§è§†è§‰è¯­è¨€èƒ½åŠ›ï¼Œç”¨äºå›¾åƒç†è§£
- **Qwen2-Audio**ï¼šéŸ³é¢‘å¤„ç†å’Œç†è§£èƒ½åŠ›
- **Qwen2-Math**ï¼šä¸“æ³¨äºæ•°å­¦æ¨ç†å’Œé—®é¢˜è§£å†³

### Qwen2.5å®¶æ—ï¼šæ€§èƒ½å¢å¼º

Qwen2.5ç³»åˆ—åœ¨å„ä¸ªç»´åº¦ä¸Šå¸¦æ¥äº†æ˜¾è‘—æ”¹è¿›ï¼š

- **æ‰©å±•è®­ç»ƒ**ï¼š180äº¿ä¸ªtokençš„è®­ç»ƒæ•°æ®ï¼Œæå‡èƒ½åŠ›
- **æ‰©å±•ä¸Šä¸‹æ–‡**ï¼šæ”¯æŒæœ€å¤š128Kä¸ªtokençš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼ŒTurboå˜ä½“æ”¯æŒ1Mä¸ªtoken
- **å¢å¼ºä¸“ä¸šåŒ–**ï¼šæ”¹è¿›çš„Qwen2.5-Coderå’ŒQwen2.5-Mathå˜ä½“
- **æ›´å¥½çš„å¤šè¯­è¨€æ”¯æŒ**ï¼šåœ¨27ç§ä»¥ä¸Šè¯­è¨€ä¸­è¡¨ç°å‡ºè‰²

### Qwen3å®¶æ—ï¼šé«˜çº§æ¨ç†

æœ€æ–°ä¸€ä»£æ¨åŠ¨äº†æ¨ç†å’Œæ€è€ƒèƒ½åŠ›çš„è¾¹ç•Œï¼š

- **Qwen3-235B-A22B**ï¼šæ——èˆ°ä¸“å®¶æ··åˆæ¨¡å‹ï¼Œæ€»å‚æ•°235B
- **Qwen3-30B-A3B**ï¼šé«˜æ•ˆMoEæ¨¡å‹ï¼Œå•ä¸ªæ´»åŠ¨å‚æ•°è¡¨ç°å¼ºåŠ²
- **å¯†é›†æ¨¡å‹**ï¼šQwen3-32Bã€14Bã€8Bã€4Bã€1.7Bã€0.6Bï¼Œé€‚åº”å„ç§éƒ¨ç½²åœºæ™¯
- **æ€è€ƒæ¨¡å¼**ï¼šæ”¯æŒå¿«é€Ÿå“åº”å’Œæ·±åº¦æ€è€ƒçš„æ··åˆæ¨ç†æ–¹æ³•
- **å¤šè¯­è¨€å“è¶Š**ï¼šæ”¯æŒ119ç§è¯­è¨€å’Œæ–¹è¨€
- **å¢å¼ºè®­ç»ƒ**ï¼š360äº¿ä¸ªtokençš„å¤šæ ·åŒ–é«˜è´¨é‡è®­ç»ƒæ•°æ®

## Qwenæ¨¡å‹çš„åº”ç”¨

### ä¼ä¸šåº”ç”¨

ç»„ç»‡ä½¿ç”¨Qwenæ¨¡å‹è¿›è¡Œæ–‡æ¡£åˆ†æã€å®¢æˆ·æœåŠ¡è‡ªåŠ¨åŒ–ã€ä»£ç ç”Ÿæˆè¾…åŠ©å’Œå•†ä¸šæ™ºèƒ½åº”ç”¨ã€‚å¼€æºç‰¹æ€§ä½¿å¾—èƒ½å¤Ÿæ ¹æ®å…·ä½“ä¸šåŠ¡éœ€æ±‚è¿›è¡Œå®šåˆ¶ï¼ŒåŒæ—¶ä¿æŒæ•°æ®éšç§å’Œæ§åˆ¶ã€‚

### ç§»åŠ¨å’Œè¾¹ç¼˜è®¡ç®—

ç§»åŠ¨åº”ç”¨åˆ©ç”¨Qwenæ¨¡å‹è¿›è¡Œå®æ—¶ç¿»è¯‘ã€æ™ºèƒ½åŠ©æ‰‹ã€å†…å®¹ç”Ÿæˆå’Œä¸ªæ€§åŒ–æ¨èã€‚å¤šç§æ¨¡å‹è§„æ¨¡ä½¿å¾—ä»ç§»åŠ¨è®¾å¤‡åˆ°è¾¹ç¼˜æœåŠ¡å™¨çš„éƒ¨ç½²æˆä¸ºå¯èƒ½ã€‚

### æ•™è‚²æŠ€æœ¯

æ•™è‚²å¹³å°ä½¿ç”¨Qwenæ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–è¾…å¯¼ã€è‡ªåŠ¨å†…å®¹ç”Ÿæˆã€è¯­è¨€å­¦ä¹ è¾…åŠ©å’Œäº’åŠ¨æ•™è‚²ä½“éªŒã€‚åƒQwen-Mathè¿™æ ·çš„ä¸“å±æ¨¡å‹æä¾›é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚

### å…¨çƒåº”ç”¨

å›½é™…åº”ç”¨å—ç›ŠäºQwenæ¨¡å‹å¼ºå¤§çš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒè¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ä¸‹æä¾›ä¸€è‡´çš„AIä½“éªŒã€‚

## æŒ‘æˆ˜å’Œå±€é™æ€§

### è®¡ç®—éœ€æ±‚

å°½ç®¡Qwenæä¾›äº†å„ç§è§„æ¨¡çš„æ¨¡å‹ï¼Œä½†è¾ƒå¤§çš„å˜ä½“ä»éœ€è¦æ˜¾è‘—çš„è®¡ç®—èµ„æºä»¥å®ç°æœ€ä½³æ€§èƒ½ï¼Œè¿™å¯èƒ½é™åˆ¶æŸäº›ç»„ç»‡çš„éƒ¨ç½²é€‰é¡¹ã€‚

### ä¸“ä¸šé¢†åŸŸæ€§èƒ½

å°½ç®¡Qwenæ¨¡å‹åœ¨ä¸€èˆ¬é¢†åŸŸè¡¨ç°è‰¯å¥½ï¼Œä½†é«˜åº¦ä¸“ä¸šåŒ–çš„åº”ç”¨å¯èƒ½éœ€è¦é¢†åŸŸä¸“å±å¾®è°ƒæˆ–ä¸“å±æ¨¡å‹ã€‚

### æ¨¡å‹é€‰æ‹©å¤æ‚æ€§

å¯ç”¨æ¨¡å‹å’Œå˜ä½“çš„å¹¿æ³›èŒƒå›´å¯èƒ½ä½¿æ–°ç”¨æˆ·åœ¨ç”Ÿæ€ç³»ç»Ÿä¸­é€‰æ‹©å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚

### è¯­è¨€ä¸å¹³è¡¡

å°½ç®¡æ”¯æŒå¤šç§è¯­è¨€ï¼Œä½†ä¸åŒè¯­è¨€çš„æ€§èƒ½å¯èƒ½æœ‰æ‰€ä¸åŒï¼Œè‹±è¯­å’Œä¸­æ–‡çš„èƒ½åŠ›æœ€å¼ºã€‚

## Qwenæ¨¡å‹å®¶æ—çš„æœªæ¥

Qwenæ¨¡å‹å®¶æ—ä»£è¡¨äº†å‘æ°‘ä¸»åŒ–ã€é«˜è´¨é‡AIå‘å±•çš„æŒç»­æ¼”å˜ã€‚æœªæ¥çš„å‘å±•åŒ…æ‹¬æ•ˆç‡ä¼˜åŒ–çš„å¢å¼ºã€å¤šæ¨¡æ€èƒ½åŠ›çš„æ‰©å±•ã€æ¨ç†æœºåˆ¶çš„æ”¹è¿›ï¼Œä»¥åŠåœ¨ä¸åŒéƒ¨ç½²åœºæ™¯ä¸­çš„æ›´å¥½æ•´åˆã€‚

éšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œæˆ‘ä»¬å¯ä»¥æœŸå¾…Qwenæ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå¼ºå¤§ï¼ŒåŒæ—¶ä¿æŒå…¶å¼€æºå¯è®¿é—®æ€§ï¼Œä½¿AIèƒ½å¤Ÿåœ¨å¤šç§åœºæ™¯å’Œç”¨ä¾‹ä¸­éƒ¨ç½²ã€‚

Qwenå®¶æ—å±•ç¤ºäº†AIå¼€å‘çš„æœªæ¥å¯ä»¥åŒæ—¶æ‹¥æŠ±å°–ç«¯æ€§èƒ½å’Œå¼€æ”¾å¯è®¿é—®æ€§ï¼Œä¸ºç»„ç»‡æä¾›å¼ºå¤§çš„å·¥å…·ï¼ŒåŒæ—¶ä¿æŒé€æ˜æ€§å’Œæ§åˆ¶ã€‚

## å¼€å‘å’Œé›†æˆç¤ºä¾‹

### ä½¿ç”¨Transformerså¿«é€Ÿå…¥é—¨

ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨Hugging Face Transformersåº“å¿«é€Ÿå¼€å§‹ä½¿ç”¨Qwenæ¨¡å‹ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### ä½¿ç”¨Qwen2.5æ¨¡å‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### ä¸“å±æ¨¡å‹ä½¿ç”¨

**ä½¿ç”¨Qwen-Coderè¿›è¡Œä»£ç ç”Ÿæˆï¼š**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**æ•°å­¦é—®é¢˜è§£å†³ï¼š**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**è§†è§‰è¯­è¨€ä»»åŠ¡ï¼š**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### æ€è€ƒæ¨¡å¼ï¼ˆQwen3ï¼‰

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### ğŸ“± ç§»åŠ¨å’Œè¾¹ç¼˜éƒ¨ç½²

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### APIéƒ¨ç½²ç¤ºä¾‹

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## æ€§èƒ½åŸºå‡†å’Œæˆå°±

Qwenæ¨¡å‹å®¶æ—åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå¼€æºå¯è®¿é—®æ€§ï¼š

### å…³é”®æ€§èƒ½äº®ç‚¹

**æ¨ç†å“è¶Šï¼š**
- Qwen3-235B-A22B åœ¨ç¼–ç ã€æ•°å­¦å’Œé€šç”¨èƒ½åŠ›çš„åŸºå‡†è¯„ä¼°ä¸­ï¼Œä¸å…¶ä»–é¡¶çº§æ¨¡å‹ï¼ˆå¦‚ DeepSeek-R1ã€o1ã€o3-miniã€Grok-3 å’Œ Gemini-2.5-Proï¼‰ç›¸æ¯”ï¼Œè¡¨ç°å‡ºç«äº‰åŠ›ã€‚
- Qwen3-30B-A3B ä»¥æ¿€æ´»å‚æ•°æ•°é‡çš„ååˆ†ä¹‹ä¸€è¶…è¶Šäº† QwQ-32Bã€‚
- Qwen3-4B çš„æ€§èƒ½å¯ä¸ Qwen2.5-72B-Instruct ç›¸åª²ç¾ã€‚

**æ•ˆç‡æˆå°±ï¼š**
- Qwen3-MoE åŸºç¡€æ¨¡å‹åœ¨ä»…ä½¿ç”¨ 10% æ¿€æ´»å‚æ•°çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¸ Qwen2.5 å¯†é›†å‹åŸºç¡€æ¨¡å‹ç›¸å½“ã€‚
- ä¸å¯†é›†å‹æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†æ–¹é¢æ˜¾è‘—èŠ‚çº¦æˆæœ¬ã€‚

**å¤šè¯­è¨€èƒ½åŠ›ï¼š**
- Qwen3 æ¨¡å‹æ”¯æŒ 119 ç§è¯­è¨€å’Œæ–¹è¨€ã€‚
- åœ¨å¤šæ ·åŒ–çš„è¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ä¸­è¡¨ç°å¼ºåŠ²ã€‚

**è®­ç»ƒè§„æ¨¡ï¼š**
- Qwen3 ä½¿ç”¨äº†çº¦ 36 ä¸‡äº¿ä¸ªæ ‡è®°ï¼Œè¦†ç›– 119 ç§è¯­è¨€å’Œæ–¹è¨€ï¼Œå‡ ä¹æ˜¯ Qwen2.5 çš„ä¸¤å€ï¼ˆ18 ä¸‡äº¿ä¸ªæ ‡è®°ï¼‰ã€‚

### æ¨¡å‹å¯¹æ¯”çŸ©é˜µ

| æ¨¡å‹ç³»åˆ— | å‚æ•°èŒƒå›´ | ä¸Šä¸‹æ–‡é•¿åº¦ | å…³é”®ä¼˜åŠ¿ | æœ€ä½³åº”ç”¨åœºæ™¯ |
|----------|----------|------------|----------|--------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | æ€§èƒ½å‡è¡¡ï¼Œå¤šè¯­è¨€æ”¯æŒ | é€šç”¨åº”ç”¨ï¼Œç”Ÿäº§éƒ¨ç½² |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | ä»£ç ç”Ÿæˆï¼Œç¼–ç¨‹ | è½¯ä»¶å¼€å‘ï¼Œç¼–ç¨‹è¾…åŠ© |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | æ•°å­¦æ¨ç† | æ•™è‚²å¹³å°ï¼ŒSTEM åº”ç”¨ |
| **Qwen2.5-VL** | å¤šç§ | å¯å˜ | è§†è§‰è¯­è¨€ç†è§£ | å¤šæ¨¡æ€åº”ç”¨ï¼Œå›¾åƒåˆ†æ |
| **Qwen3** | 0.6B-235B | å¯å˜ | é«˜çº§æ¨ç†ï¼Œæ€ç»´æ¨¡å¼ | å¤æ‚æ¨ç†ï¼Œç ”ç©¶åº”ç”¨ |
| **Qwen3 MoE** | æ€»è®¡ 30B-235B | å¯å˜ | é«˜æ•ˆçš„å¤§è§„æ¨¡æ€§èƒ½ | ä¼ä¸šåº”ç”¨ï¼Œé«˜æ€§èƒ½éœ€æ±‚ |

## æ¨¡å‹é€‰æ‹©æŒ‡å—

### åŸºç¡€åº”ç”¨
- **Qwen2.5-0.5B/1.5B**ï¼šç§»åŠ¨åº”ç”¨ï¼Œè¾¹ç¼˜è®¾å¤‡ï¼Œå®æ—¶åº”ç”¨
- **Qwen2.5-3B/7B**ï¼šé€šç”¨èŠå¤©æœºå™¨äººï¼Œå†…å®¹ç”Ÿæˆï¼Œé—®ç­”ç³»ç»Ÿ

### æ•°å­¦å’Œæ¨ç†ä»»åŠ¡
- **Qwen2.5-Math**ï¼šæ•°å­¦é—®é¢˜è§£å†³å’Œ STEM æ•™è‚²
- **Qwen3 çš„æ€ç»´æ¨¡å¼**ï¼šéœ€è¦é€æ­¥åˆ†æçš„å¤æ‚æ¨ç†

### ç¼–ç¨‹å’Œå¼€å‘
- **Qwen2.5-Coder**ï¼šä»£ç ç”Ÿæˆï¼Œè°ƒè¯•ï¼Œç¼–ç¨‹è¾…åŠ©
- **Qwen3**ï¼šå…·æœ‰æ¨ç†èƒ½åŠ›çš„é«˜çº§ç¼–ç¨‹ä»»åŠ¡

### å¤šæ¨¡æ€åº”ç”¨
- **Qwen2.5-VL**ï¼šå›¾åƒç†è§£ï¼Œè§†è§‰é—®ç­”
- **Qwen-Audio**ï¼šéŸ³é¢‘å¤„ç†å’Œè¯­éŸ³ç†è§£

### ä¼ä¸šéƒ¨ç½²
- **Qwen2.5-32B/72B**ï¼šé«˜æ€§èƒ½è¯­è¨€ç†è§£
- **Qwen3-235B-A22B**ï¼šæ»¡è¶³é«˜è¦æ±‚åº”ç”¨çš„æœ€å¤§èƒ½åŠ›

## éƒ¨ç½²å¹³å°å’Œå¯è®¿é—®æ€§
### äº‘å¹³å°
- **Hugging Face Hub**ï¼šå…¨é¢çš„æ¨¡å‹åº“å’Œç¤¾åŒºæ”¯æŒ
- **ModelScope**ï¼šé˜¿é‡Œå·´å·´çš„æ¨¡å‹å¹³å°ï¼Œæä¾›ä¼˜åŒ–å·¥å…·
- **å¤šç§äº‘æœåŠ¡æä¾›å•†**ï¼šé€šè¿‡æ ‡å‡†æœºå™¨å­¦ä¹ å¹³å°æ”¯æŒ

### æœ¬åœ°å¼€å‘æ¡†æ¶
- **Transformers**ï¼šæ ‡å‡† Hugging Face é›†æˆï¼Œä¾¿äºéƒ¨ç½²
- **vLLM**ï¼šç”Ÿäº§ç¯å¢ƒçš„é«˜æ€§èƒ½æœåŠ¡
- **Ollama**ï¼šç®€åŒ–çš„æœ¬åœ°éƒ¨ç½²å’Œç®¡ç†
- **ONNX Runtime**ï¼šè·¨å¹³å°ä¼˜åŒ–ï¼Œæ”¯æŒå¤šç§ç¡¬ä»¶
- **llama.cpp**ï¼šé«˜æ•ˆçš„ C++ å®ç°ï¼Œé€‚ç”¨äºå¤šç§å¹³å°

### å­¦ä¹ èµ„æº
- **Qwen æ–‡æ¡£**ï¼šå®˜æ–¹æ–‡æ¡£å’Œæ¨¡å‹å¡
- **Hugging Face Model Hub**ï¼šäº¤äº’å¼æ¼”ç¤ºå’Œç¤¾åŒºç¤ºä¾‹
- **ç ”ç©¶è®ºæ–‡**ï¼šarxiv ä¸Šçš„æŠ€æœ¯è®ºæ–‡ï¼Œæ·±å…¥äº†è§£
- **ç¤¾åŒºè®ºå›**ï¼šæ´»è·ƒçš„ç¤¾åŒºæ”¯æŒå’Œè®¨è®º

### Qwen æ¨¡å‹å…¥é—¨

#### å¼€å‘å¹³å°
1. **Hugging Face Transformers**ï¼šä»æ ‡å‡† Python é›†æˆå¼€å§‹
2. **ModelScope**ï¼šæ¢ç´¢é˜¿é‡Œå·´å·´çš„ä¼˜åŒ–éƒ¨ç½²å·¥å…·
3. **æœ¬åœ°éƒ¨ç½²**ï¼šä½¿ç”¨ Ollama æˆ–ç›´æ¥ Transformers è¿›è¡Œæœ¬åœ°æµ‹è¯•

#### å­¦ä¹ è·¯å¾„
1. **ç†è§£æ ¸å¿ƒæ¦‚å¿µ**ï¼šå­¦ä¹  Qwen ç³»åˆ—æ¶æ„å’Œèƒ½åŠ›
2. **å°è¯•ä¸åŒå˜ä½“**ï¼šæµ‹è¯•ä¸åŒæ¨¡å‹è§„æ¨¡ï¼Œäº†è§£æ€§èƒ½æƒè¡¡
3. **å®è·µå®æ–½**ï¼šåœ¨å¼€å‘ç¯å¢ƒä¸­éƒ¨ç½²æ¨¡å‹
4. **ä¼˜åŒ–éƒ¨ç½²**ï¼šé’ˆå¯¹ç”Ÿäº§ç”¨ä¾‹è¿›è¡Œå¾®è°ƒ

#### æœ€ä½³å®è·µ
- **ä»å°å¼€å§‹**ï¼šä»è¾ƒå°çš„æ¨¡å‹ï¼ˆ1.5B-7Bï¼‰å¼€å§‹åˆæ­¥å¼€å‘
- **ä½¿ç”¨èŠå¤©æ¨¡æ¿**ï¼šåº”ç”¨é€‚å½“çš„æ ¼å¼ä»¥è·å¾—æœ€ä½³ç»“æœ
- **ç›‘æ§èµ„æº**ï¼šè·Ÿè¸ªå†…å­˜ä½¿ç”¨å’Œæ¨ç†é€Ÿåº¦
- **è€ƒè™‘ä¸“ä¸šåŒ–**ï¼šåœ¨é€‚å½“æƒ…å†µä¸‹é€‰æ‹©é¢†åŸŸç‰¹å®šçš„å˜ä½“

## é«˜çº§ä½¿ç”¨æ¨¡å¼

### å¾®è°ƒç¤ºä¾‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### ä¸“ä¸šæç¤ºå·¥ç¨‹

**é’ˆå¯¹å¤æ‚æ¨ç†ä»»åŠ¡ï¼š**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**é’ˆå¯¹å¸¦ä¸Šä¸‹æ–‡çš„ä»£ç ç”Ÿæˆï¼š**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### å¤šè¯­è¨€åº”ç”¨

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ä¸­æ–‡)",
        "es": "Spanish (EspaÃ±ol)",
        "fr": "French (FranÃ§ais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (æ—¥æœ¬èª)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ğŸ”§ ç”Ÿäº§éƒ¨ç½²æ¨¡å¼

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### å†…å­˜ä¼˜åŒ–

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### æ¨ç†ä¼˜åŒ–

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## æœ€ä½³å®è·µå’ŒæŒ‡å—

### å®‰å…¨å’Œéšç§

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### ç›‘æ§å’Œè¯„ä¼°

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## ç»“è®º

Qwen æ¨¡å‹ç³»åˆ—ä»£è¡¨äº†ä¸€ç§å…¨é¢çš„æ–¹å¼ï¼Œæ—¨åœ¨æ™®åŠ AI æŠ€æœ¯ï¼ŒåŒæ—¶åœ¨å¤šæ ·åŒ–åº”ç”¨ä¸­ä¿æŒç«äº‰æ€§èƒ½ã€‚é€šè¿‡å…¶å¯¹å¼€æºå¯è®¿é—®æ€§ã€å¤šè¯­è¨€èƒ½åŠ›å’Œçµæ´»éƒ¨ç½²é€‰é¡¹çš„æ‰¿è¯ºï¼ŒQwen ä½¿ç»„ç»‡å’Œå¼€å‘è€…èƒ½å¤Ÿåˆ©ç”¨å¼ºå¤§çš„ AI èƒ½åŠ›ï¼Œæ— è®ºå…¶èµ„æºæˆ–å…·ä½“éœ€æ±‚å¦‚ä½•ã€‚

### å…³é”®è¦ç‚¹

**å¼€æºå“è¶Š**ï¼šQwen å±•ç¤ºäº†å¼€æºæ¨¡å‹å¯ä»¥åœ¨æ€§èƒ½ä¸Šä¸ä¸“æœ‰æ›¿ä»£å“ç«äº‰ï¼ŒåŒæ—¶æä¾›é€æ˜æ€§ã€å®šåˆ¶æ€§å’Œæ§åˆ¶åŠ›ã€‚

**å¯æ‰©å±•æ¶æ„**ï¼šä» 0.5B åˆ° 235B å‚æ•°çš„èŒƒå›´ä½¿å¾—æ¨¡å‹å¯ä»¥éƒ¨ç½²åœ¨ä»ç§»åŠ¨è®¾å¤‡åˆ°ä¼ä¸šé›†ç¾¤çš„æ‰€æœ‰è®¡ç®—ç¯å¢ƒä¸­ã€‚

**ä¸“ä¸šèƒ½åŠ›**ï¼šé¢†åŸŸç‰¹å®šçš„å˜ä½“ï¼ˆå¦‚ Qwen-Coderã€Qwen-Math å’Œ Qwen-VLï¼‰åœ¨ä¿æŒé€šç”¨è¯­è¨€ç†è§£çš„åŒæ—¶æä¾›ä¸“ä¸šçŸ¥è¯†ã€‚

**å…¨çƒå¯è®¿é—®æ€§**ï¼šå¯¹ 119+ ç§è¯­è¨€çš„å¼ºå¤§å¤šè¯­è¨€æ”¯æŒä½¿ Qwen é€‚ç”¨äºå›½é™…åº”ç”¨å’Œå¤šæ ·åŒ–ç”¨æˆ·ç¾¤ã€‚

**æŒç»­åˆ›æ–°**ï¼šä» Qwen 1.0 åˆ° Qwen3 çš„æ¼”å˜æ˜¾ç¤ºäº†èƒ½åŠ›ã€æ•ˆç‡å’Œéƒ¨ç½²é€‰é¡¹çš„æŒç»­æ”¹è¿›ã€‚

### æœªæ¥å±•æœ›

éšç€ Qwen ç³»åˆ—çš„ä¸æ–­å‘å±•ï¼Œæˆ‘ä»¬å¯ä»¥æœŸå¾…ï¼š

- **æ•ˆç‡æå‡**ï¼šæŒç»­ä¼˜åŒ–ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½å‚æ•°æ¯”
- **æ‰©å±•çš„å¤šæ¨¡æ€èƒ½åŠ›**ï¼šé›†æˆæ›´å¤æ‚çš„è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬å¤„ç†
- **æ”¹è¿›çš„æ¨ç†èƒ½åŠ›**ï¼šé«˜çº§æ€ç»´æœºåˆ¶å’Œå¤šæ­¥éª¤é—®é¢˜è§£å†³èƒ½åŠ›
- **æ›´å¥½çš„éƒ¨ç½²å·¥å…·**ï¼šé’ˆå¯¹å¤šæ ·åŒ–éƒ¨ç½²åœºæ™¯çš„å¢å¼ºæ¡†æ¶å’Œä¼˜åŒ–å·¥å…·
- **ç¤¾åŒºå¢é•¿**ï¼šæ‰©å±•çš„å·¥å…·ã€åº”ç”¨å’Œç¤¾åŒºè´¡çŒ®ç”Ÿæ€ç³»ç»Ÿ

### ä¸‹ä¸€æ­¥

æ— è®ºæ‚¨æ˜¯åœ¨æ„å»ºèŠå¤©æœºå™¨äººã€å¼€å‘æ•™è‚²å·¥å…·ã€åˆ›å»ºç¼–ç¨‹åŠ©æ‰‹ï¼Œè¿˜æ˜¯ä»äº‹å¤šè¯­è¨€åº”ç”¨ï¼ŒQwen ç³»åˆ—éƒ½æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å…·æœ‰å¼ºå¤§çš„ç¤¾åŒºæ”¯æŒå’Œå…¨é¢çš„æ–‡æ¡£ã€‚

æœ‰å…³æœ€æ–°æ›´æ–°ã€æ¨¡å‹å‘å¸ƒå’Œè¯¦ç»†æŠ€æœ¯æ–‡æ¡£ï¼Œè¯·è®¿é—® Hugging Face ä¸Šçš„ Qwen å®˜æ–¹åº“ï¼Œå¹¶æ¢ç´¢æ´»è·ƒçš„ç¤¾åŒºè®¨è®ºå’Œç¤ºä¾‹ã€‚

AI å¼€å‘çš„æœªæ¥åœ¨äºå¯è®¿é—®ã€é€æ˜å’Œå¼ºå¤§çš„å·¥å…·ï¼Œè¿™äº›å·¥å…·èƒ½å¤Ÿæ¨åŠ¨å„ä¸ªé¢†åŸŸå’Œè§„æ¨¡çš„åˆ›æ–°ã€‚Qwen ç³»åˆ—ä½“ç°äº†è¿™ä¸€æ„¿æ™¯ï¼Œä¸ºç»„ç»‡å’Œå¼€å‘è€…æä¾›äº†æ„å»ºä¸‹ä¸€ä»£ AI é©±åŠ¨åº”ç”¨çš„åŸºç¡€ã€‚

## é™„åŠ èµ„æº

- **å®˜æ–¹æ–‡æ¡£**ï¼š[Qwen æ–‡æ¡£](https://qwen.readthedocs.io/)
- **æ¨¡å‹åº“**ï¼š[Hugging Face Qwen é›†åˆ](https://huggingface.co/collections/Qwen/)
- **æŠ€æœ¯è®ºæ–‡**ï¼š[Qwen ç ”ç©¶å‡ºç‰ˆç‰©](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **ç¤¾åŒº**ï¼š[GitHub è®¨è®ºå’Œé—®é¢˜](https://github.com/QwenLM/)
- **ModelScope å¹³å°**ï¼š[é˜¿é‡Œå·´å·´ ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## å­¦ä¹ æˆæœ

å®Œæˆæœ¬æ¨¡å—åï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š

1. è§£é‡Š Qwen æ¨¡å‹ç³»åˆ—çš„æ¶æ„ä¼˜åŠ¿åŠå…¶å¼€æºæ–¹æ³•
2. æ ¹æ®å…·ä½“åº”ç”¨éœ€æ±‚å’Œèµ„æºé™åˆ¶é€‰æ‹©åˆé€‚çš„ Qwen å˜ä½“
3. åœ¨å„ç§éƒ¨ç½²åœºæ™¯ä¸­å®æ–½ Qwen æ¨¡å‹å¹¶è¿›è¡Œä¼˜åŒ–é…ç½®
4. åº”ç”¨é‡åŒ–å’Œä¼˜åŒ–æŠ€æœ¯ä»¥æå‡ Qwen æ¨¡å‹æ€§èƒ½
5. è¯„ä¼° Qwen ç³»åˆ—ä¸­æ¨¡å‹è§„æ¨¡ã€æ€§èƒ½å’Œèƒ½åŠ›ä¹‹é—´çš„æƒè¡¡

## æ¥ä¸‹æ¥

- [03: Gemma ç³»åˆ—åŸºç¡€](03.GemmaFamily.md)

---

**å…è´£å£°æ˜**ï¼š  
æœ¬æ–‡æ¡£ä½¿ç”¨AIç¿»è¯‘æœåŠ¡ [Co-op Translator](https://github.com/Azure/co-op-translator) è¿›è¡Œç¿»è¯‘ã€‚å°½ç®¡æˆ‘ä»¬åŠªåŠ›ç¡®ä¿ç¿»è¯‘çš„å‡†ç¡®æ€§ï¼Œä½†è¯·æ³¨æ„ï¼Œè‡ªåŠ¨ç¿»è¯‘å¯èƒ½åŒ…å«é”™è¯¯æˆ–ä¸å‡†ç¡®ä¹‹å¤„ã€‚åŸå§‹è¯­è¨€çš„æ–‡æ¡£åº”è¢«è§†ä¸ºæƒå¨æ¥æºã€‚å¯¹äºå…³é”®ä¿¡æ¯ï¼Œå»ºè®®ä½¿ç”¨ä¸“ä¸šäººå·¥ç¿»è¯‘ã€‚æˆ‘ä»¬å¯¹å› ä½¿ç”¨æ­¤ç¿»è¯‘è€Œäº§ç”Ÿçš„ä»»ä½•è¯¯è§£æˆ–è¯¯è¯»ä¸æ‰¿æ‹…è´£ä»»ã€‚