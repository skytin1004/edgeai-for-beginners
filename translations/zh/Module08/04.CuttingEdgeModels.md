<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "949045c54206448d55d987e8b23a82cf",
  "translation_date": "2025-09-24T09:37:09+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "zh"
}
-->
# ç¬¬å››èŠ‚ï¼šä½¿ç”¨ Chainlit æ„å»ºç”Ÿäº§çº§èŠå¤©åº”ç”¨

## æ¦‚è¿°

æœ¬èŠ‚è¯¾é‡ç‚¹è®²è§£å¦‚ä½•ä½¿ç”¨ Chainlit å’Œ Microsoft Foundry Local æ„å»ºç”Ÿäº§çº§èŠå¤©åº”ç”¨ã€‚æ‚¨å°†å­¦ä¹ å¦‚ä½•åˆ›å»ºç°ä»£åŒ–çš„ AI å¯¹è¯ç½‘é¡µç•Œé¢ã€å®ç°æµå¼å“åº”ï¼Œä»¥åŠéƒ¨ç½²å…·æœ‰å®Œå–„é”™è¯¯å¤„ç†å’Œç”¨æˆ·ä½“éªŒè®¾è®¡çš„å¼ºå¤§èŠå¤©åº”ç”¨ã€‚

**æ‚¨å°†æ„å»ºçš„å†…å®¹ï¼š**
- **Chainlit èŠå¤©åº”ç”¨**ï¼šå…·æœ‰æµå¼å“åº”çš„ç°ä»£åŒ–ç½‘é¡µç•Œé¢
- **WebGPU æ¼”ç¤º**ï¼šåŸºäºæµè§ˆå™¨çš„æ¨ç†ï¼Œæ³¨é‡éšç§ä¿æŠ¤  
- **å¼€æ”¾ WebUI é›†æˆ**ï¼šä¸ Foundry Local é›†æˆçš„ä¸“ä¸šèŠå¤©ç•Œé¢
- **ç”Ÿäº§æ¨¡å¼**ï¼šé”™è¯¯å¤„ç†ã€ç›‘æ§å’Œéƒ¨ç½²ç­–ç•¥

## å­¦ä¹ ç›®æ ‡

- ä½¿ç”¨ Chainlit æ„å»ºç”Ÿäº§çº§èŠå¤©åº”ç”¨
- å®ç°æµå¼å“åº”ä»¥æå‡ç”¨æˆ·ä½“éªŒ
- æŒæ¡ Foundry Local SDK çš„é›†æˆæ¨¡å¼
- åº”ç”¨æ­£ç¡®çš„é”™è¯¯å¤„ç†å’Œä¼˜é›…é™çº§ç­–ç•¥
- ä¸ºä¸åŒç¯å¢ƒéƒ¨ç½²å’Œé…ç½®èŠå¤©åº”ç”¨
- ç†è§£å¯¹è¯å¼ AI çš„ç°ä»£ç½‘é¡µç•Œé¢æ¨¡å¼

## å‰ç½®æ¡ä»¶

- **Foundry Local**ï¼šå·²å®‰è£…å¹¶è¿è¡Œï¼ˆ[å®‰è£…æŒ‡å—](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)ï¼‰
- **Python**ï¼š3.10 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œæ”¯æŒè™šæ‹Ÿç¯å¢ƒ
- **æ¨¡å‹**ï¼šè‡³å°‘åŠ è½½ä¸€ä¸ªæ¨¡å‹ï¼ˆ`foundry model run phi-4-mini`ï¼‰
- **æµè§ˆå™¨**ï¼šæ”¯æŒ WebGPU çš„ç°ä»£æµè§ˆå™¨ï¼ˆChrome/Edgeï¼‰
- **Docker**ï¼šç”¨äºå¼€æ”¾ WebUI é›†æˆï¼ˆå¯é€‰ï¼‰

## ç¬¬ä¸€éƒ¨åˆ†ï¼šç†è§£ç°ä»£èŠå¤©åº”ç”¨

### æ¶æ„æ¦‚è¿°

```
User Browser â†â†’ Chainlit UI â†â†’ Python Backend â†â†’ Foundry Local â†â†’ AI Model
      â†“              â†“              â†“              â†“            â†“
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### æ ¸å¿ƒæŠ€æœ¯

**Foundry Local SDK æ¨¡å¼ï¼š**
- `FoundryLocalManager(alias)`ï¼šè‡ªåŠ¨æœåŠ¡ç®¡ç†
- `manager.endpoint` å’Œ `manager.api_key`ï¼šè¿æ¥è¯¦æƒ…
- `manager.get_model_info(alias).id`ï¼šæ¨¡å‹æ ‡è¯†

**Chainlit æ¡†æ¶ï¼š**
- `@cl.on_chat_start`ï¼šåˆå§‹åŒ–èŠå¤©ä¼šè¯
- `@cl.on_message`ï¼šå¤„ç†ç”¨æˆ·æ¶ˆæ¯  
- `cl.Message().stream_token()`ï¼šå®æ—¶æµå¼å“åº”
- è‡ªåŠ¨ç”Ÿæˆ UI å’Œ WebSocket ç®¡ç†

## ç¬¬äºŒéƒ¨åˆ†ï¼šæœ¬åœ°ä¸äº‘ç«¯å†³ç­–çŸ©é˜µ

### æ€§èƒ½ç‰¹ç‚¹

| æ–¹é¢ | æœ¬åœ°ï¼ˆFoundryï¼‰ | äº‘ç«¯ï¼ˆAzure OpenAIï¼‰ |
|------|----------------|---------------------|
| **å»¶è¿Ÿ** | ğŸš€ 50-200msï¼ˆæ— ç½‘ç»œï¼‰ | â±ï¸ 200-2000msï¼ˆå–å†³äºç½‘ç»œï¼‰ |
| **éšç§** | ğŸ”’ æ•°æ®ä¸ç¦»å¼€è®¾å¤‡ | âš ï¸ æ•°æ®å‘é€è‡³äº‘ç«¯ |
| **æˆæœ¬** | ğŸ’° ç¡¬ä»¶åå…è´¹ | ğŸ’¸ æŒ‰ä»¤ç‰Œä»˜è´¹ |
| **ç¦»çº¿** | âœ… æ— éœ€äº’è”ç½‘å³å¯è¿è¡Œ | âŒ éœ€è¦äº’è”ç½‘ |
| **æ¨¡å‹è§„æ¨¡** | âš ï¸ å—ç¡¬ä»¶é™åˆ¶ | âœ… å¯è®¿é—®æœ€å¤§æ¨¡å‹ |
| **æ‰©å±•æ€§** | âš ï¸ ä¾èµ–ç¡¬ä»¶ | âœ… æ— é™æ‰©å±• |

### æ··åˆç­–ç•¥æ¨¡å¼

**æœ¬åœ°ä¼˜å…ˆï¼Œäº‘ç«¯å¤‡é€‰ï¼š**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**åŸºäºä»»åŠ¡çš„è·¯ç”±ï¼š**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šç¤ºä¾‹ 04 - Chainlit èŠå¤©åº”ç”¨

### å¿«é€Ÿå¼€å§‹

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

åº”ç”¨ä¼šè‡ªåŠ¨åœ¨ `http://localhost:8080` æ‰“å¼€ï¼Œæä¾›ç°ä»£åŒ–èŠå¤©ç•Œé¢ã€‚

### æ ¸å¿ƒå®ç°

ç¤ºä¾‹ 04 å±•ç¤ºäº†ç”Ÿäº§çº§æ¨¡å¼ï¼š

**è‡ªåŠ¨æœåŠ¡å‘ç°ï¼š**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**æµå¼èŠå¤©å¤„ç†ï¼š**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### é…ç½®é€‰é¡¹

**ç¯å¢ƒå˜é‡ï¼š**

| å˜é‡ | æè¿° | é»˜è®¤å€¼ | ç¤ºä¾‹ |
|------|------|-------|------|
| `MODEL` | ä½¿ç”¨çš„æ¨¡å‹åˆ«å | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | Foundry Local ç«¯ç‚¹ | è‡ªåŠ¨æ£€æµ‹ | `http://localhost:51211` |
| `API_KEY` | API å¯†é’¥ï¼ˆæœ¬åœ°å¯é€‰ï¼‰ | `""` | `your-api-key` |

**é«˜çº§ç”¨æ³•ï¼š**
```cmd
# Use different model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## ç¬¬å››éƒ¨åˆ†ï¼šåˆ›å»ºå’Œä½¿ç”¨ Jupyter Notebook

### Notebook æ”¯æŒæ¦‚è¿°

ç¤ºä¾‹ 04 åŒ…å«ä¸€ä¸ªå…¨é¢çš„ Jupyter Notebookï¼ˆ`chainlit_app.ipynb`ï¼‰ï¼Œæä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š

- **ğŸ“š æ•™å­¦å†…å®¹**ï¼šé€æ­¥å­¦ä¹ ææ–™
- **ğŸ”¬ äº¤äº’å¼æ¢ç´¢**ï¼šè¿è¡Œå¹¶å®éªŒä»£ç å•å…ƒ
- **ğŸ“Š å¯è§†åŒ–æ¼”ç¤º**ï¼šå›¾è¡¨ã€å›¾è§£å’Œè¾“å‡ºå¯è§†åŒ–
- **ğŸ› ï¸ å¼€å‘å·¥å…·**ï¼šæµ‹è¯•å’Œè°ƒè¯•åŠŸèƒ½

### åˆ›å»ºæ‚¨è‡ªå·±çš„ Notebook

#### ç¬¬ä¸€æ­¥ï¼šè®¾ç½® Jupyter ç¯å¢ƒ

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### ç¬¬äºŒæ­¥ï¼šåˆ›å»ºæ–° Notebook

**ä½¿ç”¨ VS Codeï¼š**
1. åœ¨ Module08 ç›®å½•ä¸­æ‰“å¼€ VS Code
2. åˆ›å»ºä¸€ä¸ª `.ipynb` æ‰©å±•åçš„æ–°æ–‡ä»¶
3. é€‰æ‹©â€œFoundry Localâ€å†…æ ¸
4. å¼€å§‹æ·»åŠ å•å…ƒå†…å®¹

**ä½¿ç”¨ Jupyter Labï¼š**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Notebook ç»“æ„æœ€ä½³å®è·µ

#### å•å…ƒç»„ç»‡

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("âœ… Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### äº¤äº’å¼ç¤ºä¾‹å’Œç»ƒä¹ 

#### ç»ƒä¹  1ï¼šå®¢æˆ·ç«¯é…ç½®æµ‹è¯•

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b-instruct"},
]

for config in configurations:
    print(f"\nğŸ§ª Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'âœ… Success' if result['status'] == 'ok' else 'âŒ Failed'}")
```

#### ç»ƒä¹  2ï¼šæµå¼å“åº”æ¨¡æ‹Ÿ

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("ğŸŒŠ Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\nâœ… Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## ç¬¬äº”éƒ¨åˆ†ï¼šWebGPU æµè§ˆå™¨æ¨ç†æ¼”ç¤º

### æ¦‚è¿°

WebGPU å…è®¸ç›´æ¥åœ¨æµè§ˆå™¨ä¸­è¿è¡Œ AI æ¨¡å‹ï¼Œæœ€å¤§é™åº¦ä¿æŠ¤éšç§å¹¶å®ç°é›¶å®‰è£…ä½“éªŒã€‚æœ¬ç¤ºä¾‹å±•ç¤ºäº†ä½¿ç”¨ ONNX Runtime Web å’Œ WebGPU æ‰§è¡Œçš„è¿‡ç¨‹ã€‚

### ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ WebGPU æ”¯æŒ

**æµè§ˆå™¨è¦æ±‚ï¼š**
- Chrome/Edge 113+ï¼Œå¯ç”¨ WebGPU
- æ£€æŸ¥ï¼š`chrome://gpu` â†’ ç¡®è®¤â€œWebGPUâ€çŠ¶æ€
- ç¨‹åºåŒ–æ£€æŸ¥ï¼š`if (!('gpu' in navigator)) { /* no WebGPU */ }`

### ç¬¬äºŒæ­¥ï¼šåˆ›å»º WebGPU æ¼”ç¤º

åˆ›å»ºç›®å½•ï¼š`samples/04/webgpu-demo/`

**index.htmlï¼š**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>ğŸš€ WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.jsï¼š**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = 'âŒ WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'ğŸ” WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('âœ… ONNX Runtime session created with WebGPU');
        log(`ğŸ“Š Input names: ${session.inputNames.join(', ')}`);
        log(`ğŸ“Š Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = 'âœ… WebGPU inference complete!';
        log(`ğŸ¯ Predicted class: ${maxIdx}`);
        log(`ğŸ“ˆ Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `âŒ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### ç¬¬ä¸‰æ­¥ï¼šè¿è¡Œæ¼”ç¤º

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## ç¬¬å…­éƒ¨åˆ†ï¼šå¼€æ”¾ WebUI é›†æˆ

### æ¦‚è¿°

å¼€æ”¾ WebUI æä¾›ä¸€ä¸ªä¸“ä¸šçš„ ChatGPT é£æ ¼ç•Œé¢ï¼Œå¯è¿æ¥åˆ° Foundry Local çš„ OpenAI å…¼å®¹ APIã€‚

### ç¬¬ä¸€æ­¥ï¼šå‰ç½®æ¡ä»¶

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### ç¬¬äºŒæ­¥ï¼šDocker è®¾ç½®ï¼ˆæ¨èï¼‰

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**æ³¨æ„ï¼š** `host.docker.internal` å…è®¸ Docker å®¹å™¨è®¿é—®ä¸»æœºæœºå™¨ï¼ˆWindowsï¼‰ã€‚

### ç¬¬ä¸‰æ­¥ï¼šé…ç½®

1. **æ‰“å¼€æµè§ˆå™¨ï¼š** è®¿é—® `http://localhost:3000`
2. **åˆå§‹è®¾ç½®ï¼š** åˆ›å»ºç®¡ç†å‘˜è´¦æˆ·
3. **æ¨¡å‹é…ç½®ï¼š**
   - è®¾ç½® â†’ æ¨¡å‹ â†’ OpenAI API  
   - åŸºç¡€ URLï¼š`http://host.docker.internal:51211/v1`
   - API å¯†é’¥ï¼š`foundry-local-key`ï¼ˆä»»æ„å€¼å‡å¯ï¼‰
4. **æµ‹è¯•è¿æ¥ï¼š** æ¨¡å‹åº”å‡ºç°åœ¨ä¸‹æ‹‰èœå•ä¸­

### æ•…éšœæ’é™¤

**å¸¸è§é—®é¢˜ï¼š**

1. **è¿æ¥è¢«æ‹’ç»ï¼š**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **æ¨¡å‹æœªå‡ºç°ï¼š**
   - éªŒè¯æ¨¡å‹æ˜¯å¦å·²åŠ è½½ï¼š`foundry model list`
   - æ£€æŸ¥ API å“åº”ï¼š`curl http://localhost:51211/v1/models`
   - é‡å¯å¼€æ”¾ WebUI å®¹å™¨

## ç¬¬ä¸ƒéƒ¨åˆ†ï¼šç”Ÿäº§éƒ¨ç½²æ³¨æ„äº‹é¡¹

### ç¯å¢ƒé…ç½®

**å¼€å‘è®¾ç½®ï¼š**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**ç”Ÿäº§éƒ¨ç½²ï¼š**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### å¸¸è§ç«¯å£é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

**ç«¯å£ 51211 å†²çªé¢„é˜²ï¼š**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### æ€§èƒ½ç›‘æ§

**å¥åº·æ£€æŸ¥å®ç°ï¼š**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## æ€»ç»“

ç¬¬å››èŠ‚è¯¾ç¨‹æ¶µç›–äº†æ„å»ºç”Ÿäº§çº§ Chainlit åº”ç”¨çš„å†…å®¹ï¼Œé‡ç‚¹åŒ…æ‹¬ï¼š

- âœ… **Chainlit æ¡†æ¶**ï¼šèŠå¤©åº”ç”¨çš„ç°ä»£åŒ– UI å’Œæµå¼æ”¯æŒ
- âœ… **Foundry Local é›†æˆ**ï¼šSDK ä½¿ç”¨å’Œé…ç½®æ¨¡å¼  
- âœ… **WebGPU æ¨ç†**ï¼šåŸºäºæµè§ˆå™¨çš„ AIï¼Œæ³¨é‡éšç§ä¿æŠ¤
- âœ… **å¼€æ”¾ WebUI è®¾ç½®**ï¼šä¸“ä¸šèŠå¤©ç•Œé¢éƒ¨ç½²
- âœ… **ç”Ÿäº§æ¨¡å¼**ï¼šé”™è¯¯å¤„ç†ã€ç›‘æ§å’Œæ‰©å±•

ç¤ºä¾‹ 04 åº”ç”¨å±•ç¤ºäº†æœ€ä½³å®è·µï¼Œå¸®åŠ©æ‚¨æ„å»ºå¼ºå¤§çš„èŠå¤©ç•Œé¢ï¼Œåˆ©ç”¨ Microsoft Foundry Local çš„æœ¬åœ° AI æ¨¡å‹ï¼ŒåŒæ—¶æä¾›å“è¶Šçš„ç”¨æˆ·ä½“éªŒã€‚

## å‚è€ƒèµ„æ–™

- **[ç¤ºä¾‹ 04ï¼šChainlit åº”ç”¨](samples/04/README.md)**ï¼šå®Œæ•´åº”ç”¨åŠæ–‡æ¡£
- **[Chainlit æ•™å­¦ Notebook](samples/04/chainlit_app.ipynb)**ï¼šäº¤äº’å¼å­¦ä¹ ææ–™
- **[Foundry Local æ–‡æ¡£](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**ï¼šå®Œæ•´å¹³å°æ–‡æ¡£
- **[Chainlit æ–‡æ¡£](https://docs.chainlit.io/)**ï¼šå®˜æ–¹æ¡†æ¶æ–‡æ¡£
- **[å¼€æ”¾ WebUI é›†æˆæŒ‡å—](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**ï¼šå®˜æ–¹æ•™ç¨‹

---

