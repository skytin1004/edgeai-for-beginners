<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6a574846c3919c56f1d02bf1de2003ca",
  "translation_date": "2025-09-30T23:19:42+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "zh"
}
-->
# ç¬¬1èŠ‚ï¼šå¼€å§‹ä½¿ç”¨ Foundry Local

## æ¦‚è¿°

Microsoft Foundry Local å°† Azure AI Foundry çš„åŠŸèƒ½ç›´æ¥å¸¦åˆ°æ‚¨çš„ Windows 11 å¼€å‘ç¯å¢ƒä¸­ï¼Œæä¾›éšç§ä¿æŠ¤ã€ä½å»¶è¿Ÿçš„ AI å¼€å‘ä½“éªŒï¼Œå¹¶é…å¤‡ä¼ä¸šçº§å·¥å…·ã€‚æœ¬èŠ‚å†…å®¹æ¶µç›–äº†æµè¡Œæ¨¡å‹ï¼ˆåŒ…æ‹¬ phiã€qwenã€deepseek å’Œ GPT-OSS-20Bï¼‰çš„å®Œæ•´å®‰è£…ã€é…ç½®å’Œå®é™…éƒ¨ç½²ã€‚

## å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬èŠ‚åï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š
- åœ¨ Windows 11 ä¸Šå®‰è£…å’Œé…ç½® Foundry Local
- æŒæ¡ CLI å‘½ä»¤å’Œé…ç½®é€‰é¡¹
- äº†è§£æ¨¡å‹ç¼“å­˜ç­–ç•¥ä»¥ä¼˜åŒ–æ€§èƒ½
- æˆåŠŸè¿è¡Œ phiã€qwenã€deepseek å’Œ GPT-OSS-20B æ¨¡å‹
- ä½¿ç”¨ Foundry Local åˆ›å»ºæ‚¨çš„ç¬¬ä¸€ä¸ª AI åº”ç”¨ç¨‹åº

## å‰ç½®æ¡ä»¶

### ç³»ç»Ÿè¦æ±‚
- **Windows 11**ï¼šç‰ˆæœ¬ 22H2 æˆ–æ›´é«˜
- **å†…å­˜**ï¼šæœ€ä½ 16GBï¼Œæ¨è 32GB
- **å­˜å‚¨**ï¼š50GB å¯ç”¨ç©ºé—´ç”¨äºæ¨¡å‹å’Œç¼“å­˜
- **ç¡¬ä»¶**ï¼šå»ºè®®ä½¿ç”¨æ”¯æŒ NPU æˆ– GPU çš„è®¾å¤‡ï¼ˆå¦‚ Copilot+ PC æˆ– NVIDIA GPUï¼‰
- **ç½‘ç»œ**ï¼šé«˜é€Ÿäº’è”ç½‘è¿æ¥ä»¥ä¸‹è½½æ¨¡å‹

### å¼€å‘ç¯å¢ƒ
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion

# Set up Python environment (recommended)
py -m venv .venv
.venv\Scripts\activate

# Install required dependencies
pip install openai foundry-local-sdk
```

## ç¬¬1éƒ¨åˆ†ï¼šå®‰è£…å’Œè®¾ç½®

### æ­¥éª¤1ï¼šå®‰è£… Foundry Local

ä½¿ç”¨ Winget å®‰è£… Foundry Local æˆ–ä» GitHub ä¸‹è½½å®‰è£…ç¨‹åºï¼š

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### æ­¥éª¤2ï¼šéªŒè¯å®‰è£…

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## ç¬¬2éƒ¨åˆ†ï¼šäº†è§£ CLI

### æ ¸å¿ƒå‘½ä»¤ç»“æ„

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## ç¬¬3éƒ¨åˆ†ï¼šæ¨¡å‹ç¼“å­˜å’Œç®¡ç†

Foundry Local å®ç°äº†æ™ºèƒ½æ¨¡å‹ç¼“å­˜ï¼Œä»¥ä¼˜åŒ–æ€§èƒ½å’Œå­˜å‚¨ï¼š

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## ç¬¬4éƒ¨åˆ†ï¼šæ¨¡å‹éƒ¨ç½²å®æ“

### è¿è¡Œ Microsoft Phi æ¨¡å‹

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### ä½¿ç”¨ Qwen æ¨¡å‹

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b
foundry model run qwen2.5-14b
```

### è¿è¡Œ DeepSeek æ¨¡å‹

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-7b
```

### è¿è¡Œ GPT-OSS-20B

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## ç¬¬5éƒ¨åˆ†ï¼šåˆ›å»ºæ‚¨çš„ç¬¬ä¸€ä¸ªåº”ç”¨ç¨‹åº

### ç°ä»£èŠå¤©åº”ç”¨ç¨‹åºï¼ˆOpenAI SDK + Foundry Localï¼‰

ä½¿ç”¨ OpenAI SDK å’Œ Foundry Local é›†æˆåˆ›å»ºä¸€ä¸ªç”Ÿäº§çº§èŠå¤©åº”ç”¨ç¨‹åºï¼Œéµå¾ªæˆ‘ä»¬çš„ç¤ºä¾‹ 01 çš„æ¨¡å¼ã€‚

```python
# chat_quickstart.py (Sample 01 pattern)
import os
import sys
from openai import OpenAI

try:
    from foundry_local import FoundryLocalManager
    FOUNDRY_SDK_AVAILABLE = True
except ImportError:
    FOUNDRY_SDK_AVAILABLE = False
    print("âš ï¸ Install foundry-local-sdk: pip install foundry-local-sdk")

def create_client():
    """Create OpenAI client with Foundry Local or Azure OpenAI."""
    # Check for Azure OpenAI configuration
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    
    if azure_endpoint and azure_api_key:
        # Azure OpenAI path
        model = os.environ.get("MODEL", "your-deployment-name")
        client = OpenAI(
            base_url=f"{azure_endpoint}/openai",
            api_key=azure_api_key,
            default_query={"api-version": "2024-08-01-preview"},
        )
        print(f"ğŸŒ Using Azure OpenAI with model: {model}")
        return client, model
    
    # Foundry Local path with SDK management
    alias = os.environ.get("MODEL", "phi-4-mini")
    if FOUNDRY_SDK_AVAILABLE:
        try:
            # Use FoundryLocalManager for proper service management
            manager = FoundryLocalManager(alias)
            model_info = manager.get_model_info(alias)
            
            client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            model = model_info.id
            print(f"ğŸ  Using Foundry Local SDK with model: {model}")
            return client, model
        except Exception as e:
            print(f"âš ï¸ Foundry SDK failed ({e}), using manual configuration")
    
    # Fallback to manual configuration
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    api_key = os.environ.get("API_KEY", "")
    model = alias
    
    client = OpenAI(
        base_url=f"{base_url}/v1",
        api_key=api_key
    )
    print(f"ğŸ”§ Manual configuration with model: {model}")
    return client, model

def main():
    """Main chat function."""
    client, model = create_client()
    
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    conversation_history = []
    
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        
        try:
            # Add user message to history
            conversation_history.append({"role": "user", "content": user_input})
            
            # Create chat completion
            response = client.chat.completions.create(
                model=model,
                messages=conversation_history,
                max_tokens=500,
                temperature=0.7
            )
            
            assistant_message = response.choices[0].message.content
            conversation_history.append({"role": "assistant", "content": assistant_message})
            
            print(f"Assistant: {assistant_message}\n")
            
        except Exception as e:
            print(f"Error: {e}\n")

if __name__ == "__main__":
    main()
```

### è¿è¡ŒèŠå¤©åº”ç”¨ç¨‹åº

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Option 1: Using FoundryLocalManager (recommended)
python chat_quickstart.py "Explain what Foundry Local is"

# Option 2: Manual configuration with environment variables
set BASE_URL=http://localhost:8000
set MODEL=phi-4-mini
set API_KEY=
python chat_quickstart.py "Write a welcome message"

# Option 3: Azure OpenAI configuration
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name
python chat_quickstart.py "Hello from Azure OpenAI"
```

## ç¬¬6éƒ¨åˆ†ï¼šæ•…éšœæ’é™¤å’Œæœ€ä½³å®è·µ

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

```powershell
# Issue: "Could not use Foundry SDK" warning
pip install foundry-local-sdk
# Or set environment variables for manual configuration

# Issue: Connection refused
foundry service status
foundry service ps  # Check loaded models

# Issue: Model not found
foundry model list
foundry model run phi-4-mini

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal

# Test API endpoint
curl http://localhost:8000/v1/models
```

### ç›‘æ§ç³»ç»Ÿèµ„æºï¼ˆWindowsï¼‰

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### ç¯å¢ƒå˜é‡

| å˜é‡ | æè¿° | é»˜è®¤å€¼ | æ˜¯å¦å¿…éœ€ |
|------|------|--------|----------|
| `MODEL` | æ¨¡å‹åˆ«åæˆ–åç§° | `phi-4-mini` | å¦ |
| `BASE_URL` | Foundry Local åŸºç¡€ URL | `http://localhost:8000` | å¦ |
| `API_KEY` | API å¯†é’¥ï¼ˆé€šå¸¸æœ¬åœ°ä¸éœ€è¦ï¼‰ | `""` | å¦ |
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI ç«¯ç‚¹ | - | ç”¨äº Azure |
| `AZURE_OPENAI_API_KEY` | Azure OpenAI API å¯†é’¥ | - | ç”¨äº Azure |
| `AZURE_OPENAI_API_VERSION` | Azure API ç‰ˆæœ¬ | `2024-08-01-preview` | å¦ |

### æœ€ä½³å®è·µ

- **ä½¿ç”¨ OpenAI SDK**ï¼šä¼˜å…ˆä½¿ç”¨ OpenAI SDK è€ŒéåŸå§‹ HTTP è¯·æ±‚ï¼Œä»¥æé«˜å¯ç»´æŠ¤æ€§
- **FoundryLocalManager**ï¼šåœ¨å¯ç”¨æ—¶ä½¿ç”¨å®˜æ–¹ SDK è¿›è¡ŒæœåŠ¡ç®¡ç†
- **é”™è¯¯å¤„ç†**ï¼šä¸ºç”Ÿäº§åº”ç”¨ç¨‹åºå®æ–½é€‚å½“çš„å›é€€ç­–ç•¥
- **å®šæœŸå‡çº§**ï¼šä¿æŒ Foundry Local æ›´æ–°ä»¥è·å–æ–°æ¨¡å‹å’Œä¿®å¤
- **ä»å°å¼€å§‹**ï¼šä»è¾ƒå°çš„æ¨¡å‹ï¼ˆPhi miniã€Qwen 7Bï¼‰å¼€å§‹ï¼Œç„¶åé€æ­¥æ‰©å±•
- **ç›‘æ§èµ„æº**ï¼šåœ¨è°ƒæ•´æç¤ºå’Œè®¾ç½®æ—¶è·Ÿè¸ª CPU/GPU/å†…å­˜ä½¿ç”¨æƒ…å†µ

## ç¬¬7éƒ¨åˆ†ï¼šå®æ“ç»ƒä¹ 

### ç»ƒä¹ 1ï¼šå¿«é€Ÿå¤šæ¨¡å‹æµ‹è¯•

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### ç»ƒä¹ 2ï¼šOpenAI SDK é›†æˆæµ‹è¯•

```python
# sdk_integration_test.py (matching Sample 01 pattern)
import os
from openai import OpenAI
from foundry_local import FoundryLocalManager

def test_model_integration(model_alias):
    """Test OpenAI SDK integration with different models."""
    try:
        # Use FoundryLocalManager for proper setup
        manager = FoundryLocalManager(model_alias)
        model_info = manager.get_model_info(model_alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Test basic completion
        response = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Say hello and state your model name."}],
            max_tokens=50
        )
        
        print(f"âœ… {model_alias}: {response.choices[0].message.content}")
        return True
    except Exception as e:
        print(f"âŒ {model_alias}: {e}")
        return False

# Test multiple models
models_to_test = ["phi-4-mini", "qwen2.5-7b"]
for model in models_to_test:
    test_model_integration(model)
```

### ç»ƒä¹ 3ï¼šå…¨é¢æœåŠ¡å¥åº·æ£€æŸ¥

```python
# health_check.py
from openai import OpenAI
from foundry_local import FoundryLocalManager

def comprehensive_health_check():
    """Perform comprehensive health check of Foundry Local service."""
    try:
        # Initialize with a common model
        manager = FoundryLocalManager("phi-4-mini")
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # 1. Check service connectivity
        models_response = client.models.list()
        available_models = [model.id for model in models_response.data]
        print(f"âœ… Service healthy - {len(available_models)} models available")
        
        # 2. Test each available model
        for model_id in available_models:
            try:
                response = client.chat.completions.create(
                    model=model_id,
                    messages=[{"role": "user", "content": "Test"}],
                    max_tokens=10
                )
                print(f"âœ… {model_id}: Working")
            except Exception as e:
                print(f"âŒ {model_id}: {e}")
        
        return True
    except Exception as e:
        print(f"âŒ Service check failed: {e}")
        return False

comprehensive_health_check()
```

## å‚è€ƒèµ„æ–™

- **å¼€å§‹ä½¿ç”¨ Foundry Local**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- **CLI å‚è€ƒå’Œå‘½ä»¤æ¦‚è¿°**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- **OpenAI SDK é›†æˆ**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- **ç¼–è¯‘ Hugging Face æ¨¡å‹**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- **Microsoft Foundry Local GitHub**: https://github.com/microsoft/Foundry-Local
- **OpenAI Python SDK**: https://github.com/openai/openai-python
- **ç¤ºä¾‹ 01ï¼šé€šè¿‡ OpenAI SDK å¿«é€ŸèŠå¤©**: samples/01/README.md
- **ç¤ºä¾‹ 02ï¼šé«˜çº§ SDK é›†æˆ**: samples/02/README.md

---

**å…è´£å£°æ˜**ï¼š  
æœ¬æ–‡æ¡£ä½¿ç”¨AIç¿»è¯‘æœåŠ¡ [Co-op Translator](https://github.com/Azure/co-op-translator) è¿›è¡Œç¿»è¯‘ã€‚å°½ç®¡æˆ‘ä»¬åŠªåŠ›ç¡®ä¿ç¿»è¯‘çš„å‡†ç¡®æ€§ï¼Œä½†è¯·æ³¨æ„ï¼Œè‡ªåŠ¨ç¿»è¯‘å¯èƒ½åŒ…å«é”™è¯¯æˆ–ä¸å‡†ç¡®ä¹‹å¤„ã€‚åŸå§‹è¯­è¨€çš„æ–‡æ¡£åº”è¢«è§†ä¸ºæƒå¨æ¥æºã€‚å¯¹äºå…³é”®ä¿¡æ¯ï¼Œå»ºè®®ä½¿ç”¨ä¸“ä¸šäººå·¥ç¿»è¯‘ã€‚æˆ‘ä»¬å¯¹å› ä½¿ç”¨æ­¤ç¿»è¯‘è€Œäº§ç”Ÿçš„ä»»ä½•è¯¯è§£æˆ–è¯¯è¯»ä¸æ‰¿æ‹…è´£ä»»ã€‚