<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a887b7e85782dadd3fd1216cd63b6c23",
  "translation_date": "2025-10-09T11:03:46+00:00",
  "source_file": "Workshop/QUICK_REFERENCE.md",
  "language_code": "it"
}
-->
# Workshop Samples - Scheda di Riferimento Rapido

**Ultimo aggiornamento**: 8 ottobre 2025

---

## üöÄ Avvio Rapido

```bash
# 1. Ensure Foundry Local is running
foundry service status
foundry model run phi-4-mini

# 2. Install dependencies
pip install -r Workshop/requirements.txt

# 3. Run a sample
cd Workshop/samples/session01
python chat_bootstrap.py "What is edge AI?"
```

---

## üìÇ Panoramica dei Campioni

| Sessione | Campione | Scopo | Tempo |
|----------|----------|-------|-------|
| 01 | `chat_bootstrap.py` | Chat di base + streaming | ~30s |
| 02 | `rag_pipeline.py` | RAG con embedding | ~45s |
| 02 | `rag_eval_ragas.py` | Valutazione RAG | ~60s |
| 03 | `benchmark_oss_models.py` | Benchmarking dei modelli | ~2m |
| 04 | `model_compare.py` | SLM vs LLM | ~45s |
| 05 | `agents_orchestrator.py` | Sistema multi-agente | ~60s |
| 06 | `models_router.py` | Routing delle intenzioni | ~45s |
| 06 | `models_pipeline.py` | Pipeline multi-step | ~60s |

---

## üõ†Ô∏è Variabili d'Ambiente

### Essenziali
```bash
# Choose model
set FOUNDRY_LOCAL_ALIAS=phi-4-mini

# Override endpoint (optional)
set FOUNDRY_LOCAL_ENDPOINT=http://localhost:8000

# Show token usage
set SHOW_USAGE=1
```

### Specifiche per Sessione
```bash
# Session 02: RAG
set RAG_QUESTION="What is local inference?"
set EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Session 03: Benchmarking
set BENCH_MODELS=phi-4-mini,qwen2.5-0.5b
set BENCH_ROUNDS=3
set BENCH_STREAM=1

# Session 04: Comparison
set SLM_ALIAS=phi-4-mini
set LLM_ALIAS=qwen2.5-7b

# Session 05: Agents
set AGENT_MODEL_PRIMARY=phi-4-mini
set AGENT_QUESTION="Why use edge AI?"

# Session 06: Pipeline
set PIPELINE_TASK="Your task here"
```

---

## ‚úÖ Validazione e Test

```bash
# Validate syntax and imports
python scripts/validate_samples.py

# Validate specific session
python scripts/validate_samples.py --session 01

# Run smoke tests
python scripts/test_samples.py --quick

# Verbose testing
python scripts/test_samples.py --verbose
```

---

## üêõ Risoluzione dei Problemi

### Errore di Connessione
```bash
# Check Foundry Local
foundry service status

# Start if needed
foundry service start
foundry model run phi-4-mini
```

### Errore di Importazione
```bash
# Install missing dependencies
pip install sentence-transformers ragas datasets

# Or install all
pip install -r Workshop/requirements.txt
```

### Modello Non Trovato
```bash
# List available models
foundry model ls

# Download model
foundry model download phi-4-mini
```

### Prestazioni Lente
```bash
# Use smaller model
set FOUNDRY_LOCAL_ALIAS=qwen2.5-0.5b

# Reduce benchmark rounds
set BENCH_ROUNDS=1
```

---

## üìñ Pattern Comuni

### Chat di Base
```python
from workshop_utils import chat_once

text, usage = chat_once(
    'phi-4-mini',
    messages=[{"role": "user", "content": "Hello"}],
    max_tokens=100,
    temperature=0.7
)
```

### Ottenere il Client
```python
from workshop_utils import get_client

manager, client, model_id = get_client(
    alias='phi-4-mini',
    endpoint=None  # Auto-detect
)
```

### Gestione degli Errori
```python
try:
    manager, client, model_id = get_client(alias)
except Exception as e:
    print(f"[ERROR] Failed: {e}")
    print("[INFO] Check: foundry service status")
    sys.exit(1)
```

### Streaming
```python
stream = client.chat.completions.create(
    model=model_id,
    messages=messages,
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

---

## üìä Selezione del Modello

| Modello | Dimensione | Ideale per | Velocit√† |
|---------|------------|------------|----------|
| `qwen2.5-0.5b` | 0.5B | Classificazione veloce | ‚ö°‚ö°‚ö° |
| `qwen2.5-coder-0.5b` | 0.5B | Generazione rapida di codice | ‚ö°‚ö°‚ö° |
| `gemma-2-2b` | 2B | Scrittura creativa | ‚ö°‚ö° |
| `phi-3.5-mini` | 3.5B | Codice, refactoring | ‚ö°‚ö° |
| `phi-4-mini` | 4B | Generale, riassunti | ‚ö°‚ö° |
| `qwen2.5-7b` | 7B | Ragionamento complesso | ‚ö° |

---

## üîó Risorse

- **Documentazione SDK**: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python
- **Riferimento Rapido**: `Workshop/FOUNDRY_SDK_QUICKREF.md`
- **Riepilogo Aggiornamenti**: `Workshop/SAMPLES_UPDATE_SUMMARY.md`
- **Note di Migrazione**: `Workshop/SDK_MIGRATION_NOTES.md`

---

## üí° Consigli

1. **Cache dei client**: `workshop_utils` gestisce la cache per te
2. **Usa modelli pi√π piccoli**: Inizia con `qwen2.5-0.5b` per i test
3. **Abilita le statistiche di utilizzo**: Imposta `SHOW_USAGE=1` per monitorare i token
4. **Elaborazione in batch**: Processa pi√π prompt in sequenza
5. **Riduci max_tokens**: Diminuisce la latenza per risposte rapide

---

## üéØ Flussi di Lavoro Campione

### Test Completo
```bash
python scripts/validate_samples.py
python scripts/test_samples.py --quick
```

### Benchmark dei Modelli
```bash
cd samples/session03
set BENCH_MODELS=phi-4-mini,qwen2.5-0.5b,gemma-2-2b
set BENCH_ROUNDS=3
python benchmark_oss_models.py
```

### Pipeline RAG
```bash
cd samples/session02
set RAG_QUESTION="What is RAG?"
python rag_pipeline.py
```

### Sistema Multi-Agente
```bash
cd samples/session05
set AGENT_QUESTION="Why edge AI for healthcare?"
python agents_orchestrator.py
```

---

**Aiuto Rapido**: Esegui qualsiasi campione con `--help` o controlla il docstring:
```bash
python chat_bootstrap.py --help
# or
python -c "import chat_bootstrap; help(chat_bootstrap)"
```

---

**Tutti i campioni aggiornati a ottobre 2025 con le migliori pratiche del Foundry Local SDK** ‚ú®

---

**Disclaimer**:  
Questo documento √® stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un esperto umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.