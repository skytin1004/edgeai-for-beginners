<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-01T21:17:50+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "fi"
}
-->
# Osa 2: Qwen-malliperheen perusteet

Qwen-malliperhe edustaa Alibaba Cloudin kokonaisvaltaista l√§hestymistapaa suuriin kielimalleihin ja multimodaaliseen teko√§lyyn, osoittaen, ett√§ avoimen l√§hdekoodin mallit voivat saavuttaa merkitt√§vi√§ tuloksia ja olla k√§ytett√§viss√§ monenlaisissa k√§ytt√∂tilanteissa. On t√§rke√§√§ ymm√§rt√§√§, kuinka Qwen-malliperhe mahdollistaa tehokkaat teko√§lyominaisuudet joustavilla k√§ytt√∂mahdollisuuksilla samalla s√§ilytt√§en kilpailukykyisen suorituskyvyn monipuolisissa teht√§viss√§.

## Resurssit kehitt√§jille

### Hugging Face -mallivarasto
Valikoituja Qwen-malliperheen malleja on saatavilla [Hugging Face](https://huggingface.co/models?search=qwen)-alustalla, mik√§ tarjoaa p√§√§syn joihinkin n√§iden mallien versioihin. Voit tutkia saatavilla olevia versioita, hienos√§√§t√§√§ niit√§ omiin k√§ytt√∂tarkoituksiisi ja ottaa ne k√§ytt√∂√∂n eri kehitysalustoilla.

### Paikalliset kehitysty√∂kalut
Paikallista kehityst√§ ja testausta varten voit k√§ytt√§√§ [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) -ty√∂kalua, joka mahdollistaa Qwen-mallien suorittamisen kehityskoneellasi optimoidulla suorituskyvyll√§.

### Dokumentaatioresurssit
- [Qwen-mallien dokumentaatio](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Qwen-mallien optimointi reunak√§ytt√∂√∂n](https://github.com/microsoft/olive)

## Johdanto

T√§ss√§ opetusohjelmassa tutustumme Alibaban Qwen-malliperheeseen ja sen perusk√§sitteisiin. K√§ymme l√§pi Qwen-malliperheen kehityksen, innovatiiviset koulutusmenetelm√§t, jotka tekev√§t Qwen-malleista tehokkaita, perheen keskeiset versiot ja k√§yt√§nn√∂n sovellukset eri tilanteissa.

## Oppimistavoitteet

Opetusohjelman lopussa osaat:

- Ymm√§rt√§√§ Alibaban Qwen-malliperheen suunnittelufilosofian ja kehityksen
- Tunnistaa keskeiset innovaatiot, jotka mahdollistavat Qwen-mallien korkean suorituskyvyn eri parametrikokoluokissa
- Tunnistaa eri Qwen-malliversioiden hy√∂dyt ja rajoitukset
- Soveltaa Qwen-mallien tiet√§myst√§ valitaksesi sopivat versiot todellisiin k√§ytt√∂tilanteisiin

## Modernin teko√§lymallien maiseman ymm√§rt√§minen

Teko√§lymallien kehitys on edennyt merkitt√§v√§sti, ja eri organisaatiot ovat omaksuneet erilaisia l√§hestymistapoja kielimallien kehitt√§miseen. Jotkut keskittyv√§t suljettuihin, yksityisiin malleihin, kun taas toiset korostavat avoimen l√§hdekoodin saavutettavuutta ja l√§pin√§kyvyytt√§. Perinteinen l√§hestymistapa sis√§lt√§√§ joko massiivisia yksityisi√§ malleja, jotka ovat k√§ytett√§viss√§ vain API:en kautta, tai avoimen l√§hdekoodin malleja, jotka saattavat j√§√§d√§ j√§lkeen ominaisuuksiltaan.

T√§m√§ paradigma luo haasteita organisaatioille, jotka etsiv√§t tehokkaita teko√§lyominaisuuksia samalla s√§ilytt√§en kontrollin datastaan, kustannuksistaan ja k√§ytt√∂mahdollisuuksistaan. Perinteinen l√§hestymistapa vaatii usein valintaa huippuluokan suorituskyvyn ja k√§yt√§nn√∂n k√§ytt√∂mahdollisuuksien v√§lill√§.

## Haaste: saavutettavuus ja huippulaatuinen teko√§ly

Tarve korkealaatuiselle ja saavutettavalle teko√§lylle on kasvanut merkitt√§v√§sti eri k√§ytt√∂tilanteissa. Esimerkkej√§ ovat sovellukset, jotka vaativat joustavia k√§ytt√∂mahdollisuuksia eri organisaatioiden tarpeisiin, kustannustehokkaita toteutuksia, joissa API-kustannukset voivat nousta merkitt√§viksi, monikielisi√§ ominaisuuksia globaaleihin sovelluksiin tai erikoistunutta osaamista esimerkiksi koodauksessa ja matematiikassa.

### Keskeiset k√§ytt√∂vaatimukset

Modernit teko√§lysovellukset kohtaavat useita perusvaatimuksia, jotka rajoittavat k√§yt√§nn√∂n soveltuvuutta:

- **Saavutettavuus**: Avoimen l√§hdekoodin saatavuus l√§pin√§kyvyyden ja r√§√§t√§l√∂innin mahdollistamiseksi
- **Kustannustehokkuus**: Kohtuulliset laskentavaatimukset eri budjeteille
- **Joustavuus**: Useita mallikokoja eri k√§ytt√∂tilanteisiin
- **Globaali ulottuvuus**: Vahvat monikieliset ja kulttuurienv√§liset ominaisuudet
- **Erikoistuminen**: Alakohtaiset versiot erityisiin k√§ytt√∂tarkoituksiin

## Qwen-mallien filosofia

Qwen-malliperhe edustaa kokonaisvaltaista l√§hestymistapaa teko√§lymallien kehitt√§miseen, painottaen avoimen l√§hdekoodin saavutettavuutta, monikielisi√§ ominaisuuksia ja k√§yt√§nn√∂n soveltuvuutta samalla s√§ilytt√§en kilpailukykyiset suorituskykyominaisuudet. Qwen-mallit saavuttavat t√§m√§n tarjoamalla monipuolisia mallikokoja, korkealaatuisia koulutusmenetelmi√§ ja erikoistuneita versioita eri aloille.

Qwen-perhe kattaa erilaisia l√§hestymistapoja, jotka tarjoavat vaihtoehtoja suorituskyvyn ja tehokkuuden spektrill√§, mahdollistaen k√§yt√∂n mobiililaitteista yrityspalvelimiin samalla tarjoten merkitt√§vi√§ teko√§lyominaisuuksia. Tavoitteena on demokratisoida p√§√§sy korkealaatuiseen teko√§lyyn samalla tarjoten joustavuutta k√§ytt√∂valinnoissa.

### Qwen-mallien keskeiset suunnitteluperiaatteet

Qwen-mallit perustuvat useisiin perusperiaatteisiin, jotka erottavat ne muista kielimalliperheist√§:

- **Avoimen l√§hdekoodin ensisijaisuus**: T√§ydellinen l√§pin√§kyvyys ja saavutettavuus tutkimus- ja kaupalliseen k√§ytt√∂√∂n
- **Kattava koulutus**: Koulutus massiivisilla, monipuolisilla tietoaineistoilla, jotka kattavat useita kieli√§ ja aloja
- **Skaalautuva arkkitehtuuri**: Useita mallikokoja eri laskentavaatimusten t√§ytt√§miseksi
- **Erikoistunut huippuosaaminen**: Alakohtaiset versiot optimoitu erityisiin teht√§viin

## Keskeiset teknologiat, jotka mahdollistavat Qwen-perheen

### Massiivinen koulutus

Yksi Qwen-perheen m√§√§ritt√§vist√§ piirteist√§ on massiivinen koulutusaineistojen ja laskentaresurssien k√§ytt√∂ mallien kehitt√§misess√§. Qwen-mallit hy√∂dynt√§v√§t huolellisesti valikoituja, monikielisi√§ tietoaineistoja, jotka kattavat biljoonia tokeneita ja tarjoavat kattavaa maailmantietoa ja p√§√§ttelykyky√§.

T√§m√§ l√§hestymistapa yhdist√§√§ korkealaatuista verkkosis√§lt√∂√§, akateemista kirjallisuutta, koodivarastoja ja monikielisi√§ resursseja. Koulutusmenetelm√§ painottaa sek√§ tiedon laajuutta ett√§ ymm√§rryksen syvyytt√§ eri aloilla ja kieliss√§.

### Kehittynyt p√§√§ttely ja ajattelu

Uusimmat Qwen-mallit sis√§lt√§v√§t kehittyneit√§ p√§√§ttelyominaisuuksia, jotka mahdollistavat monivaiheisen ongelmanratkaisun:

**Ajattelutila (Qwen3)**: Mallit voivat suorittaa yksityiskohtaista vaiheittaista p√§√§ttely√§ ennen lopullisten vastausten antamista, muistuttaen ihmisen ongelmanratkaisumenetelmi√§.

**Kaksoistilatoiminta**: Kyky vaihtaa nopean vastaustilan ja syvemm√§n ajattelutilan v√§lill√§ monimutkaisten ongelmien ratkaisemiseksi.

**Ajatusketjun integrointi**: Luonnollinen p√§√§ttelyvaiheiden sis√§llytt√§minen, joka parantaa l√§pin√§kyvyytt√§ ja tarkkuutta monimutkaisissa teht√§viss√§.

### Arkkitehtuuriset innovaatiot

Qwen-perhe sis√§lt√§√§ useita arkkitehtuurisia optimointeja, jotka on suunniteltu sek√§ suorituskyky√§ ett√§ tehokkuutta varten:

**Skaalautuva suunnittelu**: Johdonmukainen arkkitehtuuri mallikokojen v√§lill√§, mik√§ mahdollistaa helpon skaalauksen ja vertailun.

**Multimodaalinen integrointi**: Tekstin, kuvan ja √§√§nen k√§sittelyn saumaton yhdist√§minen yhten√§isiin arkkitehtuureihin.

**K√§ytt√∂optimointi**: Useita kvantisointivaihtoehtoja ja k√§ytt√∂formaatteja eri laitteistokokoonpanoille.

## Mallikoot ja k√§ytt√∂mahdollisuudet

Modernit k√§ytt√∂ymp√§rist√∂t hy√∂tyv√§t Qwen-mallien joustavuudesta eri laskentavaatimusten v√§lill√§:

### Pienet mallit (0.5B-3B)

Qwen tarjoaa tehokkaita pieni√§ malleja, jotka soveltuvat reunak√§ytt√∂√∂n, mobiilisovelluksiin ja resurssirajoitteisiin ymp√§rist√∂ihin samalla s√§ilytt√§en vaikuttavat ominaisuudet.

### Keskikokoiset mallit (7B-32B)

Keskikokoiset mallit tarjoavat parannettuja ominaisuuksia ammatillisiin sovelluksiin, tarjoten erinomaisen tasapainon suorituskyvyn ja laskentavaatimusten v√§lill√§.

### Suuret mallit (72B+)

T√§ysimittaiset mallit tarjoavat huippuluokan suorituskyvyn vaativiin sovelluksiin, tutkimukseen ja yritysk√§ytt√∂√∂n, joissa tarvitaan maksimaalista kapasiteettia.

## Qwen-malliperheen edut

### Avoimen l√§hdekoodin saavutettavuus

Qwen-mallit tarjoavat t√§ydellisen l√§pin√§kyvyyden ja r√§√§t√§l√∂intimahdollisuudet, mik√§ mahdollistaa organisaatioiden ymm√§rt√§√§, muokata ja soveltaa malleja omiin tarpeisiinsa ilman toimittajalukkoa.

### K√§ytt√∂joustavuus

Mallikokojen valikoima mahdollistaa k√§yt√∂n monenlaisissa laitteistokokoonpanoissa, mobiililaitteista huippuluokan palvelimiin, tarjoten organisaatioille joustavuutta teko√§lyinfrastruktuurivalinnoissa.

### Monikielinen huippuosaaminen

Qwen-mallit ovat erinomaisia monikielisess√§ ymm√§rryksess√§ ja tuottamisessa, tukien kymmeni√§ kieli√§ ja erityisesti vahvoja englannin ja kiinan kielen ominaisuuksia, mik√§ tekee niist√§ sopivia globaaleihin sovelluksiin.

### Kilpailukykyinen suorituskyky

Qwen-mallit saavuttavat johdonmukaisesti kilpailukykyisi√§ tuloksia vertailuissa samalla tarjoten avoimen l√§hdekoodin saavutettavuuden, osoittaen, ett√§ avoimet mallit voivat vastata yksityisi√§ vaihtoehtoja.

### Erikoistuneet ominaisuudet

Alakohtaiset versiot, kuten Qwen-Coder ja Qwen-Math, tarjoavat erikoistunutta osaamista samalla s√§ilytt√§en yleisen kieliymm√§rryksen.

## K√§yt√§nn√∂n esimerkit ja k√§ytt√∂tapaukset

Ennen teknisiin yksityiskohtiin siirtymist√§, tarkastellaan joitakin konkreettisia esimerkkej√§ siit√§, mit√§ Qwen-mallit voivat saavuttaa:

### Matemaattinen p√§√§ttelyesimerkki

Qwen-Math on erinomainen vaiheittaisessa matemaattisessa ongelmanratkaisussa. Esimerkiksi, kun mallilta pyydet√§√§n ratkaisemaan monimutkainen laskennallinen ongelma:

```
User: Find the derivative of f(x) = x¬≥ + 2x¬≤ - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x¬≥ + 2x¬≤ - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(x‚Åø) = n¬∑x‚Åø‚Åª¬π

Breaking down each term:
- d/dx(x¬≥) = 3x¬≤
- d/dx(2x¬≤) = 2 ¬∑ 2x¬π = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x¬≤ + 4x - 5
```

### Monikielinen tuki

Qwen-mallit osoittavat vahvoja monikielisi√§ ominaisuuksia eri kieliss√§:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**‰∏≠Êñá:**
‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâÊòØÊåáÊú∫Âô®ÔºàÁâπÂà´ÊòØËÆ°ÁÆóÊú∫Á≥ªÁªüÔºâÂØπ‰∫∫Á±ªÊô∫ËÉΩËøáÁ®ãÁöÑÊ®°Êãü„ÄÇËøô‰∫õËøáÁ®ãÂåÖÊã¨Â≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥ÈóÆÈ¢ò„ÄÅÊÑüÁü•ÂíåËØ≠Ë®ÄÁêÜËß£„ÄÇ‰∫∫Â∑•Êô∫ËÉΩ‰ΩøÊú∫Âô®ËÉΩÂ§üÊâßË°åÈÄöÂ∏∏ÈúÄË¶Å‰∫∫Á±ªËÆ§Áü•ËÉΩÂäõÁöÑ‰ªªÂä°„ÄÇ
```

### Multimodaaliset ominaisuudet

Qwen-VL pystyy k√§sittelem√§√§n teksti√§ ja kuvia samanaikaisesti:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Koodin generointi

Qwen-Coder on erinomainen koodin generoinnissa ja selitt√§misess√§ eri ohjelmointikielill√§:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

T√§m√§ toteutus noudattaa parhaita k√§yt√§nt√∂j√§, kuten selkeit√§ muuttujanimi√§, kattavaa dokumentaatiota ja tehokasta logiikkaa.
```

### Edge Deployment Example

Qwen models can be deployed on various edge devices with optimized configurations:

```
# Esimerkki mobiililaitteelle kvantisoinnin avulla
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Lataa kvantisoitu malli mobiilik√§ytt√∂√∂n

```
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Qwen-malliperheen kehitys

### Qwen 1.0 ja 1.5: Perusmallit

Varhaiset Qwen-mallit loivat perustan kattavalle koulutukselle ja avoimen l√§hdekoodin saavutettavuudelle:

- **Qwen-7B (7B parametria)**: Ensimm√§inen julkaisu, joka keskittyi kiinan ja englannin kielen ymm√§rt√§miseen
- **Qwen-14B (14B parametria)**: Parannetut ominaisuudet, kuten p√§√§ttely ja tiet√§mys
- **Qwen-72B (72B parametria)**: Suuri malli, joka tarjoaa huippuluokan suorituskyvyn
- **Qwen1.5-sarja**: Laajennettu useisiin kokoluokkiin (0.5B‚Äì110B) parannetulla pitk√§n kontekstin k√§sittelyll√§

### Qwen2-perhe: Multimodaalinen laajennus

Qwen2-sarja merkitsi merkitt√§v√§√§ edistyst√§ sek√§ kieli- ett√§ multimodaalisissa ominaisuuksissa:

- **Qwen2-0.5B‚Äì72B**: Kattava valikoima kielimalleja eri k√§ytt√∂tilanteisiin
- **Qwen2-57B-A14B (MoE)**: Mixture-of-experts-arkkitehtuuri tehokkaaseen parametrien k√§ytt√∂√∂n
- **Qwen2-VL**: Kehittyneet visio-kieliominaisuudet kuvien ymm√§rt√§miseen
- **Qwen2-Audio**: √Ñ√§nik√§sittely ja ymm√§rrysominaisuudet
- **Qwen2-Math**: Erikoistunut matemaattinen p√§√§ttely ja ongelmanratkaisu

### Qwen2.5-perhe: Parannettu suorituskyky

Qwen2.5-sarja toi merkitt√§vi√§ parannuksia kaikilla osa-alueilla:

- **Laajennettu koulutus**: 18 biljoonaa tokenia koulutusaineistoa parannettujen ominaisuuksien saavuttamiseksi
- **Laajennettu konteksti**: Jopa 128K tokenia kontekstipituus, Turbo-versio tukee 1M tokenia
- **Parannettu erikoistuminen**: Kehittyneet Qwen2.5-Coder- ja Qwen2.5-Math-versiot
- **Parempi monikielinen tuki**: Parannettu suorituskyky yli 27 kielell√§

### Qwen3-perhe: Kehittynyt p√§√§ttely

Uusin sukupolvi vie p√§√§ttely- ja ajattelukyvyt uudelle tasolle:

- **Qwen3-235B-A22B**: Lippulaivamalli, jossa 235B kokonaisparametria
- **Qwen3-30B-A3B**: Tehokas MoE-malli, jossa vahva suorituskyky aktiivista parametria kohden
- **Tihe√§t mallit**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B eri k√§ytt√∂tilanteisiin
- **Ajattelutila**: Hybridip√§√§ttely, joka tukee sek√§ nopeita vastauksia ett√§ syv√§llist√§ ajattelua
- **Monikielinen huippuosaaminen**: Tuki 119 kielelle ja murteelle
- **Parannettu koulutus**: 36 biljoonaa tokenia monipuolista, korkealaatuista koulutusaineistoa

## Qwen-mallien sovellukset

### Yrityssovellukset

Organisaatiot k√§ytt√§v√§t Qwen-malleja asiakirja-analyysiin, asiakaspalvelun automatisointiin, koodin generointiavustukseen ja liiketoimintatiedon sovelluksiin. Avoimen l√§hdekoodin luonne mahdollistaa r√§√§t√§l√∂innin erityisiin liiketoimintatarpeisiin samalla s√§ilytt√§en tietosuojan ja kontrollin.

### Mobiili- ja reunalaskenta

Mobiilisovellukset hy√∂dynt√§v√§t Qwen-malleja reaaliaikaiseen k√§√§nn√∂kseen, √§lykk√§isiin avustajiin, sis√§ll√∂n generointiin ja henkil√∂kohtaisiin suosituksiin. Mallikokojen valikoima mahdollistaa k√§yt√∂n mobiililaitteista reunapalvelimiin.

### Opetusteknologia

Opetusymp√§rist√∂t k√§ytt√§v√§t Qwen-malleja henkil√∂kohtaiseen ohjaukseen, automatisoituun sis√§ll√∂n generointiin, kielten oppimisen avustamiseen ja interaktiivisiin oppimiskokemuksiin. Erikoistuneet mallit, kuten Qwen-Math, tarjoavat alakohtaista osaamista.

### Globaalit sovellukset

Kansainv√§liset sovellukset hy√∂tyv√§t Qwen-mallien vahvoista monik
- Qwen3-235B-A22B saavuttaa kilpailukykyisi√§ tuloksia koodauksen, matematiikan ja yleisten kykyjen vertailuarvioinneissa verrattuna muihin huippumalleihin, kuten DeepSeek-R1, o1, o3-mini, Grok-3 ja Gemini-2.5-Pro.
- Qwen3-30B-A3B p√§ihitt√§√§ QwQ-32B:n, vaikka sill√§ on 10 kertaa v√§hemm√§n aktivoituja parametreja.
- Qwen3-4B voi kilpailla Qwen2.5-72B-Instructin suorituskyvyn kanssa.

**Tehokkuuden saavutukset:**
- Qwen3-MoE-perusmallit saavuttavat samanlaisen suorituskyvyn kuin Qwen2.5 tihe√§t perusmallit, mutta k√§ytt√§v√§t vain 10 % aktivoiduista parametreista.
- Merkitt√§vi√§ kustannuss√§√§st√∂j√§ sek√§ koulutuksessa ett√§ p√§√§ttelyss√§ verrattuna tiheisiin malleihin.

**Monikieliset kyvyt:**
- Qwen3-mallit tukevat 119 kielt√§ ja murretta.
- Vahva suorituskyky monipuolisissa kielellisiss√§ ja kulttuurisissa konteksteissa.

**Koulutuksen laajuus:**
- Qwen3 k√§ytt√§√§ l√§hes kaksinkertaisen m√§√§r√§n, noin 36 biljoonaa tokenia, jotka kattavat 119 kielt√§ ja murretta verrattuna Qwen2.5:n 18 biljoonaan tokeniin.

### Mallien vertailutaulukko

| Mallisarja      | Parametrien m√§√§r√§ | Kontekstin pituus | Keskeiset vahvuudet         | Parhaat k√§ytt√∂tapaukset         |
|------------------|-------------------|-------------------|-----------------------------|---------------------------------|
| **Qwen2.5**      | 0.5B-72B          | 32K-128K          | Tasapainoinen suorituskyky, monikielisyys | Yleiset sovellukset, tuotantok√§ytt√∂ |
| **Qwen2.5-Coder**| 1.5B-32B          | 128K              | Koodin generointi, ohjelmointi | Ohjelmistokehitys, koodausapu |
| **Qwen2.5-Math** | 1.5B-72B          | 4K-128K           | Matemaattinen p√§√§ttely      | Koulutusalustat, STEM-sovellukset |
| **Qwen2.5-VL**   | Vaihtelee         | Vaihtelee         | N√§k√∂- ja kieliymm√§rrys      | Multimodaaliset sovellukset, kuvien analyysi |
| **Qwen3**        | 0.6B-235B         | Vaihtelee         | Kehittynyt p√§√§ttely, ajattelutila | Monimutkainen p√§√§ttely, tutkimussovellukset |
| **Qwen3 MoE**    | 30B-235B yhteens√§ | Vaihtelee         | Tehokas suurimittainen suorituskyky | Yrityssovellukset, korkean suorituskyvyn tarpeet |

## Mallin valintaopas

### Perussovelluksiin
- **Qwen2.5-0.5B/1.5B**: Mobiilisovellukset, reunalaitteet, reaaliaikaiset sovellukset
- **Qwen2.5-3B/7B**: Yleiset chatbotit, sis√§ll√∂ntuotanto, kysymys-vastausj√§rjestelm√§t

### Matemaattisiin ja p√§√§ttelyteht√§viin
- **Qwen2.5-Math**: Matemaattisten ongelmien ratkaisu ja STEM-koulutus
- **Qwen3 ajattelutilalla**: Monimutkainen p√§√§ttely, joka vaatii vaiheittaista analyysi√§

### Ohjelmointiin ja kehitykseen
- **Qwen2.5-Coder**: Koodin generointi, virheenkorjaus, ohjelmointiapu
- **Qwen3**: Kehittyneet ohjelmointiteht√§v√§t p√§√§ttelykyvyill√§

### Multimodaalisille sovelluksille
- **Qwen2.5-VL**: Kuvien ymm√§rt√§minen, visuaalinen kysymys-vastaus
- **Qwen-Audio**: √Ñ√§nik√§sittely ja puheen ymm√§rt√§minen

### Yritysk√§ytt√∂√∂n
- **Qwen2.5-32B/72B**: Korkean suorituskyvyn kieliymm√§rrys
- **Qwen3-235B-A22B**: Maksimaalinen kyvykkyys vaativiin sovelluksiin

## K√§ytt√∂alustat ja saavutettavuus
### Pilvialustat
- **Hugging Face Hub**: Kattava mallivarasto yhteis√∂n tuella
- **ModelScope**: Alibaban mallialusta optimointity√∂kaluilla
- **Erilaiset pilvipalveluntarjoajat**: Tuki standardien ML-alustojen kautta

### Paikalliset kehityskehykset
- **Transformers**: Standardi Hugging Face -integraatio helppoon k√§ytt√∂√∂nottoon
- **vLLM**: Korkean suorituskyvyn palvelu tuotantoymp√§rist√∂ihin
- **Ollama**: Yksinkertaistettu paikallinen k√§ytt√∂√∂notto ja hallinta
- **ONNX Runtime**: Alustojen v√§linen optimointi eri laitteistoille
- **llama.cpp**: Tehokas C++-toteutus monipuolisille alustoille

### Oppimisresurssit
- **Qwen-dokumentaatio**: Virallinen dokumentaatio ja mallikortit
- **Hugging Face Model Hub**: Interaktiiviset demot ja yhteis√∂n esimerkit
- **Tutkimuspaperit**: Teknisi√§ artikkeleita arxivissa syv√§lliseen ymm√§rrykseen
- **Yhteis√∂foorumit**: Aktiivinen yhteis√∂n tuki ja keskustelut

### Aloittaminen Qwen-mallien kanssa

#### Kehitysalustat
1. **Hugging Face Transformers**: Aloita standardilla Python-integraatiolla
2. **ModelScope**: Tutustu Alibaban optimoituihin k√§ytt√∂√∂notto-ty√∂kaluihin
3. **Paikallinen k√§ytt√∂√∂notto**: K√§yt√§ Ollamaa tai suoria Transformers-kirjastoja paikalliseen testaukseen

#### Oppimispolku
1. **Ymm√§rr√§ ydinkonseptit**: Tutki Qwen-malliperheen arkkitehtuuria ja kykyj√§
2. **Kokeile eri versioita**: Testaa eri mallikokoja ymm√§rt√§√§ksesi suorituskyvyn kompromisseja
3. **Harjoittele toteutusta**: Ota malleja k√§ytt√∂√∂n kehitysymp√§rist√∂iss√§
4. **Optimoi k√§ytt√∂√∂notto**: Hienos√§√§d√§ tuotantok√§ytt√∂√∂n

#### Parhaat k√§yt√§nn√∂t
- **Aloita pienest√§**: K√§yt√§ pienempi√§ malleja (1.5B-7B) alkuvaiheen kehitykseen
- **K√§yt√§ chat-pohjia**: Sovella oikeaa muotoilua optimaalisten tulosten saavuttamiseksi
- **Seuraa resursseja**: Tarkkaile muistin k√§ytt√∂√§ ja p√§√§ttelynopeutta
- **Harkitse erikoistumista**: Valitse alakohtaiset versiot tarpeen mukaan

## Kehittyneet k√§ytt√∂tavat

### Hienos√§√§t√∂esimerkit

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### Erikoistunut kehotetekniikka

**Monimutkaisiin p√§√§ttelyteht√§viin:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**Koodin generointiin kontekstin kanssa:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### Monikieliset sovellukset

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (‰∏≠Êñá)",
        "es": "Spanish (Espa√±ol)",
        "fr": "French (Fran√ßais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (Êó•Êú¨Ë™û)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### üîß Tuotantok√§ytt√∂mallit

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## Suorituskyvyn optimointistrategiat

### Muistin optimointi

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### P√§√§ttelyn optimointi

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## Parhaat k√§yt√§nn√∂t ja ohjeet

### Tietoturva ja yksityisyys

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### Seuranta ja arviointi

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## Yhteenveto

Qwen-malliperhe edustaa kokonaisvaltaista l√§hestymistapaa teko√§lyteknologian demokratisointiin samalla s√§ilytt√§en kilpailukykyisen suorituskyvyn monipuolisissa sovelluksissa. Sitoutumalla avoimen l√§hdekoodin saavutettavuuteen, monikielisiin kykyihin ja joustaviin k√§ytt√∂√∂notto-vaihtoehtoihin Qwen mahdollistaa organisaatioiden ja kehitt√§jien hy√∂dynt√§√§ tehokkaita teko√§lykykyj√§ riippumatta heid√§n resursseistaan tai erityisvaatimuksistaan.

### Keskeiset huomiot

**Avoimen l√§hdekoodin huippuosaaminen**: Qwen osoittaa, ett√§ avoimen l√§hdekoodin mallit voivat saavuttaa suorituskyvyn, joka kilpailee suljettujen vaihtoehtojen kanssa, samalla tarjoten l√§pin√§kyvyytt√§, r√§√§t√§l√∂it√§vyytt√§ ja hallintaa.

**Skaalautuva arkkitehtuuri**: Parametrien vaihteluv√§li 0.5B:st√§ 235B:hen mahdollistaa k√§ytt√∂√∂noton koko laskentaymp√§rist√∂jen kirjon, mobiililaitteista yritysklustereihin.

**Erikoistuneet kyvyt**: Alakohtaiset versiot, kuten Qwen-Coder, Qwen-Math ja Qwen-VL, tarjoavat erikoistunutta asiantuntemusta samalla s√§ilytt√§en yleisen kieliymm√§rryksen.

**Globaali saavutettavuus**: Vahva monikielinen tuki yli 119 kielelle tekee Qwenist√§ sopivan kansainv√§lisiin sovelluksiin ja monipuolisille k√§ytt√§j√§ryhmille.

**Jatkuva innovaatio**: Qwen 1.0:sta Qwen3:een tapahtunut kehitys osoittaa jatkuvaa parannusta kyvyiss√§, tehokkuudessa ja k√§ytt√∂√∂notto-vaihtoehdoissa.

### Tulevaisuuden n√§kym√§t

Qwen-malliperheen kehittyess√§ voimme odottaa:

- **Parannettua tehokkuutta**: Jatkuvaa optimointia paremman suorituskyky-parametrien suhteen saavuttamiseksi
- **Laajennettuja multimodaalisia kykyj√§**: Kehittyneemp√§√§ n√§k√∂-, √§√§ni- ja tekstink√§sittely√§
- **Parannettua p√§√§ttely√§**: Kehittyneit√§ ajattelumekanismeja ja monivaiheisia ongelmanratkaisukykyj√§
- **Parempia k√§ytt√∂√∂notto-ty√∂kaluja**: Kehittyneit√§ kehyksi√§ ja optimointity√∂kaluja monipuolisille k√§ytt√∂√∂notto-skenaarioille
- **Yhteis√∂n kasvua**: Laajentuva ekosysteemi ty√∂kaluista, sovelluksista ja yhteis√∂n panoksista

### Seuraavat askeleet

Olitpa rakentamassa chatbotia, kehitt√§m√§ss√§ koulutusty√∂kaluja, luomassa koodausapureita tai ty√∂skentelem√§ss√§ monikielisten sovellusten parissa, Qwen-malliperhe tarjoaa skaalautuvia ratkaisuja vahvalla yhteis√∂n tuella ja kattavalla dokumentaatiolla.

Viimeisimm√§t p√§ivitykset, mallijulkaisut ja yksityiskohtainen tekninen dokumentaatio l√∂ytyv√§t Qwenin virallisista arkistoista Hugging Facessa. Tutustu aktiivisiin yhteis√∂keskusteluihin ja esimerkkeihin.

Teko√§lyn kehityksen tulevaisuus perustuu saavutettaviin, l√§pin√§kyviin ja tehokkaisiin ty√∂kaluihin, jotka mahdollistavat innovaation kaikilla sektoreilla ja mittakaavoilla. Qwen-malliperhe ilment√§√§ t√§t√§ visiota, tarjoten organisaatioille ja kehitt√§jille perustan seuraavan sukupolven teko√§lypohjaisten sovellusten rakentamiseen.

## Lis√§resurssit

- **Virallinen dokumentaatio**: [Qwen-dokumentaatio](https://qwen.readthedocs.io/)
- **Mallivarasto**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **Tekniset artikkelit**: [Qwen-tutkimusjulkaisut](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **Yhteis√∂**: [GitHub-keskustelut ja ongelmat](https://github.com/QwenLM/)
- **ModelScope-alusta**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## Oppimistulokset

T√§m√§n moduulin suorittamisen j√§lkeen osaat:

1. Selitt√§√§ Qwen-malliperheen arkkitehtuurin edut ja sen avoimen l√§hdekoodin l√§hestymistavan
2. Valita sopivan Qwen-version tiettyjen sovellusvaatimusten ja resurssirajoitusten perusteella
3. Toteuttaa Qwen-malleja eri k√§ytt√∂√∂notto-skenaarioissa optimoiduilla kokoonpanoilla
4. Soveltaa kvantisointi- ja optimointitekniikoita Qwen-mallien suorituskyvyn parantamiseksi
5. Arvioida mallikoon, suorituskyvyn ja kykyjen v√§lisi√§ kompromisseja Qwen-malliperheen sis√§ll√§

## Mit√§ seuraavaksi

- [03: Gemma Family Fundamentals](03.GemmaFamily.md)

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§ist√§ asiakirjaa sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.