<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "be25052ac4c842765e7f6f7eb4d7dcc5",
  "translation_date": "2025-10-20T09:50:13+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "fi"
}
-->
# Osa 1: EdgeAI:n perusteet

EdgeAI edustaa merkittÃ¤vÃ¤Ã¤ muutosta tekoÃ¤lyn kÃ¤yttÃ¶Ã¶notossa, tuoden tekoÃ¤lyominaisuudet suoraan reunalaitteisiin sen sijaan, ettÃ¤ ne perustuisi pelkÃ¤stÃ¤Ã¤n pilvipohjaiseen kÃ¤sittelyyn. On tÃ¤rkeÃ¤Ã¤ ymmÃ¤rtÃ¤Ã¤, miten EdgeAI mahdollistaa paikallisen tekoÃ¤lykÃ¤sittelyn resurssirajoitteisilla laitteilla samalla kun se sÃ¤ilyttÃ¤Ã¤ kohtuullisen suorituskyvyn ja kÃ¤sittelee haasteita, kuten yksityisyyttÃ¤, viivettÃ¤ ja offline-ominaisuuksia.

## Johdanto

TÃ¤ssÃ¤ oppitunnissa tutustumme EdgeAI:hin ja sen peruskÃ¤sitteisiin. KÃ¤ymme lÃ¤pi perinteisen tekoÃ¤lykÃ¤sittelyn paradigman, reunalaskennan haasteet, keskeiset teknologiat, jotka mahdollistavat EdgeAI:n, sekÃ¤ kÃ¤ytÃ¤nnÃ¶n sovelluksia eri toimialoilla.

## Oppimistavoitteet

Oppitunnin pÃ¤Ã¤tteeksi osaat:

- YmmÃ¤rtÃ¤Ã¤ perinteisen pilvipohjaisen tekoÃ¤lyn ja EdgeAI:n lÃ¤hestymistapojen erot.
- Tunnistaa keskeiset teknologiat, jotka mahdollistavat tekoÃ¤lykÃ¤sittelyn reunalaitteilla.
- Tunnistaa EdgeAI:n toteutusten hyÃ¶dyt ja rajoitukset.
- Soveltaa EdgeAI-tietÃ¤mystÃ¤ todellisiin tilanteisiin ja kÃ¤yttÃ¶tapauksiin.

## Perinteisen tekoÃ¤lykÃ¤sittelyn paradigma

Perinteisesti generatiiviset tekoÃ¤lysovellukset luottavat suorituskykyiseen laskentainfrastruktuuriin suurten kielimallien (LLM) tehokkaaseen kÃ¤yttÃ¶Ã¶n. Organisaatiot yleensÃ¤ ottavat nÃ¤mÃ¤ mallit kÃ¤yttÃ¶Ã¶n GPU-klustereilla pilviympÃ¤ristÃ¶issÃ¤ ja kÃ¤yttÃ¤vÃ¤t niiden ominaisuuksia API-rajapintojen kautta.

TÃ¤mÃ¤ keskitetty malli toimii hyvin monissa sovelluksissa, mutta sillÃ¤ on luontaisia rajoituksia reunalaskennan tilanteissa. Perinteinen lÃ¤hestymistapa sisÃ¤ltÃ¤Ã¤ kÃ¤yttÃ¤jÃ¤n kyselyiden lÃ¤hettÃ¤misen etÃ¤palvelimille, niiden kÃ¤sittelyn tehokkaalla laitteistolla ja tulosten palauttamisen internetin kautta. Vaikka tÃ¤mÃ¤ menetelmÃ¤ tarjoaa pÃ¤Ã¤syn huipputeknisiin malleihin, se luo riippuvuuksia internet-yhteydestÃ¤, aiheuttaa viivehuolia ja herÃ¤ttÃ¤Ã¤ yksityisyyskysymyksiÃ¤, kun arkaluontoisia tietoja tÃ¤ytyy lÃ¤hettÃ¤Ã¤ ulkoisille palvelimille.

On olemassa joitakin keskeisiÃ¤ kÃ¤sitteitÃ¤, jotka meidÃ¤n tÃ¤ytyy ymmÃ¤rtÃ¤Ã¤ tyÃ¶skennellessÃ¤mme perinteisten tekoÃ¤lykÃ¤sittelyn paradigmojen kanssa, nimittÃ¤in:

- **â˜ï¸ Pilvipohjainen kÃ¤sittely**: TekoÃ¤lymallit toimivat tehokkaalla palvelininfrastruktuurilla, jossa on korkeat laskentaresurssit.
- **ğŸ”Œ API-pohjainen kÃ¤yttÃ¶**: Sovellukset kÃ¤yttÃ¤vÃ¤t tekoÃ¤lyominaisuuksia etÃ¤-API-kutsujen kautta paikallisen kÃ¤sittelyn sijaan.
- **ğŸ›ï¸ Keskitetty mallien hallinta**: Mallit yllÃ¤pidetÃ¤Ã¤n ja pÃ¤ivitetÃ¤Ã¤n keskitetysti, mikÃ¤ varmistaa johdonmukaisuuden mutta vaatii verkkoyhteyttÃ¤.
- **ğŸ“ˆ Resurssien skaalautuvuus**: Pilvi-infrastruktuuri voi dynaamisesti skaalautua kÃ¤sittelemÃ¤Ã¤n vaihtelevia laskentatarpeita.

## Reunalaskennan haasteet

Reunalaitteet, kuten kannettavat tietokoneet, matkapuhelimet ja esineiden internetin (IoT) laitteet, kuten Raspberry Pi ja NVIDIA Orin Nano, asettavat ainutlaatuisia laskennallisia rajoitteita. NÃ¤illÃ¤ laitteilla on yleensÃ¤ vÃ¤hemmÃ¤n laskentatehoa, muistia ja energiavarantoja verrattuna datakeskusten infrastruktuuriin.

Perinteisten LLM-mallien kÃ¤yttÃ¶ tÃ¤llaisilla laitteilla on historiallisesti ollut haastavaa nÃ¤iden laitteistojen rajoitusten vuoksi. Kuitenkin tarve reunalaskennalle on kasvanut yhÃ¤ tÃ¤rkeÃ¤mmÃ¤ksi eri tilanteissa. Mieti tilanteita, joissa internet-yhteys on epÃ¤luotettava tai puuttuu kokonaan, kuten etÃ¤isillÃ¤ teollisuusalueilla, liikenteessÃ¤ olevissa ajoneuvoissa tai alueilla, joilla verkkoyhteys on heikko. LisÃ¤ksi sovellukset, jotka vaativat korkeita turvallisuusstandardeja, kuten lÃ¤Ã¤ketieteelliset laitteet, finanssijÃ¤rjestelmÃ¤t tai valtion sovellukset, saattavat tarvita arkaluontoisten tietojen kÃ¤sittelyÃ¤ paikallisesti yksityisyyden ja vaatimustenmukaisuuden yllÃ¤pitÃ¤miseksi.

### Keskeiset reunalaskennan rajoitteet

ReunalaskentaympÃ¤ristÃ¶t kohtaavat useita perustavanlaatuisia rajoitteita, joita perinteiset pilvipohjaiset tekoÃ¤lyratkaisut eivÃ¤t kohtaa:

- **Rajoitettu laskentateho**: Reunalaitteilla on yleensÃ¤ vÃ¤hemmÃ¤n prosessoriytimiÃ¤ ja alhaisemmat kellotaajuudet verrattuna palvelintason laitteistoon.
- **Muistirajoitteet**: KÃ¤ytettÃ¤vissÃ¤ oleva RAM-muisti ja tallennuskapasiteetti ovat merkittÃ¤vÃ¤sti pienemmÃ¤t reunalaitteilla.
- **Energian rajoitukset**: Akkuvirralla toimivien laitteiden on tasapainotettava suorituskyky ja energiankulutus pitkÃ¤n kÃ¤yttÃ¶ajan varmistamiseksi.
- **LÃ¤mpÃ¶hallinta**: Kompaktit muodot rajoittavat jÃ¤Ã¤hdytysmahdollisuuksia, mikÃ¤ vaikuttaa jatkuvaan suorituskykyyn kuormituksen alla.

## MikÃ¤ on EdgeAI?

### KÃ¤site: EdgeAI mÃ¤Ã¤ritelty

EdgeAI tarkoittaa tekoÃ¤lyalgoritmien kÃ¤yttÃ¶Ã¶nottoa ja suorittamista suoraan reunalaitteillaâ€”fyysisillÃ¤ laitteilla, jotka sijaitsevat verkon "reunalla", lÃ¤hellÃ¤ sitÃ¤ paikkaa, jossa dataa tuotetaan ja kerÃ¤tÃ¤Ã¤n. NÃ¤itÃ¤ laitteita ovat muun muassa Ã¤lypuhelimet, IoT-anturit, Ã¤lykamerat, autonomiset ajoneuvot, puettavat laitteet ja teollisuuslaitteet. Toisin kuin perinteiset tekoÃ¤lyjÃ¤rjestelmÃ¤t, jotka luottavat pilvipalvelimiin kÃ¤sittelyssÃ¤, EdgeAI tuo Ã¤lykkyyden suoraan datan lÃ¤hteelle.

Perusajatuksena EdgeAI:ssa on tekoÃ¤lykÃ¤sittelyn hajauttaminen, siirtÃ¤en sen pois keskitetystÃ¤ datakeskuksesta ja jakamalla se laajalle laiteverkostolle, joka muodostaa digitaalisen ekosysteemimme. TÃ¤mÃ¤ edustaa perustavanlaatuista arkkitehtuurimuutosta siinÃ¤, miten tekoÃ¤lyjÃ¤rjestelmÃ¤t suunnitellaan ja otetaan kÃ¤yttÃ¶Ã¶n.

EdgeAI:n keskeiset kÃ¤sitteelliset pilarit ovat:

- **LÃ¤heisyyskÃ¤sittely**: Laskenta tapahtuu fyysisesti lÃ¤hellÃ¤ datan alkuperÃ¤Ã¤.
- **Hajautettu Ã¤lykkyys**: PÃ¤Ã¤tÃ¶ksentekokyky jakautuu useille laitteille.
- **Tietosuvereniteetti**: Tiedot pysyvÃ¤t paikallisessa hallinnassa, eivÃ¤tkÃ¤ usein koskaan poistu laitteesta.
- **Autonominen toiminta**: Laitteet voivat toimia Ã¤lykkÃ¤Ã¤sti ilman jatkuvaa yhteyttÃ¤.
- **Upotettu tekoÃ¤ly**: Ã„lykkyys tulee osaksi jokapÃ¤ivÃ¤isten laitteiden ominaisuuksia.

### EdgeAI-arkkitehtuurin visualisointi

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TRADITIONAL AI ARCHITECTURE                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Data Transfer  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   API Response   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Edge Devices â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Cloud Servers â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ End Users â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            EDGE AI ARCHITECTURE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Direct Response  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Edge Devices with Embedded AI         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ End Users â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â”‚ Sensors â”‚â”€>â”‚ SLM Inference â”‚â”€>â”‚ Local Action â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI edustaa merkittÃ¤vÃ¤Ã¤ muutosta tekoÃ¤lyn kÃ¤yttÃ¶Ã¶notossa, tuoden tekoÃ¤lyominaisuudet suoraan reunalaitteisiin sen sijaan, ettÃ¤ ne perustuisi pelkÃ¤stÃ¤Ã¤n pilvipohjaiseen kÃ¤sittelyyn. TÃ¤mÃ¤ lÃ¤hestymistapa mahdollistaa tekoÃ¤lymallien toiminnan paikallisesti laitteilla, joilla on rajalliset laskentaresurssit, tarjoten reaaliaikaisia ennustetoimintoja ilman jatkuvaa internet-yhteyttÃ¤.

EdgeAI kattaa erilaisia teknologioita ja tekniikoita, jotka on suunniteltu tekemÃ¤Ã¤n tekoÃ¤lymalleista tehokkaampia ja soveltuvia resurssirajoitteisille laitteille. Tavoitteena on sÃ¤ilyttÃ¤Ã¤ kohtuullinen suorituskyky samalla kun merkittÃ¤vÃ¤sti vÃ¤hennetÃ¤Ã¤n tekoÃ¤lymallien laskenta- ja muistivaatimuksia.

Tarkastellaan peruslÃ¤hestymistapoja, jotka mahdollistavat EdgeAI:n toteutukset eri laitetyypeillÃ¤ ja kÃ¤yttÃ¶tapauksissa.

### EdgeAI:n keskeiset periaatteet

EdgeAI perustuu useisiin perusperiaatteisiin, jotka erottavat sen perinteisestÃ¤ pilvipohjaisesta tekoÃ¤lystÃ¤:

- **Paikallinen kÃ¤sittely**: TekoÃ¤lykÃ¤sittely tapahtuu suoraan reunalaitteella ilman ulkoista yhteyttÃ¤.
- **Resurssien optimointi**: Mallit optimoidaan erityisesti kohdelaitteiden laitteistorajoituksia varten.
- **Reaaliaikainen suorituskyky**: KÃ¤sittely tapahtuu minimaalisella viiveellÃ¤ ajankohtaisissa sovelluksissa.
- **Yksityisyys suunnittelussa**: Arkaluontoiset tiedot pysyvÃ¤t laitteessa, mikÃ¤ parantaa turvallisuutta ja vaatimustenmukaisuutta.

## Keskeiset teknologiat, jotka mahdollistavat EdgeAI:n

### Mallien kvantisointi

Yksi tÃ¤rkeimmistÃ¤ tekniikoista EdgeAI:ssa on mallien kvantisointi. TÃ¤mÃ¤ prosessi sisÃ¤ltÃ¤Ã¤ malliparametrien tarkkuuden vÃ¤hentÃ¤misen, tyypillisesti 32-bittisistÃ¤ liukuluvuista 8-bittisiin kokonaislukuihin tai jopa matalamman tarkkuuden muotoihin. Vaikka tÃ¤mÃ¤ tarkkuuden vÃ¤hentÃ¤minen saattaa vaikuttaa huolestuttavalta, tutkimukset ovat osoittaneet, ettÃ¤ monet tekoÃ¤lymallit voivat sÃ¤ilyttÃ¤Ã¤ suorituskykynsÃ¤ jopa merkittÃ¤vÃ¤sti pienemmÃ¤llÃ¤ tarkkuudella.

Kvantisointi toimii kartoittamalla liukulukuarvojen alue pienempÃ¤Ã¤n joukkoon diskreettejÃ¤ arvoja. Esimerkiksi sen sijaan, ettÃ¤ kÃ¤ytettÃ¤isiin 32 bittiÃ¤ kunkin parametrin edustamiseen, kvantisointi saattaa kÃ¤yttÃ¤Ã¤ vain 8 bittiÃ¤, mikÃ¤ johtaa 4x pienempÃ¤Ã¤n muistivaatimukseen ja usein nopeampiin kÃ¤sittelyaikoihin.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

Eri kvantisointitekniikoita ovat:

- **JÃ¤lkikoulutuskvantisointi (PTQ)**: Sovelletaan mallin koulutuksen jÃ¤lkeen ilman uudelleenkoulutusta.
- **Kvantisointitietoinen koulutus (QAT)**: SisÃ¤llyttÃ¤Ã¤ kvantisointivaikutukset koulutuksen aikana paremman tarkkuuden saavuttamiseksi.
- **Dynaaminen kvantisointi**: Kvantisoi painot int8-muotoon, mutta laskee aktivoinnit dynaamisesti.
- **Staattinen kvantisointi**: Esilaskentaa kaikki kvantisointiparametrit sekÃ¤ painoille ettÃ¤ aktivoinneille.

EdgeAI-toteutuksissa sopivan kvantisointistrategian valinta riippuu mallin arkkitehtuurista, suorituskykyvaatimuksista ja kohdelaitteen laitteistokyvyistÃ¤.

### Mallien pakkaus ja optimointi

Kvantisoinnin lisÃ¤ksi erilaiset pakkaustekniikat auttavat vÃ¤hentÃ¤mÃ¤Ã¤n mallin kokoa ja laskentavaatimuksia. NÃ¤itÃ¤ ovat:

**Karsinta**: TÃ¤mÃ¤ tekniikka poistaa tarpeettomia yhteyksiÃ¤ tai neuroneita neuroverkoista. Tunnistamalla ja eliminoimalla parametrit, jotka vaikuttavat vÃ¤hÃ¤n mallin suorituskykyyn, karsinta voi merkittÃ¤vÃ¤sti pienentÃ¤Ã¤ mallin kokoa sÃ¤ilyttÃ¤en tarkkuuden.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Tietojen tislaus**: TÃ¤mÃ¤ lÃ¤hestymistapa sisÃ¤ltÃ¤Ã¤ pienemmÃ¤n "oppilasmallin" kouluttamisen jÃ¤ljittelemÃ¤Ã¤n suuremman "opettajamallin" kÃ¤yttÃ¤ytymistÃ¤. Oppilasmalli oppii lÃ¤hentÃ¤mÃ¤Ã¤n opettajan tuottamia tuloksia, usein saavuttaen samanlaisen suorituskyvyn merkittÃ¤vÃ¤sti vÃ¤hemmillÃ¤ parametreilla.

**Mallin arkkitehtuurin optimointi**: Tutkijat ovat kehittÃ¤neet erikoistuneita arkkitehtuureja, jotka on suunniteltu erityisesti reunakÃ¤yttÃ¶Ã¶n, kuten MobileNets, EfficientNets ja muut kevyet arkkitehtuurit, jotka tasapainottavat suorituskyvyn ja laskentatehokkuuden.

### Pienet kielimallit (SLM)

Nouseva trendi EdgeAI:ssa on pienten kielimallien (SLM) kehittÃ¤minen. NÃ¤mÃ¤ mallit on suunniteltu alusta alkaen kompakteiksi ja tehokkaiksi samalla kun ne tarjoavat merkittÃ¤viÃ¤ luonnollisen kielen ominaisuuksia. SLM:t saavuttavat tÃ¤mÃ¤n huolellisilla arkkitehtuurivalinnoilla, tehokkailla koulutustekniikoilla ja keskittymÃ¤llÃ¤ tiettyihin toimialoihin tai tehtÃ¤viin.

Toisin kuin perinteiset lÃ¤hestymistavat, jotka sisÃ¤ltÃ¤vÃ¤t suurten mallien pakkaamisen, SLM:t koulutetaan usein pienemmillÃ¤ tietoaineistoilla ja optimoiduilla arkkitehtuureilla, jotka on suunniteltu erityisesti reunakÃ¤yttÃ¶Ã¶n. TÃ¤mÃ¤ lÃ¤hestymistapa voi tuottaa malleja, jotka eivÃ¤t ole vain pienempiÃ¤, vaan myÃ¶s tehokkaampia tietyissÃ¤ kÃ¤yttÃ¶tapauksissa.

## Laitteistokiihdytys EdgeAI:lle

Modernit reunalaitteet sisÃ¤ltÃ¤vÃ¤t yhÃ¤ enemmÃ¤n erikoistunutta laitteistoa, joka on suunniteltu kiihdyttÃ¤mÃ¤Ã¤n tekoÃ¤lytehtÃ¤viÃ¤:

### NeuroprosessoriyksikÃ¶t (NPU:t)

NPU:t ovat erikoistuneita prosessoreita, jotka on suunniteltu erityisesti neuroverkkojen laskentaan. NÃ¤mÃ¤ sirut voivat suorittaa tekoÃ¤lykÃ¤sittelytehtÃ¤viÃ¤ paljon tehokkaammin kuin perinteiset CPU:t, usein pienemmÃ¤llÃ¤ energiankulutuksella. Monet modernit Ã¤lypuhelimet, kannettavat tietokoneet ja IoT-laitteet sisÃ¤ltÃ¤vÃ¤t nyt NPU:ita mahdollistamaan laitekohtaisen tekoÃ¤lykÃ¤sittelyn.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

Laitteet, joissa on NPU:t, sisÃ¤ltÃ¤vÃ¤t:

- **Apple**: A-sarjan ja M-sarjan sirut Neural EnginellÃ¤
- **Qualcomm**: Snapdragon-prosessorit Hexagon DSP/NPU:lla
- **Samsung**: Exynos-prosessorit NPU:lla
- **Intel**: Movidius VPU:t ja Habana Labs -kiihdyttimet
- **Microsoft**: Windows Copilot+ PC:t NPU:illa

### ğŸ® GPU-kiihdytys

Vaikka reunalaitteilla ei ehkÃ¤ ole datakeskusten tehokkaita GPU:ita, monilla on silti integroituja tai erillisiÃ¤ GPU:ita, jotka voivat kiihdyttÃ¤Ã¤ tekoÃ¤lytehtÃ¤viÃ¤. Modernit mobiili-GPU:t ja integroidut grafiikkaprosessorit voivat tarjota merkittÃ¤viÃ¤ suorituskykyparannuksia tekoÃ¤lykÃ¤sittelytehtÃ¤viin.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### CPU-optimointi

Jopa pelkÃ¤stÃ¤Ã¤n CPU:ta kÃ¤yttÃ¤vÃ¤t laitteet voivat hyÃ¶tyÃ¤ EdgeAI:sta optimoitujen toteutusten avulla. Modernit CPU:t sisÃ¤ltÃ¤vÃ¤t erikoistuneita ohjeita tekoÃ¤lytehtÃ¤viÃ¤ varten, ja ohjelmistokehykset on kehitetty maksimoimaan CPU:n suorituskyky tekoÃ¤lykÃ¤sittelyssÃ¤.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

EdgeAI:n parissa tyÃ¶skenteleville ohjelmistosuunnittelijoille nÃ¤iden laitteistokiihdytysvaihtoehtojen hyÃ¶dyntÃ¤misen ymmÃ¤rtÃ¤minen on kriittistÃ¤ kohdelaitteiden kÃ¤sittelysuorituskyvyn ja energiatehokkuuden optimoimiseksi.

## EdgeAI:n hyÃ¶dyt

### Yksityisyys ja turvallisuus

Yksi merkittÃ¤vimmistÃ¤ EdgeAI:n eduista on parantunut yksityisyys ja turvallisuus. KÃ¤sittelemÃ¤llÃ¤ dataa paikallisesti laitteessa arkaluontoiset tiedot eivÃ¤t koskaan poistu kÃ¤yttÃ¤jÃ¤n hallinnasta. TÃ¤mÃ¤ on erityisen tÃ¤rkeÃ¤Ã¤ sovelluksille, jotka kÃ¤sittelevÃ¤t henkilÃ¶kohtaisia tietoja, lÃ¤Ã¤ketieteellistÃ¤ tietoa tai luottamuksellisia liiketoimintatietoja.

### VÃ¤hentynyt viive

EdgeAI poistaa tarpeen lÃ¤hettÃ¤Ã¤ dataa etÃ¤palvelimille kÃ¤sittelyÃ¤ varten, mikÃ¤ vÃ¤hentÃ¤Ã¤ merkittÃ¤vÃ¤sti viivettÃ¤. TÃ¤mÃ¤ on ratkaisevan tÃ¤rkeÃ¤Ã¤ reaaliaikaisissa sovelluksissa, kuten autonomisissa ajoneuvoissa, teollisuusautomaatiota tai interaktiivisissa sovelluksissa, joissa tarvitaan vÃ¤littÃ¶miÃ¤ vastauksia.

### Offline-ominaisuus

EdgeAI mahdollistaa tekoÃ¤lytoiminnallisuuden jopa silloin, kun internet-yhteys ei ole kÃ¤ytettÃ¤vissÃ¤. TÃ¤mÃ¤ on arvokasta sovelluksille etÃ¤isillÃ¤ alueilla, matkustamisen aikana tai tilanteissa, joissa verkkoyhteyden luotettavuus on huolenaihe.

### Kustannustehokkuus

VÃ¤hentÃ¤mÃ¤llÃ¤ riippuvuutta pilvipohjaisista tekoÃ¤lypalveluista EdgeAI voi auttaa vÃ¤hentÃ¤mÃ¤Ã¤n kÃ¤yttÃ¶kustannuksia, erityisesti sovelluksissa, joissa kÃ¤yttÃ¶mÃ¤Ã¤rÃ¤t ovat suuria. Organisaatiot voivat vÃ¤lttÃ¤Ã¤ jatkuvat API-kustannukset ja vÃ¤hentÃ¤Ã¤ kaistanleveysvaatimuksia.

### Skaalaut
- [02: EdgeAI-sovellukset](02.RealWorldCaseStudies.md)

---

**Vastuuvapauslauseke**:  
TÃ¤mÃ¤ asiakirja on kÃ¤Ã¤nnetty kÃ¤yttÃ¤mÃ¤llÃ¤ tekoÃ¤lypohjaista kÃ¤Ã¤nnÃ¶spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ettÃ¤ automaattiset kÃ¤Ã¤nnÃ¶kset voivat sisÃ¤ltÃ¤Ã¤ virheitÃ¤ tai epÃ¤tarkkuuksia. AlkuperÃ¤istÃ¤ asiakirjaa sen alkuperÃ¤isellÃ¤ kielellÃ¤ tulisi pitÃ¤Ã¤ ensisijaisena lÃ¤hteenÃ¤. Kriittisen tiedon osalta suositellaan ammattimaista ihmiskÃ¤Ã¤nnÃ¶stÃ¤. Emme ole vastuussa vÃ¤Ã¤rinkÃ¤sityksistÃ¤ tai virhetulkinnoista, jotka johtuvat tÃ¤mÃ¤n kÃ¤Ã¤nnÃ¶ksen kÃ¤ytÃ¶stÃ¤.