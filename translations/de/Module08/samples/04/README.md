<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "562ac0eae12d808c9f45fbb77eb5c84f",
  "translation_date": "2025-09-24T11:44:48+00:00",
  "source_file": "Module08/samples/04/README.md",
  "language_code": "de"
}
-->
# Beispiel 04: Produktionsreife Chat-Anwendungen mit Chainlit

Ein umfassendes Beispiel, das verschiedene AnsÃ¤tze zur Erstellung produktionsreifer Chat-Anwendungen mit Microsoft Foundry Local zeigt, einschlieÃŸlich moderner Webschnittstellen, Streaming-Antworten und fortschrittlicher Browsertechnologien.

## Was ist enthalten?

- **ðŸš€ Chainlit Chat App** (`app.py`): Produktionsreife Chat-Anwendung mit Streaming
- **ðŸŒ WebGPU-Demo** (`webgpu-demo/`): Browserbasierte KI-Inferenz mit Hardwarebeschleunigung
- **ðŸŽ¨ Open WebUI-Integration** (`open-webui-guide.md`): Professionelle ChatGPT-Ã¤hnliche OberflÃ¤che
- **ðŸ“š Lehrnotebook** (`chainlit_app.ipynb`): Interaktive Lernmaterialien

## Schnellstart

### 1. Chainlit Chat-Anwendung

```cmd
# Navigate to Module08 directory
cd Module08

# Start your model
foundry model run phi-4-mini

# Run Chainlit app (using port 8080 to avoid conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Ã–ffnet unter: `http://localhost:8080`

### 2. WebGPU-Browser-Demo

```cmd
# Navigate to WebGPU demo
cd Module08\samples\04\webgpu-demo

# Serve the demo
python -m http.server 5173
```

Ã–ffnet unter: `http://localhost:5173`

### 3. Open WebUI-Setup

```cmd
# Run Open WebUI with Docker
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

Ã–ffnet unter: `http://localhost:3000`

## Architektur-Muster

### Entscheidungsmatrix: Lokal vs. Cloud

| Szenario | Empfehlung | Grund |
|----------|------------|-------|
| **Datenschutzsensible Daten** | ðŸ  Lokal (Foundry) | Daten verlassen das GerÃ¤t nicht |
| **Komplexes Denken** | â˜ï¸ Cloud (Azure OpenAI) | Zugriff auf grÃ¶ÃŸere Modelle |
| **Echtzeit-Chat** | ðŸ  Lokal (Foundry) | Niedrigere Latenz, schnellere Antworten |
| **Dokumentenanalyse** | ðŸ”„ Hybrid | Lokal fÃ¼r Extraktion, Cloud fÃ¼r Analyse |
| **Code-Generierung** | ðŸ  Lokal (Foundry) | Datenschutz + spezialisierte Modelle |
| **Forschungsaufgaben** | â˜ï¸ Cloud (Azure OpenAI) | Breite Wissensbasis erforderlich |

### Technologie-Vergleich

| Technologie | Anwendungsfall | Vorteile | Nachteile |
|-------------|----------------|----------|-----------|
| **Chainlit** | Python-Entwickler, schnelles Prototyping | Einfache Einrichtung, Streaming-UnterstÃ¼tzung | Nur Python |
| **WebGPU** | Maximale PrivatsphÃ¤re, Offline-Szenarien | Browser-nativ, kein Server erforderlich | Begrenzte ModellgrÃ¶ÃŸe |
| **Open WebUI** | Produktionsbereitstellung, Teams | Professionelle UI, Benutzerverwaltung | Erfordert Docker |

## Voraussetzungen

- **Foundry Local**: Installiert und lÃ¤uft ([Download](https://aka.ms/foundry-local-installer))
- **Python**: Version 3.10+ mit virtueller Umgebung
- **Modell**: Mindestens ein geladenes Modell (`foundry model run phi-4-mini`)
- **Browser**: Chrome/Edge mit WebGPU-UnterstÃ¼tzung fÃ¼r Demos
- **Docker**: FÃ¼r Open WebUI (optional)

## Installation & Einrichtung

### 1. Python-Umgebung einrichten

```cmd
# Navigate to Module08 directory
cd Module08

# Create and activate virtual environment
py -m venv .venv
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Foundry Local einrichten

```cmd
# Verify Foundry Local installation
foundry --version

# Start the service
foundry service start

# Load a model
foundry model run phi-4-mini

# Verify model is running
foundry service ps
```

## Beispielanwendungen

### Chainlit Chat-Anwendung

**Funktionen:**
- ðŸš€ **Echtzeit-Streaming**: Tokens erscheinen, wÃ¤hrend sie generiert werden
- ðŸ›¡ï¸ **Robuste Fehlerbehandlung**: Sanfte Degradierung und Wiederherstellung
- ðŸŽ¨ **Moderne UI**: Professionelle Chat-OberflÃ¤che direkt einsatzbereit
- ðŸ”§ **Flexible Konfiguration**: Umgebungsvariablen und automatische Erkennung
- ðŸ“± **Responsive Design**: Funktioniert auf Desktop- und MobilgerÃ¤ten

**Schnellstart:**
```cmd
# Run with default settings (recommended)
chainlit run samples\04\app.py -w --port 8080

# Use specific model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Manual endpoint configuration
set BASE_URL=http://localhost:51211
set API_KEY=your-api-key
chainlit run samples\04\app.py -w --port 8080
```

### WebGPU-Browser-Demo

**Funktionen:**
- ðŸŒ **Browser-native KI**: Kein Server erforderlich, lÃ¤uft vollstÃ¤ndig im Browser
- âš¡ **WebGPU-Beschleunigung**: Hardwarebeschleunigung, wenn verfÃ¼gbar
- ðŸ”’ **Maximale PrivatsphÃ¤re**: Keine Daten verlassen Ihr GerÃ¤t
- ðŸŽ¯ **Keine Installation**: Funktioniert in jedem kompatiblen Browser
- ðŸ”„ **Sanfte Fallbacks**: FÃ¤llt auf CPU zurÃ¼ck, wenn WebGPU nicht verfÃ¼gbar ist

**AusfÃ¼hrung:**
```cmd
cd samples\04\webgpu-demo
python -m http.server 5173
# Open http://localhost:5173
```

### Open WebUI-Integration

**Funktionen:**
- ðŸŽ¨ **ChatGPT-Ã¤hnliche OberflÃ¤che**: Professionelle, vertraute UI
- ðŸ‘¥ **Multi-Benutzer-UnterstÃ¼tzung**: Benutzerkonten und GesprÃ¤chsverlauf
- ðŸ“ **Dateiverarbeitung**: Hochladen und Analysieren von Dokumenten
- ðŸ”„ **Modellwechsel**: Einfacher Wechsel zwischen verschiedenen Modellen
- ðŸ³ **Docker-Bereitstellung**: Produktionsreife containerisierte Einrichtung

**Schnelle Einrichtung:**
```cmd
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

## Konfigurationsreferenz

### Umgebungsvariablen

| Variable | Beschreibung | Standard | Beispiel |
|----------|--------------|----------|----------|
| `MODEL` | Modellalias, der verwendet werden soll | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | Foundry Local-Endpunkt | Automatisch erkannt | `http://localhost:51211` |
| `API_KEY` | API-SchlÃ¼ssel (optional fÃ¼r lokal) | `""` | `your-api-key` |

## Fehlerbehebung

### HÃ¤ufige Probleme

**Chainlit-Anwendung:**

1. **Dienst nicht verfÃ¼gbar:**
   ```cmd
   # Check Foundry Local status
   foundry service status
   foundry service ps
   
   # Validate API endpoint (note: port 51211)
   curl http://localhost:51211/v1/models
   ```

2. **Port-Konflikte:**
   ```cmd
   # Check what's using port 8080
   netstat -ano | findstr :8080
   
   # Use different port if needed
   chainlit run samples\04\app.py -w --port 3000
   ```

3. **Probleme mit der Python-Umgebung:**
   ```cmd
   # Verify correct interpreter in VS Code
   # Ctrl+Shift+P â†’ Python: Select Interpreter
   # Choose: Module08/.venv/Scripts/python.exe
   
   # Reinstall dependencies
   pip install -r requirements.txt
   ```

**WebGPU-Demo:**

1. **WebGPU nicht unterstÃ¼tzt:**
   - Aktualisieren Sie auf Chrome/Edge 113+
   - Aktivieren Sie WebGPU: `chrome://flags/#enable-unsafe-webgpu`
   - ÃœberprÃ¼fen Sie den GPU-Status: `chrome://gpu`
   - Die Demo fÃ¤llt automatisch auf die CPU zurÃ¼ck

2. **Fehler beim Laden des Modells:**
   - Stellen Sie sicher, dass eine Internetverbindung fÃ¼r den Modell-Download besteht
   - ÃœberprÃ¼fen Sie die Browser-Konsole auf CORS-Fehler
   - Vergewissern Sie sich, dass Sie Ã¼ber HTTP (nicht file://) bereitstellen

**Open WebUI:**

1. **Verbindung verweigert:**
   ```cmd
   # Check Docker is running
   docker --version
   
   # Check container status
   docker ps | findstr open-webui
   
   # View container logs
   docker logs open-webui
   ```

2. **Modelle erscheinen nicht:**
   ```cmd
   # Verify Foundry Local endpoint
   curl http://localhost:51211/v1/models
   
   # Restart Open WebUI
   docker restart open-webui
   ```

### Validierungs-Checkliste

```cmd
# âœ… 1. Foundry Local Setup
foundry --version                    # Should show version
foundry service status               # Should show "running"
foundry model list                   # Should show loaded models
curl http://localhost:51211/v1/models  # Should return JSON

# âœ… 2. Python Environment  
python --version                     # Should be 3.10+
pip list | findstr chainlit         # Should show chainlit package
pip list | findstr openai           # Should show openai package

# âœ… 3. Application Testing
chainlit run samples\04\app.py -w --port 8080  # Should open browser
# Test WebGPU demo at localhost:5173
# Test Open WebUI at localhost:3000
```

## Erweiterte Nutzung

### Leistungsoptimierung

**Chainlit:**
- Verwenden Sie Streaming fÃ¼r eine bessere wahrgenommene Leistung
- Implementieren Sie Connection-Pooling fÃ¼r hohe ParallelitÃ¤t
- Cachen Sie Modellantworten fÃ¼r wiederholte Abfragen
- Ãœberwachen Sie den Speicherverbrauch bei groÃŸen GesprÃ¤chsverlÃ¤ufen

**WebGPU:**
- Nutzen Sie WebGPU fÃ¼r maximale PrivatsphÃ¤re und Geschwindigkeit
- Implementieren Sie Modellquantisierung fÃ¼r kleinere Modelle
- Verwenden Sie Web Workers fÃ¼r Hintergrundverarbeitung
- Cachen Sie kompilierte Modelle im Browser-Speicher

**Open WebUI:**
- Verwenden Sie persistente Volumes fÃ¼r GesprÃ¤chsverlÃ¤ufe
- Konfigurieren Sie Ressourcenlimits fÃ¼r Docker-Container
- Implementieren Sie Backup-Strategien fÃ¼r Benutzerdaten
- Richten Sie einen Reverse-Proxy fÃ¼r SSL-Terminierung ein

### Integrationsmuster

**Hybrid Lokal/Cloud:**
```python
# Route based on complexity and privacy requirements
async def intelligent_routing(prompt: str, metadata: dict):
    if metadata.get("contains_pii"):
        return await foundry_local_completion(prompt)  # Privacy-sensitive
    elif len(prompt.split()) > 200:
        return await azure_openai_completion(prompt)   # Complex reasoning
    else:
        return await foundry_local_completion(prompt)  # Default local
```

**Multi-Modale Pipeline:**
```python
# Combine different AI capabilities
async def analyze_document(file_path: str):
    # 1. OCR with WebGPU (browser-based)
    text = await webgpu_ocr(file_path)
    
    # 2. Analysis with Foundry Local (private)
    summary = await foundry_local_analyze(text)
    
    # 3. Enhancement with cloud (if needed)
    if summary.confidence < 0.8:
        summary = await azure_openai_enhance(summary)
    
    return summary
```

## Produktionsbereitstellung

### SicherheitsÃ¼berlegungen

- **API-SchlÃ¼ssel**: Verwenden Sie Umgebungsvariablen, niemals fest codieren
- **Netzwerk**: Verwenden Sie HTTPS in der Produktion, ziehen Sie VPN fÃ¼r Teamzugriff in Betracht
- **Zugriffskontrolle**: Implementieren Sie Authentifizierung fÃ¼r Open WebUI
- **Datenschutz**: ÃœberprÃ¼fen Sie, welche Daten lokal bleiben und welche in die Cloud gehen
- **Updates**: Halten Sie Foundry Local und Container auf dem neuesten Stand

### Ãœberwachung und Wartung

- **Health Checks**: Implementieren Sie EndpunktÃ¼berwachung
- **Logging**: Zentralisieren Sie Logs aus allen Komponenten
- **Metriken**: Verfolgen Sie Antwortzeiten, Fehlerraten, Ressourcennutzung
- **Backup**: RegelmÃ¤ÃŸige Sicherung von GesprÃ¤chsdaten und Konfigurationen

## Referenzen und Ressourcen

### Dokumentation
- [Chainlit-Dokumentation](https://docs.chainlit.io/) - VollstÃ¤ndige Framework-Anleitung
- [Foundry Local-Dokumentation](https://learn.microsoft.com/azure/ai-foundry/foundry-local/) - Offizielle Microsoft-Dokumentation
- [ONNX Runtime Web](https://onnxruntime.ai/docs/get-started/with-javascript/web.html) - WebGPU-Integration
- [Open WebUI-Dokumentation](https://docs.openwebui.com/) - Erweiterte Konfiguration

### Beispieldateien
- [`app.py`](../../../../../Module08/samples/04/app.py) - Produktionsreife Chainlit-Anwendung
- [`chainlit_app.ipynb`](./chainlit_app.ipynb) - Lehrnotebook
- [`webgpu-demo/`](../../../../../Module08/samples/04/webgpu-demo) - Browserbasierte KI-Inferenz
- [`open-webui-guide.md`](./open-webui-guide.md) - VollstÃ¤ndige Open WebUI-Einrichtung

### Verwandte Beispiele
- [Session 4 Dokumentation](../../04.CuttingEdgeModels.md) - VollstÃ¤ndige Sitzungsanleitung
- [Foundry Local Beispiele](https://github.com/microsoft/foundry-local/tree/main/samples) - Offizielle Beispiele

---

