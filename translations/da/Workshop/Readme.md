<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "48d0fb38be925084a6ebd957d4b045e5",
  "translation_date": "2025-10-09T14:33:30+00:00",
  "source_file": "Workshop/Readme.md",
  "language_code": "da"
}
-->
# EdgeAI for Begyndere - Workshop

> **Praktisk l√¶ringsforl√∏b til at bygge produktionsklare Edge AI-applikationer**
>
> Bliv ekspert i lokal AI-implementering med Microsoft Foundry Local, fra f√∏rste chat-komplettering til multi-agent orkestrering i 6 progressive sessioner.

---

## üéØ Introduktion

Velkommen til **EdgeAI for Begyndere Workshop** - din praktiske guide til at bygge intelligente applikationer, der k√∏rer udelukkende p√• lokal hardware. Denne workshop oms√¶tter teoretiske Edge AI-koncepter til virkelige f√¶rdigheder gennem gradvist udfordrende √∏velser med Microsoft Foundry Local og Small Language Models (SLMs).

### Hvorfor denne workshop?

**Edge AI-revolutionen er her**

Organisationer verden over skifter fra cloud-afh√¶ngig AI til edge computing af tre vigtige grunde:

1. **Privatliv & Overholdelse** - Behandl f√∏lsomme data lokalt uden cloud-transmission (HIPAA, GDPR, finansielle regler)
2. **Ydeevne** - Fjern netv√¶rkslatens (50-500ms lokalt vs 500-2000ms cloud round-trip)
3. **Omkostningskontrol** - Fjern per-token API-omkostninger og skaler uden cloud-udgifter

**Men Edge AI er anderledes**

At k√∏re AI lokalt kr√¶ver nye f√¶rdigheder:
- Modelvalg og optimering til ressourcebegr√¶nsninger
- Lokal serviceadministration og hardwareacceleration
- Prompt engineering til mindre modeller
- Produktionsimplementeringsm√∏nstre til edge-enheder

**Denne workshop leverer disse f√¶rdigheder**

I 6 fokuserede sessioner (~3 timer i alt) vil du g√• fra "Hello World" til at implementere produktionsklare multi-agent systemer - alt sammen lokalt p√• din maskine.

---

## üìö L√¶ringsm√•l

Ved at gennemf√∏re denne workshop vil du kunne:

### Kernekompetencer
1. **Implementere og administrere lokale AI-tjenester**
   - Installere og konfigurere Microsoft Foundry Local
   - V√¶lge passende modeller til edge-implementering
   - Administrere modellernes livscyklus (download, load, cache)
   - Overv√•ge ressourceforbrug og optimere ydeevne

2. **Bygge AI-drevne applikationer**
   - Implementere OpenAI-kompatible chat-kompletteringer lokalt
   - Designe effektive prompts til Small Language Models
   - H√•ndtere streaming-svar for bedre brugeroplevelse
   - Integrere lokale modeller i eksisterende applikationer

3. **Skabe RAG (Retrieval Augmented Generation) systemer**
   - Bygge semantisk s√∏gning med embeddings
   - Forankre LLM-svar i dom√¶nespecifik viden
   - Evaluere RAG-kvalitet med industristandardmetrikker
   - Skalere fra prototype til produktion

4. **Optimere modelydelse**
   - Benchmarke flere modeller til din brugssag
   - M√•le latens, genneml√∏b og f√∏rste-token tid
   - V√¶lge optimale modeller baseret p√• hastighed/kvalitet-afvejninger
   - Sammenligne SLM vs LLM-afvejninger i virkelige scenarier

5. **Orkestrere multi-agent systemer**
   - Designe specialiserede agenter til forskellige opgaver
   - Implementere agenthukommelse og kontekststyring
   - Koordinere agenter i komplekse arbejdsgange
   - Rute foresp√∏rgsler intelligent p√• tv√¶rs af flere modeller

6. **Implementere produktionsklare l√∏sninger**
   - Implementere fejlh√•ndtering og retry-logik
   - Overv√•ge tokenforbrug og systemressourcer
   - Bygge skalerbare arkitekturer med model-as-tools m√∏nstre
   - Planl√¶gge migrationsveje fra edge til hybrid (edge + cloud)

---

## üéì L√¶ringsresultater

### Hvad du vil bygge

Ved afslutningen af denne workshop vil du have skabt:

| Session | Leverance | Demonstrerede f√¶rdigheder |
|---------|-----------|---------------------------|
| **1** | Chat-applikation med streaming | Serviceops√¶tning, grundl√¶ggende kompletteringer, streaming UX |
| **2** | RAG-system med evaluering | Embeddings, semantisk s√∏gning, kvalitetsmetrikker |
| **3** | Multi-model benchmark suite | Ydelsesm√•ling, model-sammenligning |
| **4** | SLM vs LLM komparator | Afvejningsanalyse, optimeringsstrategier |
| **5** | Multi-agent orkestrator | Agentdesign, hukommelsesstyring, koordinering |
| **6** | Intelligent routingsystem | Intent-detektion, modelvalg, skalerbarhed |

### Kompetencematrix

| F√¶rdighedsniveau | Session 1-2 | Session 3-4 | Session 5-6 |
|------------------|-------------|-------------|-------------|
| **Begynder** | ‚úÖ Ops√¶tning & grundl√¶ggende | ‚ö†Ô∏è Udfordrende | ‚ùå For avanceret |
| **Mellem** | ‚úÖ Hurtig gennemgang | ‚úÖ Kerneindl√¶ring | ‚ö†Ô∏è Str√¶km√•l |
| **Avanceret** | ‚úÖ Let gennemgang | ‚úÖ Forfining | ‚úÖ Produktionsm√∏nstre |

### Karriereklare f√¶rdigheder

**Efter denne workshop vil du v√¶re klar til:**

‚úÖ **Bygge privatlivsfokuserede applikationer**
- Sundhedsapps, der h√•ndterer PHI/PII lokalt
- Finansielle tjenester med overholdelseskrav
- Regeringssystemer med datasuver√¶nitet

‚úÖ **Optimere til edge-milj√∏er**
- IoT-enheder med begr√¶nsede ressourcer
- Offline-f√∏rst mobilapplikationer
- Lav-latens realtidssystemer

‚úÖ **Designe intelligente arkitekturer**
- Multi-agent systemer til komplekse arbejdsgange
- Hybrid edge-cloud implementeringer
- Omkostningsoptimeret AI-infrastruktur

‚úÖ **Lede Edge AI-initiativer**
- Evaluere Edge AI's gennemf√∏rlighed for projekter
- V√¶lge passende modeller og frameworks
- Arkitektere skalerbare lokale AI-l√∏sninger

---

## üó∫Ô∏è Workshopstruktur

### Sessionoversigt (6 sessioner √ó 30 minutter = 3 timer)

| Session | Emne | Fokus | Varighed |
|---------|------|-------|----------|
| **1** | Kom godt i gang med Foundry Local | Installere, validere, f√∏rste kompletteringer | 30 min |
| **2** | Bygge AI-l√∏sninger med RAG | Prompt engineering, embeddings, evaluering | 30 min |
| **3** | Open Source-modeller | Modelopdagelse, benchmarking, valg | 30 min |
| **4** | Cutting Edge-modeller | SLM vs LLM, optimering, frameworks | 30 min |
| **5** | AI-drevne agenter | Agentdesign, orkestrering, hukommelse | 30 min |
| **6** | Modeller som v√¶rkt√∏jer | Routing, chaining, skaleringsstrategier | 30 min |

---

## üöÄ Hurtig start

### Foruds√¶tninger

**Systemkrav:**
- **OS**: Windows 10/11, macOS 11+ eller Linux (Ubuntu 20.04+)
- **RAM**: Minimum 8GB, anbefalet 16GB+
- **Lagerplads**: 10GB+ ledig plads til modeller
- **CPU**: Moderne processor med AVX2-underst√∏ttelse
- **GPU** (valgfrit): CUDA-kompatibel eller Qualcomm NPU til acceleration

**Softwarekrav:**
- **Python 3.8+** ([Download](https://www.python.org/downloads/))
- **Microsoft Foundry Local** ([Installationsguide](../../../Workshop))
- **Git** ([Download](https://git-scm.com/downloads))
- **Visual Studio Code** (anbefalet) ([Download](https://code.visualstudio.com/))

### Ops√¶tning i 3 trin

#### 1. Installer Foundry Local

**Windows:**
```powershell
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**Verificer installation:**
```bash
foundry --version
foundry service status
```

#### 2. Klon repository & installer afh√¶ngigheder

```bash
# Clone repository
git clone https://github.com/microsoft/edgeai-for-beginners.git
cd edgeai-for-beginners/Workshop

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.\.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

#### 3. K√∏r din f√∏rste pr√∏ve

```bash
# Start Foundry Local and load a model
foundry model run phi-4-mini

# Run the chat bootstrap sample
cd samples/session01
python chat_bootstrap.py "What is edge AI?"
```

**‚úÖ Succes!** Du b√∏r se et streaming-svar om edge AI.

---

## üì¶ Workshopressourcer

### Python-eksempler

Progressive praktiske eksempler, der demonstrerer hvert koncept:

| Session | Eksempel | Beskrivelse | K√∏rselstid |
|---------|----------|-------------|------------|
| 1 | [`chat_bootstrap.py`](../../../Workshop/samples/session01/chat_bootstrap.py) | Grundl√¶ggende & streaming chat | ~30s |
| 2 | [`rag_pipeline.py`](../../../Workshop/samples/session02/rag_pipeline.py) | RAG med embeddings | ~45s |
| 2 | [`rag_eval_ragas.py`](../../../Workshop/samples/session02/rag_eval_ragas.py) | RAG-kvalitetsevaluering | ~60s |
| 3 | [`benchmark_oss_models.py`](../../../Workshop/samples/session03/benchmark_oss_models.py) | Multi-model benchmarking | ~2-3m |
| 4 | [`model_compare.py`](../../../Workshop/samples/session04/model_compare.py) | SLM vs LLM sammenligning | ~45s |
| 5 | [`agents_orchestrator.py`](../../../Workshop/samples/session05/agents_orchestrator.py) | Multi-agent system | ~60s |
| 6 | [`models_router.py`](../../../Workshop/samples/session06/models_router.py) | Intent-baseret routing | ~45s |
| 6 | [`models_pipeline.py`](../../../Workshop/samples/session06/models_pipeline.py) | Multi-step pipeline | ~60s |

### Jupyter Notebooks

Interaktiv udforskning med forklaringer og visualiseringer:

| Session | Notebook | Beskrivelse | Sv√¶rhedsgrad |
|---------|----------|-------------|--------------|
| 1 | [`session01_chat_bootstrap.ipynb`](./notebooks/session01_chat_bootstrap.ipynb) | Chat-grundl√¶ggende & streaming | ‚≠ê Begynder |
| 2 | [`session02_rag_pipeline.ipynb`](./notebooks/session02_rag_pipeline.ipynb) | Bygge RAG-system | ‚≠ê‚≠ê Mellem |
| 2 | [`session02_rag_eval_ragas.ipynb`](./notebooks/session02_rag_eval_ragas.ipynb) | Evaluere RAG-kvalitet | ‚≠ê‚≠ê Mellem |
| 3 | [`session03_benchmark_oss_models.ipynb`](./notebooks/session03_benchmark_oss_models.ipynb) | Modelbenchmarking | ‚≠ê‚≠ê Mellem |
| 4 | [`session04_model_compare.ipynb`](./notebooks/session04_model_compare.ipynb) | Modelsammenligning | ‚≠ê‚≠ê Mellem |
| 5 | [`session05_agents_orchestrator.ipynb`](./notebooks/session05_agents_orchestrator.ipynb) | Agentorkestrering | ‚≠ê‚≠ê‚≠ê Avanceret |
| 6 | [`session06_models_router.ipynb`](./notebooks/session06_models_router.ipynb) | Intent-routing | ‚≠ê‚≠ê‚≠ê Avanceret |
| 6 | [`session06_models_pipeline.ipynb`](./notebooks/session06_models_pipeline.ipynb) | Pipeline-orkestrering | ‚≠ê‚≠ê‚≠ê Avanceret |

### Dokumentation

Omfattende vejledninger og referencer:

| Dokument | Beskrivelse | Brug n√•r |
|----------|-------------|----------|
| [QUICK_START.md](./QUICK_START.md) | Hurtig ops√¶tningsguide | Starter fra bunden |
| [QUICK_REFERENCE.md](./QUICK_REFERENCE.md) | Kommando- & API-cheat sheet | Har brug for hurtige svar |
| [FOUNDRY_SDK_QUICKREF.md](./FOUNDRY_SDK_QUICKREF.md) | SDK-m√∏nstre & eksempler | Skriver kode |
| [ENV_CONFIGURATION.md](./ENV_CONFIGURATION.md) | Guide til milj√∏variabler | Konfigurerer eksempler |
| [SAMPLES_UPDATE_SUMMARY.md](./SAMPLES_UPDATE_SUMMARY.md) | Seneste forbedringer af eksempler | Forst√•r √¶ndringer |
| [SDK_MIGRATION_NOTES.md](./SDK_MIGRATION_NOTES.md) | Migrationsguide | Opgraderer kode |
| [notebooks/TROUBLESHOOTING.md](./notebooks/TROUBLESHOOTING.md) | Almindelige problemer & l√∏sninger | Fejlfinding |

---

## üéì Anbefalinger til l√¶ringsforl√∏b

### For begyndere (3-4 timer)
1. ‚úÖ Session 1: Kom godt i gang (fokus p√• ops√¶tning og grundl√¶ggende chat)
2. ‚úÖ Session 2: RAG-grundl√¶ggende (spring evaluering over i starten)
3. ‚úÖ Session 3: Enkel benchmarking (kun 2 modeller)
4. ‚è≠Ô∏è Spring sessionerne 4-6 over for nu
5. üîÑ Vend tilbage til sessionerne 4-6 efter at have bygget den f√∏rste applikation

### For mellemudviklere (3 timer)
1. ‚ö° Session 1: Hurtig ops√¶tningsvalidering
2. ‚úÖ Session 2: Fuld RAG-pipeline med evaluering
3. ‚úÖ Session 3: Fuldt benchmark-suite
4. ‚úÖ Session 4: Modeloptimering
5. ‚úÖ Sessionerne 5-6: Fokus p√• arkitekturm√∏nstre

### For avancerede praktikere (2-3 timer)
1. ‚ö° Sessionerne 1-3: Hurtig gennemgang og validering
2. ‚úÖ Session 4: Optimeringsdybdeg√•ende
3. ‚úÖ Session 5: Multi-agent arkitektur
4. ‚úÖ Session 6: Produktionsm√∏nstre og skalering
5. üöÄ Udvid: Byg brugerdefineret routing-logik og hybridimplementeringer

---

## Workshop Session Pack (Fokuserede 30-minutters labs)

Hvis du f√∏lger det kondenserede 6-session workshopformat, brug disse dedikerede vejledninger (hver matcher og supplerer de bredere modul-dokumenter ovenfor):

| Workshop Session | Guide | Kernefokus |
|------------------|-------|------------|
| 1 | [Session01-GettingStartedFoundryLocal](./Session01-GettingStartedFoundryLocal.md) | Installere, validere, k√∏re phi & GPT-OSS-20B, acceleration |
| 2 | [Session02-BuildAISolutionsRAG](./Session02-BuildAISolutionsRAG.md) | Prompt engineering, RAG-m√∏nstre, CSV & dokumentforankring, migration |
| 3 | [Session03-OpenSourceModels](./Session03-OpenSourceModels.md) | Hugging Face-integration, benchmarking, modelvalg |
| 4 | [Session04-CuttingEdgeModels](./Session04-CuttingEdgeModels.md) | SLM vs LLM, WebGPU, Chainlit RAG, ONNX-acceleration |
| 5 | [Session05-AIPoweredAgents](./Session05-AIPoweredAgents.md) | Agentroller, hukommelse, v√¶rkt√∏jer, orkestrering |
| 6 | [Session06-ModelsAsTools](./Session06-ModelsAsTools.md) | Routing, chaining, skaleringsvej til Azure |
Hver sessionsfil inkluderer: abstrakt, l√¶ringsm√•l, 30-minutters demo-flow, startprojekt, valideringscheckliste, fejlfinding og referencer til den officielle Foundry Local Python SDK.

### Eksempelscripts

Installer workshop-afh√¶ngigheder (Windows):

```powershell
cd Workshop
py -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt
```

macOS / Linux:

```bash
cd Workshop
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Hvis Foundry Local-tjenesten k√∏rer p√• en anden (Windows) maskine eller VM fra macOS, eksport√©r endpointet:

```bash
export FOUNDRY_LOCAL_ENDPOINT=http://<windows-host>:5273/v1
```

| Session | Script(s) | Beskrivelse |
|---------|-----------|-------------|
| 1 | `samples/session01/chat_bootstrap.py` | Bootstrap-tjeneste & streaming-chat |
| 2 | `samples/session02/rag_pipeline.py` | Minimal RAG (in-memory embeddings) |
|   | `samples/session02/rag_eval_ragas.py` | RAG-evaluering med ragas-metrics |
| 3 | `samples/session03/benchmark_oss_models.py` | Multi-model latenstid & throughput benchmarking |
| 4 | `samples/session04/model_compare.py` | SLM vs LLM sammenligning (latenstid & pr√∏veoutput) |
| 5 | `samples/session05/agents_orchestrator.py` | To-agent forsknings- ‚Üí redaktionel pipeline |
| 6 | `samples/session06/models_router.py` | Intent-baseret routing-demo |
|   | `samples/session06/models_pipeline.py` | Multi-step planl√¶g/udf√∏r/forfin k√¶de |

### Milj√∏variabler (F√¶lles for alle eksempler)

| Variabel | Form√•l | Eksempel |
|----------|---------|---------|
| `FOUNDRY_LOCAL_ALIAS` | Standard enkeltmodel-alias for basale eksempler | `phi-4-mini` |
| `SLM_ALIAS` / `LLM_ALIAS` | Eksplicit SLM vs st√∏rre model til sammenligning | `phi-4-mini` / `gpt-oss-20b` |
| `BENCH_MODELS` | Komma-separeret liste over aliaser til benchmarking | `qwen2.5-0.5b,gemma-2-2b,mistral-7b` |
| `BENCH_ROUNDS` | Benchmark-repetitioner pr. model | `3` |
| `BENCH_PROMPT` | Prompt brugt i benchmarking | `Forklar retrieval augmented generation kort.` |
| `EMBED_MODEL` | Sentence-transformers embedding model | `sentence-transformers/all-MiniLM-L6-v2` |
| `RAG_QUESTION` | Overstyr testforesp√∏rgsel for RAG-pipeline | `Hvorfor bruge RAG med lokal inferens?` |
| `AGENT_QUESTION` | Overstyr foresp√∏rgsel for agents-pipeline | `Forklar hvorfor edge AI er vigtigt for compliance.` |
| `AGENT_MODEL_PRIMARY` | Model-alias for forskningsagent | `phi-4-mini` |
| `AGENT_MODEL_EDITOR` | Model-alias for redakt√∏ragent (kan v√¶re anderledes) | `gpt-oss-20b` |
| `SHOW_USAGE` | N√•r `1`, udskriver tokenforbrug pr. completion | `1` |
| `RETRY_ON_FAIL` | N√•r `1`, pr√∏v igen √©n gang ved midlertidige chatfejl | `1` |
| `RETRY_BACKOFF` | Sekunder at vente f√∏r nyt fors√∏g | `1.0` |

Hvis en variabel ikke er sat, falder scripts tilbage til fornuftige standardindstillinger. For enkeltmodel-demoer beh√∏ver du typisk kun `FOUNDRY_LOCAL_ALIAS`.

### Hj√¶lpemodul

Alle eksempler deler nu en hj√¶lper `samples/workshop_utils.py`, der tilbyder:

* Cached `FoundryLocalManager` + OpenAI-klientoprettelse
* `chat_once()`-hj√¶lper med valgfri retry + forbrugsudskrift
* Enkel tokenforbrugsrapportering (aktiver via `SHOW_USAGE=1`)

Dette reducerer duplikering og fremh√¶ver bedste praksis for effektiv lokal modelorkestrering.

## Valgfrie forbedringer (P√• tv√¶rs af sessioner)

| Tema | Forbedring | Sessioner | Milj√∏ / Skift |
|-------|-------------|----------|--------------|
| Determinisme | Fast temperatur + stabile prompt-s√¶t | 1‚Äì6 | S√¶t `temperature=0`, `top_p=1` |
| Synlighed af tokenforbrug | Konsistent undervisning i omkostninger/effektivitet | 1‚Äì6 | `SHOW_USAGE=1` |
| Streaming af f√∏rste token | Opfattet latenstidsm√•ling | 1,3,4,6 | `BENCH_STREAM=1` (benchmark) |
| Retry-robusthed | H√•ndterer midlertidig cold-start | Alle | `RETRY_ON_FAIL=1` + `RETRY_BACKOFF` |
| Multi-model-agenter | Heterogen rolle-specialisering | 5 | `AGENT_MODEL_PRIMARY`, `AGENT_MODEL_EDITOR` |
| Adaptiv routing | Intent + omkostningsheuristikker | 6 | Udvid router med eskaleringslogik |
| Vektormemory | Langtids semantisk hukommelse | 2,5,6 | Integrer FAISS/Chroma embedding index |
| Trace-eksport | Revision & evaluering | 2,5,6 | Tilf√∏j JSON-linjer pr. trin |
| Kvalitetsrubrikker | Kvalitativ sporing | 3‚Äì6 | Sekund√¶re scoring-prompts |
| Smoke-tests | Hurtig workshop-validering | Alle | `python Workshop/tests/smoke.py` |

### Deterministisk Quick Start

```powershell
set FOUNDRY_LOCAL_ALIAS=phi-4-mini
set SHOW_USAGE=1
python Workshop\tests\smoke.py
```

Forvent stabile tokenantal p√• tv√¶rs af gentagne identiske input.

### RAG-evaluering (Session 2)

Brug `rag_eval_ragas.py` til at beregne svarrelevans, trov√¶rdighed og kontekstpr√¶cision p√• et lille syntetisk datas√¶t:

```powershell
python samples/session02/rag_eval_ragas.py
```

Udvid ved at levere en st√∏rre JSONL med sp√∏rgsm√•l, kontekster og sandheder, og konverter derefter til et Hugging Face `Dataset`.

## CLI-kommandon√∏jagtighed Appendix

Workshoppen bruger bevidst kun aktuelt dokumenterede / stabile Foundry Local CLI-kommandoer.

### Refererede stabile kommandoer

| Kategori | Kommando | Form√•l |
|----------|---------|---------|
| Core | `foundry --version` | Vis installeret version |
| Core | `foundry init` | Initialiser konfiguration |
| Service | `foundry service start` | Start lokal tjeneste (hvis ikke auto) |
| Service | `foundry status` | Vis tjenestestatus |
| Models | `foundry model list` | Liste over katalog / tilg√¶ngelige modeller |
| Models | `foundry model download <alias>` | Download modelv√¶gt til cache |
| Models | `foundry model run <alias>` | Start (load) en model lokalt; kombiner med `--prompt` for one-shot |
| Models | `foundry model unload <alias>` / `foundry model stop <alias>` | Fjern en model fra hukommelsen (hvis underst√∏ttet) |
| Cache | `foundry cache list` | Liste over cachede (downloadede) modeller |
| System | `foundry system info` | Snapshot af hardware & accelerationskapaciteter |
| System | `foundry system gpu-info` | GPU-diagnostisk info |
| Config | `foundry config list` | Vis aktuelle konfigurationsv√¶rdier |
| Config | `foundry config set <key> <value>` | Opdater konfiguration |

### One‚ÄëShot Prompt Pattern

I stedet for en for√¶ldet `model chat`-underkommando, brug:

```powershell
foundry model run <alias> --prompt "Your question here"
```

Dette udf√∏rer en enkelt prompt/svar-cyklus og afslutter derefter.

### Fjernede / Undg√•ede m√∏nstre

| For√¶ldet / Udokumenteret | Erstatning / Vejledning |
|---------------------------|------------------------|
| `foundry model chat <model> "..."` | `foundry model run <model> --prompt "..."` |
| `foundry model list --running` | Brug almindelig `foundry model list` + nylig aktivitet / logs |
| `foundry model list --cached` | `foundry cache list` |
| `foundry model stats <model>` | Brug benchmark Python-script + OS-v√¶rkt√∏jer (Task Manager / `nvidia-smi`) |
| `foundry model benchmark ...` | `samples/session03/benchmark_oss_models.py` |

### Benchmarking & Telemetri

- Latenstid, p95, tokens/sek.: `samples/session03/benchmark_oss_models.py`
- F√∏rste-token latenstid (streaming): s√¶t `BENCH_STREAM=1`
- Ressourceforbrug: OS-monitorer (Task Manager, Activity Monitor, `nvidia-smi`) + `foundry system info`.

N√•r nye CLI-telemetrikommandoer stabiliseres upstream, kan de integreres med minimale √¶ndringer i session-markdowns.

### Automatisk Lint Guard

En automatisk linter forhindrer genindf√∏relse af for√¶ldede CLI-m√∏nstre inden for afgr√¶nsede kodeblokke i markdown-filer:

Script: `Workshop/scripts/lint_markdown_cli.py`

For√¶ldede m√∏nstre blokeres inden for kodeafgr√¶nsninger.

Anbefalede erstatninger:
| For√¶ldet | Erstatning |
|------------|-------------|
| `foundry model chat <a> "..."` | `foundry model run <a> --prompt "..."` |
| `model list --running` | `model list` |
| `model list --cached` | `cache list` |
| `model stats` | Benchmark-script + systemv√¶rkt√∏jer |
| `model benchmark` | `samples/session03/benchmark_oss_models.py` |
| `model list --available` | `model list` |

K√∏r lokalt:
```powershell
python Workshop\scripts\lint_markdown_cli.py --verbose
```

GitHub Action: `.github/workflows/markdown-cli-lint.yml` k√∏rer ved hver push & PR.

Valgfri pre-commit hook:
```bash
echo "python Workshop/scripts/lint_markdown_cli.py" > .git/hooks/pre-commit
chmod +x .git/hooks/pre-commit
```

## Hurtig CLI ‚Üí SDK Migration Table

| Opgave | CLI One-Liner | SDK (Python) Equivalent | Noter |
|------|---------------|-------------------------|-------|
| K√∏r en model √©n gang (prompt) | `foundry model run phi-4-mini --prompt "Hello"` | `manager=FoundryLocalManager("phi-4-mini"); client=OpenAI(base_url=manager.endpoint, api_key=manager.api_key or "not-needed"); client.chat.completions.create(model=manager.get_model_info("phi-4-mini").id, messages=[{"role":"user","content":"Hello"}])` | SDK bootstraps tjeneste & caching automatisk |
| Download (cache) model | `foundry model download qwen2.5-0.5b` | `FoundryLocalManager("qwen2.5-0.5b")  # triggers download/load` | Manager v√¶lger bedste variant, hvis alias maps til flere builds |
| Liste over katalog | `foundry model list` | `# brug manager for hvert alias eller vedligehold kendt liste` | CLI aggregerer; SDK i √∏jeblikket pr. alias-initialisering |
| Liste over cachede modeller | `foundry cache list` | `manager.list_cached_models()` | Efter manager-init (ethvert alias) |
| Aktiver GPU-acceleration | `foundry config set compute.onnx.enable_gpu true` | `# CLI-handling; SDK antager, at konfiguration allerede er anvendt` | Konfiguration er ekstern sideeffekt |
| Hent endpoint-URL | (implicit) | `manager.endpoint` | Bruges til at oprette OpenAI-kompatibel klient |
| Varm en model op | `foundry model run <alias>` derefter f√∏rste prompt | `chat_once(alias, messages=[...])` (utility) | Utilities h√•ndterer initial cold latency warmup |
| M√•l latenstid | `python benchmark_oss_models.py` | `import benchmark_oss_models` (eller nyt eksportscript) | Foretr√¶k script for konsistente metrics |
| Stop / fjern model | `foundry model unload <alias>` | (Ikke eksponeret ‚Äì genstart tjeneste / proces) | Typisk ikke n√∏dvendigt for workshop-flow |
| Hent tokenforbrug | (vis output) | `resp.usage.total_tokens` | Tilbydes, hvis backend returnerer usage-objekt |

## Benchmark Markdown Export

Brug scriptet `Workshop/scripts/export_benchmark_markdown.py` til at k√∏re en frisk benchmark (samme logik som `samples/session03/benchmark_oss_models.py`) og generere en GitHub-venlig Markdown-tabel plus r√• JSON.

### Eksempel

```powershell
python Workshop\scripts\export_benchmark_markdown.py --models "qwen2.5-0.5b,gemma-2-2b,mistral-7b" --prompt "Explain retrieval augmented generation briefly." --rounds 3 --output benchmark_report.md
```

Genererede filer:
| Fil | Indhold |
|------|----------|
| `benchmark_report.md` | Markdown-tabel + fortolkningshint |
| `benchmark_report.json` | R√• metrics-array (til diffing / trendsporing) |

S√¶t `BENCH_STREAM=1` i milj√∏et for at inkludere f√∏rste-token latenstid, hvis underst√∏ttet.

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, skal det bem√¶rkes, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os ikke ansvar for misforst√•elser eller fejltolkninger, der m√•tte opst√• som f√∏lge af brugen af denne overs√¶ttelse.