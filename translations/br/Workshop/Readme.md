<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "48d0fb38be925084a6ebd957d4b045e5",
  "translation_date": "2025-10-09T10:52:51+00:00",
  "source_file": "Workshop/Readme.md",
  "language_code": "br"
}
-->
# EdgeAI para Iniciantes - Workshop

> **Caminho de Aprendizado Pr√°tico para Construir Aplica√ß√µes de IA na Borda Prontas para Produ√ß√£o**
>
> Domine a implanta√ß√£o de IA local com o Microsoft Foundry Local, desde a primeira conclus√£o de chat at√© a orquestra√ß√£o de m√∫ltiplos agentes em 6 sess√µes progressivas.

---

## üéØ Introdu√ß√£o

Bem-vindo ao **Workshop EdgeAI para Iniciantes** - seu guia pr√°tico e pr√°tico para construir aplica√ß√µes inteligentes que funcionam inteiramente em hardware local. Este workshop transforma conceitos te√≥ricos de IA na borda em habilidades do mundo real por meio de exerc√≠cios progressivamente desafiadores usando o Microsoft Foundry Local e Modelos de Linguagem Pequenos (SLMs).

### Por que este Workshop?

**A Revolu√ß√£o da IA na Borda J√° Chegou**

Organiza√ß√µes em todo o mundo est√£o migrando da IA dependente da nuvem para a computa√ß√£o na borda por tr√™s raz√µes cr√≠ticas:

1. **Privacidade e Conformidade** - Processar dados sens√≠veis localmente sem transmiss√£o para a nuvem (HIPAA, GDPR, regulamenta√ß√µes financeiras)
2. **Desempenho** - Eliminar a lat√™ncia de rede (50-500ms local vs 500-2000ms ida e volta na nuvem)
3. **Controle de Custos** - Eliminar custos por token de APIs e escalar sem despesas com a nuvem

**Mas IA na Borda √© Diferente**

Executar IA no local exige novas habilidades:
- Sele√ß√£o e otimiza√ß√£o de modelos para restri√ß√µes de recursos
- Gerenciamento de servi√ßos locais e acelera√ß√£o de hardware
- Engenharia de prompts para modelos menores
- Padr√µes de implanta√ß√£o em produ√ß√£o para dispositivos de borda

**Este Workshop Entrega Essas Habilidades**

Em 6 sess√µes focadas (~3 horas no total), voc√™ progredir√° de "Hello World" para implantar sistemas de m√∫ltiplos agentes prontos para produ√ß√£o - todos rodando localmente na sua m√°quina.

---

## üìö Objetivos de Aprendizado

Ao concluir este workshop, voc√™ ser√° capaz de:

### Compet√™ncias Centrais
1. **Implantar e Gerenciar Servi√ßos de IA Locais**
   - Instalar e configurar o Microsoft Foundry Local
   - Selecionar modelos apropriados para implanta√ß√£o na borda
   - Gerenciar o ciclo de vida dos modelos (download, carregamento, cache)
   - Monitorar o uso de recursos e otimizar o desempenho

2. **Construir Aplica√ß√µes com IA**
   - Implementar conclus√µes de chat compat√≠veis com OpenAI localmente
   - Projetar prompts eficazes para Modelos de Linguagem Pequenos
   - Lidar com respostas em streaming para uma melhor experi√™ncia do usu√°rio
   - Integrar modelos locais em aplica√ß√µes existentes

3. **Criar Sistemas RAG (Gera√ß√£o Aumentada por Recupera√ß√£o)**
   - Construir busca sem√¢ntica com embeddings
   - Basear respostas de LLM em conhecimento espec√≠fico do dom√≠nio
   - Avaliar a qualidade do RAG com m√©tricas padr√£o da ind√∫stria
   - Escalar de prot√≥tipo para produ√ß√£o

4. **Otimizar o Desempenho do Modelo**
   - Avaliar m√∫ltiplos modelos para seu caso de uso
   - Medir lat√™ncia, throughput e tempo do primeiro token
   - Selecionar modelos ideais com base em trade-offs de velocidade/qualidade
   - Comparar trade-offs entre SLM e LLM em cen√°rios reais

5. **Orquestrar Sistemas de M√∫ltiplos Agentes**
   - Projetar agentes especializados para diferentes tarefas
   - Implementar mem√≥ria e gerenciamento de contexto de agentes
   - Coordenar agentes em fluxos de trabalho complexos
   - Roteamento inteligente de solicita√ß√µes entre m√∫ltiplos modelos

6. **Implantar Solu√ß√µes Prontas para Produ√ß√£o**
   - Implementar tratamento de erros e l√≥gica de repeti√ß√£o
   - Monitorar o uso de tokens e recursos do sistema
   - Construir arquiteturas escal√°veis com padr√µes de modelo-como-ferramentas
   - Planejar caminhos de migra√ß√£o da borda para h√≠brido (borda + nuvem)

---

## üéì Resultados de Aprendizado

### O Que Voc√™ Vai Construir

Ao final deste workshop, voc√™ ter√° criado:

| Sess√£o | Entreg√°vel | Habilidades Demonstradas |
|--------|------------|--------------------------|
| **1** | Aplica√ß√£o de chat com streaming | Configura√ß√£o do servi√ßo, conclus√µes b√°sicas, UX de streaming |
| **2** | Sistema RAG com avalia√ß√£o | Embeddings, busca sem√¢ntica, m√©tricas de qualidade |
| **3** | Su√≠te de benchmark de m√∫ltiplos modelos | Medi√ß√£o de desempenho, compara√ß√£o de modelos |
| **4** | Comparador SLM vs LLM | An√°lise de trade-offs, estrat√©gias de otimiza√ß√£o |
| **5** | Orquestrador de m√∫ltiplos agentes | Design de agentes, gerenciamento de mem√≥ria, coordena√ß√£o |
| **6** | Sistema de roteamento inteligente | Detec√ß√£o de inten√ß√£o, sele√ß√£o de modelo, escalabilidade |

### Matriz de Compet√™ncias

| N√≠vel de Habilidade | Sess√£o 1-2 | Sess√£o 3-4 | Sess√£o 5-6 |
|---------------------|------------|------------|------------|
| **Iniciante**       | ‚úÖ Configura√ß√£o e b√°sicos | ‚ö†Ô∏è Desafiador | ‚ùå Muito avan√ßado |
| **Intermedi√°rio**   | ‚úÖ Revis√£o r√°pida | ‚úÖ Aprendizado principal | ‚ö†Ô∏è Metas desafiadoras |
| **Avan√ßado**        | ‚úÖ Tranquilo | ‚úÖ Refinamento | ‚úÖ Padr√µes de produ√ß√£o |

### Habilidades para o Mercado de Trabalho

**Ap√≥s este workshop, voc√™ estar√° preparado para:**

‚úÖ **Construir Aplica√ß√µes com Foco em Privacidade**
- Aplicativos de sa√∫de que lidam com PHI/PII localmente
- Servi√ßos financeiros com requisitos de conformidade
- Sistemas governamentais com necessidades de soberania de dados

‚úÖ **Otimizar para Ambientes de Borda**
- Dispositivos IoT com recursos limitados
- Aplicativos m√≥veis offline-first
- Sistemas em tempo real de baixa lat√™ncia

‚úÖ **Projetar Arquiteturas Inteligentes**
- Sistemas de m√∫ltiplos agentes para fluxos de trabalho complexos
- Implanta√ß√µes h√≠bridas borda-nuvem
- Infraestrutura de IA otimizada para custos

‚úÖ **Liderar Iniciativas de IA na Borda**
- Avaliar a viabilidade de IA na borda para projetos
- Selecionar modelos e frameworks apropriados
- Arquitetar solu√ß√µes locais de IA escal√°veis

---

## üó∫Ô∏è Estrutura do Workshop

### Vis√£o Geral das Sess√µes (6 Sess√µes √ó 30 Minutos = 3 Horas)

| Sess√£o | T√≥pico | Foco | Dura√ß√£o |
|--------|--------|------|---------|
| **1** | Come√ßando com o Foundry Local | Instalar, validar, primeiras conclus√µes | 30 min |
| **2** | Construindo Solu√ß√µes de IA com RAG | Engenharia de prompts, embeddings, avalia√ß√£o | 30 min |
| **3** | Modelos Open Source | Descoberta, benchmarking, sele√ß√£o de modelos | 30 min |
| **4** | Modelos de Ponta | SLM vs LLM, otimiza√ß√£o, frameworks | 30 min |
| **5** | Agentes com IA | Design de agentes, orquestra√ß√£o, mem√≥ria | 30 min |
| **6** | Modelos como Ferramentas | Roteamento, encadeamento, estrat√©gias de escalabilidade | 30 min |

---

## üöÄ In√≠cio R√°pido

### Pr√©-requisitos

**Requisitos do Sistema:**
- **SO**: Windows 10/11, macOS 11+ ou Linux (Ubuntu 20.04+)
- **RAM**: M√≠nimo de 8GB, recomendado 16GB+
- **Armazenamento**: 10GB+ de espa√ßo livre para modelos
- **CPU**: Processador moderno com suporte AVX2
- **GPU** (opcional): Compat√≠vel com CUDA ou Qualcomm NPU para acelera√ß√£o

**Requisitos de Software:**
- **Python 3.8+** ([Download](https://www.python.org/downloads/))
- **Microsoft Foundry Local** ([Guia de Instala√ß√£o](../../../Workshop))
- **Git** ([Download](https://git-scm.com/downloads))
- **Visual Studio Code** (recomendado) ([Download](https://code.visualstudio.com/))

### Configura√ß√£o em 3 Passos

#### 1. Instale o Foundry Local

**Windows:**
```powershell
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**Verifique a Instala√ß√£o:**
```bash
foundry --version
foundry service status
```

#### 2. Clone o Reposit√≥rio e Instale Depend√™ncias

```bash
# Clone repository
git clone https://github.com/microsoft/edgeai-for-beginners.git
cd edgeai-for-beginners/Workshop

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.\.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

#### 3. Execute Seu Primeiro Exemplo

```bash
# Start Foundry Local and load a model
foundry model run phi-4-mini

# Run the chat bootstrap sample
cd samples/session01
python chat_bootstrap.py "What is edge AI?"
```

**‚úÖ Sucesso!** Voc√™ deve ver uma resposta em streaming sobre IA na borda.

---

## üì¶ Recursos do Workshop

### Exemplos em Python

Exemplos pr√°ticos e progressivos demonstrando cada conceito:

| Sess√£o | Exemplo | Descri√ß√£o | Tempo de Execu√ß√£o |
|--------|---------|-----------|-------------------|
| 1 | [`chat_bootstrap.py`](../../../Workshop/samples/session01/chat_bootstrap.py) | Chat b√°sico e streaming | ~30s |
| 2 | [`rag_pipeline.py`](../../../Workshop/samples/session02/rag_pipeline.py) | RAG com embeddings | ~45s |
| 2 | [`rag_eval_ragas.py`](../../../Workshop/samples/session02/rag_eval_ragas.py) | Avalia√ß√£o de qualidade RAG | ~60s |
| 3 | [`benchmark_oss_models.py`](../../../Workshop/samples/session03/benchmark_oss_models.py) | Benchmark de m√∫ltiplos modelos | ~2-3m |
| 4 | [`model_compare.py`](../../../Workshop/samples/session04/model_compare.py) | Compara√ß√£o SLM vs LLM | ~45s |
| 5 | [`agents_orchestrator.py`](../../../Workshop/samples/session05/agents_orchestrator.py) | Sistema de m√∫ltiplos agentes | ~60s |
| 6 | [`models_router.py`](../../../Workshop/samples/session06/models_router.py) | Roteamento baseado em inten√ß√£o | ~45s |
| 6 | [`models_pipeline.py`](../../../Workshop/samples/session06/models_pipeline.py) | Pipeline de m√∫ltiplas etapas | ~60s |

### Notebooks Jupyter

Explora√ß√£o interativa com explica√ß√µes e visualiza√ß√µes:

| Sess√£o | Notebook | Descri√ß√£o | Dificuldade |
|--------|----------|-----------|-------------|
| 1 | [`session01_chat_bootstrap.ipynb`](./notebooks/session01_chat_bootstrap.ipynb) | B√°sico de chat e streaming | ‚≠ê Iniciante |
| 2 | [`session02_rag_pipeline.ipynb`](./notebooks/session02_rag_pipeline.ipynb) | Construir sistema RAG | ‚≠ê‚≠ê Intermedi√°rio |
| 2 | [`session02_rag_eval_ragas.ipynb`](./notebooks/session02_rag_eval_ragas.ipynb) | Avaliar qualidade RAG | ‚≠ê‚≠ê Intermedi√°rio |
| 3 | [`session03_benchmark_oss_models.ipynb`](./notebooks/session03_benchmark_oss_models.ipynb) | Benchmark de modelos | ‚≠ê‚≠ê Intermedi√°rio |
| 4 | [`session04_model_compare.ipynb`](./notebooks/session04_model_compare.ipynb) | Compara√ß√£o de modelos | ‚≠ê‚≠ê Intermedi√°rio |
| 5 | [`session05_agents_orchestrator.ipynb`](./notebooks/session05_agents_orchestrator.ipynb) | Orquestra√ß√£o de agentes | ‚≠ê‚≠ê‚≠ê Avan√ßado |
| 6 | [`session06_models_router.ipynb`](./notebooks/session06_models_router.ipynb) | Roteamento por inten√ß√£o | ‚≠ê‚≠ê‚≠ê Avan√ßado |
| 6 | [`session06_models_pipeline.ipynb`](./notebooks/session06_models_pipeline.ipynb) | Orquestra√ß√£o de pipeline | ‚≠ê‚≠ê‚≠ê Avan√ßado |

### Documenta√ß√£o

Guias e refer√™ncias abrangentes:

| Documento | Descri√ß√£o | Quando Usar |
|-----------|-----------|-------------|
| [QUICK_START.md](./QUICK_START.md) | Guia de configura√ß√£o r√°pida | Come√ßando do zero |
| [QUICK_REFERENCE.md](./QUICK_REFERENCE.md) | Resumo de comandos e APIs | Precisa de respostas r√°pidas |
| [FOUNDRY_SDK_QUICKREF.md](./FOUNDRY_SDK_QUICKREF.md) | Padr√µes e exemplos do SDK | Escrevendo c√≥digo |
| [ENV_CONFIGURATION.md](./ENV_CONFIGURATION.md) | Guia de vari√°veis de ambiente | Configurando exemplos |
| [SAMPLES_UPDATE_SUMMARY.md](./SAMPLES_UPDATE_SUMMARY.md) | Melhorias recentes nos exemplos | Entendendo mudan√ßas |
| [SDK_MIGRATION_NOTES.md](./SDK_MIGRATION_NOTES.md) | Guia de migra√ß√£o | Atualizando c√≥digo |
| [notebooks/TROUBLESHOOTING.md](./notebooks/TROUBLESHOOTING.md) | Problemas comuns e solu√ß√µes | Depurando problemas |

---

## üéì Recomenda√ß√µes de Caminho de Aprendizado

### Para Iniciantes (3-4 horas)
1. ‚úÖ Sess√£o 1: Come√ßando (foco na configura√ß√£o e chat b√°sico)
2. ‚úÖ Sess√£o 2: B√°sicos de RAG (pule a avalia√ß√£o inicialmente)
3. ‚úÖ Sess√£o 3: Benchmarking simples (apenas 2 modelos)
4. ‚è≠Ô∏è Pule as Sess√µes 4-6 por enquanto
5. üîÑ Retorne √†s Sess√µes 4-6 ap√≥s construir a primeira aplica√ß√£o

### Para Desenvolvedores Intermedi√°rios (3 horas)
1. ‚ö° Sess√£o 1: Valida√ß√£o r√°pida da configura√ß√£o
2. ‚úÖ Sess√£o 2: Pipeline completo de RAG com avalia√ß√£o
3. ‚úÖ Sess√£o 3: Su√≠te completa de benchmarking
4. ‚úÖ Sess√£o 4: Otimiza√ß√£o de modelos
5. ‚úÖ Sess√µes 5-6: Foco em padr√µes de arquitetura

### Para Praticantes Avan√ßados (2-3 horas)
1. ‚ö° Sess√µes 1-3: Revis√£o e valida√ß√£o r√°pidas
2. ‚úÖ Sess√£o 4: Mergulho profundo na otimiza√ß√£o
3. ‚úÖ Sess√£o 5: Arquitetura de m√∫ltiplos agentes
4. ‚úÖ Sess√£o 6: Padr√µes de produ√ß√£o e escalabilidade
5. üöÄ Extens√£o: Construa l√≥gica de roteamento personalizada e implanta√ß√µes h√≠bridas

---

## Pacote de Sess√µes do Workshop (Laborat√≥rios Focados de 30 Minutos)

Se voc√™ estiver seguindo o formato condensado de 6 sess√µes do workshop, use estes guias dedicados (cada um mapeia e complementa os m√≥dulos mais amplos acima):

| Sess√£o do Workshop | Guia | Foco Principal |
|--------------------|------|----------------|
| 1 | [Session01-GettingStartedFoundryLocal](./Session01-GettingStartedFoundryLocal.md) | Instalar, validar, rodar phi & GPT-OSS-20B, acelera√ß√£o |
| 2 | [Session02-BuildAISolutionsRAG](./Session02-BuildAISolutionsRAG.md) | Engenharia de prompts, padr√µes RAG, grounding em CSV e documentos, migra√ß√£o |
| 3 | [Session03-OpenSourceModels](./Session03-OpenSourceModels.md) | Integra√ß√£o com Hugging Face, benchmarking, sele√ß√£o de modelos |
| 4 | [Session04-CuttingEdgeModels](./Session04-CuttingEdgeModels.md) | SLM vs LLM, WebGPU, Chainlit RAG, acelera√ß√£o ONNX |
| 5 | [Session05-AIPoweredAgents](./Session05-AIPoweredAgents.md) | Pap√©is de agentes, mem√≥ria, ferramentas, orquestra√ß√£o |
| 6 | [Session06-ModelsAsTools](./Session06-ModelsAsTools.md) | Roteamento, encadeamento, caminho de escalabilidade para Azure |
Cada arquivo de sess√£o inclui: resumo, objetivos de aprendizado, fluxo de demonstra√ß√£o de 30 minutos, projeto inicial, lista de verifica√ß√£o de valida√ß√£o, solu√ß√£o de problemas e refer√™ncias ao SDK oficial Foundry Local Python.

### Scripts de Exemplo

Instalar depend√™ncias do workshop (Windows):

```powershell
cd Workshop
py -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt
```

macOS / Linux:

```bash
cd Workshop
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Se estiver executando o servi√ßo Foundry Local em uma m√°quina ou VM diferente (Windows) a partir de macOS, exporte o endpoint:

```bash
export FOUNDRY_LOCAL_ENDPOINT=http://<windows-host>:5273/v1
```

| Sess√£o | Script(s) | Descri√ß√£o |
|--------|-----------|-----------|
| 1 | `samples/session01/chat_bootstrap.py` | Servi√ßo inicial e chat em streaming |
| 2 | `samples/session02/rag_pipeline.py` | RAG m√≠nimo (embeddings em mem√≥ria) |
|   | `samples/session02/rag_eval_ragas.py` | Avalia√ß√£o de RAG com m√©tricas ragas |
| 3 | `samples/session03/benchmark_oss_models.py` | Benchmark de lat√™ncia e throughput multi-modelo |
| 4 | `samples/session04/model_compare.py` | Compara√ß√£o SLM vs LLM (lat√™ncia e sa√≠da de exemplo) |
| 5 | `samples/session05/agents_orchestrator.py` | Pipeline de pesquisa ‚Üí editorial com dois agentes |
| 6 | `samples/session06/models_router.py` | Demonstra√ß√£o de roteamento baseado em inten√ß√£o |
|   | `samples/session06/models_pipeline.py` | Cadeia de planejamento/execu√ß√£o/refinamento em m√∫ltiplas etapas |

### Vari√°veis de Ambiente (Comuns Entre Exemplos)

| Vari√°vel | Finalidade | Exemplo |
|----------|------------|---------|
| `FOUNDRY_LOCAL_ALIAS` | Alias padr√£o de modelo √∫nico para exemplos b√°sicos | `phi-4-mini` |
| `SLM_ALIAS` / `LLM_ALIAS` | Modelos expl√≠citos SLM vs maiores para compara√ß√£o | `phi-4-mini` / `gpt-oss-20b` |
| `BENCH_MODELS` | Lista de aliases para benchmark | `qwen2.5-0.5b,gemma-2-2b,mistral-7b` |
| `BENCH_ROUNDS` | Repeti√ß√µes de benchmark por modelo | `3` |
| `BENCH_PROMPT` | Prompt usado no benchmark | `Explain retrieval augmented generation briefly.` |
| `EMBED_MODEL` | Modelo de embeddings do Sentence-transformers | `sentence-transformers/all-MiniLM-L6-v2` |
| `RAG_QUESTION` | Substituir consulta de teste para pipeline RAG | `Why use RAG with local inference?` |
| `AGENT_QUESTION` | Substituir consulta para pipeline de agentes | `Explain why edge AI matters for compliance.` |
| `AGENT_MODEL_PRIMARY` | Alias de modelo para agente de pesquisa | `phi-4-mini` |
| `AGENT_MODEL_EDITOR` | Alias de modelo para agente editor (pode ser diferente) | `gpt-oss-20b` |
| `SHOW_USAGE` | Quando `1`, imprime uso de tokens por conclus√£o | `1` |
| `RETRY_ON_FAIL` | Quando `1`, tenta novamente em erros transit√≥rios de chat | `1` |
| `RETRY_BACKOFF` | Segundos de espera antes de tentar novamente | `1.0` |

Se uma vari√°vel n√£o estiver definida, os scripts usam valores padr√£o sensatos. Para demonstra√ß√µes de modelo √∫nico, normalmente voc√™ s√≥ precisa de `FOUNDRY_LOCAL_ALIAS`.

### M√≥dulo Utilit√°rio

Todos os exemplos agora compartilham um helper `samples/workshop_utils.py` que fornece:

* Cria√ß√£o de cliente OpenAI + `FoundryLocalManager` com cache
* Helper `chat_once()` com op√ß√£o de retry + impress√£o de uso
* Relat√≥rio simples de uso de tokens (ativado via `SHOW_USAGE=1`)

Isso reduz duplica√ß√£o e destaca boas pr√°ticas para orquestra√ß√£o eficiente de modelos locais.

## Melhorias Opcionais (Entre Sess√µes)

| Tema | Melhoria | Sess√µes | Env / Alternador |
|------|----------|---------|------------------|
| Determinismo | Temperatura fixa + conjuntos de prompts est√°veis | 1‚Äì6 | Definir `temperature=0`, `top_p=1` |
| Visibilidade de Uso de Tokens | Ensino consistente de custo/efici√™ncia | 1‚Äì6 | `SHOW_USAGE=1` |
| Primeiro Token em Streaming | M√©trica de lat√™ncia percebida | 1,3,4,6 | `BENCH_STREAM=1` (benchmark) |
| Resili√™ncia de Retry | Lida com inicializa√ß√£o fria transit√≥ria | Todas | `RETRY_ON_FAIL=1` + `RETRY_BACKOFF` |
| Agentes Multi-Modelo | Especializa√ß√£o de pap√©is heterog√™neos | 5 | `AGENT_MODEL_PRIMARY`, `AGENT_MODEL_EDITOR` |
| Roteamento Adaptativo | Heur√≠sticas de inten√ß√£o + custo | 6 | Estender roteador com l√≥gica de escalonamento |
| Mem√≥ria Vetorial | Recall sem√¢ntico de longo prazo | 2,5,6 | Integrar √≠ndice de embeddings FAISS/Chroma |
| Exporta√ß√£o de Tra√ßos | Auditoria e avalia√ß√£o | 2,5,6 | Adicionar linhas JSON por etapa |
| Rubricas de Qualidade | Rastreamento qualitativo | 3‚Äì6 | Prompts de pontua√ß√£o secund√°ria |
| Testes de Fuma√ßa | Valida√ß√£o r√°pida pr√©-workshop | Todas | `python Workshop/tests/smoke.py` |

### In√≠cio R√°pido Determin√≠stico

```powershell
set FOUNDRY_LOCAL_ALIAS=phi-4-mini
set SHOW_USAGE=1
python Workshop\tests\smoke.py
```

Espere contagens de tokens est√°veis em entradas id√™nticas repetidas.

### Avalia√ß√£o de RAG (Sess√£o 2)

Use `rag_eval_ragas.py` para calcular relev√¢ncia da resposta, fidelidade e precis√£o do contexto em um pequeno conjunto de dados sint√©tico:

```powershell
python samples/session02/rag_eval_ragas.py
```

Amplie fornecendo um JSONL maior de perguntas, contextos e verdades base, convertendo para um `Dataset` do Hugging Face.

## Ap√™ndice de Precis√£o de Comandos CLI

O workshop utiliza deliberadamente apenas comandos CLI Foundry Local atualmente documentados/est√°veis.

### Comandos Est√°veis Referenciados

| Categoria | Comando | Finalidade |
|-----------|---------|-----------|
| Core | `foundry --version` | Mostrar vers√£o instalada |
| Core | `foundry init` | Inicializar configura√ß√£o |
| Servi√ßo | `foundry service start` | Iniciar servi√ßo local (se n√£o autom√°tico) |
| Servi√ßo | `foundry status` | Mostrar status do servi√ßo |
| Modelos | `foundry model list` | Listar cat√°logo / modelos dispon√≠veis |
| Modelos | `foundry model download <alias>` | Baixar pesos do modelo para cache |
| Modelos | `foundry model run <alias>` | Executar (carregar) um modelo localmente; combinar com `--prompt` para execu√ß√£o √∫nica |
| Modelos | `foundry model unload <alias>` / `foundry model stop <alias>` | Descarregar um modelo da mem√≥ria (se suportado) |
| Cache | `foundry cache list` | Listar modelos em cache (baixados) |
| Sistema | `foundry system info` | Snapshot de capacidades de hardware e acelera√ß√£o |
| Sistema | `foundry system gpu-info` | Informa√ß√µes de diagn√≥stico de GPU |
| Configura√ß√£o | `foundry config list` | Mostrar valores de configura√ß√£o atuais |
| Configura√ß√£o | `foundry config set <key> <value>` | Atualizar configura√ß√£o |

### Padr√£o de Prompt √önico

Em vez de um subcomando `model chat` obsoleto, use:

```powershell
foundry model run <alias> --prompt "Your question here"
```

Isso executa um ciclo √∫nico de prompt/resposta e ent√£o sai.

### Padr√µes Removidos / Evitados

| Obsoleto / N√£o Documentado | Substitui√ß√£o / Orienta√ß√£o |
|----------------------------|--------------------------|
| `foundry model chat <model> "..."` | `foundry model run <model> --prompt "..."` |
| `foundry model list --running` | Use `foundry model list` simples + atividade recente / logs |
| `foundry model list --cached` | `foundry cache list` |
| `foundry model stats <model>` | Use script de benchmark Python + ferramentas do sistema operacional (Gerenciador de Tarefas / `nvidia-smi`) |
| `foundry model benchmark ...` | `samples/session03/benchmark_oss_models.py` |

### Benchmarking e Telemetria

- Lat√™ncia, p95, tokens/segundo: `samples/session03/benchmark_oss_models.py`
- Lat√™ncia do primeiro token (streaming): definir `BENCH_STREAM=1`
- Uso de recursos: monitores do sistema operacional (Gerenciador de Tarefas, Monitor de Atividade, `nvidia-smi`) + `foundry system info`.

√Ä medida que novos comandos de telemetria CLI se estabilizam, podem ser incorporados com edi√ß√µes m√≠nimas aos markdowns das sess√µes.

### Linter Autom√°tico de CLI

Um linter automatizado impede a reintrodu√ß√£o de padr√µes CLI obsoletos dentro de blocos de c√≥digo cercados em arquivos markdown:

Script: `Workshop/scripts/lint_markdown_cli.py`

Padr√µes obsoletos s√£o bloqueados dentro de cercas de c√≥digo.

Substitui√ß√µes recomendadas:
| Obsoleto | Substitui√ß√£o |
|----------|-------------|
| `foundry model chat <a> "..."` | `foundry model run <a> --prompt "..."` |
| `model list --running` | `model list` |
| `model list --cached` | `cache list` |
| `model stats` | Script de benchmark + ferramentas do sistema |
| `model benchmark` | `samples/session03/benchmark_oss_models.py` |
| `model list --available` | `model list` |

Executar localmente:
```powershell
python Workshop\scripts\lint_markdown_cli.py --verbose
```

GitHub Action: `.github/workflows/markdown-cli-lint.yml` executa em cada push e PR.

Hook opcional de pr√©-commit:
```bash
echo "python Workshop/scripts/lint_markdown_cli.py" > .git/hooks/pre-commit
chmod +x .git/hooks/pre-commit
```

## Tabela R√°pida de Migra√ß√£o CLI ‚Üí SDK

| Tarefa | CLI One-Liner | Equivalente SDK (Python) | Notas |
|--------|---------------|--------------------------|-------|
| Executar um modelo uma vez (prompt) | `foundry model run phi-4-mini --prompt "Hello"` | `manager=FoundryLocalManager("phi-4-mini"); client=OpenAI(base_url=manager.endpoint, api_key=manager.api_key or "not-needed"); client.chat.completions.create(model=manager.get_model_info("phi-4-mini").id, messages=[{"role":"user","content":"Hello"}])` | SDK inicializa servi√ßo e cache automaticamente |
| Baixar (cache) modelo | `foundry model download qwen2.5-0.5b` | `FoundryLocalManager("qwen2.5-0.5b")  # triggers download/load` | Manager escolhe melhor variante se alias mapear para m√∫ltiplas builds |
| Listar cat√°logo | `foundry model list` | `# use manager for each alias or maintain known list` | CLI agrega; SDK atualmente por inst√¢ncia de alias |
| Listar modelos em cache | `foundry cache list` | `manager.list_cached_models()` | Ap√≥s inicializa√ß√£o do manager (qualquer alias) |
| Ativar acelera√ß√£o GPU | `foundry config set compute.onnx.enable_gpu true` | `# CLI action; SDK assumes config already applied` | Configura√ß√£o √© efeito colateral externo |
| Obter URL do endpoint | (impl√≠cito) | `manager.endpoint` | Usado para criar cliente compat√≠vel com OpenAI |
| Aquecer um modelo | `foundry model run <alias>` ent√£o primeiro prompt | `chat_once(alias, messages=[...])` (utilit√°rio) | Utilit√°rios lidam com aquecimento inicial de lat√™ncia fria |
| Medir lat√™ncia | `python benchmark_oss_models.py` | `import benchmark_oss_models` (ou novo script exportador) | Prefira script para m√©tricas consistentes |
| Parar / descarregar modelo | `foundry model unload <alias>` | (N√£o exposto ‚Äì reiniciar servi√ßo/processo) | Normalmente n√£o necess√°rio para fluxo de workshop |
| Recuperar uso de tokens | (ver sa√≠da) | `resp.usage.total_tokens` | Fornecido se backend retornar objeto de uso |

## Exporta√ß√£o de Markdown de Benchmark

Use o script `Workshop/scripts/export_benchmark_markdown.py` para executar um benchmark novo (mesma l√≥gica de `samples/session03/benchmark_oss_models.py`) e emitir uma tabela Markdown amig√°vel ao GitHub mais JSON bruto.

### Exemplo

```powershell
python Workshop\scripts\export_benchmark_markdown.py --models "qwen2.5-0.5b,gemma-2-2b,mistral-7b" --prompt "Explain retrieval augmented generation briefly." --rounds 3 --output benchmark_report.md
```

Arquivos gerados:
| Arquivo | Conte√∫do |
|---------|----------|
| `benchmark_report.md` | Tabela Markdown + dicas de interpreta√ß√£o |
| `benchmark_report.json` | Array de m√©tricas brutas (para compara√ß√£o/rastreamento de tend√™ncias) |

Defina `BENCH_STREAM=1` no ambiente para incluir lat√™ncia do primeiro token, se suportado.

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte oficial. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.