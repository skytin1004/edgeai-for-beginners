<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6a574846c3919c56f1d02bf1de2003ca",
  "translation_date": "2025-09-30T23:07:53+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "ar"
}
-->
# Ø§Ù„Ø¬Ù„Ø³Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ù„Ø¨Ø¯Ø¡ Ù…Ø¹ Foundry Local

## Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©

ÙŠÙ‚Ø¯Ù… Microsoft Foundry Local Ø¥Ù…ÙƒØ§Ù†ÙŠØ§Øª Azure AI Foundry Ù…Ø¨Ø§Ø´Ø±Ø© Ø¥Ù„Ù‰ Ø¨ÙŠØ¦Ø© ØªØ·ÙˆÙŠØ± Windows 11 Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙƒØŒ Ù…Ù…Ø§ ÙŠØªÙŠØ­ ØªØ·ÙˆÙŠØ± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø®ØµÙˆØµÙŠØ© ÙˆØ²Ù…Ù† Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…Ù†Ø®ÙØ¶ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø¤Ø³Ø³Ø§Øª. ØªØºØ·ÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¬Ù„Ø³Ø© Ø§Ù„ØªØ«Ø¨ÙŠØª Ø§Ù„ÙƒØ§Ù…Ù„ØŒ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ØŒ ÙˆØ§Ù„Ù†Ø´Ø± Ø§Ù„Ø¹Ù…Ù„ÙŠ Ù„Ù†Ù…Ø§Ø°Ø¬ Ø´Ù‡ÙŠØ±Ø© Ù…Ø«Ù„ phiØŒ qwenØŒ deepseekØŒ ÙˆGPT-OSS-20B.

## Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…

Ø¨Ù†Ù‡Ø§ÙŠØ© Ù‡Ø°Ù‡ Ø§Ù„Ø¬Ù„Ø³Ø©ØŒ Ø³ØªØªÙ…ÙƒÙ† Ù…Ù†:
- ØªØ«Ø¨ÙŠØª ÙˆØ¥Ø¹Ø¯Ø§Ø¯ Foundry Local Ø¹Ù„Ù‰ Windows 11
- Ø¥ØªÙ‚Ø§Ù† Ø£ÙˆØ§Ù…Ø± CLI ÙˆØ®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯
- ÙÙ‡Ù… Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
- ØªØ´ØºÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ phiØŒ qwenØŒ deepseekØŒ ÙˆGPT-OSS-20B Ø¨Ù†Ø¬Ø§Ø­
- Ø¥Ù†Ø´Ø§Ø¡ Ø£ÙˆÙ„ ØªØ·Ø¨ÙŠÙ‚ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Foundry Local

## Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©

### Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
- **Windows 11**: Ø§Ù„Ø¥ØµØ¯Ø§Ø± 22H2 Ø£Ùˆ Ø£Ø­Ø¯Ø«
- **RAM**: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ 16GBØŒ ÙŠÙˆØµÙ‰ Ø¨Ù€ 32GB
- **Ø§Ù„ØªØ®Ø²ÙŠÙ†**: Ù…Ø³Ø§Ø­Ø© Ø®Ø§Ù„ÙŠØ© 50GB Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
- **Ø§Ù„Ø£Ø¬Ù‡Ø²Ø©**: ÙŠÙØ¶Ù„ Ø¬Ù‡Ø§Ø² Ù…Ø²ÙˆØ¯ Ø¨Ù€ NPU Ø£Ùˆ GPU (Ù…Ø«Ù„ Copilot+ PC Ø£Ùˆ NVIDIA GPU)
- **Ø§Ù„Ø´Ø¨ÙƒØ©**: Ø¥Ù†ØªØ±Ù†Øª Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø³Ø±Ø¹Ø© Ù„ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬

### Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ·ÙˆÙŠØ±
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion

# Set up Python environment (recommended)
py -m venv .venv
.venv\Scripts\activate

# Install required dependencies
pip install openai foundry-local-sdk
```

## Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø§Ù„ØªØ«Ø¨ÙŠØª ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯

### Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ«Ø¨ÙŠØª Foundry Local

Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª Foundry Local Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Winget Ø£Ùˆ Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ø«Ø¨Øª Ù…Ù† GitHub:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ«Ø¨ÙŠØª

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: ÙÙ‡Ù… CLI

### Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù„Ø«: Ø¥Ø¯Ø§Ø±Ø© ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬

ÙŠØ³ØªØ®Ø¯Ù… Foundry Local ØªØ®Ø²ÙŠÙ†Ù‹Ø§ Ø°ÙƒÙŠÙ‹Ø§ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„ØªØ®Ø²ÙŠÙ†:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø±Ø§Ø¨Ø¹: Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ø¹Ù…Ù„ÙŠ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬

### ØªØ´ØºÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Microsoft Phi

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### Ø§Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ù†Ù…Ø§Ø°Ø¬ Qwen

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b
foundry model run qwen2.5-14b
```

### ØªØ´ØºÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ DeepSeek

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-7b
```

### ØªØ´ØºÙŠÙ„ GPT-OSS-20B

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§Ù…Ø³: Ø¥Ù†Ø´Ø§Ø¡ Ø£ÙˆÙ„ ØªØ·Ø¨ÙŠÙ‚

### ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø±Ø¯Ø´Ø© Ø­Ø¯ÙŠØ« (OpenAI SDK + Foundry Local)

Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø±Ø¯Ø´Ø© Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¥Ù†ØªØ§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAI SDK Ù…Ø¹ ØªÙƒØ§Ù…Ù„ Foundry LocalØŒ Ø¨Ø§ØªØ¨Ø§Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ù…Ù† Ù†Ù…ÙˆØ°Ø¬Ù†Ø§ Sample 01.

```python
# chat_quickstart.py (Sample 01 pattern)
import os
import sys
from openai import OpenAI

try:
    from foundry_local import FoundryLocalManager
    FOUNDRY_SDK_AVAILABLE = True
except ImportError:
    FOUNDRY_SDK_AVAILABLE = False
    print("âš ï¸ Install foundry-local-sdk: pip install foundry-local-sdk")

def create_client():
    """Create OpenAI client with Foundry Local or Azure OpenAI."""
    # Check for Azure OpenAI configuration
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    
    if azure_endpoint and azure_api_key:
        # Azure OpenAI path
        model = os.environ.get("MODEL", "your-deployment-name")
        client = OpenAI(
            base_url=f"{azure_endpoint}/openai",
            api_key=azure_api_key,
            default_query={"api-version": "2024-08-01-preview"},
        )
        print(f"ğŸŒ Using Azure OpenAI with model: {model}")
        return client, model
    
    # Foundry Local path with SDK management
    alias = os.environ.get("MODEL", "phi-4-mini")
    if FOUNDRY_SDK_AVAILABLE:
        try:
            # Use FoundryLocalManager for proper service management
            manager = FoundryLocalManager(alias)
            model_info = manager.get_model_info(alias)
            
            client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            model = model_info.id
            print(f"ğŸ  Using Foundry Local SDK with model: {model}")
            return client, model
        except Exception as e:
            print(f"âš ï¸ Foundry SDK failed ({e}), using manual configuration")
    
    # Fallback to manual configuration
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    api_key = os.environ.get("API_KEY", "")
    model = alias
    
    client = OpenAI(
        base_url=f"{base_url}/v1",
        api_key=api_key
    )
    print(f"ğŸ”§ Manual configuration with model: {model}")
    return client, model

def main():
    """Main chat function."""
    client, model = create_client()
    
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    conversation_history = []
    
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        
        try:
            # Add user message to history
            conversation_history.append({"role": "user", "content": user_input})
            
            # Create chat completion
            response = client.chat.completions.create(
                model=model,
                messages=conversation_history,
                max_tokens=500,
                temperature=0.7
            )
            
            assistant_message = response.choices[0].message.content
            conversation_history.append({"role": "assistant", "content": assistant_message})
            
            print(f"Assistant: {assistant_message}\n")
            
        except Exception as e:
            print(f"Error: {e}\n")

if __name__ == "__main__":
    main()
```

### ØªØ´ØºÙŠÙ„ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Option 1: Using FoundryLocalManager (recommended)
python chat_quickstart.py "Explain what Foundry Local is"

# Option 2: Manual configuration with environment variables
set BASE_URL=http://localhost:8000
set MODEL=phi-4-mini
set API_KEY=
python chat_quickstart.py "Write a welcome message"

# Option 3: Azure OpenAI configuration
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name
python chat_quickstart.py "Hello from Azure OpenAI"
```

## Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø³Ø§Ø¯Ø³: Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ¥ØµÙ„Ø§Ø­Ù‡Ø§ ÙˆØ£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª

### Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙˆØ­Ù„ÙˆÙ„Ù‡Ø§

```powershell
# Issue: "Could not use Foundry SDK" warning
pip install foundry-local-sdk
# Or set environment variables for manual configuration

# Issue: Connection refused
foundry service status
foundry service ps  # Check loaded models

# Issue: Model not found
foundry model list
foundry model run phi-4-mini

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal

# Test API endpoint
curl http://localhost:8000/v1/models
```

### Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù… (Windows)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©

| Ø§Ù„Ù…ØªØºÙŠØ± | Ø§Ù„ÙˆØµÙ | Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ | Ù…Ø·Ù„ÙˆØ¨ |
|---------|-------|-----------|--------|
| `MODEL` | Ø§Ø³Ù… Ø£Ùˆ Ø§Ø®ØªØµØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ | `phi-4-mini` | Ù„Ø§ |
| `BASE_URL` | Ø¹Ù†ÙˆØ§Ù† Foundry Local Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ | `http://localhost:8000` | Ù„Ø§ |
| `API_KEY` | Ù…ÙØªØ§Ø­ API (Ø¹Ø§Ø¯Ø©Ù‹ ØºÙŠØ± Ù…Ø·Ù„ÙˆØ¨ Ù…Ø­Ù„ÙŠÙ‹Ø§) | `""` | Ù„Ø§ |
| `AZURE_OPENAI_ENDPOINT` | Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© Azure OpenAI | - | Ù„Ù€ Azure |
| `AZURE_OPENAI_API_KEY` | Ù…ÙØªØ§Ø­ API Ù„Ù€ Azure OpenAI | - | Ù„Ù€ Azure |
| `AZURE_OPENAI_API_VERSION` | Ø¥ØµØ¯Ø§Ø± API Ù„Ù€ Azure | `2024-08-01-preview` | Ù„Ø§ |

### Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª

- **Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAI SDK**: ÙŠÙØ¶Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAI SDK Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø·Ù„Ø¨Ø§Øª HTTP Ø§Ù„Ø®Ø§Ù… Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙŠØ§Ù†Ø©
- **FoundryLocalManager**: Ø§Ø³ØªØ®Ø¯Ù… SDK Ø§Ù„Ø±Ø³Ù…ÙŠ Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø¹Ù†Ø¯ ØªÙˆÙØ±Ù‡
- **Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡**: Ù‚Ù… Ø¨ØªÙ†ÙÙŠØ° Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©
- **Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¨Ø§Ù†ØªØ¸Ø§Ù…**: Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠØ« Foundry Local Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù†Ù…Ø§Ø°Ø¬ ÙˆØ¥ØµÙ„Ø§Ø­Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©
- **Ø§Ø¨Ø¯Ø£ ØµØºÙŠØ±Ù‹Ø§**: Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØµØºÙŠØ±Ø© (Phi miniØŒ Qwen 7B) Ø«Ù… Ù‚Ù… Ø¨Ø§Ù„ØªÙˆØ³Ø¹
- **Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯**: ØªØªØ¨Ø¹ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ©/ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª/Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø£Ø«Ù†Ø§Ø¡ Ø¶Ø¨Ø· Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø§Øª ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª

## Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø³Ø§Ø¨Ø¹: Ø§Ù„ØªÙ…Ø§Ø±ÙŠÙ† Ø§Ù„Ø¹Ù…Ù„ÙŠØ©

### Ø§Ù„ØªÙ…Ø±ÙŠÙ† 1: Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### Ø§Ù„ØªÙ…Ø±ÙŠÙ† 2: Ø§Ø®ØªØ¨Ø§Ø± ØªÙƒØ§Ù…Ù„ OpenAI SDK

```python
# sdk_integration_test.py (matching Sample 01 pattern)
import os
from openai import OpenAI
from foundry_local import FoundryLocalManager

def test_model_integration(model_alias):
    """Test OpenAI SDK integration with different models."""
    try:
        # Use FoundryLocalManager for proper setup
        manager = FoundryLocalManager(model_alias)
        model_info = manager.get_model_info(model_alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Test basic completion
        response = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Say hello and state your model name."}],
            max_tokens=50
        )
        
        print(f"âœ… {model_alias}: {response.choices[0].message.content}")
        return True
    except Exception as e:
        print(f"âŒ {model_alias}: {e}")
        return False

# Test multiple models
models_to_test = ["phi-4-mini", "qwen2.5-7b"]
for model in models_to_test:
    test_model_integration(model)
```

### Ø§Ù„ØªÙ…Ø±ÙŠÙ† 3: ÙØ­Øµ Ø´Ø§Ù…Ù„ Ù„ØµØ­Ø© Ø§Ù„Ø®Ø¯Ù…Ø©

```python
# health_check.py
from openai import OpenAI
from foundry_local import FoundryLocalManager

def comprehensive_health_check():
    """Perform comprehensive health check of Foundry Local service."""
    try:
        # Initialize with a common model
        manager = FoundryLocalManager("phi-4-mini")
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # 1. Check service connectivity
        models_response = client.models.list()
        available_models = [model.id for model in models_response.data]
        print(f"âœ… Service healthy - {len(available_models)} models available")
        
        # 2. Test each available model
        for model_id in available_models:
            try:
                response = client.chat.completions.create(
                    model=model_id,
                    messages=[{"role": "user", "content": "Test"}],
                    max_tokens=10
                )
                print(f"âœ… {model_id}: Working")
            except Exception as e:
                print(f"âŒ {model_id}: {e}")
        
        return True
    except Exception as e:
        print(f"âŒ Service check failed: {e}")
        return False

comprehensive_health_check()
```

## Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹

- **Ø§Ù„Ø¨Ø¯Ø¡ Ù…Ø¹ Foundry Local**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- **Ù…Ø±Ø¬Ø¹ CLI ÙˆÙ†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙˆØ§Ù…Ø±**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- **ØªÙƒØ§Ù…Ù„ OpenAI SDK**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- **ØªØ¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Hugging Face**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- **GitHub Ù„Ù€ Microsoft Foundry Local**: https://github.com/microsoft/Foundry-Local
- **OpenAI Python SDK**: https://github.com/openai/openai-python
- **Sample 01: Ø¯Ø±Ø¯Ø´Ø© Ø³Ø±ÙŠØ¹Ø© Ø¹Ø¨Ø± OpenAI SDK**: samples/01/README.md
- **Sample 02: ØªÙƒØ§Ù…Ù„ SDK Ù…ØªÙ‚Ø¯Ù…**: samples/02/README.md

---

**Ø¥Ø®Ù„Ø§Ø¡ Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ©**:  
ØªÙ… ØªØ±Ø¬Ù…Ø© Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ [Co-op Translator](https://github.com/Azure/co-op-translator). Ø¨ÙŠÙ†Ù…Ø§ Ù†Ø³Ø¹Ù‰ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¯Ù‚Ø©ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¹Ù„Ù… Ø£Ù† Ø§Ù„ØªØ±Ø¬Ù…Ø§Øª Ø§Ù„Ø¢Ù„ÙŠØ© Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ø®Ø·Ø§Ø¡ Ø£Ùˆ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ø¯Ù‚ÙŠÙ‚Ø©. ÙŠØ¬Ø¨ Ø§Ø¹ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø§Ù„Ø£ØµÙ„ÙŠ Ø¨Ù„ØºØªÙ‡ Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø±Ø³Ù…ÙŠ. Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­Ø§Ø³Ù…Ø©ØŒ ÙŠÙÙˆØµÙ‰ Ø¨Ø§Ù„Ø§Ø³ØªØ¹Ø§Ù†Ø© Ø¨ØªØ±Ø¬Ù…Ø© Ø¨Ø´Ø±ÙŠØ© Ø§Ø­ØªØ±Ø§ÙÙŠØ©. Ù†Ø­Ù† ØºÙŠØ± Ù…Ø³Ø¤ÙˆÙ„ÙŠÙ† Ø¹Ù† Ø£ÙŠ Ø³ÙˆØ¡ ÙÙ‡Ù… Ø£Ùˆ ØªÙØ³ÙŠØ±Ø§Øª Ø®Ø§Ø·Ø¦Ø© Ù†Ø§ØªØ¬Ø© Ø¹Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„ØªØ±Ø¬Ù…Ø©.