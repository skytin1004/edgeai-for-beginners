<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "27be883865b4bad1e3c7e02c696da642",
  "translation_date": "2025-09-19T01:38:54+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "hr"
}
-->
# Odjeljak 1: Napredno uÄenje SLM-a - Osnove i optimizacija

Mali jeziÄni modeli (SLM) predstavljaju kljuÄni napredak u EdgeAI-u, omoguÄ‡ujuÄ‡i sofisticirane sposobnosti obrade prirodnog jezika na ureÄ‘ajima s ograniÄenim resursima. Razumijevanje kako uÄinkovito implementirati, optimizirati i koristiti SLM-ove kljuÄno je za izgradnju praktiÄnih AI rjeÅ¡enja temeljenih na rubnim ureÄ‘ajima.

## Uvod

U ovoj lekciji istraÅ¾it Ä‡emo male jeziÄne modele (SLM) i njihove napredne strategije implementacije. Pokrit Ä‡emo osnovne koncepte SLM-ova, njihove parametarske granice i klasifikacije, tehnike optimizacije te praktiÄne strategije implementacije za okruÅ¾enja rubnog raÄunalstva.

## Ciljevi uÄenja

Na kraju ove lekcije moÄ‡i Ä‡ete:

- ğŸ”¢ Razumjeti parametarske granice i klasifikacije malih jeziÄnih modela.
- ğŸ› ï¸ Identificirati kljuÄne tehnike optimizacije za implementaciju SLM-ova na rubnim ureÄ‘ajima.
- ğŸš€ NauÄiti primijeniti napredne strategije kvantizacije i kompresije za SLM-ove.

## Razumijevanje parametarskih granica i klasifikacija SLM-ova

Mali jeziÄni modeli (SLM) su AI modeli dizajnirani za obradu, razumijevanje i generiranje sadrÅ¾aja prirodnog jezika s znaÄajno manje parametara u usporedbi s njihovim velikim pandanima. Dok veliki jeziÄni modeli (LLM) sadrÅ¾e stotine milijardi do trilijuna parametara, SLM-ovi su posebno dizajnirani za uÄinkovitost i implementaciju na rubnim ureÄ‘ajima.

Okvir klasifikacije parametara pomaÅ¾e nam razumjeti razliÄite kategorije SLM-ova i njihove odgovarajuÄ‡e sluÄajeve upotrebe. Ova klasifikacija je kljuÄna za odabir pravog modela za specifiÄne scenarije rubnog raÄunalstva.

### Okvir klasifikacije parametara

Razumijevanje parametarskih granica pomaÅ¾e u odabiru odgovarajuÄ‡ih modela za razliÄite scenarije rubnog raÄunalstva:

- **ğŸ”¬ Mikro SLM-ovi**: 100M - 1.4B parametara (ultra-lagani za mobilne ureÄ‘aje)
- **ğŸ“± Mali SLM-ovi**: 1.5B - 13.9B parametara (uravnoteÅ¾ene performanse i uÄinkovitost)
- **âš–ï¸ Srednji SLM-ovi**: 14B - 30B parametara (pribliÅ¾avanje sposobnostima LLM-a uz odrÅ¾avanje uÄinkovitosti)

ToÄna granica ostaje fluidna u istraÅ¾ivaÄkoj zajednici, ali veÄ‡ina praktiÄara smatra modele s manje od 30 milijardi parametara "malima", dok neki izvori postavljaju prag Äak niÅ¾e, na 10 milijardi parametara.

### KljuÄne prednosti SLM-ova

SLM-ovi nude nekoliko temeljnih prednosti koje ih Äine idealnima za aplikacije rubnog raÄunalstva:

**Operativna uÄinkovitost**: SLM-ovi omoguÄ‡uju brÅ¾e vrijeme izvoÄ‘enja zbog manjeg broja parametara za obradu, Å¡to ih Äini idealnima za aplikacije u stvarnom vremenu. Zahtijevaju manje raÄunalnih resursa, omoguÄ‡ujuÄ‡i implementaciju na ureÄ‘ajima s ograniÄenim resursima uz manju potroÅ¡nju energije i smanjen ugljiÄni otisak.

**Fleksibilnost implementacije**: Ovi modeli omoguÄ‡uju AI sposobnosti na ureÄ‘aju bez potrebe za internetskom povezanoÅ¡Ä‡u, poboljÅ¡avaju privatnost i sigurnost lokalnom obradom, mogu se prilagoditi za aplikacije specifiÄne za odreÄ‘ene domene i prikladni su za razliÄita okruÅ¾enja rubnog raÄunalstva.

**Isplativost**: SLM-ovi nude isplativu obuku i implementaciju u usporedbi s LLM-ovima, uz smanjene operativne troÅ¡kove i niÅ¾e zahtjeve za propusnost za aplikacije na rubu.

## Napredne strategije stjecanja modela

### Ekosustav Hugging Face

Hugging Face sluÅ¾i kao primarno srediÅ¡te za otkrivanje i pristup najnaprednijim SLM-ovima. Platforma pruÅ¾a sveobuhvatne resurse za otkrivanje i implementaciju modela:

**ZnaÄajke otkrivanja modela**: Platforma nudi napredno filtriranje prema broju parametara, vrsti licence i metrikama performansi. Korisnici mogu pristupiti alatima za usporedbu modela, rezultatima evaluacije u stvarnom vremenu i WebGPU demonstracijama za trenutna testiranja.

**Kolekcije SLM-ova**: Popularni modeli ukljuÄuju Phi-4-mini-3.8B za zadatke naprednog zakljuÄivanja, seriju Qwen3 (0.6B/1.7B/4B) za viÅ¡ejeziÄne aplikacije, Google Gemma3 za uÄinkovite zadatke opÄ‡e namjene te eksperimentalne modele poput BitNET-a za ultra-nisku preciznost implementacije. Platforma takoÄ‘er sadrÅ¾i kolekcije voÄ‘ene zajednicom sa specijaliziranim modelima za specifiÄne domene te unaprijed obuÄene i varijante prilagoÄ‘ene instrukcijama optimizirane za razliÄite sluÄajeve upotrebe.

### Katalog modela Azure AI Foundry

Katalog modela Azure AI Foundry pruÅ¾a pristup SLM-ovima na razini poduzeÄ‡a s poboljÅ¡anim moguÄ‡nostima integracije:

**Integracija za poduzeÄ‡a**: Katalog ukljuÄuje modele koje Azure prodaje izravno s podrÅ¡kom na razini poduzeÄ‡a i SLA-ovima, ukljuÄujuÄ‡i Phi-4-mini-3.8B za napredne sposobnosti zakljuÄivanja i Llama 3-8B za produkcijsku implementaciju. TakoÄ‘er sadrÅ¾i modele poput Qwen3 8B od pouzdanih treÄ‡ih strana otvorenog koda.

**Prednosti za poduzeÄ‡a**: UgraÄ‘eni alati za fino podeÅ¡avanje, promatranje i odgovorni AI integrirani su s fleksibilnim Provisioned Throughputom meÄ‘u obiteljima modela. Izravna Microsoftova podrÅ¡ka s SLA-ovima na razini poduzeÄ‡a, integrirane znaÄajke sigurnosti i usklaÄ‘enosti te sveobuhvatni tijekovi implementacije poboljÅ¡avaju iskustvo poduzeÄ‡a.

## Napredne tehnike kvantizacije i optimizacije

### Llama.cpp okvir za optimizaciju

Llama.cpp pruÅ¾a najnaprednije tehnike kvantizacije za maksimalnu uÄinkovitost u implementaciji na rubu:

**Metode kvantizacije**: Okvir podrÅ¾ava razliÄite razine kvantizacije, ukljuÄujuÄ‡i Q4_0 (4-bitna kvantizacija s izvrsnim smanjenjem veliÄine - idealno za mobilnu implementaciju Qwen3-0.6B), Q5_1 (5-bitna kvantizacija koja balansira kvalitetu i kompresiju - prikladno za Phi-4-mini-3.8B rubnu inferenciju) i Q8_0 (8-bitna kvantizacija za gotovo originalnu kvalitetu - preporuÄeno za produkcijsku upotrebu Google Gemma3). BitNET predstavlja vrhunac s 1-bitnom kvantizacijom za ekstremne scenarije kompresije.

**Prednosti implementacije**: Optimizirana inferencija za CPU s SIMD ubrzanjem omoguÄ‡uje uÄinkovito uÄitavanje i izvoÄ‘enje modela. Kompatibilnost na viÅ¡e platformi, ukljuÄujuÄ‡i x86, ARM i Apple Silicon arhitekture, omoguÄ‡uje hardverski neovisne moguÄ‡nosti implementacije.

**PraktiÄni primjer implementacije**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Usporedba memorijskog otiska**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Microsoft Olive Suite za optimizaciju

Microsoft Olive nudi sveobuhvatne tijekove optimizacije modela dizajnirane za produkcijska okruÅ¾enja:

**Tehnike optimizacije**: Suite ukljuÄuje dinamiÄku kvantizaciju za automatski odabir preciznosti (posebno uÄinkovito s modelima serije Qwen3), optimizaciju grafa i fuziju operatora (optimizirano za arhitekturu Google Gemma3), hardverske specifiÄne optimizacije za CPU, GPU i NPU (s posebnom podrÅ¡kom za Phi-4-mini-3.8B na ARM ureÄ‘ajima) te viÅ¡estupanjske tijekove optimizacije. BitNET modeli zahtijevaju specijalizirane tijekove rada za 1-bitnu kvantizaciju unutar Olive okvira.

**Automatizacija tijeka rada**: Automatizirano testiranje meÄ‘u varijantama optimizacije osigurava oÄuvanje kvalitete metrika tijekom optimizacije. Integracija s popularnim ML okvirima poput PyTorcha i ONNX-a pruÅ¾a optimizaciju za implementaciju u oblaku i na rubu.

**PraktiÄni primjer implementacije**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Apple MLX okvir

Apple MLX pruÅ¾a nativnu optimizaciju posebno dizajniranu za Apple Silicon ureÄ‘aje:

**Optimizacija za Apple Silicon**: Okvir koristi arhitekturu ujedinjene memorije s integracijom Metal Performance Shaders, automatsku inferenciju mjeÅ¡ovite preciznosti (posebno uÄinkovito s Google Gemma3) i optimiziranu iskoriÅ¡tenost memorijske Å¡irine. Phi-4-mini-3.8B pokazuje iznimne performanse na M-seriji Äipova, dok Qwen3-1.7B pruÅ¾a optimalnu ravnoteÅ¾u za implementaciju na MacBook Air ureÄ‘ajima.

**ZnaÄajke razvoja**: PodrÅ¡ka za Python i Swift API s operacijama kompatibilnim s NumPy-jem, moguÄ‡nosti automatske diferencijacije i besprijekorna integracija s Appleovim alatima za razvoj pruÅ¾aju sveobuhvatno razvojno okruÅ¾enje.

**PraktiÄni primjer implementacije**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Strategije implementacije i inferencije u produkciji

### Ollama: Pojednostavljena lokalna implementacija

Ollama pojednostavljuje implementaciju SLM-ova s funkcijama spremnim za poduzeÄ‡a u lokalnim i rubnim okruÅ¾enjima:

**MoguÄ‡nosti implementacije**: Instalacija i izvoÄ‘enje modela jednim naredbom s automatskim povlaÄenjem i predmemoriranjem modela. PodrÅ¡ka za Phi-4-mini-3.8B, cijelu seriju Qwen3 (0.6B/1.7B/4B) i Google Gemma3 s REST API-jem za integraciju aplikacija te moguÄ‡nostima upravljanja i prebacivanja izmeÄ‘u viÅ¡e modela. BitNET modeli zahtijevaju eksperimentalne konfiguracije za podrÅ¡ku 1-bitne kvantizacije.

**Napredne znaÄajke**: PodrÅ¡ka za prilagoÄ‘eno fino podeÅ¡avanje modela, generiranje Dockerfile-a za implementaciju u kontejnerima, GPU ubrzanje s automatskim otkrivanjem te opcije kvantizacije i optimizacije modela pruÅ¾aju sveobuhvatnu fleksibilnost implementacije.

### VLLM: Inferencija visokih performansi

VLLM omoguÄ‡uje optimizaciju inferencije na razini produkcije za scenarije visokog protoka:

**Optimizacije performansi**: PagedAttention za memorijski uÄinkovitu obradu paÅ¾nje (posebno korisno za transformacijsku arhitekturu Phi-4-mini-3.8B), dinamiÄko grupiranje za optimizaciju protoka (optimizirano za paralelnu obradu serije Qwen3), paralelizam tenzora za skaliranje na viÅ¡e GPU-ova (podrÅ¡ka za Google Gemma3) i spekulativno dekodiranje za smanjenje kaÅ¡njenja. BitNET modeli zahtijevaju specijalizirane jezgre za inferenciju za 1-bitne operacije.

**Integracija za poduzeÄ‡a**: API krajnje toÄke kompatibilne s OpenAI-jem, podrÅ¡ka za implementaciju na Kubernetesu, integracija za praÄ‡enje i promatranje te moguÄ‡nosti automatskog skaliranja pruÅ¾aju rjeÅ¡enja za implementaciju na razini poduzeÄ‡a.

### Foundry Local: Microsoftovo rjeÅ¡enje za rub

Foundry Local pruÅ¾a sveobuhvatne moguÄ‡nosti implementacije na rubu za okruÅ¾enja poduzeÄ‡a:

**ZnaÄajke rubnog raÄunalstva**: Dizajn arhitekture s prioritetom offline rada uz optimizaciju za ograniÄene resurse, upravljanje lokalnim registrima modela i moguÄ‡nosti sinkronizacije izmeÄ‘u ruba i oblaka osiguravaju pouzdanu implementaciju na rubu.

**Sigurnost i usklaÄ‘enost**: Lokalna obrada podataka za oÄuvanje privatnosti, kontrole sigurnosti na razini poduzeÄ‡a, zapisivanje revizija i izvjeÅ¡tavanje o usklaÄ‘enosti te upravljanje pristupom na temelju uloga pruÅ¾aju sveobuhvatnu sigurnost za implementacije na rubu.

## Najbolje prakse za implementaciju SLM-ova

### Smjernice za odabir modela

Pri odabiru SLM-ova za implementaciju na rubu, razmotrite sljedeÄ‡e faktore:

**Razmatranja o broju parametara**: Odaberite mikro SLM-ove poput Qwen3-0.6B za ultra-lagane mobilne aplikacije, male SLM-ove poput Qwen3-1.7B ili Google Gemma3 za scenarije uravnoteÅ¾enih performansi te srednje SLM-ove poput Phi-4-mini-3.8B ili Qwen3-4B kada se pribliÅ¾avate sposobnostima LLM-a uz odrÅ¾avanje uÄinkovitosti. BitNET modeli nude eksperimentalnu ultra-kompresiju za specifiÄne istraÅ¾ivaÄke aplikacije.

**UsklaÄ‘enost s sluÄajem upotrebe**: Uskladite sposobnosti modela sa specifiÄnim zahtjevima aplikacije, uzimajuÄ‡i u obzir faktore poput kvalitete odgovora, brzine inferencije, ograniÄenja memorije i zahtjeva za offline radom.

### Odabir strategije optimizacije

**Pristup kvantizaciji**: Odaberite odgovarajuÄ‡e razine kvantizacije na temelju zahtjeva za kvalitetom i hardverskih ograniÄenja. Razmotrite Q4_0 za maksimalnu kompresiju (idealno za mobilnu implementaciju Qwen3-0.6B), Q5_1 za uravnoteÅ¾en omjer kvalitete i kompresije (prikladno za Phi-4-mini-3.8B i Google Gemma3) te Q8_0 za oÄuvanje gotovo originalne kvalitete (preporuÄeno za produkcijska okruÅ¾enja Qwen3-4B). BitNET-ova 1-bitna kvantizacija predstavlja ekstremnu granicu kompresije za specijalizirane aplikacije.

**Odabir okvira**: Odaberite okvire za optimizaciju na temelju ciljanog hardvera i zahtjeva za implementacijom. Koristite Llama.cpp za optimiziranu implementaciju na CPU-u, Microsoft Olive za sveobuhvatne tijekove optimizacije i Apple MLX za Apple Silicon ureÄ‘aje.

## PraktiÄni primjeri modela i sluÄajevi upotrebe

### Scenariji implementacije u stvarnom svijetu

**Mobilne aplikacije**: Qwen3-0.6B izvrsno se snalazi u aplikacijama za chatbotove na pametnim telefonima s minimalnim memorijskim otiskom, dok Google Gemma3 pruÅ¾a uravnoteÅ¾ene performanse za obrazovne alate na tabletima. Phi-4-mini-3.8B nudi vrhunske sposobnosti zakljuÄivanja za mobilne aplikacije produktivnosti.

**Desktop i rubno raÄunalstvo**: Qwen3-1.7B pruÅ¾a optimalne performanse za aplikacije pomoÄ‡nika na desktopu, Phi-4-mini-3.8B omoguÄ‡uje napredne sposobnosti generiranja koda za alate za razvojne programere, a Qwen3-4B omoguÄ‡uje sofisticiranu analizu dokumenata na radnim stanicama.

**IstraÅ¾ivanje i eksperimentiranje**: BitNET modeli omoguÄ‡uju istraÅ¾ivanje ultra-niske preciznosti inferencije za akademska istraÅ¾ivanja i aplikacije dokazivanja koncepta koje zahtijevaju ekstremna ograniÄenja resursa.

### Benchmarking performansi i usporedbe

**Brzina inferencije**: Qwen3-0.6B postiÅ¾e najbrÅ¾e vrijeme inferencije na mobilnim CPU-ima, Google Gemma3 pruÅ¾a uravnoteÅ¾en omjer brzine i kvalitete za opÄ‡e aplikacije, Phi-4-mini-3.8B nudi vrhunsku brzinu zakljuÄivanja za sloÅ¾ene zadatke, a BitNET pruÅ¾a teorijski maksimalan protok uz specijalizirani hardver.

**Zahtjevi za memorijom**: Memorijski otisci modela kreÄ‡u se od Qwen3-0.6B (manje od 1GB kvantizirano) do Phi-4-mini-3.8B (otprilike 3-4GB kvantizirano), dok BitNET postiÅ¾e otiske ispod 500MB u eksperimentalnim konfiguracijama.

## Izazovi i razmatranja

### Kompromisi performansi

Implementacija SLM-ova ukljuÄuje paÅ¾ljivo razmatranje kompromisa izmeÄ‘u veliÄine modela, brzine inferencije i kvalitete izlaza. Na primjer, dok Qwen3-0.6B nudi iznimnu brzinu i uÄinkovitost, Phi-4-mini-3.8B pruÅ¾a vrhunske sposobnosti zakljuÄivanja uz poveÄ‡ane zahtjeve za resursima. Google Gemma3 predstavlja sredinu prikladnu za veÄ‡inu opÄ‡ih aplikacija.

### Kompatibilnost hardvera

RazliÄiti rubni

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoÄ‡u AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kritiÄne informacije preporuÄuje se profesionalni prijevod od strane ljudskog prevoditelja. Ne preuzimamo odgovornost za bilo kakve nesporazume ili pogreÅ¡ne interpretacije koje proizlaze iz koriÅ¡tenja ovog prijevoda.