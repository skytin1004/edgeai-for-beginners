<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "308873206bc5ec5d87e34ca2907d2c9c",
  "translation_date": "2025-09-19T01:29:48+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "hr"
}
-->
# Odjeljak 2: Implementacija lokalnog okruÅ¾enja - RjeÅ¡enja koja Å¡tite privatnost

Lokalna implementacija malih jeziÄnih modela (SLM-ova) predstavlja promjenu paradigme prema AI rjeÅ¡enjima koja Å¡tite privatnost i smanjuju troÅ¡kove. Ovaj sveobuhvatni vodiÄ istraÅ¾uje dva moÄ‡na okviraâ€”Ollama i Microsoft Foundry Localâ€”koji omoguÄ‡uju programerima da iskoriste puni potencijal SLM-ova uz potpunu kontrolu nad svojim okruÅ¾enjem implementacije.

## Uvod

U ovoj lekciji istraÅ¾it Ä‡emo napredne strategije implementacije malih jeziÄnih modela u lokalnim okruÅ¾enjima. Pokrit Ä‡emo osnovne koncepte lokalne AI implementacije, analizirati dvije vodeÄ‡e platforme (Ollama i Microsoft Foundry Local) te pruÅ¾iti praktiÄne smjernice za implementaciju rjeÅ¡enja spremnih za produkciju.

## Ciljevi uÄenja

Na kraju ove lekcije moÄ‡i Ä‡ete:

- Razumjeti arhitekturu i prednosti okvira za lokalnu implementaciju SLM-ova.
- Implementirati produkcijska rjeÅ¡enja koristeÄ‡i Ollama i Microsoft Foundry Local.
- Usporediti i odabrati odgovarajuÄ‡u platformu na temelju specifiÄnih zahtjeva i ograniÄenja.
- Optimizirati lokalne implementacije za performanse, sigurnost i skalabilnost.

## Razumijevanje arhitektura lokalne implementacije SLM-ova

Lokalna implementacija SLM-ova predstavlja temeljnu promjenu od AI usluga ovisnih o oblaku prema rjeÅ¡enjima koja Å¡tite privatnost i omoguÄ‡uju rad na vlastitim sustavima. Ovaj pristup omoguÄ‡uje organizacijama da zadrÅ¾e potpunu kontrolu nad svojom AI infrastrukturom uz osiguranje suvereniteta podataka i operativne neovisnosti.

### Klasifikacija okvira implementacije

Razumijevanje razliÄitih pristupa implementaciji pomaÅ¾e u odabiru prave strategije za specifiÄne sluÄajeve upotrebe:

- **Usmjereno na razvoj**: Jednostavna postava za eksperimentiranje i prototipiranje.
- **Razina poduzeÄ‡a**: RjeÅ¡enja spremna za produkciju s moguÄ‡nostima integracije u poduzeÄ‡e.
- **MeÄ‘usustavno**: Univerzalna kompatibilnost na razliÄitim operativnim sustavima i hardveru.

### KljuÄne prednosti lokalne implementacije SLM-ova

Lokalna implementacija SLM-ova nudi nekoliko temeljnih prednosti koje je Äine idealnom za aplikacije osjetljive na privatnost i potrebe poduzeÄ‡a:

**Privatnost i sigurnost**: Lokalna obrada osigurava da osjetljivi podaci nikada ne napuÅ¡taju infrastrukturu organizacije, omoguÄ‡ujuÄ‡i usklaÄ‘enost s GDPR-om, HIPAA-om i drugim regulatornim zahtjevima. Implementacije u izoliranim sustavima moguÄ‡e su za klasificirana okruÅ¾enja, dok potpuni revizijski tragovi odrÅ¾avaju sigurnosni nadzor.

**Isplativost**: Eliminacija modela naplate po tokenu znaÄajno smanjuje operativne troÅ¡kove. NiÅ¾i zahtjevi za propusnost i smanjena ovisnost o oblaku omoguÄ‡uju predvidljive troÅ¡kovne strukture za proraÄune poduzeÄ‡a.

**Performanse i pouzdanost**: BrÅ¾e vrijeme izvoÄ‘enja bez mreÅ¾ne latencije omoguÄ‡uje aplikacije u stvarnom vremenu. Offline funkcionalnost osigurava kontinuirani rad bez obzira na povezanost s internetom, dok lokalna optimizacija resursa pruÅ¾a dosljedne performanse.

## Ollama: Univerzalna platforma za lokalnu implementaciju

### Osnovna arhitektura i filozofija

Ollama je dizajnirana kao univerzalna, programerima prilagoÄ‘ena platforma koja demokratizira lokalnu implementaciju LLM-ova na raznim hardverskim konfiguracijama i operativnim sustavima.

**TehniÄka osnova**: IzgraÄ‘ena na robusnom okviru llama.cpp, Ollama koristi uÄinkovit GGUF format modela za optimalne performanse. Kompatibilnost na razliÄitim platformama osigurava dosljedno ponaÅ¡anje na Windows, macOS i Linux okruÅ¾enjima, dok inteligentno upravljanje resursima optimizira koriÅ¡tenje CPU-a, GPU-a i memorije.

**Filozofija dizajna**: Ollama daje prednost jednostavnosti bez Å¾rtvovanja funkcionalnosti, nudeÄ‡i implementaciju bez konfiguracije za trenutnu produktivnost. Platforma odrÅ¾ava Å¡iroku kompatibilnost modela uz dosljedne API-je za razliÄite arhitekture modela.

### Napredne znaÄajke i moguÄ‡nosti

**Izvrsno upravljanje modelima**: Ollama pruÅ¾a sveobuhvatno upravljanje Å¾ivotnim ciklusom modela s automatskim preuzimanjem, predmemoriranjem i verzioniranjem. Platforma podrÅ¾ava opseÅ¾an ekosustav modela ukljuÄujuÄ‡i Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral i specijalizirane modele za ugraÄ‘ivanje.

**Prilagodba putem datoteka modela**: Napredni korisnici mogu kreirati prilagoÄ‘ene konfiguracije modela sa specifiÄnim parametrima, sistemskim upitima i modifikacijama ponaÅ¡anja. To omoguÄ‡uje optimizacije specifiÄne za domenu i specijalizirane zahtjeve aplikacija.

**Optimizacija performansi**: Ollama automatski detektira i koristi dostupno hardversko ubrzanje ukljuÄujuÄ‡i NVIDIA CUDA, Apple Metal i OpenCL. Inteligentno upravljanje memorijom osigurava optimalno koriÅ¡tenje resursa na razliÄitim hardverskim konfiguracijama.

### Strategije implementacije u produkciji

**Instalacija i postavljanje**: Ollama omoguÄ‡uje jednostavnu instalaciju na razliÄitim platformama putem nativnih instalacijskih programa, upravitelja paketa (WinGet, Homebrew, APT) i Docker kontejnera za kontejnerske implementacije.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Osnovne naredbe i operacije**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Napredna konfiguracija**: Datoteke modela omoguÄ‡uju sofisticiranu prilagodbu za zahtjeve poduzeÄ‡a:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Primjeri integracije za programere

**Integracija Python API-ja**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**Integracija JavaScript/TypeScript (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**KoriÅ¡tenje RESTful API-ja s cURL-om**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### PodeÅ¡avanje performansi i optimizacija

**Konfiguracija memorije i niti**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Odabir kvantizacije za razliÄiti hardver**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Platforma za AI na rubu poduzeÄ‡a

### Arhitektura razine poduzeÄ‡a

Microsoft Foundry Local predstavlja sveobuhvatno rjeÅ¡enje za poduzeÄ‡a dizajnirano posebno za produkcijske AI implementacije na rubu s dubokom integracijom u Microsoftov ekosustav.

**Osnova temeljena na ONNX-u**: IzgraÄ‘ena na industrijskom standardu ONNX Runtime, Foundry Local pruÅ¾a optimizirane performanse na raznim hardverskim arhitekturama. Platforma koristi integraciju Windows ML-a za nativnu optimizaciju na Windowsu uz odrÅ¾avanje meÄ‘usustavne kompatibilnosti.

**Izvrsno hardversko ubrzanje**: Foundry Local ima inteligentnu detekciju i optimizaciju hardvera na CPU-ima, GPU-ima i NPU-ima. Duboka suradnja s proizvoÄ‘aÄima hardvera (AMD, Intel, NVIDIA, Qualcomm) osigurava optimalne performanse na hardverskim konfiguracijama poduzeÄ‡a.

### Napredno iskustvo za programere

**Pristup putem viÅ¡e suÄelja**: Foundry Local pruÅ¾a sveobuhvatna razvojna suÄelja ukljuÄujuÄ‡i moÄ‡an CLI za upravljanje modelima i implementaciju, SDK-ove za viÅ¡e jezika (Python, NodeJS) za nativnu integraciju i RESTful API-je s kompatibilnoÅ¡Ä‡u OpenAI-ja za jednostavnu migraciju.

**Integracija s Visual Studiom**: Platforma se besprijekorno integrira s AI alatima za VS Code, pruÅ¾ajuÄ‡i alate za konverziju modela, kvantizaciju i optimizaciju unutar razvojnog okruÅ¾enja. Ova integracija ubrzava razvojne procese i smanjuje sloÅ¾enost implementacije.

**Pipeline za optimizaciju modela**: Integracija s Microsoft Olive omoguÄ‡uje sofisticirane procese optimizacije modela ukljuÄujuÄ‡i dinamiÄku kvantizaciju, optimizaciju grafova i podeÅ¡avanje specifiÄno za hardver. MoguÄ‡nosti konverzije putem Azure ML-a pruÅ¾aju skalabilnu optimizaciju za velike modele.

### Strategije implementacije u produkciji

**Instalacija i konfiguracija**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Operacije upravljanja modelima**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Napredna konfiguracija implementacije**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Integracija u ekosustav poduzeÄ‡a

**Sigurnost i usklaÄ‘enost**: Foundry Local pruÅ¾a sigurnosne znaÄajke razine poduzeÄ‡a ukljuÄujuÄ‡i kontrolu pristupa temeljenog na ulogama, zapisivanje revizija, izvjeÅ¡tavanje o usklaÄ‘enosti i Å¡ifrirano pohranjivanje modela. Integracija s Microsoftovom sigurnosnom infrastrukturom osigurava pridrÅ¾avanje sigurnosnih politika poduzeÄ‡a.

**UgraÄ‘ene AI usluge**: Platforma nudi gotove AI moguÄ‡nosti ukljuÄujuÄ‡i Phi Silica za lokalnu obradu jezika, AI Imaging za poboljÅ¡anje i analizu slika te specijalizirane API-je za uobiÄajene AI zadatke u poduzeÄ‡u.

## Komparativna analiza: Ollama vs Foundry Local

### Usporedba tehniÄke arhitekture

| **Aspekt** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **Format modela** | GGUF (preko llama.cpp) | ONNX (preko ONNX Runtime) |
| **Fokus platforme** | Univerzalna meÄ‘usustavna | Optimizacija za Windows/poduzeÄ‡a |
| **Integracija hardvera** | PodrÅ¡ka za generiÄki GPU/CPU | Duboka integracija Windows ML-a, NPU podrÅ¡ka |
| **Optimizacija** | Kvantizacija llama.cpp | Microsoft Olive + ONNX Runtime |
| **ZnaÄajke za poduzeÄ‡a** | VoÄ‘eno zajednicom | Razina poduzeÄ‡a s SLA-ovima |

### Karakteristike performansi

**Prednosti performansi Ollame**:
- Izvrsne performanse CPU-a zahvaljujuÄ‡i optimizaciji llama.cpp-a.
- Dosljedno ponaÅ¡anje na razliÄitim platformama i hardveru.
- UÄinkovito koriÅ¡tenje memorije s inteligentnim uÄitavanjem modela.
- Brzo pokretanje za razvojne i testne scenarije.

**Prednosti performansi Foundry Local-a**:
- Superiorno koriÅ¡tenje NPU-a na modernom Windows hardveru.
- Optimizirano GPU ubrzanje kroz partnerstva s proizvoÄ‘aÄima.
- PraÄ‡enje i optimizacija performansi razine poduzeÄ‡a.
- Skalabilne moguÄ‡nosti implementacije za produkcijska okruÅ¾enja.

### Analiza iskustva za programere

**Iskustvo programera s Ollamom**:
- Minimalni zahtjevi za postavljanje uz trenutnu produktivnost.
- Intuitivno suÄelje naredbenog retka za sve operacije.
- OpseÅ¾na podrÅ¡ka zajednice i dokumentacija.
- Fleksibilna prilagodba putem datoteka modela.

**Iskustvo programera s Foundry Local-om**:
- Sveobuhvatna integracija IDE-a s ekosustavom Visual Studija.
- Razvojni procesi razine poduzeÄ‡a s znaÄajkama za timsku suradnju.
- Profesionalni kanali podrÅ¡ke uz Microsoftovu podrÅ¡ku.
- Napredni alati za otklanjanje pogreÅ¡aka i optimizaciju.

### Optimizacija sluÄajeva upotrebe

**Odaberite Ollamu kada**:
- Razvijate meÄ‘usustavne aplikacije koje zahtijevaju dosljedno ponaÅ¡anje.
- Prioritet je transparentnost otvorenog koda i doprinosi zajednice.
- Radite s ograniÄenim resursima ili proraÄunskim ograniÄenjima.
- Gradite eksperimentalne ili istraÅ¾ivaÄke aplikacije.
- Potrebna je Å¡iroka kompatibilnost modela na razliÄitim arhitekturama.

**Odaberite Foundry Local kada**:
- Implementirate aplikacije za poduzeÄ‡a sa strogim zahtjevima za performanse.
- Koristite optimizacije specifiÄne za Windows hardver (NPU, Windows ML).
- Potrebna je podrÅ¡ka za poduzeÄ‡a, SLA-ovi i znaÄajke usklaÄ‘enosti.
- Gradite produkcijske aplikacije s integracijom u Microsoftov ekosustav.
- Trebate napredne alate za optimizaciju i profesionalne razvojne procese.

## Napredne strategije implementacije

### Obrasci implementacije u kontejnerima

**Kontejnerizacija Ollame**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Implementacija Foundry Local-a za poduzeÄ‡a**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Tehnike optimizacije performansi

**Strategije optimizacije Ollame**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Optimizacija Foundry Local-a**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Sigurnosni i regulatorni aspekti

### Implementacija sigurnosti u poduzeÄ‡u

**Najbolje prakse za sigurnost Ollame**:
- Izolacija mreÅ¾e s pravilima vatrozida i VPN pristupom.
- Autentifikacija putem integracije reverznog proxyja.
- Provjera integriteta modela i sigurna distribucija modela.
- Zapisivanje revizija za pristup API-ju i operacije modela.

**Sigurnost Foundry Local-a za poduzeÄ‡a**:
- UgraÄ‘ena kontrola pristupa temeljenog na ulogama s integracijom Active Directoryja.
- Sveobuhvatni revizijski tragovi s izvjeÅ¡tavanjem o usklaÄ‘enosti.
- Å ifrirano pohranjivanje modela i sigurna implementacija modela.
- Integracija s Microsoftovom sigurnosnom infrastrukturom.

### UsklaÄ‘enost i regulatorni zahtjevi

Obje platforme podrÅ¾avaju regulatornu usklaÄ‘enost kroz:
- Kontrole rezidencije podataka koje osiguravaju lokalnu obradu.
- Zapisivanje revizija za zahtjeve regulatornog izvjeÅ¡tavanja.
- Kontrole pristupa za rukovanje osjetljivim podacima.
- Å ifriranje u mirovanju i tijekom prijenosa za zaÅ¡titu podataka.

## Najbolje prakse za produkcijsku implementaciju

### PraÄ‡enje i preglednost

**KljuÄne metrike za praÄ‡enje**:
- Latencija i propusnost izvoÄ‘enja modela.
- KoriÅ¡tenje resursa (CPU, GPU, memorija).
- Vrijeme odgovora API-ja i stope pogreÅ¡aka.
- ToÄnost modela i odstupanje performansi.

**Implementacija praÄ‡enja**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### Kontinuirana integracija i implementacija

**Integracija CI/CD pipeline-a**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## BuduÄ‡i trendovi i razmatranja

### Novi tehnologijski trendovi

Lokalni SLM ekosustav nastavlja se razvijati s nekoliko kljuÄnih trendova:

**Napredne arhitekture modela**: Pojavljuju se sljedeÄ‡e generacije SLM-ova s poboljÅ¡anim omjerima uÄinkovitosti i sposobnosti, ukljuÄujuÄ‡i modele struÄnjaka za dinamiÄko skaliranje i specijalizirane arhitekture za implementaciju na rubu.

**Integracija hardvera**: Dublja integracija sa specijaliziranim AI hardverom ukljuÄujuÄ‡i NPU-e, prilagoÄ‘eni silicij i ubrzivaÄe za rubno raÄunalstvo pruÅ¾it Ä‡e poboljÅ¡ane performanse.

**Evolucija ekosustava**: Napori za standardizaciju meÄ‘u platformama za implementaciju i poboljÅ¡ana interoperabilnost izmeÄ‘u razliÄitih okvira pojednostavit Ä‡e implementacije na viÅ¡e platformi.

### Obrasci usvajanja u industriji

**Usvajanje u poduzeÄ‡ima**: PoveÄ‡ano usvajanje u poduzeÄ‡ima potaknuto zahtjevima za privatnost, optimizacijom troÅ¡kova i potrebama za regulatornom usklaÄ‘enoÅ¡Ä‡u. Vladini i obrambeni sektori posebno su usmjereni na implementacije u izoliranim sustavima.

**Globalna razmatranja**: MeÄ‘unarodni zahtjevi za suverenitet podataka potiÄu usvajanje lokalnih implementacija, posebno u regijama sa strogim propisima o zaÅ¡titi podataka.

## Izazovi i razmatranja

### TehniÄki izazovi

**Zahtjevi infrastrukture**: Lokalna implementacija zahtijeva paÅ¾ljivo planiranje kapaciteta i odabir hardvera. Organizacije moraju balansirati zahtjeve za performansama s troÅ¡kovnim ograniÄenjima uz osiguranje skalabilnosti za rastuÄ‡e radne optereÄ‡enje.

**ğŸ”§ OdrÅ¾avanje i aÅ¾uriranja**: Redovita aÅ¾uriranja modela, sigurnosne zakrpe i optimizacija performansi zahtijevaju posveÄ‡ene resurse i struÄnost. Automatizirani pipeline-i implementacije postaju kljuÄni za produkcijska okruÅ¾enja.

### Sigurnosna razmatranja

**Sigurnost modela**: ZaÅ¡tita vlasniÄ

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoÄ‡u AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kljuÄne informacije preporuÄuje se profesionalni prijevod od strane Äovjeka. Ne preuzimamo odgovornost za bilo kakve nesporazume ili pogreÅ¡ne interpretacije koje proizlaze iz koriÅ¡tenja ovog prijevoda.