<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-18T22:22:19+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "hr"
}
-->
# Odjeljak 2: Osnove Qwen obitelji

Modeli iz obitelji Qwen predstavljaju sveobuhvatan pristup Alibaba Clouda velikim jeziÄnim modelima i multimodalnoj umjetnoj inteligenciji, pokazujuÄ‡i da modeli otvorenog koda mogu postiÄ‡i izvanredne performanse dok su dostupni u raznim scenarijima implementacije. VaÅ¾no je razumjeti kako obitelj Qwen omoguÄ‡uje moÄ‡ne AI sposobnosti s fleksibilnim opcijama implementacije, istovremeno odrÅ¾avajuÄ‡i konkurentne performanse u raznovrsnim zadacima.

## Resursi za programere

### Hugging Face Model Repository
Odabrani modeli iz obitelji Qwen dostupni su putem [Hugging Face](https://huggingface.co/models?search=qwen), pruÅ¾ajuÄ‡i pristup nekim varijantama ovih modela. MoÅ¾ete istraÅ¾iti dostupne varijante, prilagoditi ih za svoje specifiÄne potrebe i implementirati ih kroz razliÄite okvire.

### Alati za lokalni razvoj
Za lokalni razvoj i testiranje moÅ¾ete koristiti [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) za pokretanje dostupnih Qwen modela na vaÅ¡em razvojnom raÄunalu s optimiziranim performansama.

### Dokumentacijski resursi
- [Qwen Model Documentation](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Optimizacija Qwen modela za implementaciju na rubnim ureÄ‘ajima](https://github.com/microsoft/olive)

## Uvod

U ovom vodiÄu istraÅ¾it Ä‡emo obitelj modela Qwen i njezine osnovne koncepte. Pokrit Ä‡emo evoluciju obitelji Qwen, inovativne metodologije treniranja koje Äine Qwen modele uÄinkovitim, kljuÄne varijante u obitelji i praktiÄne primjene u razliÄitim scenarijima.

## Ciljevi uÄenja

Na kraju ovog vodiÄa moÄ‡i Ä‡ete:

- Razumjeti filozofiju dizajna i evoluciju obitelji Qwen modela Alibaba Clouda
- Identificirati kljuÄne inovacije koje omoguÄ‡uju Qwen modelima postizanje visokih performansi u razliÄitim veliÄinama parametara
- Prepoznati prednosti i ograniÄenja razliÄitih varijanti Qwen modela
- Primijeniti znanje o Qwen modelima za odabir odgovarajuÄ‡ih varijanti za stvarne scenarije

## Razumijevanje suvremenog krajolika AI modela

Krajolik umjetne inteligencije znaÄajno se razvio, s razliÄitim organizacijama koje slijede razliÄite pristupe razvoju jeziÄnih modela. Dok se neki fokusiraju na vlasniÄke modele zatvorenog koda, drugi naglaÅ¡avaju dostupnost i transparentnost otvorenog koda. Tradicionalni pristup ukljuÄuje ili masivne vlasniÄke modele dostupne samo putem API-ja ili modele otvorenog koda koji mogu zaostajati u sposobnostima.

Ova paradigma stvara izazove za organizacije koje traÅ¾e moÄ‡ne AI sposobnosti dok odrÅ¾avaju kontrolu nad svojim podacima, troÅ¡kovima i fleksibilnoÅ¡Ä‡u implementacije. Konvencionalni pristup Äesto zahtijeva izbor izmeÄ‘u vrhunskih performansi i praktiÄnih razmatranja implementacije.

## Izazov dostupne AI izvrsnosti

Potreba za visokokvalitetnom, dostupnom umjetnom inteligencijom postaje sve vaÅ¾nija u raznim scenarijima. Razmotrite primjene koje zahtijevaju fleksibilne opcije implementacije za razliÄite organizacijske potrebe, isplative implementacije gdje troÅ¡kovi API-ja mogu postati znaÄajni, viÅ¡ejeziÄne sposobnosti za globalne aplikacije ili specijaliziranu struÄnost u podruÄjima poput programiranja i matematike.

### KljuÄni zahtjevi za implementaciju

Suvremene AI implementacije suoÄavaju se s nekoliko temeljnih zahtjeva koji ograniÄavaju praktiÄnu primjenjivost:

- **Dostupnost**: Dostupnost otvorenog koda za transparentnost i prilagodbu
- **Isplativost**: Razumni raÄunalni zahtjevi za razliÄite proraÄune
- **Fleksibilnost**: ViÅ¡e veliÄina modela za razliÄite scenarije implementacije
- **Globalni doseg**: SnaÅ¾ne viÅ¡ejeziÄne i meÄ‘ukulturalne sposobnosti
- **Specijalizacija**: Varijante specifiÄne za domenu za odreÄ‘ene sluÄajeve upotrebe

## Filozofija Qwen modela

Obitelj Qwen modela predstavlja sveobuhvatan pristup razvoju AI modela, prioritizirajuÄ‡i dostupnost otvorenog koda, viÅ¡ejeziÄne sposobnosti i praktiÄnu implementaciju dok odrÅ¾ava konkurentne karakteristike performansi. Qwen modeli to postiÅ¾u kroz raznolike veliÄine modela, visokokvalitetne metodologije treniranja i specijalizirane varijante za razliÄite domene.

Obitelj Qwen obuhvaÄ‡a razliÄite pristupe dizajnirane za pruÅ¾anje opcija kroz spektar performansi i uÄinkovitosti, omoguÄ‡ujuÄ‡i implementaciju od mobilnih ureÄ‘aja do korporativnih servera dok pruÅ¾a znaÄajne AI sposobnosti. Cilj je demokratizirati pristup visokokvalitetnoj umjetnoj inteligenciji dok se pruÅ¾a fleksibilnost u izborima implementacije.

### Temeljna naÄela dizajna Qwen modela

Qwen modeli temelje se na nekoliko osnovnih naÄela koja ih razlikuju od drugih obitelji jeziÄnih modela:

- **Otvoreni kod na prvom mjestu**: Potpuna transparentnost i dostupnost za istraÅ¾ivanje i komercijalnu upotrebu
- **Sveobuhvatno treniranje**: Treniranje na masivnim, raznolikim skupovima podataka koji pokrivaju viÅ¡e jezika i domena
- **Skalabilna arhitektura**: ViÅ¡e veliÄina modela za usklaÄ‘ivanje s razliÄitim raÄunalnim zahtjevima
- **Specijalizirana izvrsnost**: Varijante specifiÄne za domenu optimizirane za odreÄ‘ene zadatke

## KljuÄne tehnologije koje omoguÄ‡uju obitelj Qwen

### Masivno treniranje

Jedan od definirajuÄ‡ih aspekata obitelji Qwen je masivna skala podataka za treniranje i raÄunalnih resursa uloÅ¾enih u razvoj modela. Qwen modeli koriste paÅ¾ljivo odabrane, viÅ¡ejeziÄne skupove podataka koji obuhvaÄ‡aju trilijune tokena, dizajnirane za pruÅ¾anje sveobuhvatnog znanja o svijetu i sposobnosti zakljuÄivanja.

Ovaj pristup kombinira visokokvalitetni web sadrÅ¾aj, akademsku literaturu, repozitorije koda i viÅ¡ejeziÄne resurse. Metodologija treniranja naglaÅ¡ava i Å¡irinu znanja i dubinu razumijevanja u raznim domenama i jezicima.

### Napredno zakljuÄivanje i razmiÅ¡ljanje

Nedavni Qwen modeli ukljuÄuju sofisticirane sposobnosti zakljuÄivanja koje omoguÄ‡uju sloÅ¾eno viÅ¡ekorakovno rjeÅ¡avanje problema:

**NaÄin razmiÅ¡ljanja (Qwen3)**: Modeli se mogu upustiti u detaljno zakljuÄivanje korak po korak prije nego Å¡to pruÅ¾e konaÄne odgovore, sliÄno ljudskim pristupima rjeÅ¡avanju problema.

**Dvo-modni rad**: Sposobnost prebacivanja izmeÄ‘u brzog naÄina odgovora za jednostavne upite i dubljeg naÄina razmiÅ¡ljanja za sloÅ¾ene probleme.

**Integracija lanca misli**: Prirodna integracija koraka zakljuÄivanja koji poboljÅ¡avaju transparentnost i toÄnost u sloÅ¾enim zadacima.

### Arhitektonske inovacije

Obitelj Qwen ukljuÄuje nekoliko arhitektonskih optimizacija dizajniranih za performanse i uÄinkovitost:

**Skalabilni dizajn**: Dosljedna arhitektura kroz veliÄine modela omoguÄ‡uje jednostavno skaliranje i usporedbu.

**Multimodalna integracija**: Besprijekorna integracija obrade teksta, vizije i zvuka unutar jedinstvenih arhitektura.

**Optimizacija implementacije**: ViÅ¡e opcija kvantizacije i formata implementacije za razliÄite hardverske konfiguracije.

## VeliÄina modela i opcije implementacije

Suvremena okruÅ¾enja implementacije imaju koristi od fleksibilnosti Qwen modela u razliÄitim raÄunalnim zahtjevima:

### Mali modeli (0.5B-3B)

Qwen pruÅ¾a uÄinkovite male modele prikladne za implementaciju na rubnim ureÄ‘ajima, mobilne aplikacije i okruÅ¾enja s ograniÄenim resursima, dok odrÅ¾ava impresivne sposobnosti.

### Srednji modeli (7B-32B)

Modeli srednjeg raspona nude poboljÅ¡ane sposobnosti za profesionalne aplikacije, pruÅ¾ajuÄ‡i izvrsnu ravnoteÅ¾u izmeÄ‘u performansi i raÄunalnih zahtjeva.

### Veliki modeli (72B+)

Modeli pune skale pruÅ¾aju vrhunske performanse za zahtjevne aplikacije, istraÅ¾ivanje i korporativne implementacije koje zahtijevaju maksimalne sposobnosti.

## Prednosti obitelji Qwen modela

### Dostupnost otvorenog koda

Qwen modeli pruÅ¾aju potpunu transparentnost i moguÄ‡nosti prilagodbe, omoguÄ‡ujuÄ‡i organizacijama da razumiju, modificiraju i prilagode modele svojim specifiÄnim potrebama bez vezanja za dobavljaÄa.

### Fleksibilnost implementacije

Raspon veliÄina modela omoguÄ‡uje implementaciju u raznim hardverskim konfiguracijama, od mobilnih ureÄ‘aja do vrhunskih servera, pruÅ¾ajuÄ‡i organizacijama fleksibilnost u izboru AI infrastrukture.

### ViÅ¡ejeziÄna izvrsnost

Qwen modeli izvrsni su u viÅ¡ejeziÄnom razumijevanju i generiranju, podrÅ¾avajuÄ‡i desetke jezika s posebnom snagom u engleskom i kineskom, Å¡to ih Äini prikladnima za globalne aplikacije.

### Konkurentne performanse

Qwen modeli dosljedno postiÅ¾u konkurentne rezultate na referentnim testovima dok pruÅ¾aju dostupnost otvorenog koda, pokazujuÄ‡i da otvoreni modeli mogu parirati vlasniÄkim alternativama.

### Specijalizirane sposobnosti

Varijante specifiÄne za domenu poput Qwen-Coder i Qwen-Math pruÅ¾aju specijaliziranu struÄnost dok odrÅ¾avaju opÄ‡e sposobnosti razumijevanja jezika.

## PraktiÄni primjeri i sluÄajevi upotrebe

Prije nego Å¡to se upustimo u tehniÄke detalje, istraÅ¾imo neke konkretne primjere onoga Å¡to Qwen modeli mogu postiÄ‡i:

### Primjer matematiÄkog zakljuÄivanja

Qwen-Math izvrsno rjeÅ¡ava matematiÄke probleme korak po korak. Na primjer, kada se zatraÅ¾i rjeÅ¡avanje sloÅ¾enog problema iz raÄuna:

```
User: Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = xÂ³ + 2xÂ² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹

Breaking down each term:
- d/dx(xÂ³) = 3xÂ²
- d/dx(2xÂ²) = 2 Â· 2xÂ¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3xÂ² + 4x - 5
```

### Primjer viÅ¡ejeziÄne podrÅ¡ke

Qwen modeli pokazuju snaÅ¾ne viÅ¡ejeziÄne sposobnosti u raznim jezicima:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**ä¸­æ–‡:**
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡æœºå™¨ï¼ˆç‰¹åˆ«æ˜¯è®¡ç®—æœºç³»ç»Ÿï¼‰å¯¹äººç±»æ™ºèƒ½è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚è¿™äº›è¿‡ç¨‹åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€è§£å†³é—®é¢˜ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚äººå·¥æ™ºèƒ½ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»è®¤çŸ¥èƒ½åŠ›çš„ä»»åŠ¡ã€‚
```

### Primjer multimodalnih sposobnosti

Qwen-VL moÅ¾e istovremeno obraÄ‘ivati tekst i slike:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Primjer generiranja koda

Qwen-Coder izvrsno generira i objaÅ¡njava kod u viÅ¡e programskih jezika:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    IzvrÅ¡i binarno pretraÅ¾ivanje na sortiranoj listi kako bi pronaÅ¡ao ciljnu vrijednost.
    
    Args:
        arr (list): Sortirana lista usporedivih elemenata
        target: Vrijednost koju treba pretraÅ¾iti
        
    Returns:
        int: Indeks cilja ako je pronaÄ‘en, -1 ako nije pronaÄ‘en
        
    Vremenska sloÅ¾enost: O(log n)
    Prostorna sloÅ¾enost: O(1)
    """
    # Inicijaliziraj lijeve i desne pokazivaÄe
    left, right = 0, len(arr) - 1
    
    # Nastavi pretraÅ¾ivanje dok je prostor pretraÅ¾ivanja valjan
    while left <= right:
        # IzraÄunaj srednji indeks kako bi izbjegao prelijevanje cijelog broja
        mid = left + (right - left) // 2
        
        # Provjeri jesmo li pronaÅ¡li cilj
        if arr[mid] == target:
            return mid
        
        # Ako je cilj manji, pretraÅ¾i lijevu polovicu
        elif arr[mid] > target:
            right = mid - 1
        
        # Ako je cilj veÄ‡i, pretraÅ¾i desnu polovicu
        else:
            left = mid + 1
    
    # Cilj nije pronaÄ‘en
    return -1

# Primjer upotrebe:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Indeks 7: {result}")  # Output: Indeks 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### Primjer implementacije na rubnim ureÄ‘ajima

Qwen modeli mogu se implementirati na raznim rubnim ureÄ‘ajima s optimiziranim konfiguracijama:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Evolucija obitelji Qwen

### Qwen 1.0 i 1.5: Temeljni modeli

Rani Qwen modeli uspostavili su temeljna naÄela sveobuhvatnog treniranja i dostupnosti otvorenog koda:

- **Qwen-7B (7B parametara)**: PoÄetno izdanje fokusirano na razumijevanje kineskog i engleskog jezika
- **Qwen-14B (14B parametara)**: PoboljÅ¡ane sposobnosti s unaprijeÄ‘enim zakljuÄivanjem i znanjem
- **Qwen-72B (72B parametara)**: Model velikih razmjera koji pruÅ¾a vrhunske performanse
- **Serija Qwen1.5**: ProÅ¡irena na viÅ¡e veliÄina (0.5B do 110B) s poboljÅ¡anim rukovanjem dugim kontekstom

### Obitelj Qwen2: Multimodalna ekspanzija

Serija Qwen2 oznaÄila je znaÄajan napredak u jeziÄnim i multimodalnim sposobnostima:

- **Qwen2-0.5B do 72B**: Sveobuhvatan raspon jeziÄnih modela za razliÄite potrebe implementacije
- **Qwen2-57B-A14B (MoE)**: Arhitektura mjeÅ¡avine struÄnjaka za uÄinkovito koriÅ¡tenje parametara
- **Qwen2-VL**: Napredne sposobnosti vizije i jezika za razumijevanje slika
- **Qwen2-Audio**: Obrada i razumijevanje zvuka
- **Qwen2-Math**: Specijalizirano matematiÄko zakljuÄivanje i rjeÅ¡avanje problema

### Obitelj Qwen2.5: PoboljÅ¡ane performanse

Serija Qwen2.5 donijela je znaÄajna poboljÅ¡anja u svim dimenzijama:

- **ProÅ¡ireno treniranje**: 18 trilijuna tokena podataka za treniranje za poboljÅ¡ane sposobnosti
- **ProÅ¡ireni kontekst**: Do 128K tokena duljine konteksta, s Turbo varijantom koja podrÅ¾ava 1M tokena
- **PoboljÅ¡ana specijalizacija**: PoboljÅ¡ane varijante Qwen2.5-Coder i Qwen2.5-Math
- **Bolja viÅ¡ejeziÄna podrÅ¡ka**: PoboljÅ¡ane performanse u 27+ jezika

### Obitelj Qwen3: Napredno zakljuÄivanje

Najnovija generacija pomiÄe granice sposobnosti zakljuÄivanja i razmiÅ¡ljanja:

- **Qwen3-235B-A22B**: VodeÄ‡i model mjeÅ¡avine struÄnjaka s ukupno 235B parametara
- **Qwen3-30B-A3B**: UÄinkovit MoE model s jakim performansama po aktivnom parametru
- **Gusti modeli**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B za razliÄite scenarije implementacije
- **NaÄin razmiÅ¡ljanja**: Hibridni pristup zakljuÄivanju koji podrÅ¾ava i brze odgovore i duboko razmiÅ¡ljanje
- **ViÅ¡ejeziÄna izvrsnost**: PodrÅ¡ka za 119 jezika i dijalekata
- **PoboljÅ¡ano treniranje**: 36 trilijuna tokena raznolikih, visokokvalitetnih podataka za treniranje

## Primjene Qwen modela

### Korporativne aplikacije

Organizacije koriste Qwen modele za analizu dokumenata, automatizaciju korisniÄke podrÅ¡ke, pomoÄ‡ u generiranju koda i aplikacije poslovne inteligencije. Priroda otvorenog koda omoguÄ‡uje prilagodbu za specifiÄne poslovne potrebe uz odrÅ¾avanje privatnosti i kontrole podataka.

### Mobilno i rubno raÄunalstvo

Mobilne aplikacije koriste Qwen modele za prijevod u stvarnom vremenu, inteligentne asistente, generiranje sadrÅ¾aja i personalizirane preporuke. Raspon veliÄina modela omoguÄ‡uje implementaciju od mobilnih ureÄ‘aja do rubnih servera.

### Edukacijska tehnologija

Edukacijske platforme koriste Qwen modele za personalizirano poduÄavanje, automatizirano generiranje sadrÅ¾aja, pomoÄ‡ u uÄenju jezika i interaktivna edukacijska iskustva. Specijalizirani modeli poput Qwen-Math pruÅ¾aju struÄnost specifiÄnu za domenu.

### Globalne aplikacije

MeÄ‘unarodne aplikacije imaju koristi od snaÅ¾nih viÅ¡ejeziÄnih sposobnosti Qwen modela, omoguÄ‡ujuÄ‡i dosljedna AI iskustva u razliÄitim jezicima i kulturnim kontekstima.

## Izazovi i ograniÄenja

### RaÄunalni zahtjevi

Iako Qwen pruÅ¾a modele u raznim veliÄinama, veÄ‡e varijante i dalje zahtijevaju znaÄajne raÄunalne resurse za optimalne performanse
Evo kako zapoÄeti s Qwen modelima koristeÄ‡i Hugging Face Transformers biblioteku:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### KoriÅ¡tenje Qwen2.5 modela

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### Specijalizirana upotreba modela

**Generiranje koda s Qwen-Coder:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**RjeÅ¡avanje matematiÄkih problema:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**Zadaci vezani uz viziju i jezik:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### NaÄin razmiÅ¡ljanja (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### ğŸ“± Mobilna i rubna implementacija

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### Primjer API implementacije

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## Benchmarkovi performansi i postignuÄ‡a

Obitelj Qwen modela postigla je izvanredne rezultate na raznim benchmarkovima, uz odrÅ¾avanje otvorenog pristupa:

### KljuÄni naglasci performansi

**Izvrsnost u zakljuÄivanju:**
- Qwen3-235B-A22B postiÅ¾e konkurentne rezultate u evaluacijama kodiranja, matematike i opÄ‡ih sposobnosti u usporedbi s drugim vrhunskim modelima poput DeepSeek-R1, o1, o3-mini, Grok-3 i Gemini-2.5-Pro
- Qwen3-30B-A3B nadmaÅ¡uje QwQ-32B s 10 puta viÅ¡e aktiviranih parametara
- Qwen3-4B moÅ¾e se usporediti s performansama Qwen2.5-72B-Instruct

**PostignuÄ‡a u uÄinkovitosti:**
- Osnovni modeli Qwen3-MoE postiÅ¾u sliÄne performanse kao Qwen2.5 gusti osnovni modeli, koristeÄ‡i samo 10% aktivnih parametara
- ZnaÄajne uÅ¡tede u troÅ¡kovima treninga i inferencije u usporedbi s gustim modelima

**ViÅ¡ejeziÄne sposobnosti:**
- Qwen3 modeli podrÅ¾avaju 119 jezika i dijalekata
- SnaÅ¾ne performanse u raznolikim jeziÄnim i kulturnim kontekstima

**Skala treninga:**
- Qwen3 koristi gotovo dvostruko viÅ¡e podataka, s pribliÅ¾no 36 trilijuna tokena koji pokrivaju 119 jezika i dijalekata u usporedbi s Qwen2.5 koji koristi 18 trilijuna tokena

### Matrica usporedbe modela

| Serija modela | Raspon parametara | Duljina konteksta | KljuÄne snage | Najbolje primjene |
|---------------|-------------------|-------------------|---------------|-------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | UravnoteÅ¾ene performanse, viÅ¡ejeziÄnost | OpÄ‡e aplikacije, produkcijska implementacija |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | Generiranje koda, programiranje | Razvoj softvera, pomoÄ‡ pri kodiranju |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | MatematiÄko zakljuÄivanje | Edukacijske platforme, STEM aplikacije |
| **Qwen2.5-VL** | Razni | Promjenjivo | Razumijevanje vizije i jezika | Multimodalne aplikacije, analiza slika |
| **Qwen3** | 0.6B-235B | Promjenjivo | Napredno zakljuÄivanje, naÄin razmiÅ¡ljanja | Kompleksno zakljuÄivanje, istraÅ¾ivaÄke aplikacije |
| **Qwen3 MoE** | 30B-235B ukupno | Promjenjivo | UÄinkovite performanse velikih razmjera | Poslovne aplikacije, potrebe visokih performansi |

## VodiÄ za odabir modela

### Za osnovne aplikacije
- **Qwen2.5-0.5B/1.5B**: Mobilne aplikacije, rubni ureÄ‘aji, aplikacije u stvarnom vremenu
- **Qwen2.5-3B/7B**: OpÄ‡i chatbotovi, generiranje sadrÅ¾aja, sustavi pitanja i odgovora

### Za matematiÄke i zakljuÄivaÄke zadatke
- **Qwen2.5-Math**: RjeÅ¡avanje matematiÄkih problema i STEM edukacija
- **Qwen3 s naÄinom razmiÅ¡ljanja**: Kompleksno zakljuÄivanje koje zahtijeva analizu korak po korak

### Za programiranje i razvoj
- **Qwen2.5-Coder**: Generiranje koda, otklanjanje greÅ¡aka, pomoÄ‡ pri programiranju
- **Qwen3**: Napredni zadaci programiranja s sposobnostima zakljuÄivanja

### Za multimodalne aplikacije
- **Qwen2.5-VL**: Razumijevanje slika, odgovaranje na vizualna pitanja
- **Qwen-Audio**: Obrada zvuka i razumijevanje govora

### Za poslovnu implementaciju
- **Qwen2.5-32B/72B**: JeziÄno razumijevanje visokih performansi
- **Qwen3-235B-A22B**: Maksimalne sposobnosti za zahtjevne aplikacije

## Platforme za implementaciju i dostupnost
### Cloud platforme
- **Hugging Face Hub**: Sveobuhvatan repozitorij modela s podrÅ¡kom zajednice
- **ModelScope**: Alibaba platforma za modele s alatima za optimizaciju
- **Razni pruÅ¾atelji usluga u oblaku**: PodrÅ¡ka putem standardnih ML platformi

### Okviri za lokalni razvoj
- **Transformers**: Standardna Hugging Face integracija za jednostavnu implementaciju
- **vLLM**: PosluÅ¾itelj visokih performansi za produkcijska okruÅ¾enja
- **Ollama**: Pojednostavljena lokalna implementacija i upravljanje
- **ONNX Runtime**: Optimizacija za razliÄite hardverske platforme
- **llama.cpp**: UÄinkovita C++ implementacija za razne platforme

### Resursi za uÄenje
- **Qwen dokumentacija**: SluÅ¾bena dokumentacija i kartice modela
- **Hugging Face Model Hub**: Interaktivni demo primjeri i primjeri zajednice
- **IstraÅ¾ivaÄki radovi**: TehniÄki radovi na arxivu za detaljno razumijevanje
- **Forumi zajednice**: Aktivna podrÅ¡ka zajednice i rasprave

### PoÄetak rada s Qwen modelima

#### Platforme za razvoj
1. **Hugging Face Transformers**: ZapoÄnite sa standardnom Python integracijom
2. **ModelScope**: IstraÅ¾ite Alibaba alate za optimiziranu implementaciju
3. **Lokalna implementacija**: Koristite Ollama ili direktne transformere za lokalno testiranje

#### Put uÄenja
1. **Razumijevanje osnovnih pojmova**: ProuÄite arhitekturu i sposobnosti obitelji Qwen
2. **Eksperimentiranje s varijantama**: Isprobajte razliÄite veliÄine modela za razumijevanje kompromisa u performansama
3. **Praksa implementacije**: Implementirajte modele u razvojnim okruÅ¾enjima
4. **Optimizacija implementacije**: Fino podeÅ¡avanje za produkcijske sluÄajeve

#### Najbolje prakse
- **ZapoÄnite s manjim modelima**: PoÄnite s manjim modelima (1.5B-7B) za poÄetni razvoj
- **Koristite predloÅ¡ke za chat**: Primijenite pravilno formatiranje za optimalne rezultate
- **Pratite resurse**: Pratite koriÅ¡tenje memorije i brzinu inferencije
- **Razmotrite specijalizaciju**: Odaberite varijante specifiÄne za domenu kada je to prikladno

## Napredni obrasci koriÅ¡tenja

### Primjeri fino podeÅ¡avanja

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### Specijalizirano inÅ¾enjerstvo upita

**Za zadatke kompleksnog zakljuÄivanja:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**Za generiranje koda s kontekstom:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### ViÅ¡ejeziÄne aplikacije

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ä¸­æ–‡)",
        "es": "Spanish (EspaÃ±ol)",
        "fr": "French (FranÃ§ais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (æ—¥æœ¬èª)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ğŸ”§ Obrasci produkcijske implementacije

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## Strategije optimizacije performansi

### Optimizacija memorije

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### Optimizacija inferencije

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## Najbolje prakse i smjernice

### Sigurnost i privatnost

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### PraÄ‡enje i evaluacija

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## ZakljuÄak

Obitelj Qwen modela predstavlja sveobuhvatan pristup demokratizaciji AI tehnologije uz odrÅ¾avanje konkurentnih performansi u raznolikim aplikacijama. Kroz svoju predanost otvorenom pristupu, viÅ¡ejeziÄnim sposobnostima i fleksibilnim opcijama implementacije, Qwen omoguÄ‡uje organizacijama i developerima koriÅ¡tenje moÄ‡nih AI sposobnosti bez obzira na njihove resurse ili specifiÄne zahtjeve.

### KljuÄni zakljuÄci

**Izvrsnost otvorenog pristupa**: Qwen pokazuje da otvoreni modeli mogu postiÄ‡i performanse konkurentne vlasniÄkim alternativama, uz pruÅ¾anje transparentnosti, prilagodljivosti i kontrole.

**Skalabilna arhitektura**: Raspon od 0.5B do 235B parametara omoguÄ‡uje implementaciju u cijelom spektru raÄunalnih okruÅ¾enja, od mobilnih ureÄ‘aja do poslovnih klastera.

**Specijalizirane sposobnosti**: Varijante specifiÄne za domenu poput Qwen-Coder, Qwen-Math i Qwen-VL pruÅ¾aju specijaliziranu ekspertizu uz odrÅ¾avanje opÄ‡eg jeziÄnog razumijevanja.

**Globalna dostupnost**: SnaÅ¾na podrÅ¡ka za viÅ¡e jezika (119+ jezika) Äini Qwen prikladnim za meÄ‘unarodne aplikacije i raznolike korisniÄke baze.

**Kontinuirana inovacija**: Evolucija od Qwen 1.0 do Qwen3 pokazuje dosljedno poboljÅ¡anje sposobnosti, uÄinkovitosti i opcija implementacije.

### Pogled u buduÄ‡nost

Kako se obitelj Qwen nastavlja razvijati, moÅ¾emo oÄekivati:

- **PoboljÅ¡anu uÄinkovitost**: Kontinuiranu optimizaciju za bolje omjere performansi po parametru
- **ProÅ¡irene multimodalne sposobnosti**: Integraciju sofisticiranijih obrada vizije, zvuka i teksta
- **Naprednije zakljuÄivanje**: PoboljÅ¡ane mehanizme razmiÅ¡ljanja i sposobnosti rjeÅ¡avanja problema u viÅ¡e koraka
- **Bolje alate za implementaciju**: PoboljÅ¡ane okvire i alate za optimizaciju za raznolike scenarije implementacije
- **Rast zajednice**: ProÅ¡ireni ekosustav alata, aplikacija i doprinosa zajednice

### SljedeÄ‡i koraci

Bez obzira razvijate li chatbot, edukacijske alate, asistente za kodiranje ili radite na viÅ¡ejeziÄnim aplikacijama, obitelj Qwen pruÅ¾a skalabilna rjeÅ¡enja uz snaÅ¾nu podrÅ¡ku zajednice i sveobuhvatnu dokumentaciju.

Za najnovije informacije, izdanja modela i detaljnu tehniÄku dokumentaciju, posjetite sluÅ¾bene Qwen repozitorije na Hugging Faceu i istraÅ¾ite aktivne rasprave i primjere zajednice.

BuduÄ‡nost razvoja AI-a leÅ¾i u dostupnim, transparentnim i moÄ‡nim alatima koji omoguÄ‡uju inovaciju u svim sektorima i razmjerima. Obitelj Qwen utjelovljuje ovu viziju, pruÅ¾ajuÄ‡i organizacijama i developerima temelj za izgradnju sljedeÄ‡e generacije AI aplikacija.

## Dodatni resursi

- **SluÅ¾bena dokumentacija**: [Qwen Dokumentacija](https://qwen.readthedocs.io/)
- **Model Hub**: [Hugging Face Qwen Kolekcije](https://huggingface.co/collections/Qwen/)
- **TehniÄki radovi**: [Qwen IstraÅ¾ivaÄke Publikacije](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **Zajednica**: [GitHub Rasprave i Problemi](https://github.com/QwenLM/)
- **ModelScope Platforma**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## Ishodi uÄenja

Nakon zavrÅ¡etka ovog modula, moÄ‡i Ä‡ete:

1. Objasniti arhitektonske prednosti obitelji Qwen modela i njihov pristup otvorenom kodu
2. Odabrati odgovarajuÄ‡u Qwen varijantu na temelju specifiÄnih zahtjeva aplikacije i ograniÄenja resursa
3. Implementirati Qwen modele u raznim scenarijima implementacije s optimiziranim konfiguracijama
4. Primijeniti tehnike kvantizacije i optimizacije za poboljÅ¡anje performansi Qwen modela
5. Procijeniti kompromise izmeÄ‘u veliÄine modela, performansi i sposobnosti unutar obitelji Qwen

## Å to slijedi

- [03: Osnove Gemma obitelji](03.GemmaFamily.md)

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoÄ‡u AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kljuÄne informacije preporuÄuje se profesionalni prijevod od strane ljudskog prevoditelja. Ne preuzimamo odgovornost za bilo kakva nesporazuma ili pogreÅ¡na tumaÄenja koja proizlaze iz koriÅ¡tenja ovog prijevoda.