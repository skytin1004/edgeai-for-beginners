<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-01T21:53:08+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "hr"
}
-->
# Odjeljak 2: Osnove Qwen obitelji

Modeli iz Qwen obitelji predstavljaju sveobuhvatan pristup Alibaba Clouda velikim jeziÄnim modelima i multimodalnoj umjetnoj inteligenciji, pokazujuÄ‡i da otvoreni modeli mogu postiÄ‡i izvanredne rezultate dok ostaju dostupni u raznim scenarijima implementacije. VaÅ¾no je razumjeti kako Qwen obitelj omoguÄ‡uje moÄ‡ne AI sposobnosti s fleksibilnim opcijama implementacije, istovremeno odrÅ¾avajuÄ‡i konkurentne performanse u raznovrsnim zadacima.

## Resursi za programere

### Hugging Face repozitorij modela
Odabrani modeli iz Qwen obitelji dostupni su putem [Hugging Face](https://huggingface.co/models?search=qwen), omoguÄ‡ujuÄ‡i pristup nekim varijantama ovih modela. MoÅ¾ete istraÅ¾iti dostupne varijante, prilagoditi ih za svoje specifiÄne potrebe i implementirati ih kroz razliÄite okvire.

### Alati za lokalni razvoj
Za lokalni razvoj i testiranje moÅ¾ete koristiti [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) za pokretanje dostupnih Qwen modela na vaÅ¡em razvojnom raÄunalu s optimiziranim performansama.

### Dokumentacija
- [Dokumentacija Qwen modela](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Optimizacija Qwen modela za implementaciju na rubnim ureÄ‘ajima](https://github.com/microsoft/olive)

## Uvod

U ovom vodiÄu istraÅ¾it Ä‡emo obitelj Qwen modela i njezine osnovne koncepte. Pokrit Ä‡emo evoluciju Qwen obitelji, inovativne metodologije treniranja koje Äine Qwen modele uÄinkovitim, kljuÄne varijante u obitelji i praktiÄne primjene u razliÄitim scenarijima.

## Ciljevi uÄenja

Na kraju ovog vodiÄa moÄ‡i Ä‡ete:

- Razumjeti filozofiju dizajna i evoluciju Qwen obitelji modela
- Identificirati kljuÄne inovacije koje omoguÄ‡uju Qwen modelima postizanje visokih performansi u razliÄitim veliÄinama parametara
- Prepoznati prednosti i ograniÄenja razliÄitih varijanti Qwen modela
- Primijeniti znanje o Qwen modelima za odabir odgovarajuÄ‡ih varijanti za stvarne scenarije

## Razumijevanje suvremenog AI pejzaÅ¾a

AI pejzaÅ¾ znaÄajno se razvio, s razliÄitim organizacijama koje slijede razliÄite pristupe razvoju jeziÄnih modela. Dok se neki fokusiraju na vlasniÄke zatvorene modele, drugi naglaÅ¡avaju otvorenost i transparentnost. Tradicionalni pristup ukljuÄuje ili masivne vlasniÄke modele dostupne samo putem API-ja ili otvorene modele koji mogu zaostajati u sposobnostima.

Ovaj paradigmatski pristup stvara izazove za organizacije koje traÅ¾e moÄ‡ne AI sposobnosti uz zadrÅ¾avanje kontrole nad svojim podacima, troÅ¡kovima i fleksibilnoÅ¡Ä‡u implementacije. Konvencionalni pristup Äesto zahtijeva izbor izmeÄ‘u vrhunskih performansi i praktiÄnih razmatranja implementacije.

## Izazov dostupne AI izvrsnosti

Potreba za visokokvalitetnom, dostupnom AI tehnologijom postaje sve vaÅ¾nija u raznim scenarijima. Razmotrite primjene koje zahtijevaju fleksibilne opcije implementacije za razliÄite organizacijske potrebe, isplative implementacije gdje troÅ¡kovi API-ja mogu postati znaÄajni, viÅ¡ejeziÄne sposobnosti za globalne aplikacije ili specijalizirano znanje u podruÄjima poput programiranja i matematike.

### KljuÄni zahtjevi za implementaciju

Suvremene AI implementacije suoÄavaju se s nekoliko temeljnih zahtjeva koji ograniÄavaju praktiÄnu primjenjivost:

- **Dostupnost**: Otvoreni kod za transparentnost i prilagodbu
- **Isplativost**: Razumni raÄunalni zahtjevi za razliÄite budÅ¾ete
- **Fleksibilnost**: ViÅ¡e veliÄina modela za razliÄite scenarije implementacije
- **Globalni doseg**: SnaÅ¾ne viÅ¡ejeziÄne i meÄ‘ukulturalne sposobnosti
- **Specijalizacija**: Varijante specifiÄne za domenu za odreÄ‘ene primjene

## Filozofija Qwen modela

Qwen obitelj modela predstavlja sveobuhvatan pristup razvoju AI modela, prioritizirajuÄ‡i otvorenost, viÅ¡ejeziÄne sposobnosti i praktiÄnu implementaciju uz odrÅ¾avanje konkurentnih performansi. Qwen modeli to postiÅ¾u kroz raznovrsne veliÄine modela, visokokvalitetne metodologije treniranja i specijalizirane varijante za razliÄite domene.

Qwen obitelj obuhvaÄ‡a razliÄite pristupe dizajnirane za pruÅ¾anje opcija kroz spektar performansi i uÄinkovitosti, omoguÄ‡ujuÄ‡i implementaciju od mobilnih ureÄ‘aja do korporativnih servera uz pruÅ¾anje znaÄajnih AI sposobnosti. Cilj je demokratizirati pristup visokokvalitetnoj AI tehnologiji uz pruÅ¾anje fleksibilnosti u izboru implementacije.

### Temeljna naÄela dizajna Qwen modela

Qwen modeli temelje se na nekoliko osnovnih naÄela koja ih razlikuju od drugih obitelji jeziÄnih modela:

- **Otvoreni kod na prvom mjestu**: Potpuna transparentnost i dostupnost za istraÅ¾ivanje i komercijalnu upotrebu
- **Sveobuhvatno treniranje**: Treniranje na masivnim, raznovrsnim skupovima podataka koji pokrivaju viÅ¡e jezika i domena
- **Skalabilna arhitektura**: ViÅ¡e veliÄina modela za prilagodbu razliÄitim raÄunalnim zahtjevima
- **Specijalizirana izvrsnost**: Varijante specifiÄne za domenu optimizirane za odreÄ‘ene zadatke

## KljuÄne tehnologije koje omoguÄ‡uju Qwen obitelj

### Masivno treniranje

Jedan od kljuÄnih aspekata Qwen obitelji je masivna skala podataka za treniranje i raÄunalnih resursa uloÅ¾enih u razvoj modela. Qwen modeli koriste paÅ¾ljivo odabrane, viÅ¡ejeziÄne skupove podataka koji obuhvaÄ‡aju trilijune tokena, dizajnirane za pruÅ¾anje sveobuhvatnog znanja o svijetu i sposobnosti zakljuÄivanja.

Ovaj pristup kombinira visokokvalitetni web sadrÅ¾aj, akademsku literaturu, repozitorije koda i viÅ¡ejeziÄne resurse. Metodologija treniranja naglaÅ¡ava i Å¡irinu znanja i dubinu razumijevanja u raznim domenama i jezicima.

### Napredno zakljuÄivanje i razmiÅ¡ljanje

Nedavni Qwen modeli ukljuÄuju sofisticirane sposobnosti zakljuÄivanja koje omoguÄ‡uju sloÅ¾eno viÅ¡ekorakovno rjeÅ¡avanje problema:

**NaÄin razmiÅ¡ljanja (Qwen3)**: Modeli se mogu upustiti u detaljno zakljuÄivanje korak po korak prije nego Å¡to pruÅ¾e konaÄne odgovore, sliÄno ljudskom pristupu rjeÅ¡avanju problema.

**Dvostruki naÄin rada**: Sposobnost prebacivanja izmeÄ‘u brzog naÄina odgovora za jednostavne upite i dubljeg naÄina razmiÅ¡ljanja za sloÅ¾ene probleme.

**Integracija lanca misli**: Prirodna integracija koraka zakljuÄivanja koja poboljÅ¡ava transparentnost i toÄnost u sloÅ¾enim zadacima.

### Arhitektonske inovacije

Qwen obitelj ukljuÄuje nekoliko arhitektonskih optimizacija dizajniranih za performanse i uÄinkovitost:

**Skalabilni dizajn**: Dosljedna arhitektura kroz veliÄine modela omoguÄ‡uje jednostavno skaliranje i usporedbu.

**Multimodalna integracija**: Besprijekorna integracija obrade teksta, slike i zvuka unutar jedinstvenih arhitektura.

**Optimizacija implementacije**: ViÅ¡e opcija kvantizacije i formata implementacije za razliÄite hardverske konfiguracije.

## VeliÄine modela i opcije implementacije

Suvremena okruÅ¾enja implementacije imaju koristi od fleksibilnosti Qwen modela u razliÄitim raÄunalnim zahtjevima:

### Mali modeli (0.5B-3B)

Qwen pruÅ¾a uÄinkovite male modele prikladne za implementaciju na rubnim ureÄ‘ajima, mobilnim aplikacijama i okruÅ¾enjima s ograniÄenim resursima, uz odrÅ¾avanje impresivnih sposobnosti.

### Srednji modeli (7B-32B)

Modeli srednjeg raspona nude poboljÅ¡ane sposobnosti za profesionalne primjene, pruÅ¾ajuÄ‡i odliÄnu ravnoteÅ¾u izmeÄ‘u performansi i raÄunalnih zahtjeva.

### Veliki modeli (72B+)

Modeli pune skale pruÅ¾aju vrhunske performanse za zahtjevne primjene, istraÅ¾ivanje i korporativne implementacije koje zahtijevaju maksimalne sposobnosti.

## Prednosti Qwen obitelji modela

### Dostupnost otvorenog koda

Qwen modeli pruÅ¾aju potpunu transparentnost i moguÄ‡nosti prilagodbe, omoguÄ‡ujuÄ‡i organizacijama da razumiju, modificiraju i prilagode modele svojim specifiÄnim potrebama bez vezanja za dobavljaÄa.

### Fleksibilnost implementacije

Raspon veliÄina modela omoguÄ‡uje implementaciju na raznovrsnim hardverskim konfiguracijama, od mobilnih ureÄ‘aja do vrhunskih servera, pruÅ¾ajuÄ‡i organizacijama fleksibilnost u izboru AI infrastrukture.

### ViÅ¡ejeziÄna izvrsnost

Qwen modeli briljiraju u razumijevanju i generiranju na viÅ¡e jezika, podrÅ¾avajuÄ‡i desetke jezika s posebnim naglaskom na engleski i kineski, Å¡to ih Äini prikladnima za globalne primjene.

### Konkurentne performanse

Qwen modeli dosljedno postiÅ¾u konkurentne rezultate na referentnim testovima uz pruÅ¾anje otvorenog koda, pokazujuÄ‡i da otvoreni modeli mogu parirati vlasniÄkim alternativama.

### Specijalizirane sposobnosti

Varijante specifiÄne za domenu, poput Qwen-Coder i Qwen-Math, pruÅ¾aju specijalizirano znanje uz odrÅ¾avanje opÄ‡ih sposobnosti razumijevanja jezika.

## PraktiÄni primjeri i primjene

Prije nego Å¡to se upustimo u tehniÄke detalje, istraÅ¾imo konkretne primjere onoga Å¡to Qwen modeli mogu postiÄ‡i:

### Primjer matematiÄkog zakljuÄivanja

Qwen-Math briljira u rjeÅ¡avanju matematiÄkih problema korak po korak. Na primjer, kada se traÅ¾i rjeÅ¡enje sloÅ¾enog problema iz raÄuna:

```
User: Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = xÂ³ + 2xÂ² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹

Breaking down each term:
- d/dx(xÂ³) = 3xÂ²
- d/dx(2xÂ²) = 2 Â· 2xÂ¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3xÂ² + 4x - 5
```

### Primjer viÅ¡ejeziÄne podrÅ¡ke

Qwen modeli pokazuju snaÅ¾ne viÅ¡ejeziÄne sposobnosti u raznim jezicima:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**ä¸­æ–‡:**
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡æœºå™¨ï¼ˆç‰¹åˆ«æ˜¯è®¡ç®—æœºç³»ç»Ÿï¼‰å¯¹äººç±»æ™ºèƒ½è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚è¿™äº›è¿‡ç¨‹åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€è§£å†³é—®é¢˜ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚äººå·¥æ™ºèƒ½ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»è®¤çŸ¥èƒ½åŠ›çš„ä»»åŠ¡ã€‚
```

### Primjer multimodalnih sposobnosti

Qwen-VL moÅ¾e istovremeno obraÄ‘ivati tekst i slike:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Primjer generiranja koda

Qwen-Coder briljira u generiranju i objaÅ¡njavanju koda u raznim programskim jezicima:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

Ova implementacija slijedi najbolje prakse s jasnim nazivima varijabli, sveobuhvatnom dokumentacijom i uÄinkovitom logikom.
```

### Edge Deployment Example

Qwen models can be deployed on various edge devices with optimized configurations:

```
# Primjer implementacije na mobilnom ureÄ‘aju s kvantizacijom
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# UÄitavanje kvantiziranog modela za mobilnu implementaciju

```
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Evolucija Qwen obitelji

### Qwen 1.0 i 1.5: Temeljni modeli

Rani Qwen modeli uspostavili su temeljna naÄela sveobuhvatnog treniranja i dostupnosti otvorenog koda:

- **Qwen-7B (7B parametara)**: PoÄetno izdanje s fokusom na razumijevanje kineskog i engleskog jezika
- **Qwen-14B (14B parametara)**: PoboljÅ¡ane sposobnosti s unaprijeÄ‘enim zakljuÄivanjem i znanjem
- **Qwen-72B (72B parametara)**: Model velike skale koji pruÅ¾a vrhunske performanse
- **Serija Qwen1.5**: ProÅ¡irena na viÅ¡e veliÄina (0.5B do 110B) s poboljÅ¡anim rukovanjem dugim kontekstima

### Qwen2 obitelj: Multimodalna ekspanzija

Serija Qwen2 oznaÄila je znaÄajan napredak u jeziÄnim i multimodalnim sposobnostima:

- **Qwen2-0.5B do 72B**: Sveobuhvatan raspon jeziÄnih modela za razliÄite potrebe implementacije
- **Qwen2-57B-A14B (MoE)**: Arhitektura mjeÅ¡avine struÄnjaka za uÄinkovito koriÅ¡tenje parametara
- **Qwen2-VL**: Napredne sposobnosti razumijevanja slike i jezika
- **Qwen2-Audio**: Obrada i razumijevanje zvuka
- **Qwen2-Math**: Specijalizirano matematiÄko zakljuÄivanje i rjeÅ¡avanje problema

### Qwen2.5 obitelj: PoboljÅ¡ane performanse

Serija Qwen2.5 donijela je znaÄajna poboljÅ¡anja u svim dimenzijama:

- **ProÅ¡ireno treniranje**: 18 trilijuna tokena podataka za treniranje za poboljÅ¡ane sposobnosti
- **ProÅ¡ireni kontekst**: Do 128K tokena duljine konteksta, s Turbo varijantom koja podrÅ¾ava 1M tokena
- **PoboljÅ¡ana specijalizacija**: PoboljÅ¡ane varijante Qwen2.5-Coder i Qwen2.5-Math
- **Bolja viÅ¡ejeziÄna podrÅ¡ka**: PoboljÅ¡ane performanse u 27+ jezika

### Qwen3 obitelj: Napredno zakljuÄivanje

Najnovija generacija pomiÄe granice sposobnosti zakljuÄivanja i razmiÅ¡ljanja:

- **Qwen3-235B-A22B**: VodeÄ‡i model mjeÅ¡avine struÄnjaka s ukupno 235B parametara
- **Qwen3-30B-A3B**: UÄinkovit MoE model s jakim performansama po aktivnom parametru
- **Gusti modeli**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B za razne scenarije implementacije
- **NaÄin razmiÅ¡ljanja**: Hibridni pristup zakljuÄivanju koji podrÅ¾ava i brze odgovore i duboko razmiÅ¡ljanje
- **ViÅ¡ejeziÄna izvrsnost**: PodrÅ¡ka za 119 jezika i dijalekata
- **PoboljÅ¡ano treniranje**: 36 trilijuna tokena raznovrsnih, visokokvalitetnih podataka za treniranje

## Primjene Qwen modela

### Korporativne primjene

Organizacije koriste Qwen modele za analizu dokumenata, automatizaciju korisniÄke podrÅ¡ke, pomoÄ‡ u generiranju koda i aplikacije poslovne inteligencije. Otvoreni kod omoguÄ‡uje prilagodbu specifiÄnim poslovnim potrebama uz zadrÅ¾avanje privatnosti podataka i kontrole.

### Mobilno i rubno raÄunalstvo

Mobilne aplikacije koriste Qwen modele za prijevod u stvarnom vremenu, inteligentne asistente, generiranje sadrÅ¾aja i personalizirane preporuke. Raspon veliÄina modela omoguÄ‡uje implementaciju od mobilnih ureÄ‘aja do rubnih servera.

### Edukacijska tehnologija

Edukacijske platforme koriste Qwen modele za personalizirano poduÄavanje, automatizirano generiranje sadrÅ¾aja, pomoÄ‡ u uÄenju jezika i interaktivna edukacijska iskustva. Specijalizirani modeli poput Qwen-Math pruÅ¾aju ekspertizu specifiÄnu za domenu.

### Globalne primjene

MeÄ‘unarodne aplikacije imaju koristi od snaÅ¾nih viÅ¡ejeziÄnih sposobnosti Qwen modela, omoguÄ‡ujuÄ‡i dosljedna AI iskustva u razliÄitim jezicima i kulturnim kontekstima.

## Izazovi i ograniÄenja

### RaÄunalni zahtjevi

Iako Qwen pruÅ¾a modele u raznim veliÄinama, veÄ‡e varijante i dalje zahtijevaju znaÄajne raÄunalne resurse za optimalne performanse, Å¡to moÅ¾e ograniÄiti opcije implementacije za neke organizacije.

### Performanse u specijaliziranim domenama

Iako Qwen modeli dobro funkcioniraju u opÄ‡im domenama, visoko specijalizirane primjene mogu imati koristi od prilagodbe ili specijaliziranih modela.

### Kompleksnost odabira modela

Å irok raspon dostupnih modela i varijanti moÅ¾e oteÅ¾ati odabir korisnicima koji su novi u ekosustavu.

### JeziÄna neravnoteÅ¾a

Iako podrÅ¾avaju mnoge jezike, performanse mogu varirati meÄ‘u razliÄitim jezicima, s najjaÄim sposobnostima u engleskom i kineskom.

## BuduÄ‡nost Qwen obitelji modela

Qwen obitelj modela predstavlja kontinuiranu evoluciju prema demokratiziranoj, visokokvalitetnoj AI tehnologiji. BuduÄ‡i razvoj ukljuÄuje optimizacije uÄinkovitosti, proÅ¡irene multimodalne sposobnosti, poboljÅ¡ane mehanizme zakljuÄivanja i bolju integraciju u razliÄite scenarije implementacije.

Kako se tehnologija nastavlja razvijati, moÅ¾emo oÄekivati da Ä‡e Qwen modeli postati sve sposobniji uz odrÅ¾avanje otvorenosti, omoguÄ‡ujuÄ‡i AI implementaciju u raznovrsnim scenarijima i primjenama.

Qwen obitelj pokazuje da buduÄ‡nost razvoja AI tehnologije moÅ¾e obuhvatiti i vrhunske performanse i otvorenost, pruÅ¾ajuÄ‡i organizacijama moÄ‡ne alate uz odrÅ¾avanje transparentnosti i kontrole.

## Primjeri razvoja i integracije

### Brzi poÄetak s Transformers knjiÅ¾nicom

Evo kako zapoÄeti s Qwen model
- Qwen3-235B-A22B postiÅ¾e konkurentne rezultate u evaluacijama kodiranja, matematike i opÄ‡ih sposobnosti u usporedbi s drugim vrhunskim modelima kao Å¡to su DeepSeek-R1, o1, o3-mini, Grok-3 i Gemini-2.5-Pro.
- Qwen3-30B-A3B nadmaÅ¡uje QwQ-32B s 10 puta viÅ¡e aktiviranih parametara.
- Qwen3-4B moÅ¾e se usporediti s performansama Qwen2.5-72B-Instruct.

**PostignuÄ‡a u uÄinkovitosti:**
- Osnovni modeli Qwen3-MoE postiÅ¾u sliÄne performanse kao Qwen2.5 gusti osnovni modeli, koristeÄ‡i samo 10% aktivnih parametara.
- ZnaÄajne uÅ¡tede u troÅ¡kovima treninga i inferencije u usporedbi s gustim modelima.

**ViÅ¡ejeziÄne sposobnosti:**
- Qwen3 modeli podrÅ¾avaju 119 jezika i dijalekata.
- SnaÅ¾ne performanse u raznolikim jeziÄnim i kulturnim kontekstima.

**Skala treninga:**
- Qwen3 koristi gotovo dvostruko viÅ¡e podataka, s pribliÅ¾no 36 trilijuna tokena koji pokrivaju 119 jezika i dijalekata, u usporedbi s Qwen2.5 koji koristi 18 trilijuna tokena.

### Matrica usporedbe modela

| Serija modela | Raspon parametara | Duljina konteksta | KljuÄne prednosti | Najbolje primjene |
|---------------|-------------------|-------------------|-------------------|-------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | UravnoteÅ¾ene performanse, viÅ¡ejeziÄnost | OpÄ‡e aplikacije, produkcijska implementacija |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | Generiranje koda, programiranje | Razvoj softvera, pomoÄ‡ pri kodiranju |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | MatematiÄko zakljuÄivanje | Edukacijske platforme, STEM aplikacije |
| **Qwen2.5-VL** | Razni | Varijabilno | Razumijevanje vizije i jezika | Multimodalne aplikacije, analiza slika |
| **Qwen3** | 0.6B-235B | Varijabilno | Napredno zakljuÄivanje, naÄin razmiÅ¡ljanja | Kompleksno zakljuÄivanje, istraÅ¾ivaÄke aplikacije |
| **Qwen3 MoE** | 30B-235B ukupno | Varijabilno | UÄinkovite performanse velikih razmjera | Poslovne aplikacije, potrebe visokih performansi |

## VodiÄ za odabir modela

### Za osnovne aplikacije
- **Qwen2.5-0.5B/1.5B**: Mobilne aplikacije, rubni ureÄ‘aji, aplikacije u stvarnom vremenu.
- **Qwen2.5-3B/7B**: OpÄ‡i chatbotovi, generiranje sadrÅ¾aja, sustavi pitanja i odgovora.

### Za matematiÄke i logiÄke zadatke
- **Qwen2.5-Math**: RjeÅ¡avanje matematiÄkih problema i STEM edukacija.
- **Qwen3 s naÄinom razmiÅ¡ljanja**: Kompleksno zakljuÄivanje koje zahtijeva analizu korak po korak.

### Za programiranje i razvoj
- **Qwen2.5-Coder**: Generiranje koda, otklanjanje greÅ¡aka, pomoÄ‡ pri programiranju.
- **Qwen3**: Napredni zadaci programiranja s moguÄ‡nostima zakljuÄivanja.

### Za multimodalne aplikacije
- **Qwen2.5-VL**: Razumijevanje slika, vizualno odgovaranje na pitanja.
- **Qwen-Audio**: Obrada zvuka i razumijevanje govora.

### Za poslovnu implementaciju
- **Qwen2.5-32B/72B**: JeziÄno razumijevanje visokih performansi.
- **Qwen3-235B-A22B**: Maksimalne sposobnosti za zahtjevne aplikacije.

## Platforme za implementaciju i dostupnost
### Cloud platforme
- **Hugging Face Hub**: Sveobuhvatan repozitorij modela s podrÅ¡kom zajednice.
- **ModelScope**: Alibaba platforma za modele s alatima za optimizaciju.
- **Razni pruÅ¾atelji usluga u oblaku**: PodrÅ¡ka putem standardnih ML platformi.

### Okviri za lokalni razvoj
- **Transformers**: Standardna integracija Hugging Face za jednostavnu implementaciju.
- **vLLM**: PosluÅ¾itelj visokih performansi za produkcijska okruÅ¾enja.
- **Ollama**: Pojednostavljena lokalna implementacija i upravljanje.
- **ONNX Runtime**: Optimizacija za razliÄite hardverske platforme.
- **llama.cpp**: UÄinkovita C++ implementacija za razne platforme.

### Resursi za uÄenje
- **Qwen dokumentacija**: SluÅ¾bena dokumentacija i kartice modela.
- **Hugging Face Model Hub**: Interaktivni demo primjeri i primjeri zajednice.
- **IstraÅ¾ivaÄki radovi**: TehniÄki radovi na arxivu za detaljno razumijevanje.
- **Forumi zajednice**: Aktivna podrÅ¡ka zajednice i rasprave.

### PoÄetak rada s Qwen modelima

#### Platforme za razvoj
1. **Hugging Face Transformers**: ZapoÄnite sa standardnom Python integracijom.
2. **ModelScope**: IstraÅ¾ite Alibaba alate za optimiziranu implementaciju.
3. **Lokalna implementacija**: Koristite Ollama ili direktne transformere za lokalno testiranje.

#### Put uÄenja
1. **Razumijevanje osnovnih pojmova**: ProuÄite arhitekturu i moguÄ‡nosti obitelji Qwen.
2. **Eksperimentiranje s varijantama**: Isprobajte razliÄite veliÄine modela kako biste razumjeli kompromis izmeÄ‘u performansi.
3. **Praksa implementacije**: Implementirajte modele u razvojnim okruÅ¾enjima.
4. **Optimizacija implementacije**: Fino podeÅ¡avanje za produkcijske sluÄajeve.

#### Najbolje prakse
- **ZapoÄnite s manjim modelima**: PoÄnite s manjim modelima (1.5B-7B) za poÄetni razvoj.
- **Koristite predloÅ¡ke za chat**: Primijenite pravilno formatiranje za optimalne rezultate.
- **Pratite resurse**: Pratite koriÅ¡tenje memorije i brzinu inferencije.
- **Razmotrite specijalizaciju**: Odaberite varijante specifiÄne za domenu kada je to prikladno.

## Napredni obrasci koriÅ¡tenja

### Primjeri fino podeÅ¡avanja

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### Specijalizirano oblikovanje upita

**Za zadatke kompleksnog zakljuÄivanja:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**Za generiranje koda s kontekstom:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### ViÅ¡ejeziÄne aplikacije

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ä¸­æ–‡)",
        "es": "Spanish (EspaÃ±ol)",
        "fr": "French (FranÃ§ais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (æ—¥æœ¬èª)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ğŸ”§ Obrasci za produkcijsku implementaciju

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## Strategije optimizacije performansi

### Optimizacija memorije

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### Optimizacija inferencije

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## Najbolje prakse i smjernice

### Sigurnost i privatnost

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### PraÄ‡enje i evaluacija

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## ZakljuÄak

Obitelj Qwen modela predstavlja sveobuhvatan pristup demokratizaciji AI tehnologije uz odrÅ¾avanje konkurentnih performansi u raznolikim aplikacijama. Kroz svoju predanost otvorenom pristupu, viÅ¡ejeziÄnim sposobnostima i fleksibilnim opcijama implementacije, Qwen omoguÄ‡uje organizacijama i developerima koriÅ¡tenje moÄ‡nih AI moguÄ‡nosti bez obzira na njihove resurse ili specifiÄne zahtjeve.

### KljuÄne toÄke

**Izvrsnost otvorenog koda**: Qwen pokazuje da modeli otvorenog koda mogu postiÄ‡i performanse konkurentne vlasniÄkim alternativama, uz pruÅ¾anje transparentnosti, prilagodljivosti i kontrole.

**Skalabilna arhitektura**: Raspon od 0.5B do 235B parametara omoguÄ‡uje implementaciju u cijelom spektru raÄunalnih okruÅ¾enja, od mobilnih ureÄ‘aja do poslovnih klastera.

**Specijalizirane sposobnosti**: Varijante specifiÄne za domenu, poput Qwen-Coder, Qwen-Math i Qwen-VL, pruÅ¾aju specijaliziranu ekspertizu uz odrÅ¾avanje opÄ‡eg jeziÄnog razumijevanja.

**Globalna dostupnost**: SnaÅ¾na podrÅ¡ka za viÅ¡e jezika u viÅ¡e od 119 jezika Äini Qwen prikladnim za meÄ‘unarodne aplikacije i raznolike korisniÄke baze.

**Kontinuirana inovacija**: Evolucija od Qwen 1.0 do Qwen3 pokazuje dosljedno poboljÅ¡anje sposobnosti, uÄinkovitosti i opcija implementacije.

### Pogled u buduÄ‡nost

Kako se obitelj Qwen nastavlja razvijati, moÅ¾emo oÄekivati:

- **PoboljÅ¡anu uÄinkovitost**: Kontinuiranu optimizaciju za bolje omjere performansi po parametru.
- **ProÅ¡irene multimodalne sposobnosti**: Integraciju sofisticiranije obrade vizije, zvuka i teksta.
- **Naprednije zakljuÄivanje**: PoboljÅ¡ani mehanizmi razmiÅ¡ljanja i sposobnosti rjeÅ¡avanja problema u viÅ¡e koraka.
- **Bolje alate za implementaciju**: PoboljÅ¡ane okvire i alate za optimizaciju za raznolike scenarije implementacije.
- **Rast zajednice**: ProÅ¡ireni ekosustav alata, aplikacija i doprinosa zajednice.

### SljedeÄ‡i koraci

Bilo da gradite chatbot, razvijate edukacijske alate, stvarate asistente za kodiranje ili radite na viÅ¡ejeziÄnim aplikacijama, obitelj Qwen pruÅ¾a skalabilna rjeÅ¡enja uz snaÅ¾nu podrÅ¡ku zajednice i sveobuhvatnu dokumentaciju.

Za najnovije informacije, izdanja modela i detaljnu tehniÄku dokumentaciju, posjetite sluÅ¾bene Qwen repozitorije na Hugging Faceu i istraÅ¾ite aktivne rasprave i primjere zajednice.

BuduÄ‡nost razvoja AI-a leÅ¾i u dostupnim, transparentnim i moÄ‡nim alatima koji omoguÄ‡uju inovaciju u svim sektorima i razmjerima. Obitelj Qwen utjelovljuje ovu viziju, pruÅ¾ajuÄ‡i organizacijama i developerima temelj za izgradnju sljedeÄ‡e generacije aplikacija pokretanih AI-jem.

## Dodatni resursi

- **SluÅ¾bena dokumentacija**: [Qwen Dokumentacija](https://qwen.readthedocs.io/)
- **Model Hub**: [Hugging Face Qwen Kolekcije](https://huggingface.co/collections/Qwen/)
- **TehniÄki radovi**: [Qwen IstraÅ¾ivaÄke Publikacije](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **Zajednica**: [GitHub Rasprave i Problemi](https://github.com/QwenLM/)
- **ModelScope Platforma**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## Ishodi uÄenja

Nakon zavrÅ¡etka ovog modula, moÄ‡i Ä‡ete:

1. Objasniti arhitektonske prednosti obitelji Qwen modela i njihov pristup otvorenom kodu.
2. Odabrati odgovarajuÄ‡u varijantu Qwen modela na temelju specifiÄnih zahtjeva aplikacije i ograniÄenja resursa.
3. Implementirati Qwen modele u raznim scenarijima implementacije s optimiziranim konfiguracijama.
4. Primijeniti tehnike kvantizacije i optimizacije za poboljÅ¡anje performansi Qwen modela.
5. Procijeniti kompromis izmeÄ‘u veliÄine modela, performansi i sposobnosti unutar obitelji Qwen.

## Å to slijedi

- [03: Osnove obitelji Gemma](03.GemmaFamily.md)

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoÄ‡u AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kljuÄne informacije preporuÄuje se profesionalni prijevod od strane struÄnjaka. Ne preuzimamo odgovornost za nesporazume ili pogreÅ¡na tumaÄenja koja mogu proizaÄ‡i iz koriÅ¡tenja ovog prijevoda.