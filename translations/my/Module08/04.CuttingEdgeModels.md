<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "949045c54206448d55d987e8b23a82cf",
  "translation_date": "2025-09-25T02:21:07+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "my"
}
-->
# á€¡á€á€”á€ºá€¸ á„: Chainlit á€–á€¼á€„á€·á€º á€‘á€¯á€á€ºá€œá€¯á€•á€ºá€™á€¾á€¯á€¡á€†á€„á€·á€º Chat Applications á€á€Šá€ºá€†á€±á€¬á€€á€ºá€á€¼á€„á€ºá€¸

## á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

á€’á€®á€¡á€á€”á€ºá€¸á€™á€¾á€¬ Chainlit á€”á€²á€· Microsoft Foundry Local á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€¼á€®á€¸ á€‘á€¯á€á€ºá€œá€¯á€•á€ºá€™á€¾á€¯á€¡á€†á€„á€·á€º chat applications á€á€Šá€ºá€†á€±á€¬á€€á€ºá€•á€¯á€¶á€€á€­á€¯ á€œá€±á€·á€œá€¬á€•á€«á€™á€šá€ºá‹ AI á€…á€€á€¬á€¸á€á€­á€¯á€„á€ºá€¸á€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€á€±á€á€ºá€™á€® web interface á€™á€»á€¬á€¸á€–á€”á€ºá€á€®á€¸á€á€¼á€„á€ºá€¸áŠ streaming responses á€€á€­á€¯ á€¡á€€á€±á€¬á€„á€ºá€¡á€‘á€Šá€ºá€–á€±á€¬á€ºá€á€¼á€„á€ºá€¸áŠ error handling á€”á€²á€· user experience design á€€á€­á€¯ á€‘á€­á€›á€±á€¬á€€á€ºá€…á€½á€¬ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€‘á€¬á€¸á€á€²á€· chat applications á€™á€»á€¬á€¸á€€á€­á€¯ á€á€Šá€ºá€†á€±á€¬á€€á€ºá€á€¼á€„á€ºá€¸á€á€­á€¯á€·á€€á€­á€¯ á€á€„á€ºá€šá€°á€”á€­á€¯á€„á€ºá€•á€«á€™á€šá€ºá‹

**á€á€„á€ºá€á€Šá€ºá€†á€±á€¬á€€á€ºá€™á€šá€·á€ºá€¡á€›á€¬á€™á€»á€¬á€¸:**
- **Chainlit Chat App**: Streaming responses á€•á€«á€á€„á€ºá€á€²á€· á€á€±á€á€ºá€™á€® web UI
- **WebGPU Demo**: Privacy-first applications á€¡á€á€½á€€á€º browser-based inference  
- **Open WebUI Integration**: Foundry Local á€–á€¼á€„á€·á€º á€•á€›á€±á€¬á€ºá€–á€€á€ºá€›á€¾á€„á€ºá€”á€šá€º chat interface
- **Production Patterns**: Error handling, monitoring, á€”á€²á€· deployment strategies

## á€á€„á€ºá€šá€°á€›á€™á€šá€·á€ºá€¡á€›á€¬á€™á€»á€¬á€¸

- Chainlit á€–á€¼á€„á€·á€º á€‘á€¯á€á€ºá€œá€¯á€•á€ºá€™á€¾á€¯á€¡á€†á€„á€·á€º chat applications á€á€Šá€ºá€†á€±á€¬á€€á€ºá€á€¼á€„á€ºá€¸
- User experience á€€á€­á€¯ á€™á€¼á€¾á€„á€·á€ºá€á€„á€ºá€•á€±á€¸á€á€²á€· streaming responses á€€á€­á€¯ á€¡á€€á€±á€¬á€„á€ºá€¡á€‘á€Šá€ºá€–á€±á€¬á€ºá€á€¼á€„á€ºá€¸
- Foundry Local SDK integration patterns á€€á€­á€¯ á€€á€»á€½á€™á€ºá€¸á€€á€»á€„á€ºá€…á€½á€¬ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¼á€„á€ºá€¸
- Error handling á€”á€²á€· graceful degradation á€€á€­á€¯ á€¡á€€á€±á€¬á€„á€ºá€¸á€†á€¯á€¶á€¸ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¼á€„á€ºá€¸
- Chat applications á€™á€»á€¬á€¸á€€á€­á€¯ á€¡á€á€¼á€±á€¡á€”á€±á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€¡á€á€½á€€á€º deploy á€”á€²á€· configure á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸
- Conversational AI á€¡á€á€½á€€á€º á€á€±á€á€ºá€™á€® web UI patterns á€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€á€¼á€„á€ºá€¸

## á€€á€¼á€­á€¯á€á€„á€ºá€œá€­á€¯á€¡á€•á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸

- **Foundry Local**: Install á€œá€¯á€•á€ºá€•á€¼á€®á€¸ run á€œá€¯á€•á€ºá€‘á€¬á€¸ ([Installation Guide](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: 3.10 á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º á€¡á€‘á€€á€º version á€”á€²á€· virtual environment capability
- **Model**: á€¡á€”á€Šá€ºá€¸á€†á€¯á€¶á€¸ model á€á€…á€ºá€á€¯ load á€œá€¯á€•á€ºá€‘á€¬á€¸ (`foundry model run phi-4-mini`)
- **Browser**: WebGPU support á€›á€¾á€­á€á€²á€· á€á€±á€á€ºá€™á€® web browser (Chrome/Edge)
- **Docker**: Open WebUI integration á€¡á€á€½á€€á€º (optional)

## á€¡á€•á€­á€¯á€„á€ºá€¸ á: á€á€±á€á€ºá€™á€® Chat Applications á€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€á€¼á€„á€ºá€¸

### Architecture á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

```
User Browser â†â†’ Chainlit UI â†â†’ Python Backend â†â†’ Foundry Local â†â†’ AI Model
      â†“              â†“              â†“              â†“            â†“
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### á€¡á€“á€­á€€ á€”á€Šá€ºá€¸á€•á€Šá€¬á€™á€»á€¬á€¸

**Foundry Local SDK Patterns:**
- `FoundryLocalManager(alias)`: Service á€€á€­á€¯ á€¡á€œá€­á€¯á€¡á€œá€»á€±á€¬á€€á€º á€…á€®á€™á€¶á€á€”á€·á€ºá€á€½á€²á€á€¼á€„á€ºá€¸
- `manager.endpoint` á€”á€²á€· `manager.api_key`: Connection details
- `manager.get_model_info(alias).id`: Model identification

**Chainlit Framework:**
- `@cl.on_chat_start`: Chat session á€™á€»á€¬á€¸á€€á€­á€¯ initialize á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸
- `@cl.on_message`: User message á€™á€»á€¬á€¸á€€á€­á€¯ handle á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸  
- `cl.Message().stream_token()`: Real-time streaming
- UI á€€á€­á€¯ á€¡á€œá€­á€¯á€¡á€œá€»á€±á€¬á€€á€º á€–á€”á€ºá€á€®á€¸á€á€¼á€„á€ºá€¸ á€”á€²á€· WebSocket á€…á€®á€™á€¶á€á€”á€·á€ºá€á€½á€²á€™á€¾á€¯

## á€¡á€•á€­á€¯á€„á€ºá€¸ á‚: Local á€”á€²á€· Cloud Decision Matrix

### Performance Characteristics

| Aspect | Local (Foundry) | Cloud (Azure OpenAI) |
|--------|-----------------|---------------------|
| **Latency** | ğŸš€ 50-200ms (network á€™á€œá€­á€¯á€¡á€•á€º) | â±ï¸ 200-2000ms (network á€¡á€•á€±á€«á€º á€™á€°á€á€Šá€º) |
| **Privacy** | ğŸ”’ Data á€€ device á€‘á€²á€™á€¾á€¬á€•á€² á€›á€¾á€­ | âš ï¸ Data á€€á€­á€¯ cloud á€á€­á€¯á€· á€•á€­á€¯á€· |
| **Cost** | ğŸ’° Hardware á€›á€¾á€­á€•á€¼á€®á€¸á€›á€„á€º á€¡á€á€™á€²á€· | ğŸ’¸ Token á€¡á€œá€­á€¯á€€á€º á€•á€±á€¸á€› |
| **Offline** | âœ… Internet á€™á€œá€­á€¯á€¡á€•á€º | âŒ Internet á€œá€­á€¯á€¡á€•á€º |
| **Model Size** | âš ï¸ Hardware á€¡á€•á€±á€«á€º á€™á€°á€á€Šá€º | âœ… á€¡á€€á€¼á€®á€¸á€†á€¯á€¶á€¸ models á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€”á€­á€¯á€„á€º |
| **Scaling** | âš ï¸ Hardware á€¡á€•á€±á€«á€º á€™á€°á€á€Šá€º | âœ… á€¡á€€á€”á€·á€ºá€¡á€á€á€ºá€™á€›á€¾á€­ scaling |

### Hybrid Strategy Patterns

**Local-First with Fallback:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Task-Based Routing:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## á€¡á€•á€­á€¯á€„á€ºá€¸ áƒ: Sample 04 - Chainlit Chat Application

### Quick Start

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Application á€€ `http://localhost:8080` á€™á€¾á€¬ á€á€±á€á€ºá€™á€® chat interface á€”á€²á€· á€¡á€œá€­á€¯á€¡á€œá€»á€±á€¬á€€á€º á€–á€½á€„á€·á€ºá€•á€«á€™á€šá€ºá‹

### Core Implementation

Sample 04 application á€€ á€‘á€¯á€á€ºá€œá€¯á€•á€ºá€™á€¾á€¯á€¡á€†á€„á€·á€º patterns á€™á€»á€¬á€¸á€€á€­á€¯ á€•á€¼á€á€‘á€¬á€¸á€•á€«á€á€šá€º:

**Automatic Service Discovery:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Streaming Chat Handler:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Configuration Options

**Environment Variables:**

| Variable | Description | Default | Example |
|----------|-------------|---------|----------|
| `MODEL` | á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€™á€šá€·á€º model alias | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | Foundry Local endpoint | Auto-detected | `http://localhost:51211` |
| `API_KEY` | API key (optional for local) | `""` | `your-api-key` |

**Advanced Usage:**
```cmd
# Use different model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## á€¡á€•á€­á€¯á€„á€ºá€¸ á„: Jupyter Notebooks á€–á€”á€ºá€á€®á€¸á€á€¼á€„á€ºá€¸ á€”á€²á€· á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¼á€„á€ºá€¸

### Notebook Support á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

Sample 04 á€™á€¾á€¬ comprehensive Jupyter notebook (`chainlit_app.ipynb`) á€•á€«á€á€„á€ºá€•á€¼á€®á€¸:

- **ğŸ“š á€•á€Šá€¬á€›á€±á€¸á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬á€™á€»á€¬á€¸**: á€¡á€†á€„á€·á€ºá€†á€„á€·á€º á€œá€±á€·á€œá€¬á€›á€±á€¸á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬á€™á€»á€¬á€¸
- **ğŸ”¬ á€¡á€•á€¼á€”á€ºá€¡á€œá€¾á€”á€º á€…á€™á€ºá€¸á€á€•á€ºá€™á€¾á€¯**: Code cells á€™á€»á€¬á€¸á€€á€­á€¯ run á€”á€²á€· á€…á€™á€ºá€¸á€á€•á€ºá€á€¼á€„á€ºá€¸
- **ğŸ“Š á€¡á€™á€¼á€„á€ºá€¡á€¬á€›á€¯á€¶ á€•á€¼á€á€™á€¾á€¯á€™á€»á€¬á€¸**: Charts, diagrams, á€”á€²á€· output visualization
- **ğŸ› ï¸ á€–á€½á€¶á€·á€–á€¼á€­á€¯á€¸á€›á€±á€¸ Tools**: Testing á€”á€²á€· debugging á€¡á€á€½á€„á€·á€ºá€¡á€›á€±á€¸á€™á€»á€¬á€¸

### á€á€„á€·á€ºá€€á€­á€¯á€šá€ºá€•á€­á€¯á€„á€º Notebooks á€–á€”á€ºá€á€®á€¸á€á€¼á€„á€ºá€¸

#### á€¡á€†á€„á€·á€º á: Jupyter Environment á€€á€­á€¯ Set Up á€œá€¯á€•á€ºá€•á€«

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### á€¡á€†á€„á€·á€º á‚: Notebook á€¡á€á€…á€º á€á€…á€ºá€á€¯ á€–á€”á€ºá€á€®á€¸á€•á€«

**VS Code á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¼á€„á€ºá€¸:**
1. Module08 directory á€€á€­á€¯ VS Code á€™á€¾á€¬ á€–á€½á€„á€·á€ºá€•á€«
2. `.ipynb` extension á€”á€²á€· á€–á€­á€¯á€„á€ºá€¡á€á€…á€º á€á€…á€ºá€á€¯ á€–á€”á€ºá€á€®á€¸á€•á€«
3. Prompt á€œá€¯á€•á€ºá€á€²á€·á€¡á€á€« "Foundry Local" kernel á€€á€­á€¯ á€›á€½á€±á€¸á€•á€«
4. á€á€„á€·á€ºá€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬á€™á€»á€¬á€¸á€”á€²á€· cells á€™á€»á€¬á€¸á€€á€­á€¯ á€…á€á€„á€ºá€‘á€Šá€·á€ºá€•á€«

**Jupyter Lab á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¼á€„á€ºá€¸:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Notebook Structure Best Practices

#### Cell Organization

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("âœ… Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### á€¡á€•á€¼á€”á€ºá€¡á€œá€¾á€”á€º á€”á€™á€°á€”á€¬á€™á€»á€¬á€¸ á€”á€²á€· á€œá€±á€·á€€á€»á€„á€·á€ºá€á€”á€ºá€¸á€™á€»á€¬á€¸

#### Exercise 1: Client Configuration Testing

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b-instruct"},
]

for config in configurations:
    print(f"\nğŸ§ª Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'âœ… Success' if result['status'] == 'ok' else 'âŒ Failed'}")
```

#### Exercise 2: Streaming Response Simulation

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("ğŸŒŠ Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\nâœ… Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## á€¡á€•á€­á€¯á€„á€ºá€¸ á…: WebGPU Browser Inference Demo

### á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

WebGPU á€€ AI models á€€á€­á€¯ browser á€‘á€²á€™á€¾á€¬ run á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€¡á€±á€¬á€„á€º á€œá€¯á€•á€ºá€•á€±á€¸á€•á€¼á€®á€¸ privacy á€”á€²á€· zero-install experiences á€¡á€á€½á€€á€º á€¡á€€á€±á€¬á€„á€ºá€¸á€†á€¯á€¶á€¸á€–á€¼á€…á€ºá€…á€±á€•á€«á€á€šá€ºá‹ á€’á€® sample á€€ ONNX Runtime Web á€”á€²á€· WebGPU execution á€€á€­á€¯ á€•á€¼á€á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹

### á€¡á€†á€„á€·á€º á: WebGPU Support á€€á€­á€¯ á€…á€…á€ºá€†á€±á€¸á€•á€«

**Browser Requirements:**
- Chrome/Edge 113+ WebGPU enabled
- á€…á€…á€ºá€†á€±á€¸á€›á€”á€º: `chrome://gpu` â†’ "WebGPU" status á€€á€­á€¯ á€¡á€á€Šá€ºá€•á€¼á€¯á€•á€«
- Programmatic check: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### á€¡á€†á€„á€·á€º á‚: WebGPU Demo á€á€…á€ºá€á€¯ á€–á€”á€ºá€á€®á€¸á€•á€«

Directory á€–á€”á€ºá€á€®á€¸á€•á€«: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>ğŸš€ WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = 'âŒ WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'ğŸ” WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('âœ… ONNX Runtime session created with WebGPU');
        log(`ğŸ“Š Input names: ${session.inputNames.join(', ')}`);
        log(`ğŸ“Š Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = 'âœ… WebGPU inference complete!';
        log(`ğŸ¯ Predicted class: ${maxIdx}`);
        log(`ğŸ“ˆ Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `âŒ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### á€¡á€†á€„á€·á€º áƒ: Demo á€€á€­á€¯ Run á€œá€¯á€•á€ºá€•á€«

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## á€¡á€•á€­á€¯á€„á€ºá€¸ á†: Open WebUI Integration

### á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

Open WebUI á€€ Foundry Local á€›á€²á€· OpenAI-compatible API á€€á€­á€¯ á€á€»á€­á€á€ºá€†á€€á€ºá€‘á€¬á€¸á€á€²á€· á€•á€›á€±á€¬á€ºá€–á€€á€ºá€›á€¾á€„á€ºá€”á€šá€º ChatGPT-like interface á€€á€­á€¯ á€•á€±á€¸á€…á€½á€™á€ºá€¸á€•á€«á€á€šá€ºá‹

### á€¡á€†á€„á€·á€º á: Prerequisites

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### á€¡á€†á€„á€·á€º á‚: Docker Setup (á€¡á€€á€¼á€¶á€•á€¼á€¯á€‘á€¬á€¸)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**á€™á€¾á€á€ºá€á€»á€€á€º:** `host.docker.internal` á€€ Windows á€™á€¾á€¬ Docker containers á€á€½á€±á€€á€­á€¯ host machine á€€á€­á€¯ á€á€»á€­á€á€ºá€†á€€á€ºá€”á€­á€¯á€„á€ºá€¡á€±á€¬á€„á€º á€œá€¯á€•á€ºá€•á€±á€¸á€•á€«á€á€šá€ºá‹

### á€¡á€†á€„á€·á€º áƒ: Configuration

1. **Browser á€€á€­á€¯ á€–á€½á€„á€·á€ºá€•á€«:** `http://localhost:3000` á€á€­á€¯á€· á€á€½á€¬á€¸á€•á€«
2. **Initial Setup:** Admin account á€á€…á€ºá€á€¯ á€–á€”á€ºá€á€®á€¸á€•á€«
3. **Model Configuration:**
   - Settings â†’ Models â†’ OpenAI API  
   - Base URL: `http://host.docker.internal:51211/v1`
   - API Key: `foundry-local-key` (á€™á€Šá€ºá€á€Šá€·á€º value á€™á€†á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€”á€­á€¯á€„á€º)
4. **Test Connection:** Models á€á€½á€± dropdown á€™á€¾á€¬ á€•á€±á€«á€ºá€œá€¬á€›á€™á€šá€º

### Troubleshooting

**á€¡á€™á€»á€¬á€¸á€†á€¯á€¶á€¸ á€€á€¼á€¯á€¶á€›á€á€²á€· á€•á€¼á€¿á€”á€¬á€™á€»á€¬á€¸:**

1. **Connection Refused:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Models á€™á€•á€±á€«á€ºá€œá€¬á€á€¼á€„á€ºá€¸:**
   - Model load á€œá€¯á€•á€ºá€‘á€¬á€¸á€™á€¾á€¯á€€á€­á€¯ á€…á€…á€ºá€†á€±á€¸á€•á€«: `foundry model list`
   - API response á€€á€­á€¯ á€…á€…á€ºá€†á€±á€¸á€•á€«: `curl http://localhost:51211/v1/models`
   - Open WebUI container á€€á€­á€¯ á€•á€¼á€”á€ºá€…á€á€„á€ºá€•á€«

## á€¡á€•á€­á€¯á€„á€ºá€¸ á‡: Production Deployment á€¡á€á€½á€€á€º á€…á€‰á€ºá€¸á€…á€¬á€¸á€…á€›á€¬á€™á€»á€¬á€¸

### Environment Configuration

**Development Setup:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Production Deployment:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Common Port Issues á€”á€²á€· Solutions

**Port 51211 Conflict á€€á€­á€¯ á€€á€¬á€€á€½á€šá€ºá€á€¼á€„á€ºá€¸:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Performance Monitoring

**Health Check Implementation:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

Session 4 á€™á€¾á€¬ conversational AI á€¡á€á€½á€€á€º production-ready Chainlit applications á€á€Šá€ºá€†á€±á€¬á€€á€ºá€á€¼á€„á€ºá€¸á€€á€­á€¯ á€œá€±á€·á€œá€¬á€á€²á€·á€•á€«á€á€šá€ºá‹ á€á€„á€ºá€œá€±á€·á€œá€¬á€á€²á€·á€á€¬á€á€½á€±á€€:

- âœ… **Chainlit Framework**: Chat applications á€¡á€á€½á€€á€º á€á€±á€á€ºá€™á€® UI á€”á€²á€· streaming support
- âœ… **Foundry Local Integration**: SDK á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€™á€¾á€¯ á€”á€²á€· configuration patterns  
- âœ… **WebGPU Inference**: Privacy á€¡á€á€½á€€á€º browser-based AI
- âœ… **Open WebUI Setup**: á€•á€›á€±á€¬á€ºá€–á€€á€ºá€›á€¾á€„á€ºá€”á€šá€º chat interface deployment
- âœ… **Production Patterns**: Error handling, monitoring, á€”á€²á€· scaling

Sample 04 application á€€ Microsoft Foundry Local á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€¼á€®á€¸ local AI models á€€á€­á€¯ leverage á€œá€¯á€•á€ºá€‘á€¬á€¸á€á€²á€· á€á€­á€¯á€„á€ºá€á€¶á€·á€á€²á€· chat interfaces á€á€Šá€ºá€†á€±á€¬á€€á€ºá€•á€¯á€¶á€¡á€á€½á€€á€º á€¡á€€á€±á€¬á€„á€ºá€¸á€†á€¯á€¶á€¸á€”á€™á€°á€”á€¬á€™á€»á€¬á€¸á€€á€­á€¯ á€•á€¼á€á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹

## References

- **[Sample 04: Chainlit Application](samples/04/README.md)**: Documentation á€•á€«á€á€„á€ºá€á€²á€· á€¡á€•á€¼á€Šá€·á€ºá€¡á€…á€¯á€¶ application
- **[Chainlit Educational Notebook](samples/04/chainlit_app.ipynb)**: á€¡á€•á€¼á€”á€ºá€¡á€œá€¾á€”á€º á€œá€±á€·á€œá€¬á€›á€±á€¸á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬á€™á€»á€¬á€¸
- **[Foundry Local Documentation](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Platform documentation á€¡á€•á€¼á€Šá€·á€ºá€¡á€…á€¯á€¶
- **[Chainlit Documentation](https://docs.chainlit.io/)**: Framework documentation
- **[Open WebUI Integration Guide](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: á€á€›á€¬á€¸á€á€„á€º tutorial

---

