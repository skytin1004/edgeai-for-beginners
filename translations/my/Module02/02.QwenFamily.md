<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-18T22:27:13+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "my"
}
-->
# á€¡á€•á€­á€¯á€„á€ºá€¸ á‚: Qwen á€™á€­á€á€¬á€¸á€…á€¯á€¡á€á€¼á€±á€á€¶á€¡á€á€»á€€á€ºá€™á€»á€¬á€¸

Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€­á€á€¬á€¸á€…á€¯á€á€Šá€º Alibaba Cloud á á€¡á€€á€¼á€®á€¸á€…á€¬á€¸á€˜á€¬á€á€¬á€…á€€á€¬á€¸á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€™á€»á€­á€¯á€¸á€…á€¯á€¶ AI á€¡á€á€½á€€á€º á€¡á€€á€»á€šá€ºá€•á€¼á€”á€·á€ºá€á€±á€¬á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€€á€­á€¯ á€€á€­á€¯á€šá€ºá€…á€¬á€¸á€•á€¼á€¯á€á€Šá€ºá‹ áá€„á€ºá€¸á€á€Šá€º á€¡á€á€½á€„á€·á€ºá€¡á€›á€±á€¸á€™á€»á€¬á€¸á€…á€½á€¬á€›á€¾á€­á€á€±á€¬ deployment á€›á€¾á€±á€·á€†á€±á€¬á€„á€ºá€™á€»á€¬á€¸á€á€½á€„á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€”á€­á€¯á€„á€ºá€…á€½á€™á€ºá€¸á€›á€¾á€­á€á€Šá€·á€ºá€¡á€•á€¼á€„á€º á€–á€½á€„á€·á€ºá€œá€¾á€…á€ºá€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€á€Šá€º á€‘á€°á€¸á€á€¼á€¬á€¸á€á€±á€¬á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€€á€­á€¯ á€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€€á€¼á€±á€¬á€„á€ºá€¸ á€•á€¼á€á€á€Šá€ºá‹ Qwen á€™á€­á€á€¬á€¸á€…á€¯á€á€Šá€º á€¡á€œá€½á€”á€ºá€¡á€…á€½á€™á€ºá€¸á€‘á€€á€ºá€á€±á€¬ AI á€…á€½á€™á€ºá€¸á€›á€Šá€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€¡á€œá€½á€šá€ºá€á€€á€° á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€”á€­á€¯á€„á€ºá€…á€½á€™á€ºá€¸á€”á€¾á€„á€·á€º á€¡á€•á€¼á€±á€¬á€„á€ºá€¸á€¡á€œá€²á€™á€»á€¬á€¸á€…á€½á€¬á€›á€¾á€­á€á€±á€¬ á€¡á€á€¼á€±á€¡á€”á€±á€™á€»á€¬á€¸á€á€½á€„á€º á€•á€¼á€­á€¯á€„á€ºá€†á€­á€¯á€„á€ºá€™á€¾á€¯á€›á€¾á€­á€á€±á€¬ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€€á€­á€¯ á€‘á€­á€”á€ºá€¸á€á€­á€™á€ºá€¸á€‘á€¬á€¸á€”á€­á€¯á€„á€ºá€…á€½á€™á€ºá€¸á€›á€¾á€­á€€á€¼á€±á€¬á€„á€ºá€¸ á€”á€¬á€¸á€œá€Šá€ºá€›á€”á€º á€¡á€›á€±á€¸á€€á€¼á€®á€¸á€á€Šá€ºá‹

## Developer á€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€¡á€›á€„á€ºá€¸á€¡á€™á€¼á€…á€ºá€™á€»á€¬á€¸

### Hugging Face Model Repository
á€›á€½á€±á€¸á€á€»á€šá€ºá€‘á€¬á€¸á€á€±á€¬ Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€­á€á€¬á€¸á€…á€¯á€™á€»á€¬á€¸á€€á€­á€¯ [Hugging Face](https://huggingface.co/models?search=qwen) á€™á€¾á€á€†á€„á€·á€º á€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€•á€¼á€®á€¸áŠ áá€„á€ºá€¸á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á á€¡á€á€»á€­á€¯á€·á€á€±á€¬á€™á€»á€­á€¯á€¸á€€á€½á€²á€™á€»á€¬á€¸á€€á€­á€¯ á€›á€šá€°á€”á€­á€¯á€„á€ºá€á€Šá€ºá‹ á€á€„á€·á€ºá€›á€²á€· á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€™á€¾á€¯á€¡á€á€½á€€á€º á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€•á€¼á€¯á€•á€¼á€„á€ºá€á€­á€¯á€¸á€á€€á€ºá€…á€±á€á€¼á€„á€ºá€¸á€”á€¾á€„á€·á€º framework á€™á€»á€¬á€¸á€…á€½á€¬á€™á€¾á€á€†á€„á€·á€º deployment á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€á€Šá€ºá‹

### Local Development Tools
á€’á€±á€á€á€½á€„á€ºá€¸á€á€½á€„á€º á€–á€½á€¶á€·á€–á€¼á€­á€¯á€¸á€á€­á€¯á€¸á€á€€á€ºá€™á€¾á€¯á€”á€¾á€„á€·á€º á€…á€™á€ºá€¸á€á€•á€ºá€™á€¾á€¯á€¡á€á€½á€€á€º [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€á€„á€·á€ºá€›á€²á€· á€–á€½á€¶á€·á€–á€¼á€­á€¯á€¸á€á€­á€¯á€¸á€á€€á€ºá€™á€¾á€¯á€…á€€á€ºá€•á€±á€«á€ºá€á€½á€„á€º á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€™á€¼á€¾á€„á€·á€ºá€á€„á€ºá€‘á€¬á€¸á€á€±á€¬á€¡á€á€¼á€±á€¡á€”á€±á€–á€¼á€„á€·á€º run á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€á€Šá€ºá‹

### Documentation á€¡á€›á€„á€ºá€¸á€¡á€™á€¼á€…á€ºá€™á€»á€¬á€¸
- [Qwen Model Documentation](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Optimizing Qwen Models for Edge Deployment](https://github.com/microsoft/olive)

## á€™á€­á€á€ºá€†á€€á€º

á€’á€®á€á€„á€ºá€á€”á€ºá€¸á€…á€¬á€™á€¾á€¬ Alibaba á€›á€²á€· Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€­á€á€¬á€¸á€…á€¯á€”á€¾á€„á€·á€º áá€„á€ºá€¸á á€¡á€á€¼á€±á€á€¶á€¡á€šá€°á€¡á€†á€™á€»á€¬á€¸á€€á€­á€¯ á€œá€±á€·á€œá€¬á€•á€«á€™á€Šá€ºá‹ Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€­á€á€¬á€¸á€…á€¯á á€á€­á€¯á€¸á€á€€á€ºá€™á€¾á€¯áŠ Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€‘á€­á€›á€±á€¬á€€á€ºá€…á€±á€á€±á€¬ á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€á€…á€ºá€™á€»á€¬á€¸áŠ á€™á€­á€á€¬á€¸á€…á€¯á€¡á€á€½á€„á€ºá€¸á€›á€¾á€­ á€¡á€“á€­á€€á€™á€»á€­á€¯á€¸á€€á€½á€²á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€¡á€á€¼á€±á€¡á€”á€±á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€á€½á€„á€º á€¡á€á€¯á€¶á€¸á€á€»á€”á€­á€¯á€„á€ºá€™á€¾á€¯á€™á€»á€¬á€¸á€€á€­á€¯ á€–á€±á€¬á€ºá€•á€¼á€•á€«á€™á€Šá€ºá‹

## á€á€„á€ºá€šá€°á€›á€™á€Šá€·á€ºá€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€ºá€™á€»á€¬á€¸

á€’á€®á€á€„á€ºá€á€”á€ºá€¸á€…á€¬á€¡á€†á€¯á€¶á€¸á€á€½á€„á€º á€á€„á€ºá€á€Šá€º á€¡á€±á€¬á€€á€ºá€•á€«á€¡á€á€»á€€á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€”á€­á€¯á€„á€ºá€•á€«á€™á€Šá€º-

- Alibaba á€›á€²á€· Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€­á€á€¬á€¸á€…á€¯á á€’á€®á€‡á€­á€¯á€„á€ºá€¸á€¡á€šá€°á€¡á€†á€”á€¾á€„á€·á€º á€á€­á€¯á€¸á€á€€á€ºá€™á€¾á€¯á€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€á€¼á€„á€ºá€¸
- á€™á€±á€¬á€ºá€’á€šá€ºá€¡á€›á€½á€šá€ºá€¡á€…á€¬á€¸á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€á€½á€„á€º Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€‘á€­á€›á€±á€¬á€€á€ºá€…á€±á€á€±á€¬ á€¡á€“á€­á€€á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€á€…á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€–á€±á€¬á€ºá€‘á€¯á€á€ºá€á€¼á€„á€ºá€¸
- Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€­á€¯á€¸á€€á€½á€²á€™á€»á€¬á€¸á á€¡á€€á€»á€­á€¯á€¸á€€á€»á€±á€¸á€‡á€°á€¸á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€€á€”á€·á€ºá€á€á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€á€­á€›á€¾á€­á€á€¼á€„á€ºá€¸
- Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á á€¡á€á€¯á€¶á€¸á€á€»á€™á€¾á€¯á€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€•á€¼á€®á€¸ á€¡á€™á€¾á€”á€ºá€á€€á€šá€ºá€¡á€á€¼á€±á€¡á€”á€±á€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€á€„á€·á€ºá€œá€»á€±á€¬á€ºá€á€±á€¬á€™á€»á€­á€¯á€¸á€€á€½á€²á€™á€»á€¬á€¸á€€á€­á€¯ á€›á€½á€±á€¸á€á€»á€šá€ºá€á€¼á€„á€ºá€¸

## á€á€±á€á€ºá€á€…á€º AI á€™á€±á€¬á€ºá€’á€šá€º Landscape á€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€á€¼á€„á€ºá€¸

AI landscape á€á€Šá€º á€¡á€œá€½á€”á€ºá€á€­á€¯á€¸á€á€€á€ºá€œá€¬á€•á€¼á€®á€¸áŠ á€¡á€–á€½á€²á€·á€¡á€…á€Šá€ºá€¸á€™á€»á€¬á€¸á€€ á€˜á€¬á€á€¬á€…á€€á€¬á€¸á€™á€±á€¬á€ºá€’á€šá€ºá€–á€½á€¶á€·á€–á€¼á€­á€¯á€¸á€á€­á€¯á€¸á€á€€á€ºá€™á€¾á€¯á€¡á€á€½á€€á€º á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€€á€­á€¯ á€œá€­á€¯á€€á€ºá€”á€¬á€”á€±á€á€Šá€ºá‹ á€á€á€»á€­á€¯á€·á€€ á€•á€­á€¯á€„á€ºá€†á€­á€¯á€„á€ºá€™á€¾á€¯á€›á€¾á€­á€á€±á€¬ á€–á€½á€„á€·á€ºá€™á€‘á€¬á€¸á€á€±á€¬á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€¡á€¬á€›á€¯á€¶á€…á€­á€¯á€€á€ºá€•á€¼á€®á€¸áŠ á€á€á€»á€­á€¯á€·á€€ á€–á€½á€„á€·á€ºá€œá€¾á€…á€ºá€™á€¾á€¯á€”á€¾á€„á€·á€º á€‘á€„á€ºá€›á€¾á€¬á€¸á€™á€¾á€¯á€€á€­á€¯ á€¡á€›á€±á€¸á€•á€±á€«á€ºá€‘á€¬á€¸á€á€Šá€ºá‹ á€›á€­á€¯á€¸á€›á€¬á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€á€Šá€º API á€™á€»á€¬á€¸á€™á€¾á€á€†á€„á€·á€ºá€á€¬ á€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€á€±á€¬ á€¡á€€á€¼á€®á€¸á€…á€¬á€¸á€•á€­á€¯á€„á€ºá€†á€­á€¯á€„á€ºá€™á€¾á€¯á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸ á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º á€…á€½á€™á€ºá€¸á€›á€Šá€ºá€”á€±á€¬á€€á€ºá€€á€»á€”á€±á€á€±á€¬ á€–á€½á€„á€·á€ºá€œá€¾á€…á€ºá€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€›á€½á€±á€¸á€á€»á€šá€ºá€›á€”á€ºá€œá€­á€¯á€¡á€•á€ºá€á€Šá€ºá‹

á€’á€® paradigm á€á€Šá€º á€¡á€–á€½á€²á€·á€¡á€…á€Šá€ºá€¸á€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€…á€½á€™á€ºá€¸á€›á€Šá€ºá€™á€¼á€„á€·á€º AI á€€á€­á€¯ á€›á€›á€¾á€­á€›á€”á€ºá€¡á€á€€á€ºá€¡á€á€²á€™á€»á€¬á€¸á€€á€­á€¯ á€–á€”á€ºá€á€®á€¸á€•á€¼á€®á€¸áŠ áá€„á€ºá€¸á€á€­á€¯á€·á á€’á€±á€á€¬áŠ á€€á€¯á€”á€ºá€€á€»á€…á€›á€­á€á€ºá€”á€¾á€„á€·á€º deployment á€¡á€œá€½á€šá€ºá€á€€á€°á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€”á€­á€¯á€„á€ºá€™á€¾á€¯á€€á€­á€¯ á€‘á€­á€”á€ºá€¸á€á€­á€™á€ºá€¸á€‘á€¬á€¸á€›á€”á€º á€¡á€á€€á€ºá€¡á€á€²á€™á€»á€¬á€¸á€€á€­á€¯ á€–á€¼á€…á€ºá€…á€±á€á€Šá€ºá‹ á€›á€­á€¯á€¸á€›á€¬á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€á€Šá€º cutting-edge á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€”á€¾á€„á€·á€º á€¡á€€á€»á€­á€¯á€¸á€›á€¾á€­á€á€±á€¬ deployment á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬á€™á€»á€¬á€¸á€¡á€€á€¼á€¬á€¸ á€›á€½á€±á€¸á€á€»á€šá€ºá€›á€”á€ºá€œá€­á€¯á€¡á€•á€ºá€á€Šá€ºá‹

## á€¡á€›á€Šá€ºá€¡á€á€½á€±á€¸á€™á€¼á€„á€·á€º AI á€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€™á€¾á€¯á á€¡á€á€€á€ºá€¡á€á€²

á€¡á€›á€Šá€ºá€¡á€á€½á€±á€¸á€™á€¼á€„á€·á€ºáŠ á€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€á€±á€¬ AI á€á€Šá€º á€¡á€á€¼á€±á€¡á€”á€±á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€á€½á€„á€º á€¡á€›á€±á€¸á€€á€¼á€®á€¸á€œá€¬á€á€Šá€ºá‹ á€¡á€–á€½á€²á€·á€¡á€…á€Šá€ºá€¸á€¡á€œá€­á€¯á€¡á€œá€»á€±á€¬á€€á€º deployment á€›á€½á€±á€¸á€á€»á€šá€ºá€™á€¾á€¯á€™á€»á€¬á€¸áŠ API á€€á€¯á€”á€ºá€€á€»á€…á€›á€­á€á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€‘á€­á€”á€ºá€¸á€á€»á€¯á€•á€ºá€”á€­á€¯á€„á€ºá€á€±á€¬ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€›á€¾á€­á€á€±á€¬ implementation á€™á€»á€¬á€¸áŠ á€€á€™á€¹á€˜á€¬á€œá€¯á€¶á€¸á€†á€­á€¯á€„á€ºá€›á€¬ application á€™á€»á€¬á€¸á€¡á€á€½á€€á€º multilingual á€…á€½á€™á€ºá€¸á€›á€Šá€ºá€™á€»á€¬á€¸ á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º coding á€”á€¾á€„á€·á€º á€á€„á€ºá€¹á€á€»á€¬á€€á€²á€·á€á€­á€¯á€·á€á€±á€¬ á€”á€šá€ºá€•á€šá€ºá€¡á€‘á€°á€¸á€€á€»á€½á€™á€ºá€¸á€€á€»á€„á€ºá€™á€¾á€¯á€œá€­á€¯á€¡á€•á€ºá€á€±á€¬ application á€™á€»á€¬á€¸á€€á€­á€¯ á€…á€‰á€ºá€¸á€…á€¬á€¸á€•á€«á‹

### á€¡á€“á€­á€€ Deployment á€œá€­á€¯á€¡á€•á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸

á€á€±á€á€ºá€á€…á€º AI deployment á€™á€»á€¬á€¸á€á€Šá€º á€¡á€œá€±á€·á€¡á€€á€»á€„á€·á€ºá€¡á€á€½á€€á€º á€¡á€“á€­á€€á€¡á€€á€”á€·á€ºá€¡á€á€á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€›á€„á€ºá€†á€­á€¯á€„á€ºá€›á€á€Šá€º-

- **á€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€™á€¾á€¯**: á€–á€½á€„á€·á€ºá€œá€¾á€…á€ºá€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€™á€¾á€¯á€á€Šá€º á€‘á€„á€ºá€›á€¾á€¬á€¸á€™á€¾á€¯á€”á€¾á€„á€·á€º customization á€¡á€á€½á€€á€º á€¡á€›á€±á€¸á€€á€¼á€®á€¸á€á€Šá€º
- **á€€á€¯á€”á€ºá€€á€»á€…á€›á€­á€á€ºá€á€€á€ºá€á€¬á€™á€¾á€¯**: á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€á€±á€¬ budget á€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€á€„á€·á€ºá€œá€»á€±á€¬á€ºá€á€±á€¬ á€€á€½á€”á€ºá€•á€»á€°á€á€¬á€œá€­á€¯á€¡á€•á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸
- **á€¡á€•á€¼á€±á€¬á€„á€ºá€¸á€¡á€œá€²á€”á€­á€¯á€„á€ºá€™á€¾á€¯**: á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€á€±á€¬ deployment á€¡á€á€¼á€±á€¡á€”á€±á€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€™á€±á€¬á€ºá€’á€šá€ºá€¡á€›á€½á€šá€ºá€¡á€…á€¬á€¸á€™á€»á€¬á€¸
- **á€€á€™á€¹á€˜á€¬á€œá€¯á€¶á€¸á€†á€­á€¯á€„á€ºá€›á€¬á€›á€±á€¬á€€á€ºá€›á€¾á€­á€”á€­á€¯á€„á€ºá€™á€¾á€¯**: multilingual á€”á€¾á€„á€·á€º cross-cultural á€…á€½á€™á€ºá€¸á€›á€Šá€ºá€™á€»á€¬á€¸
- **á€¡á€‘á€°á€¸á€€á€»á€½á€™á€ºá€¸á€€á€»á€„á€ºá€™á€¾á€¯**: á€¡á€‘á€°á€¸á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€™á€¾á€¯á€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€”á€šá€ºá€•á€šá€ºá€¡á€‘á€°á€¸á€™á€»á€­á€¯á€¸á€€á€½á€²á€™á€»á€¬á€¸

## Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€¡á€šá€°á€¡á€†

Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€­á€á€¬á€¸á€…á€¯á€á€Šá€º AI á€™á€±á€¬á€ºá€’á€šá€ºá€–á€½á€¶á€·á€–á€¼á€­á€¯á€¸á€á€­á€¯á€¸á€á€€á€ºá€™á€¾á€¯á€¡á€á€½á€€á€º á€¡á€€á€»á€šá€ºá€•á€¼á€”á€·á€ºá€á€±á€¬á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€€á€­á€¯ á€€á€­á€¯á€šá€ºá€…á€¬á€¸á€•á€¼á€¯á€•á€¼á€®á€¸áŠ á€–á€½á€„á€·á€ºá€œá€¾á€…á€ºá€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€™á€¾á€¯áŠ multilingual á€…á€½á€™á€ºá€¸á€›á€Šá€ºá€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€¡á€€á€»á€­á€¯á€¸á€›á€¾á€­á€á€±á€¬ deployment á€€á€­á€¯ á€¦á€¸á€…á€¬á€¸á€•á€±á€¸á€‘á€¬á€¸á€á€Šá€ºá‹ Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€á€Šá€º á€™á€±á€¬á€ºá€’á€šá€ºá€¡á€›á€½á€šá€ºá€¡á€…á€¬á€¸á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸áŠ á€¡á€›á€Šá€ºá€¡á€á€½á€±á€¸á€™á€¼á€„á€·á€º training á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€”á€šá€ºá€•á€šá€ºá€¡á€‘á€°á€¸á€™á€»á€­á€¯á€¸á€€á€½á€²á€™á€»á€¬á€¸á€™á€¾á€á€†á€„á€·á€º á€•á€¼á€­á€¯á€„á€ºá€†á€­á€¯á€„á€ºá€™á€¾á€¯á€›á€¾á€­á€á€±á€¬ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€€á€­á€¯ á€›á€›á€¾á€­á€…á€±á€á€Šá€ºá‹

Qwen á€™á€­á€á€¬á€¸á€…á€¯á€á€Šá€º á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€º-á€‘á€­á€›á€±á€¬á€€á€ºá€™á€¾á€¯ spectrum á€¡á€€á€»á€šá€ºá€¡á€á€”á€ºá€¸á€€á€­á€¯ á€•á€±á€¸á€…á€½á€™á€ºá€¸á€›á€”á€º á€›á€Šá€ºá€›á€½á€šá€ºá€‘á€¬á€¸á€•á€¼á€®á€¸áŠ mobile device á€™á€»á€¬á€¸á€™á€¾ á€…á€®á€¸á€•á€½á€¬á€¸á€›á€±á€¸ server á€™á€»á€¬á€¸á€¡á€‘á€­ deployment á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€…á€½á€™á€ºá€¸á€›á€¾á€­á€á€Šá€ºá‹ áá€„á€ºá€¸áá€›á€Šá€ºá€™á€¾á€”á€ºá€¸á€á€»á€€á€ºá€™á€¾á€¬ á€¡á€›á€Šá€ºá€¡á€á€½á€±á€¸á€™á€¼á€„á€·á€º AI á€€á€­á€¯ á€œá€°á€á€­á€¯á€„á€ºá€¸á€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€…á€±á€•á€¼á€®á€¸áŠ deployment á€›á€½á€±á€¸á€á€»á€šá€ºá€™á€¾á€¯á€™á€»á€¬á€¸á€á€½á€„á€º á€¡á€œá€½á€šá€ºá€á€€á€°á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€”á€­á€¯á€„á€ºá€…á€½á€™á€ºá€¸á€€á€­á€¯ á€•á€±á€¸á€…á€½á€™á€ºá€¸á€á€¼á€„á€ºá€¸á€–á€¼á€…á€ºá€á€Šá€ºá‹

### Qwen á á€¡á€“á€­á€€á€’á€®á€‡á€­á€¯á€„á€ºá€¸á€¡á€šá€°á€¡á€†á€™á€»á€¬á€¸

Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€á€Šá€º á€¡á€á€¼á€¬á€¸á€˜á€¬á€á€¬á€…á€€á€¬á€¸á€™á€±á€¬á€ºá€’á€šá€ºá€™á€­á€á€¬á€¸á€…á€¯á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€€á€½á€²á€•á€¼á€¬á€¸á€…á€±á€á€±á€¬ á€¡á€á€¼á€±á€á€¶á€¡á€šá€°á€¡á€†á€™á€»á€¬á€¸á€¡á€•á€±á€«á€º á€¡á€á€¼á€±á€á€¶á€‘á€¬á€¸á€á€Šá€º-

- **Open Source First**: á€á€¯á€á€±á€á€”á€”á€¾á€„á€·á€º á€…á€®á€¸á€•á€½á€¬á€¸á€›á€±á€¸á€¡á€á€½á€€á€º á€¡á€•á€¼á€Šá€·á€ºá€¡á€á€‘á€„á€ºá€›á€¾á€¬á€¸á€™á€¾á€¯á€”á€¾á€„á€·á€º á€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€™á€¾á€¯
- **Comprehensive Training**: á€˜á€¬á€á€¬á€…á€€á€¬á€¸á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€”á€šá€ºá€•á€šá€ºá€™á€»á€¬á€¸á€…á€½á€¬á€€á€­á€¯ á€¡á€€á€»á€šá€ºá€¡á€á€”á€ºá€¸á€–á€¯á€¶á€¸á€œá€½á€¾á€™á€ºá€¸á€‘á€¬á€¸á€á€±á€¬ á€¡á€€á€¼á€®á€¸á€…á€¬á€¸áŠ á€™á€»á€­á€¯á€¸á€…á€¯á€¶ dataset á€™á€»á€¬á€¸á€•á€±á€«á€ºá€á€½á€„á€º training á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸
- **Scalable Architecture**: á€€á€½á€”á€ºá€•á€»á€°á€á€¬á€œá€­á€¯á€¡á€•á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€€á€­á€¯ á€€á€­á€¯á€€á€ºá€Šá€®á€…á€±á€›á€”á€º á€™á€±á€¬á€ºá€’á€šá€ºá€¡á€›á€½á€šá€ºá€¡á€…á€¬á€¸á€™á€»á€¬á€¸á€…á€½á€¬
- **Specialized Excellence**: á€¡á€‘á€°á€¸á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€™á€¾á€¯á€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€”á€šá€ºá€•á€šá€ºá€¡á€‘á€°á€¸á€™á€»á€­á€¯á€¸á€€á€½á€²á€™á€»á€¬á€¸

## Qwen á€™á€­á€á€¬á€¸á€…á€¯á€€á€­á€¯ á€•á€¶á€·á€•á€­á€¯á€¸á€•á€±á€¸á€á€±á€¬ á€¡á€“á€­á€€á€”á€Šá€ºá€¸á€•á€Šá€¬á€™á€»á€¬á€¸

### Massive Scale Training

Qwen á€™á€­á€á€¬á€¸á€…á€¯á á€¡á€“á€­á€€á€¡á€á€»á€€á€ºá€á€…á€ºá€á€¯á€™á€¾á€¬ á€™á€±á€¬á€ºá€’á€šá€ºá€–á€½á€¶á€·á€–á€¼á€­á€¯á€¸á€á€­á€¯á€¸á€á€€á€ºá€™á€¾á€¯á€¡á€á€½á€€á€º training data á€”á€¾á€„á€·á€º á€€á€½á€”á€ºá€•á€»á€°á€á€¬á€¡á€›á€„á€ºá€¸á€¡á€™á€¼á€…á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€¡á€€á€¼á€®á€¸á€…á€¬á€¸á€¡á€á€­á€¯á€„á€ºá€¸á€¡á€á€¬á€–á€¼á€„á€·á€º á€›á€„á€ºá€¸á€”á€¾á€®á€¸á€™á€¼á€¾á€¯á€•á€ºá€”á€¾á€¶á€‘á€¬á€¸á€á€¼á€„á€ºá€¸á€–á€¼á€…á€ºá€á€Šá€ºá‹ Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€á€Šá€º multilingual dataset á€™á€»á€¬á€¸á€€á€­á€¯ á€á€”á€ºá€¸á€•á€±á€«á€„á€ºá€¸á€™á€»á€¬á€¸á€…á€½á€¬á€á€±á€¬ token á€™á€»á€¬á€¸á€–á€¼á€„á€·á€º á€…á€”á€…á€ºá€á€€á€»á€›á€½á€±á€¸á€á€»á€šá€ºá€‘á€¬á€¸á€•á€¼á€®á€¸áŠ á€€á€™á€¹á€˜á€¬á€·á€¡á€á€­á€•á€Šá€¬á€”á€¾á€„á€·á€º reasoning á€…á€½á€™á€ºá€¸á€›á€Šá€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€•á€±á€¸á€…á€½á€™á€ºá€¸á€›á€”á€º á€›á€Šá€ºá€›á€½á€šá€ºá€‘á€¬á€¸á€á€Šá€ºá‹

á€’á€®á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€á€Šá€º á€¡á€›á€Šá€ºá€¡á€á€½á€±á€¸á€™á€¼á€„á€·á€º web content, academic literature, code repositories, á€”á€¾á€„á€·á€º multilingual resources á€™á€»á€¬á€¸á€€á€­á€¯ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€„á€·á€º á€¡á€œá€¯á€•á€ºá€œá€¯á€•á€ºá€á€Šá€ºá‹ training á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€á€Šá€º á€˜á€¬á€á€¬á€…á€€á€¬á€¸á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€”á€šá€ºá€•á€šá€ºá€™á€»á€¬á€¸á€¡á€€á€»á€šá€ºá€¡á€á€”á€ºá€¸á€€á€­á€¯ á€–á€¯á€¶á€¸á€œá€½á€¾á€™á€ºá€¸á€‘á€¬á€¸á€•á€¼á€®á€¸áŠ á€”á€¬á€¸á€œá€Šá€ºá€™á€¾á€¯á á€¡á€”á€€á€ºá€¡á€”á€€á€ºá€€á€­á€¯ á€¡á€›á€±á€¸á€•á€±á€«á€ºá€‘á€¬á€¸á€á€Šá€ºá‹

### Advanced Reasoning á€”á€¾á€„á€·á€º Thinking

á€”á€±á€¬á€€á€ºá€†á€¯á€¶á€¸ Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€á€Šá€º á€›á€¾á€¯á€•á€ºá€‘á€½á€±á€¸á€á€±á€¬ multi-step á€•á€¼á€¿á€”á€¬á€™á€»á€¬á€¸á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€”á€­á€¯á€„á€ºá€…á€½á€™á€ºá€¸á€›á€¾á€­á€á€±á€¬ reasoning á€…á€½á€™á€ºá€¸á€›á€Šá€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€á€Šá€º-

**Thinking Mode (Qwen3)**: á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€á€Šá€º á€œá€°á€·á€•á€¼á€¿á€”á€¬á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€”á€Šá€ºá€¸á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€†á€„á€ºá€á€°á€á€±á€¬ á€¡á€†á€„á€·á€ºá€†á€„á€·á€º reasoning á€€á€­á€¯ á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€¼á€®á€¸áŠ á€”á€±á€¬á€€á€ºá€†á€¯á€¶á€¸á€¡á€–á€¼á€±á€™á€»á€¬á€¸á€€á€­á€¯ á€•á€±á€¸á€”á€­á€¯á€„á€ºá€á€Šá€ºá‹

**Dual-Mode Operation**: á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€á€±á€¬á€™á€±á€¸á€á€½á€”á€ºá€¸á€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€¡á€™á€¼á€”á€ºá€á€¯á€¶á€·á€•á€¼á€”á€ºá€™á€¾á€¯ mode á€”á€¾á€„á€·á€º á€›á€¾á€¯á€•á€ºá€‘á€½á€±á€¸á€á€±á€¬á€•á€¼á€¿á€”á€¬á€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€¡á€”á€€á€ºá€¡á€”á€€á€ºá€á€½á€±á€¸á€á€±á€«á€ºá€™á€¾á€¯ mode á€¡á€€á€¼á€¬á€¸ switch á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€…á€½á€™á€ºá€¸á‹

**Chain-of-Thought Integration**: á€›á€¾á€¯á€•á€ºá€‘á€½á€±á€¸á€á€±á€¬á€¡á€œá€¯á€•á€ºá€™á€»á€¬á€¸á€á€½á€„á€º á€‘á€„á€ºá€›á€¾á€¬á€¸á€™á€¾á€¯á€”á€¾á€„á€·á€º á€á€­á€€á€»á€™á€¾á€¯á€€á€­á€¯ á€á€­á€¯á€¸á€á€€á€ºá€…á€±á€á€±á€¬ reasoning á€¡á€†á€„á€·á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€á€˜á€¬á€á€€á€»á€€á€» á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€á€¼á€„á€ºá€¸á‹

### Architectural Innovations

Qwen á€™á€­á€á€¬á€¸á€…á€¯á€á€Šá€º á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€”á€¾á€„á€·á€º á€‘á€­á€›á€±á€¬á€€á€ºá€™á€¾á€¯á€¡á€á€½á€€á€º á€¡á€†á€„á€·á€ºá€™á€¼á€¾á€„á€·á€ºá€á€„á€ºá€™á€¾á€¯á€™á€»á€¬á€¸á€…á€½á€¬á€€á€­á€¯ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€á€Šá€º-

**Scalable Design**: á€™á€±á€¬á€ºá€’á€šá€ºá€¡á€›á€½á€šá€ºá€¡á€…á€¬á€¸á€™á€»á€¬á€¸á€¡á€€á€»á€šá€ºá€¡á€á€”á€ºá€¸á€á€½á€„á€º á€¡á€œá€½á€šá€ºá€á€€á€° scale á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€…á€½á€™á€ºá€¸á€”á€¾á€„á€·á€º á€”á€¾á€­á€¯á€„á€ºá€¸á€šá€¾á€‰á€ºá€”á€­á€¯á€„á€ºá€…á€½á€™á€ºá€¸á€›á€¾á€­á€á€±á€¬ architecture á€á€…á€ºá€á€¯á‹

**Multimodal Integration**: text, vision, á€”á€¾á€„á€·á€º audio á€€á€­á€¯ á€á€…á€ºá€á€¯á€á€Šá€ºá€¸á€á€±á€¬ architecture á€¡á€á€½á€„á€ºá€¸ seamless á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€á€¼á€„á€ºá€¸á‹

**Deployment Optimization**: hardware configuration á€™á€»á€¬á€¸á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€¡á€á€½á€€á€º quantization á€›á€½á€±á€¸á€á€»á€šá€ºá€™á€¾á€¯á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º deployment format á€™á€»á€¬á€¸á‹

## Model Size á€”á€¾á€„á€·á€º Deployment á€›á€½á€±á€¸á€á€»á€šá€ºá€™á€¾á€¯á€™á€»á€¬á€¸

á€á€±á€á€ºá€á€…á€º deployment á€•á€á€ºá€á€”á€ºá€¸á€€á€»á€„á€ºá€™á€»á€¬á€¸á€á€Šá€º Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á á€€á€½á€”á€ºá€•á€»á€°á€á€¬á€œá€­á€¯á€¡á€•á€ºá€á€»á€€á€ºá€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€¡á€á€½á€€á€º á€¡á€•á€¼á€±á€¬á€„á€ºá€¸á€¡á€œá€²á€”á€­á€¯á€„á€ºá€…á€½á€™á€ºá€¸á€€á€­á€¯ á€¡á€€á€»á€­á€¯á€¸á€á€¶á€…á€¬á€¸á€›á€á€Šá€º-

### Small Models (0.5B-3B)

Qwen á€á€Šá€º edge deployment, mobile application á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º resource á€€á€”á€·á€ºá€á€á€ºá€‘á€¬á€¸á€á€±á€¬á€•á€á€ºá€á€”á€ºá€¸á€€á€»á€„á€ºá€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€á€„á€·á€ºá€œá€»á€±á€¬á€ºá€á€±á€¬ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€™á€¼á€„á€·á€º small models á€™á€»á€¬á€¸á€€á€­á€¯ á€•á€±á€¸á€…á€½á€™á€ºá€¸á€á€Šá€ºá‹

### Medium Models (7B-32B)

Mid-range á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€á€Šá€º á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€”á€¾á€„á€·á€º á€€á€½á€”á€ºá€•á€»á€°á€á€¬á€œá€­á€¯á€¡á€•á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸á€¡á€€á€¼á€¬á€¸ balance á€€á€±á€¬á€„á€ºá€¸á€™á€½á€”á€ºá€á€±á€¬ professional application á€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€™á€¼á€¾á€„á€·á€ºá€á€„á€ºá€‘á€¬á€¸á€á€Šá€ºá‹

### Large Models (72B+)

Full-scale á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€á€Šá€º á€¡á€™á€»á€¬á€¸á€†á€¯á€¶á€¸á€…á€½á€™á€ºá€¸á€›á€Šá€ºá€œá€­á€¯á€¡á€•á€ºá€á€±á€¬ application á€™á€»á€¬á€¸áŠ á€á€¯á€á€±á€á€”á€”á€¾á€„á€·á€º á€…á€®á€¸á€•á€½á€¬á€¸á€›á€±á€¸ deployment á€™á€»á€¬á€¸á€¡á€á€½á€€á€º state-of-the-art á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€€á€­á€¯ á€•á€±á€¸á€…á€½á€™á€ºá€¸á€á€Šá€ºá‹

## Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€­á€á€¬á€¸á€…á€¯á á€¡á€€á€»á€­á€¯á€¸á€€á€»á€±á€¸á€‡á€°á€¸á€™á€»á€¬á€¸

### Open Source á€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€™á€¾á€¯

Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€á€Šá€º vendor lock-in á€™á€›á€¾á€­á€˜á€² á€¡á€–á€½á€²á€·á€¡á€…á€Šá€ºá€¸á€™á€»á€¬á€¸á€€á€­á€¯ áá€„á€ºá€¸á€á€­á€¯á€·áá€œá€­á€¯á€¡á€•á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€á€¼á€„á€ºá€¸áŠ á€•á€¼á€¯á€•á€¼á€„á€ºá€á€¼á€„á€ºá€¸á€”á€¾á€„á€·á€º á€¡á€œá€½á€šá€ºá€á€€á€°á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€”á€­á€¯á€„á€ºá€…á€½á€™á€ºá€¸á€€á€­á€¯ á€•á€±á€¸á€…á€½á€™á€ºá€¸á€á€Šá€ºá‹

### Deployment á€¡á€•á€¼á€±á€¬á€„á€ºá€¸á€¡á€œá€²á€”á€­á€¯á€„á€ºá€…á€½á€™á€ºá€¸

á€™á€±á€¬á€ºá€’á€šá€ºá€¡á€›á€½á€šá€ºá€¡á€…á€¬á€¸á€™á€»á€¬á€¸á á€¡á€€á€»á€šá€ºá€¡á€á€”á€ºá€¸á€á€Šá€º mobile device á€™á€»á€¬á€¸á€™á€¾ high-end server á€™á€»á€¬á€¸á€¡á€‘á€­ hardware configuration á€™á€»á€¬á€¸á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€á€½á€„á€º deployment á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€…á€½á€™á€ºá€¸á€€á€­á€¯ á€•á€±á€¸á€…á€½á€™á€ºá€¸á€á€Šá€ºá‹

### Multilingual á€…á€½á€™á€ºá€¸á€›á€Šá€º

Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€á€Šá€º multilingual á€”á€¬á€¸á€œá€Šá€ºá€™á€¾á€¯á€”á€¾á€„á€·á€º á€–á€”á€ºá€á€®á€¸á€™á€¾á€¯á€á€½á€„á€º á€‘á€°á€¸á€á€»á€½á€”á€ºá€•á€¼á€®á€¸áŠ á€¡á€„á€ºá€¹á€‚á€œá€­á€•á€ºá€”á€¾á€„á€·á€º á€á€›á€¯á€á€ºá€˜á€¬á€á€¬á€…á€€á€¬á€¸á€á€½á€„á€º á€¡á€‘á€°á€¸á€¡á€¬á€¸á€€á€±á€¬á€„á€ºá€¸á€á€Šá€ºá‹ áá€„á€ºá€¸á€á€Šá€º á€€á€™á€¹á€˜á€¬á€œá€¯á€¶á€¸á€†á€­á€¯á€„á€ºá€›á€¬ application á€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€á€„á€·á€ºá€œá€»á€±á€¬á€ºá€á€Šá€ºá‹

### á€•á€¼á€­á€¯á€„á€ºá€†á€­á€¯á€„á€ºá€™á€¾á€¯á€›á€¾á€­á€á€±á€¬ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€º

Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€á€Šá€º benchmark á€™á€»á€¬á€¸á€á€½á€„á€º á€•á€¼á€­á€¯á€„á€ºá€†á€­á€¯á€„á€ºá€™á€¾á€¯á€›á€¾á€­á€á€±á€¬á€›á€œá€’á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€¡á€™á€¼á€²á€›á€›á€¾á€­á€•á€¼á€®á€¸áŠ á€–á€½á€„á€·á€ºá€œá€¾á€…á€ºá€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€™á€¾á€¯á€€á€­á€¯ á€•á€±á€¸á€…á€½á€™á€ºá€¸á€á€Šá€ºá‹ áá€„á€ºá€¸á€á€Šá€º á€–á€½á€„á€·á€ºá€œá€¾á€…á€ºá€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€á€Šá€º á€•á€­á€¯á€„á€ºá€†á€­á€¯á€„á€ºá€™á€¾á€¯á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€á€”á€ºá€¸á€á€°á€”á€­á€¯á€„á€ºá€€á€¼á€±á€¬á€„á€ºá€¸ á€•á€¼á€á€á€Šá€ºá‹

### á€¡á€‘á€°á€¸á€…á€½á€™á€ºá€¸á€›á€Šá€º

Qwen-Coder á€”á€¾á€„á€·á€º Qwen-Math á€€á€²á€·á€á€­á€¯á€·á€á€±á€¬ á€”á€šá€ºá€•á€šá€ºá€¡á€‘á€°á€¸á€™á€»á€­á€¯á€¸á€€á€½á€²á€™á€»á€¬á€¸á€á€Šá€º á€¡á€‘á€°á€¸á€€á€»á€½á€™á€ºá€¸á€€á€»á€„á€ºá€™á€¾á€¯á€€á€­á€¯ á€•á€±á€¸á€…á€½á€™á€ºá€¸á€•á€¼á€®á€¸áŠ á€˜á€¬á€á€¬á€…á€€á€¬á€¸á€”á€¬á€¸á€œá€Šá€ºá€™á€¾á€¯á€…á€½á€™á€ºá€¸á€›á€Šá€ºá€€á€­á€¯ á€‘á€­á€”á€ºá€¸á€á€­á€™á€ºá€¸á€‘á€¬á€¸á€á€Šá€ºá‹

## á€¡á€œá€±á€·á€¡á€€á€»á€„á€·á€ºá€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€™á€¾á€¯á€™á€»á€¬á€¸

á€”á€Šá€ºá€¸á€•á€Šá€¬á€†á€­á€¯á€„á€ºá€›á€¬á€¡á€á€±á€¸á€…á€­á€á€ºá€¡á€á€»á€€á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€™á€œá€±á€·á€œá€¬á€™á€®áŠ Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€€ á€˜á€¬á€á€½á€±á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€á€šá€ºá€†á€­á€¯á€á€¬á€€á€­á€¯ concrete á€¥á€•á€™á€¬á€™á€»á€¬á€¸á€–á€¼á€„á€·á€º á€œá€±á€·á€œá€¬á€€á€¼á€Šá€·á€ºá€•á€«-

### á€á€„á€ºá€¹á€á€»á€¬ reasoning á€¥á€•á€™á€¬

Qwen-Math á€á€Šá€º á€¡á€†á€„á€·á€ºá€†á€„á€·á€º á€á€„á€ºá€¹á€á€»á€¬á€•á€¼á€¿á€”á€¬á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€™á€¾á€¯á€á€½á€„á€º á€‘á€°á€¸á€á€»á€½á€”á€ºá€á€Šá€ºá‹ á€¥á€•á€™á€¬á€¡á€¬á€¸á€–á€¼á€„á€·á€ºáŠ á€›á€¾á€¯á€•á€ºá€‘á€½á€±á€¸á€á€±á€¬ calculus á€•á€¼á€¿á€”á€¬á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€›á€”á€ºá€™á€±á€¸á€œá€»á€¾á€„á€º-

```
User: Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = xÂ³ + 2xÂ² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹

Breaking down each term:
- d/dx(xÂ³) = 3xÂ²
- d/dx(2xÂ²) = 2 Â· 2xÂ¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3xÂ² + 4x - 5
```

### Multilingual Support á€¥á€•á€™á€¬

Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€á€Šá€º á€˜á€¬á€á€¬á€…á€€á€¬á€¸á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€á€½á€„á€º multilingual á€…á€½á€™á€ºá€¸á€›á€Šá€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€•á€¼á€á€á€Šá€º-

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**ä¸­æ–‡:**
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡æœºå™¨ï¼ˆç‰¹åˆ«æ˜¯è®¡ç®—æœºç³»ç»Ÿï¼‰å¯¹äººç±»æ™ºèƒ½è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚è¿™äº›è¿‡ç¨‹åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€è§£å†³é—®é¢˜ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚äººå·¥æ™ºèƒ½ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»è®¤çŸ¥èƒ½åŠ›çš„ä»»åŠ¡ã€‚
```

### Multimodal á€…á€½á€™á€ºá€¸á€›á€Šá€º á€¥á€•á€™á€¬

Qwen-VL á€á€Šá€º text á€”á€¾á€„á€·á€º image á€™á€»á€¬á€¸á€€á€­á€¯ á€á€…á€ºá€á€»á€­á€”á€ºá€á€Šá€ºá€¸á€á€½á€„á€º process á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€á€Šá€º-

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Code Generation á€¥á€•á€™á€¬

Qwen-Coder á€á€Šá€º programming language á€™á€»á€¬á€¸á€…á€½á€¬á€á€½á€„á€º code á€–á€”á€ºá€á€®á€¸á€á€¼á€„á€ºá€¸á€”á€¾á€„á€·á€º á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€•á€¼á€±á€¬á€•á€¼á€á€¼á€„á€ºá€¸á€á€½á€„á€º á€‘á€°á€¸á€á€»á€½á€”á€ºá€á€Šá€º-

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found
Here's how to get started with Qwen models using the Hugging Face Transformers library:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Qwen2.5 á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¼á€„á€ºá€¸

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### á€¡á€‘á€°á€¸á€•á€¼á€¯ á€™á€±á€¬á€ºá€’á€šá€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¼á€„á€ºá€¸

**Qwen-Coder á€–á€¼á€„á€·á€º á€€á€¯á€’á€ºá€‘á€¯á€á€ºá€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**á€á€„á€ºá€¹á€á€»á€¬á€†á€­á€¯á€„á€ºá€›á€¬ á€•á€¼á€¿á€”á€¬á€™á€»á€¬á€¸ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€á€¼á€„á€ºá€¸:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**á€™á€¼á€„á€ºá€€á€½á€„á€ºá€¸-á€˜á€¬á€á€¬á€…á€€á€¬á€¸ á€¡á€œá€¯á€•á€ºá€™á€»á€¬á€¸:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### á€…á€‰á€ºá€¸á€…á€¬á€¸á€™á€¾á€¯ á€¡á€”á€±á€¡á€‘á€¬á€¸ (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### ğŸ“± á€™á€­á€¯á€˜á€­á€¯á€„á€ºá€¸á€”á€¾á€„á€·á€º á€¡á€”á€¬á€¸á€á€á€º á€…á€€á€ºá€™á€»á€¬á€¸á€á€½á€„á€º á€á€„á€ºá€á€½á€„á€ºá€¸á€á€¼á€„á€ºá€¸

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### API á€á€„á€ºá€á€½á€„á€ºá€¸á€™á€¾á€¯ á€”á€™á€°á€”á€¬

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€º á€…á€™á€ºá€¸á€á€•á€ºá€™á€¾á€¯á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€¡á€±á€¬á€„á€ºá€™á€¼á€„á€ºá€™á€¾á€¯á€™á€»á€¬á€¸

Qwen á€™á€±á€¬á€ºá€’á€šá€º á€™á€­á€á€¬á€¸á€…á€¯á€á€Šá€º á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€á€±á€¬ á€…á€™á€ºá€¸á€á€•á€ºá€™á€¾á€¯á€™á€»á€¬á€¸á€á€½á€„á€º á€‘á€°á€¸á€á€»á€½á€”á€ºá€á€±á€¬ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€€á€­á€¯ á€›á€›á€¾á€­á€‘á€¬á€¸á€•á€¼á€®á€¸ open-source á€¡á€”á€±á€”á€¾á€„á€·á€º á€œá€½á€šá€ºá€€á€°á€…á€½á€¬ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€”á€­á€¯á€„á€ºá€…á€±á€•á€«á€á€Šá€ºá‹

### á€¡á€“á€­á€€ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€º á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸

**á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬ á€…á€‰á€ºá€¸á€…á€¬á€¸á€™á€¾á€¯ á€‘á€°á€¸á€á€»á€½á€”á€ºá€™á€¾á€¯:**
- Qwen3-235B-A22B á€á€Šá€º coding, math, general capabilities á€…á€™á€ºá€¸á€á€•á€ºá€™á€¾á€¯á€™á€»á€¬á€¸á€á€½á€„á€º DeepSeek-R1, o1, o3-mini, Grok-3, Gemini-2.5-Pro á€á€­á€¯á€·á€”á€¾á€„á€·á€º á€šá€¾á€‰á€ºá€•á€¼á€­á€¯á€„á€ºá€”á€­á€¯á€„á€ºá€á€±á€¬ á€›á€œá€’á€ºá€™á€»á€¬á€¸ á€›á€›á€¾á€­á€‘á€¬á€¸á€á€Šá€ºá‹
- Qwen3-30B-A3B á€á€Šá€º QwQ-32B á€€á€­á€¯ 10 á€† á€•á€­á€¯á€™á€­á€¯ á€¡á€€á€»á€­á€¯á€¸á€›á€¾á€­á€á€±á€¬ parameters á€–á€¼á€„á€·á€º á€€á€»á€±á€¬á€ºá€œá€½á€”á€ºá€”á€­á€¯á€„á€ºá€á€Šá€ºá‹
- Qwen3-4B á€á€Šá€º Qwen2.5-72B-Instruct á€”á€¾á€„á€·á€º á€á€”á€ºá€¸á€á€°á€á€±á€¬ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€º á€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€á€Šá€ºá‹

**á€‘á€­á€›á€±á€¬á€€á€ºá€™á€¾á€¯ á€¡á€±á€¬á€„á€ºá€™á€¼á€„á€ºá€™á€¾á€¯á€™á€»á€¬á€¸:**
- Qwen3-MoE base models á€á€Šá€º Qwen2.5 dense base models á€”á€¾á€„á€·á€º á€á€°á€Šá€®á€á€±á€¬ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€º á€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€•á€¼á€®á€¸ active parameters 10% á€á€¬ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€Šá€ºá‹
- Dense models á€”á€¾á€„á€·á€º á€”á€¾á€­á€¯á€„á€ºá€¸á€šá€¾á€‰á€ºá€•á€«á€€ training á€”á€¾á€„á€·á€º inference á€¡á€á€½á€€á€º á€¡á€œá€½á€”á€ºá€á€€á€ºá€á€¬á€á€±á€¬ á€€á€¯á€”á€ºá€€á€»á€…á€›á€­á€á€ºá‹

**á€˜á€¬á€á€¬á€…á€€á€¬á€¸ á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€”á€­á€¯á€„á€ºá€™á€¾á€¯:**
- Qwen3 á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€á€Šá€º 119 á€˜á€¬á€á€¬á€…á€€á€¬á€¸á€”á€¾á€„á€·á€º á€’á€±á€á€­á€šá€…á€€á€¬á€¸á€™á€»á€¬á€¸á€€á€­á€¯ á€•á€¶á€·á€•á€­á€¯á€¸á€‘á€¬á€¸á€á€Šá€ºá‹
- á€˜á€¬á€á€¬á€…á€€á€¬á€¸á€”á€¾á€„á€·á€º á€šá€‰á€ºá€€á€»á€±á€¸á€™á€¾á€¯ á€¡á€á€¼á€±á€¡á€”á€±á€™á€»á€¬á€¸ á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€á€½á€„á€º á€‘á€°á€¸á€á€»á€½á€”á€ºá€á€±á€¬ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá‹

**Training á€¡á€á€­á€¯á€„á€ºá€¸á€¡á€á€¬:**
- Qwen3 á€á€Šá€º Qwen2.5 á 18 trillion tokens á€”á€¾á€„á€·á€º á€”á€¾á€­á€¯á€„á€ºá€¸á€šá€¾á€‰á€ºá€•á€«á€€ 36 trillion tokens á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€‘á€¬á€¸á€•á€¼á€®á€¸ 119 á€˜á€¬á€á€¬á€…á€€á€¬á€¸á€”á€¾á€„á€·á€º á€’á€±á€á€­á€šá€…á€€á€¬á€¸á€™á€»á€¬á€¸á€€á€­á€¯ á€¡á€€á€»á€¯á€¶á€¸á€á€„á€ºá€…á€±á€á€Šá€ºá‹

### á€™á€±á€¬á€ºá€’á€šá€º á€”á€¾á€­á€¯á€„á€ºá€¸á€šá€¾á€‰á€ºá€™á€¾á€¯ á€‡á€šá€¬á€¸

| á€™á€±á€¬á€ºá€’á€šá€º á€™á€­á€á€¬á€¸á€…á€¯ | Parameters á€¡á€á€­á€¯á€„á€ºá€¸á€¡á€á€¬ | Context Length | á€¡á€“á€­á€€ á€¡á€¬á€¸á€á€¬á€á€»á€€á€ºá€™á€»á€¬á€¸ | á€¡á€€á€±á€¬á€„á€ºá€¸á€†á€¯á€¶á€¸ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€™á€¾á€¯á€™á€»á€¬á€¸ |
|------------------|------------------------|----------------|---------------------|----------------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | Balanced performance, multilingual | General applications, production deployment |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | Code generation, programming | Software development, coding assistance |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | Mathematical reasoning | Educational platforms, STEM applications |
| **Qwen2.5-VL** | Various | Variable | Vision-language understanding | Multimodal applications, image analysis |
| **Qwen3** | 0.6B-235B | Variable | Advanced reasoning, thinking mode | Complex reasoning, research applications |
| **Qwen3 MoE** | 30B-235B total | Variable | Efficient large-scale performance | Enterprise applications, high-performance needs |

## á€™á€±á€¬á€ºá€’á€šá€º á€›á€½á€±á€¸á€á€»á€šá€ºá€™á€¾á€¯ á€œá€™á€ºá€¸á€Šá€½á€¾á€”á€º

### á€¡á€á€¼á€±á€á€¶ á€¡á€€á€ºá€•á€œá€®á€€á€±á€¸á€›á€¾á€„á€ºá€¸á€™á€»á€¬á€¸á€¡á€á€½á€€á€º
- **Qwen2.5-0.5B/1.5B**: á€™á€­á€¯á€˜á€­á€¯á€„á€ºá€¸ app á€™á€»á€¬á€¸áŠ á€¡á€”á€¬á€¸á€á€á€º á€…á€€á€ºá€™á€»á€¬á€¸áŠ á€¡á€á€»á€­á€”á€ºá€”á€¾á€„á€·á€º á€á€•á€¼á€±á€¸á€Šá€® á€¡á€€á€ºá€•á€œá€®á€€á€±á€¸á€›á€¾á€„á€ºá€¸á€™á€»á€¬á€¸
- **Qwen2.5-3B/7B**: General chatbots, content generation, Q&A systems

### á€á€„á€ºá€¹á€á€»á€¬á€”á€¾á€„á€·á€º á€…á€‰á€ºá€¸á€…á€¬á€¸á€™á€¾á€¯ á€¡á€œá€¯á€•á€ºá€™á€»á€¬á€¸á€¡á€á€½á€€á€º
- **Qwen2.5-Math**: á€á€„á€ºá€¹á€á€»á€¬á€†á€­á€¯á€„á€ºá€›á€¬ á€•á€¼á€¿á€”á€¬á€™á€»á€¬á€¸ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€á€¼á€„á€ºá€¸á€”á€¾á€„á€·á€º STEM á€•á€Šá€¬á€›á€±á€¸
- **Qwen3 with Thinking Mode**: á€¡á€†á€„á€·á€ºá€™á€¼á€„á€·á€º reasoning á€œá€­á€¯á€¡á€•á€ºá€á€±á€¬ á€¡á€œá€¯á€•á€ºá€™á€»á€¬á€¸

### Programming á€”á€¾á€„á€·á€º Development á€¡á€á€½á€€á€º
- **Qwen2.5-Coder**: Code generation, debugging, programming assistance
- **Qwen3**: Reasoning capabilities á€•á€«á€á€„á€ºá€á€±á€¬ á€¡á€†á€„á€·á€ºá€™á€¼á€„á€·á€º programming á€¡á€œá€¯á€•á€ºá€™á€»á€¬á€¸

### Multimodal á€¡á€€á€ºá€•á€œá€®á€€á€±á€¸á€›á€¾á€„á€ºá€¸á€™á€»á€¬á€¸á€¡á€á€½á€€á€º
- **Qwen2.5-VL**: á€•á€¯á€¶á€”á€¬á€¸á€œá€Šá€ºá€™á€¾á€¯áŠ visual question answering
- **Qwen-Audio**: Audio processing á€”á€¾á€„á€·á€º á€…á€€á€¬á€¸á€á€¶á€”á€¬á€¸á€œá€Šá€ºá€™á€¾á€¯

### á€…á€®á€¸á€•á€½á€¬á€¸á€›á€±á€¸ á€á€„á€ºá€á€½á€„á€ºá€¸á€™á€¾á€¯á€¡á€á€½á€€á€º
- **Qwen2.5-32B/72B**: á€¡á€™á€¼á€„á€·á€ºá€†á€¯á€¶á€¸ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€º language understanding
- **Qwen3-235B-A22B**: á€¡á€œá€½á€”á€ºá€á€±á€¬á€„á€ºá€¸á€†á€­á€¯á€™á€¾á€¯á€™á€»á€¬á€¸á€›á€¾á€­á€á€±á€¬ á€¡á€€á€ºá€•á€œá€®á€€á€±á€¸á€›á€¾á€„á€ºá€¸á€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€¡á€™á€¼á€„á€·á€ºá€†á€¯á€¶á€¸ capability

## á€á€„á€ºá€á€½á€„á€ºá€¸á€™á€¾á€¯ á€•á€œá€€á€ºá€–á€±á€¬á€„á€ºá€¸á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€”á€­á€¯á€„á€ºá€™á€¾á€¯
### Cloud Platforms
- **Hugging Face Hub**: Model repository á€¡á€•á€¼á€Šá€·á€ºá€¡á€…á€¯á€¶á€”á€¾á€„á€·á€º community á€•á€¶á€·á€•á€­á€¯á€¸á€™á€¾á€¯
- **ModelScope**: Alibaba á model platform á€”á€¾á€„á€·á€º optimization tools
- **Various Cloud Providers**: ML platforms á€™á€»á€¬á€¸á€™á€¾ á€•á€¶á€·á€•á€­á€¯á€¸á€™á€¾á€¯

### á€’á€±á€á€á€½á€„á€ºá€¸ Development Frameworks
- **Transformers**: Hugging Face integration á€¡á€†á€„á€·á€ºá€™á€¼á€„á€·á€º
- **vLLM**: Production á€¡á€á€½á€€á€º á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€™á€¼á€„á€·á€º serving
- **Ollama**: á€’á€±á€á€á€½á€„á€ºá€¸ á€á€„á€ºá€á€½á€„á€ºá€¸á€™á€¾á€¯á€”á€¾á€„á€·á€º á€…á€®á€™á€¶á€á€”á€·á€ºá€á€½á€²á€™á€¾á€¯ á€œá€½á€šá€ºá€€á€°á€…á€±á€á€±á€¬ tool
- **ONNX Runtime**: Hardware á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€¡á€á€½á€€á€º optimization
- **llama.cpp**: Platform á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€¡á€á€½á€€á€º C++ á€¡á€€á€»á€­á€¯á€¸á€›á€¾á€­á€á€±á€¬ implementation

### á€á€„á€ºá€šá€°á€›á€±á€¸ á€›á€„á€ºá€¸á€™á€¼á€…á€ºá€™á€»á€¬á€¸
- **Qwen Documentation**: á€á€›á€¬á€¸á€á€„á€º documentation á€”á€¾á€„á€·á€º model cards
- **Hugging Face Model Hub**: Interactive demos á€”á€¾á€„á€·á€º community á€”á€™á€°á€”á€¬á€™á€»á€¬á€¸
- **Research Papers**: arxiv á€á€½á€„á€ºá€›á€¾á€­á€á€±á€¬ á€”á€Šá€ºá€¸á€•á€Šá€¬á€†á€­á€¯á€„á€ºá€›á€¬ á€…á€¬á€á€™á€ºá€¸á€™á€»á€¬á€¸
- **Community Forums**: Active community á€•á€¶á€·á€•á€­á€¯á€¸á€™á€¾á€¯á€”á€¾á€„á€·á€º á€†á€½á€±á€¸á€”á€½á€±á€¸á€™á€¾á€¯á€™á€»á€¬á€¸

### Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€…á€á€„á€ºá€á€¼á€„á€ºá€¸

#### Development Platforms
1. **Hugging Face Transformers**: Python integration á€–á€¼á€„á€·á€º á€…á€á€„á€ºá€•á€«
2. **ModelScope**: Alibaba á optimized deployment tools á€€á€­á€¯ á€…á€™á€ºá€¸á€á€•á€ºá€•á€«
3. **Local Deployment**: Ollama á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º direct transformers á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á á€…á€™á€ºá€¸á€á€•á€ºá€•á€«

#### Learning Path
1. **Core Concepts á€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€•á€«**: Qwen á€™á€±á€¬á€ºá€’á€šá€º á€™á€­á€á€¬á€¸á€…á€¯á architecture á€”á€¾á€„á€·á€º capabilities á€€á€­á€¯ á€œá€±á€·á€œá€¬á€•á€«
2. **Variants á€™á€»á€¬á€¸á€€á€­á€¯ á€…á€™á€ºá€¸á€á€•á€ºá€•á€«**: á€™á€±á€¬á€ºá€’á€šá€º á€¡á€›á€½á€šá€ºá€¡á€…á€¬á€¸ á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€€á€­á€¯ á€…á€™á€ºá€¸á€á€•á€ºá€•á€¼á€®á€¸ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€º trade-offs á€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€•á€«
3. **Implementation á€€á€­á€¯ á€œá€±á€·á€€á€»á€„á€·á€ºá€•á€«**: Development environments á€á€½á€„á€º á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€á€„á€ºá€á€½á€„á€ºá€¸á€•á€«
4. **Deployment á€€á€­á€¯ Optimize á€œá€¯á€•á€ºá€•á€«**: Production use cases á€¡á€á€½á€€á€º Fine-tune á€œá€¯á€•á€ºá€•á€«

#### Best Practices
- **Start Small**: á€…á€á€„á€ºá€–á€½á€„á€·á€ºá€œá€¾á€…á€ºá€™á€¾á€¯á€¡á€á€½á€€á€º á€™á€±á€¬á€ºá€’á€šá€ºá€¡á€á€±á€¸á€™á€»á€¬á€¸ (1.5B-7B) á€–á€¼á€„á€·á€º á€…á€á€„á€ºá€•á€«
- **Chat Templates á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€«**: á€¡á€€á€±á€¬á€„á€ºá€¸á€†á€¯á€¶á€¸á€›á€œá€’á€ºá€™á€»á€¬á€¸á€¡á€á€½á€€á€º formatting á€€á€­á€¯ á€á€„á€·á€ºá€á€±á€¬á€ºá€…á€½á€¬ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€«
- **Resources á€€á€­á€¯ á€…á€±á€¬á€„á€·á€ºá€€á€¼á€Šá€·á€ºá€•á€«**: memory usage á€”á€¾á€„á€·á€º inference speed á€€á€­á€¯ á€…á€±á€¬á€„á€·á€ºá€€á€¼á€Šá€·á€ºá€•á€«
- **Specialization á€€á€­á€¯ á€…á€‰á€ºá€¸á€…á€¬á€¸á€•á€«**: domain-specific variants á€€á€­á€¯ á€á€„á€·á€ºá€á€±á€¬á€ºá€á€±á€¬á€¡á€á€« á€›á€½á€±á€¸á€á€»á€šá€ºá€•á€«

## á€¡á€†á€„á€·á€ºá€™á€¼á€„á€·á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€™á€¾á€¯ á€•á€¯á€¶á€…á€¶á€™á€»á€¬á€¸

### Fine-tuning á€”á€™á€°á€”á€¬á€™á€»á€¬á€¸

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### á€¡á€‘á€°á€¸á€•á€¼á€¯ Prompt Engineering

**á€¡á€†á€„á€·á€ºá€™á€¼á€„á€·á€º reasoning á€¡á€œá€¯á€•á€ºá€™á€»á€¬á€¸á€¡á€á€½á€€á€º:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**Context á€•á€«á€á€„á€ºá€á€±á€¬ Code Generation á€¡á€á€½á€€á€º:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### Multilingual Applications

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ä¸­æ–‡)",
        "es": "Spanish (EspaÃ±ol)",
        "fr": "French (FranÃ§ais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (æ—¥æœ¬èª)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ğŸ”§ Production Deployment á€•á€¯á€¶á€…á€¶á€™á€»á€¬á€¸

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€º á€¡á€‘á€°á€¸á€•á€¼á€¯á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€™á€»á€¬á€¸

### Memory Optimization

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### Inference Optimization

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## Best Practices á€”á€¾á€„á€·á€º á€œá€™á€ºá€¸á€Šá€½á€¾á€”á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸

### Security á€”á€¾á€„á€·á€º Privacy

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### Monitoring á€”á€¾á€„á€·á€º Evaluation

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## á€”á€­á€‚á€¯á€¶á€¸

Qwen á€™á€±á€¬á€ºá€’á€šá€º á€™á€­á€á€¬á€¸á€…á€¯á€á€Šá€º AI á€”á€Šá€ºá€¸á€•á€Šá€¬á€€á€­á€¯ á€œá€°á€á€­á€¯á€„á€ºá€¸á€›á€±á€¬á€€á€ºá€›á€¾á€­á€”á€­á€¯á€„á€ºá€…á€±á€›á€”á€º á€¡á€‘á€°á€¸á€•á€¼á€¯ approach á€á€…á€ºá€á€¯á€€á€­á€¯ á€€á€­á€¯á€šá€ºá€…á€¬á€¸á€•á€¼á€¯á€‘á€¬á€¸á€•á€¼á€®á€¸ á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€á€±á€¬ á€¡á€€á€ºá€•á€œá€®á€€á€±á€¸á€›á€¾á€„á€ºá€¸á€™á€»á€¬á€¸á€á€½á€„á€º á€šá€¾á€‰á€ºá€•á€¼á€­á€¯á€„á€ºá€”á€­á€¯á€„á€ºá€á€±á€¬ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€€á€­á€¯ á€‘á€­á€”á€ºá€¸á€á€­á€™á€ºá€¸á€‘á€¬á€¸á€•á€«á€á€Šá€ºá‹ Open-source á€¡á€”á€±á€”á€¾á€„á€·á€º multilingual capabilities á€”á€¾á€„á€·á€º flexible deployment options á€™á€»á€¬á€¸á€–á€¼á€„á€·á€º Qwen á€á€Šá€º á€¡á€–á€½á€²á€·á€¡á€…á€Šá€ºá€¸á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º Developer á€™á€»á€¬á€¸á€€á€­á€¯ á€á€€á€ºá€†á€­á€¯á€„á€ºá€›á€¬ á€œá€­á€¯á€¡á€•á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸á€”á€¾á€„á€·á€º resource á€™á€»á€¬á€¸á€™á€›á€½á€±á€¸ á€¡á€„á€ºá€¡á€¬á€¸á€€á€¼á€®á€¸ AI capabilities á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€”á€­á€¯á€„á€ºá€…á€±á€•á€«á€á€Šá€ºá‹

### á€¡á€“á€­á€€ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸

**Open Source Excellence**: Qwen á€á€Šá€º open-source á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€á€Šá€º proprietary á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€šá€¾á€‰á€ºá€•á€¼á€­á€¯á€„á€ºá€”á€­á€¯á€„á€ºá€á€±á€¬ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€º á€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€€á€¼á€±á€¬á€„á€ºá€¸ á€•á€¼á€á€‘á€¬á€¸á€•á€¼á€®á€¸ transparency, customization, á€”á€¾á€„á€·á€º control á€€á€­á€¯ á€•á€±á€¸á€…á€½á€™á€ºá€¸á€•á€«á€á€Šá€ºá‹

**Scalable Architecture**: 0.5B á€™á€¾ 235B parameters á€¡á€‘á€­ á€¡á€€á€»á€¯á€¶á€¸á€á€„á€ºá€á€±á€¬ range á€á€Šá€º mobile devices á€™á€¾ enterprise clusters á€¡á€‘á€­ computational environments á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€á€½á€„á€º á€á€„á€ºá€á€½á€„á€ºá€¸á€”á€­á€¯á€„á€ºá€…á€±á€•á€«á€á€Šá€ºá‹

**Specialized Capabilities**: Qwen-Coder, Qwen-Math, Qwen-VL á€€á€²á€·á€á€­á€¯á€·á€á€±á€¬ domain-specific variants á€™á€»á€¬á€¸á€á€Šá€º general language understanding á€€á€­á€¯ á€‘á€­á€”á€ºá€¸á€á€­á€™á€ºá€¸á€‘á€¬á€¸á€•á€¼á€®á€¸ á€¡á€‘á€°á€¸á€•á€¼á€¯ expertise á€€á€­á€¯ á€•á€±á€¸á€…á€½á€™á€ºá€¸á€•á€«á€á€Šá€ºá‹

**Global Accessibility**: 119+ á€˜á€¬á€á€¬á€…á€€á€¬á€¸á€™á€»á€¬á€¸á€€á€­á€¯ á€•á€¶á€·á€•á€­á€¯á€¸á€‘á€¬á€¸á€á€±á€¬ multilingual support á€á€Šá€º Qwen á€€á€­á€¯ á€¡á€•á€¼á€Šá€ºá€•á€¼á€Šá€ºá€†á€­á€¯á€„á€ºá€›á€¬ á€¡á€€á€ºá€•á€œá€®á€€á€±á€¸á€›á€¾á€„á€ºá€¸á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€° á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€¡á€á€½á€€á€º á€á€„á€·á€ºá€œá€»á€±á€¬á€ºá€…á€±á€•á€«á€á€Šá€ºá‹

**Continuous Innovation**: Qwen 1.0 á€™á€¾ Qwen3 á€¡á€‘á€­ á€¡á€†á€„á€·á€ºá€™á€¼á€¾á€„á€·á€ºá€á€„á€ºá€™á€¾á€¯á€™á€»á€¬á€¸á€á€Šá€º capabilities, efficiency, á€”á€¾á€„á€·á€º deployment options á€™á€»á€¬á€¸á€á€½á€„á€º á€á€­á€¯á€¸á€á€€á€ºá€™á€¾á€¯á€€á€­á€¯ á€•á€¼á€á€‘á€¬á€¸á€•á€«á€á€Šá€ºá‹

### á€¡á€”á€¬á€‚á€á€º á€¡á€™á€¼á€„á€º

Qwen á€™á€±á€¬á€ºá€’á€šá€º á€™á€­á€á€¬á€¸á€…á€¯á€á€Šá€º á€†á€€á€ºá€œá€€á€ºá€á€­á€¯á€¸á€á€€á€ºá€œá€¬á€á€Šá€·á€ºá€¡á€á€«á€á€½á€„á€º á€€á€»á€½á€”á€ºá€¯á€•á€ºá€á€­á€¯á€·á€á€Šá€º á€¡á€±á€¬á€€á€ºá€•á€«á€¡á€›á€¬á€™á€»á€¬á€¸á€€á€­á€¯ á€™á€»á€¾á€±á€¬á€ºá€œá€„á€·á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€Šá€º-

- **Enhanced Efficiency**: á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€º-per-parameter ratios á€•á€­á€¯á€™á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€™á€½á€”á€ºá€…á€±á€›á€”á€º optimization á€†á€€á€ºá€œá€€á€ºá€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€á€¼á€„á€ºá€¸
- **Expanded Multimodal Capabilities**: á€™á€¼á€„á€ºá€€á€½á€„á€ºá€¸áŠ á€¡á€á€¶áŠ á€”á€¾á€„á€·á€º á€…á€¬á€á€¬á€¸ processing á€•á€­á€¯á€™á€­á€¯á€á€­á€¯á€¸á€á€€á€ºá€…á€±á€›á€”á€º á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€á€¼á€„á€ºá€¸
- **Improved Reasoning**: á€…á€‰á€ºá€†á€€á€ºá€™á€•á€¼á€á€º reasoning mechanisms á€”á€¾á€„á€·á€º multi-step problem-solving capabilities
- **Better Deployment Tools**: Deployment scenarios á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€¡á€á€½á€€á€º frameworks á€”á€¾á€„á€·á€º optimization tools á€•á€­á€¯á€™á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€™á€½á€”á€ºá€…á€±á€›á€”á€º
- **Community Growth**: Tools, applications, á€”á€¾á€„á€·á€º community contributions ecosystem á€á€­á€¯á€¸á€á€€á€ºá€œá€¬á€á€¼á€„á€ºá€¸

### á€”á€±á€¬á€€á€ºá€á€…á€ºá€†á€„á€·á€º

Chatbot á€á€…á€ºá€á€¯á€á€Šá€ºá€†á€±á€¬á€€á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€…á€ºá€…á€±áŠ á€•á€Šá€¬á€›á€±á€¸á€†á€­á€¯á€„á€ºá€›á€¬ tools á€á€®á€‘á€½á€„á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€…á€ºá€…á€±áŠ coding assistants á€–á€”á€ºá€á€®á€¸á€á€¼á€„á€ºá€¸á€–á€¼á€…á€ºá€…á€±áŠ multilingual applications á€á€½á€„á€º á€¡á€œá€¯á€•á€ºá€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€…á€ºá€…á€±áŠ Qwen á€™á€±á€¬á€ºá€’á€šá€º á€™á€­á€á€¬á€¸á€…á€¯á€á€Šá€º scalable solutions á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€¡á€¬á€¸á€€á€±á€¬á€„á€ºá€¸á€á€±á€¬ community á€•á€¶á€·á€•á€­á€¯á€¸á€™á€¾á€¯á€”á€¾á€„á€·á€º á€¡á€€á€»á€¯á€¶á€¸á€á€„á€ºá€á€±á€¬ documentation á€€á€­á€¯ á€•á€±á€¸á€…á€½á€™á€ºá€¸á€•á€«á€á€Šá€ºá‹

á€”á€±á€¬á€€á€ºá€†á€¯á€¶á€¸á€› updates, á€™á€±á€¬á€ºá€’á€šá€º á€‘á€¯á€á€ºá€á€±á€™á€¾á€¯á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€”á€Šá€ºá€¸á€•á€Šá€¬á€†á€­á€¯á€„á€ºá€›á€¬ documentation á€¡á€•á€¼á€Šá€·á€ºá€¡á€…á€¯á€¶á€€á€­á€¯ Hugging Face á€á€½á€„á€ºá€›á€¾á€­á€á€±á€¬ Qwen á á€á€›á€¬á€¸á€á€„á€º repositories á€á€½á€„á€º á€œá€±á€·á€œá€¬á€•á€«á‹ Community discussions á€”á€¾á€„á€·á€º á€”á€™á€°á€”á€¬á€™á€»á€¬á€¸á€€á€­á€¯á€œá€Šá€ºá€¸ á€…á€™á€ºá€¸á€á€•á€ºá€•á€«á‹

AI development á á€¡á€”á€¬á€‚á€á€ºá€á€Šá€º accessible, transparent, á€”á€¾á€„á€·á€º á€¡á€„á€ºá€¡á€¬á€¸á€€á€¼á€®á€¸á€á€±á€¬ tools á€™á€»á€¬á€¸á€á€½á€„á€º á€›á€¾á€­á€”á€±á€•á€¼á€®á€¸ á€¡á€”á€šá€ºá€”á€šá€ºá€¡á€›á€•á€ºá€›á€•á€ºá€”á€¾á€„á€·á€º á€¡á€›á€½á€šá€ºá€¡á€…á€¬á€¸á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€á€½á€„á€º innovation á€€á€­á€¯ á€¡á€¬á€¸á€•á€±á€¸á€”á€­á€¯á€„á€ºá€…á€±á€•á€«á€á€Šá€ºá‹ Qwen á€™á€±á€¬á€ºá€’á€šá€º á€™á€­á€á€¬á€¸á€…á€¯á€á€Šá€º á€¤á€›á€Šá€ºá€™á€¾á€”á€ºá€¸á€á€»á€€á€ºá€€á€­á€¯ á€€á€­á€¯á€šá€ºá€…á€¬á€¸á€•á€¼á€¯á€‘á€¬á€¸á€•á€¼á€®á€¸ AI-powered applications á€™á€»á€¬á€¸á á€”á€±á€¬á€€á€ºá€†á€¯á€¶á€¸á€™á€»á€­á€¯á€¸á€†á€€á€ºá€€á€­á€¯ á€á€Šá€ºá€†á€±á€¬á€€á€ºá€›á€”á€º á€¡á€–á€½á€²á€·á€¡á€…á€Šá€ºá€¸á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º Developer á€™á€»á€¬á€¸á€€á€­á€¯ á€¡á€á€¼á€±á€á€¶á€¡á€¯á€á€ºá€™á€¼á€…á€ºá€•á€±á€¸á€…á€½á€™á€ºá€¸á€•á€«á€á€Šá€ºá‹

## á€¡á€•á€­á€¯á€†á€±á€¬á€„á€ºá€¸ á€›á€„á€ºá€¸á€™á€¼á€…á€ºá€™á€»á€¬á€¸

- **á€á€›á€¬á€¸á€á€„á€º Documentation**: [Qwen Documentation](https://qwen.readthedocs.io/)
- **Model Hub**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **Technical Papers**: [Qwen Research Publications](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **Community**: [GitHub Discussions and Issues](https://github.com/QwenLM/)
- **ModelScope Platform**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## á€á€„á€ºá€šá€°á€™á€¾á€¯á€›á€œá€’á€ºá€™á€»á€¬á€¸

á€¤ module á€€á€­á€¯ á€•á€¼á€®á€¸á€™á€¼á€±á€¬á€€á€ºá€•á€¼á€®á€¸á€”á€±á€¬á€€á€ºá€á€½á€„á€º á€á€„á€ºá€á€Šá€º-

1. Qwen á€™á€±á€¬á€ºá€’á€šá€º á€™á€­á€á€¬á€¸á€…á€¯á architectural á€¡á€¬á€¸á€á€¬á€á€»á€€á€ºá€™á€»á€¬á€¸á€”á€¾á€„á€·á€º open-source approach á€€á€­á€¯ á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€•á€¼á€±á€¬á€”á€­á€¯á€„á€ºá€™á€Šá€ºá‹
2. á€á€á€ºá€™á€¾á€á€º application á€œá€­á€¯á€¡á€•á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸á€”á€¾á€„á€·á€º resource á€¡á€€á€”á€·á€ºá€¡á€á€á€ºá€™á€»á€¬á€¸á€¡á€•á€±á€«á€º á€™á€°á€á€Šá€ºá á€á€„á€·á€ºá€á€±á€¬á€ºá€á€±á€¬ Qwen variant á€€á€­á€¯ á€›á€½á€±á€¸á€á€»á€šá€ºá€”á€­á€¯á€„á€ºá€™á€Šá€ºá‹
3. Development scenarios á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€á€½á€„á€º optimized configurations á€–á€¼á€„á€·á€º Qwen á€™á€±á€¬á€ºá€’á€šá€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€á€„á€ºá€á€½á€„á€ºá€¸á€”á€­á€¯á€„á€ºá€™á€Šá€ºá‹
4. Qwen á€™á€±á€¬á€ºá€’á€šá€º á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€€á€­á€¯ á€á€­á€¯á€¸á€á€€á€ºá€…á€±á€›á€”á€º quantization á€”á€¾á€„á€·á€º optimization techniques á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€”á€­á€¯á€„á€ºá€™á€Šá€ºá‹
5. Qwen á€™á€±á€¬á€ºá€’á€šá€º á€™á€­á€á€¬á€¸á€…á€¯á€¡á€á€½á€„á€ºá€¸ á€™á€±á€¬á€ºá€’á€šá€º á€¡á€›á€½á€šá€ºá€¡á€…á€¬á€¸áŠ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºáŠ á€”á€¾á€„á€·á€º capabilities á€¡á€€á€¼á€¬á€¸ trade-offs á€™á€»á€¬á€¸á€€á€­á€¯ á€á€¯á€¶á€¸á€á€•á€ºá€”á€­á€¯á€„á€ºá€™á€Šá€ºá‹

## á€”á€±á€¬á€€á€ºá€á€…á€ºá€†á€„á€·á€º

- [03: Gemma Family Fundamentals](03.GemmaFamily.md)

---

**á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€€á€¼á€¬á€¸á€á€»á€€á€º**:  
á€¤á€…á€¬á€›á€½á€€á€ºá€…á€¬á€á€™á€ºá€¸á€€á€­á€¯ AI á€˜á€¬á€á€¬á€•á€¼á€”á€ºá€á€”á€ºá€†á€±á€¬á€„á€ºá€™á€¾á€¯ [Co-op Translator](https://github.com/Azure/co-op-translator) á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á á€˜á€¬á€á€¬á€•á€¼á€”á€ºá€‘á€¬á€¸á€•á€«á€á€Šá€ºá‹ á€€á€»á€½á€”á€ºá€¯á€•á€ºá€á€­á€¯á€·á€á€Šá€º á€á€­á€€á€»á€™á€¾á€¯á€¡á€á€½á€€á€º á€€á€¼á€­á€¯á€¸á€…á€¬á€¸á€”á€±á€á€±á€¬á€ºá€œá€Šá€ºá€¸áŠ á€¡á€œá€­á€¯á€¡á€œá€»á€±á€¬á€€á€º á€˜á€¬á€á€¬á€•á€¼á€”á€ºá€á€¼á€„á€ºá€¸á€á€½á€„á€º á€¡á€™á€¾á€¬á€¸á€™á€»á€¬á€¸ á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º á€™á€á€­á€€á€»á€™á€¾á€¯á€™á€»á€¬á€¸ á€•á€«á€›á€¾á€­á€”á€­á€¯á€„á€ºá€á€Šá€ºá€€á€­á€¯ á€á€á€­á€•á€¼á€¯á€•á€«á‹ á€™á€°á€›á€„á€ºá€¸á€˜á€¬á€á€¬á€…á€€á€¬á€¸á€–á€¼á€„á€·á€º á€›á€±á€¸á€á€¬á€¸á€‘á€¬á€¸á€á€±á€¬ á€…á€¬á€›á€½á€€á€ºá€…á€¬á€á€™á€ºá€¸á€€á€­á€¯ á€¡á€¬á€á€¬á€á€› á€¡á€›á€„á€ºá€¸á€¡á€™á€¼á€…á€ºá€¡á€–á€¼á€…á€º á€á€á€ºá€™á€¾á€á€ºá€á€„á€·á€ºá€•á€«á€á€Šá€ºá‹ á€¡á€›á€±á€¸á€€á€¼á€®á€¸á€á€±á€¬ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€œá€°á€·á€˜á€¬á€á€¬á€•á€¼á€”á€ºá€•á€Šá€¬á€›á€¾á€„á€ºá€™á€»á€¬á€¸á€™á€¾ á€•á€›á€±á€¬á€ºá€–á€€á€ºá€›á€¾á€„á€ºá€”á€šá€º á€˜á€¬á€á€¬á€•á€¼á€”á€ºá€á€¼á€„á€ºá€¸á€€á€­á€¯ á€¡á€€á€¼á€¶á€•á€¼á€¯á€•á€«á€á€Šá€ºá‹ á€¤á€˜á€¬á€á€¬á€•á€¼á€”á€ºá€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¼á€„á€ºá€¸á€™á€¾ á€–á€¼á€…á€ºá€•á€±á€«á€ºá€œá€¬á€á€±á€¬ á€¡á€œá€½á€²á€¡á€œá€½á€á€ºá€™á€»á€¬á€¸ á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º á€¡á€”á€¬á€¸á€šá€°á€™á€¾á€¬á€¸á€™á€¾á€¯á€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€€á€»á€½á€”á€ºá€¯á€•á€ºá€á€­á€¯á€·á€á€Šá€º á€á€¬á€á€”á€ºá€™á€šá€°á€•á€«á‹