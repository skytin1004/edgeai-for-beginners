<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "308873206bc5ec5d87e34ca2907d2c9c",
  "translation_date": "2025-09-19T01:28:26+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "sr"
}
-->
# Одељак 2: Развој локалног окружења - Решeња која стављају приватност на прво место

Локално постављање малих језичких модела (SLM) представља промену парадигме ка решењима за вештачку интелигенцију која чувају приватност и смањују трошкове. Овај свеобухватни водич истражује два моћна оквира—Ollama и Microsoft Foundry Local—који омогућавају програмерима да искористе пун потенцијал SLM-а уз потпуну контролу над окружењем за постављање.

## Увод

У овој лекцији истражићемо напредне стратегије за постављање малих језичких модела у локалним окружењима. Покрићемо основне концепте локалног постављања вештачке интелигенције, анализирати две водеће платформе (Ollama и Microsoft Foundry Local) и пружити практичне смернице за имплементацију решења спремних за производњу.

## Циљеви учења

На крају ове лекције, моћи ћете да:

- Разумете архитектуру и предности оквира за локално постављање SLM-а.
- Имплементирате решења спремна за производњу користећи Ollama и Microsoft Foundry Local.
- Упоредите и изаберете одговарајућу платформу на основу специфичних захтева и ограничења.
- Оптимизујете локална постављања за перформансе, безбедност и скалабилност.

## Разумевање архитектура локалног постављања SLM-а

Локално постављање SLM-а представља фундаменталну промену од услуга вештачке интелигенције зависних од облака ка решењима која чувају приватност и функционишу на лицу места. Овај приступ омогућава организацијама да задрже потпуну контролу над својом инфраструктуром за вештачку интелигенцију уз обезбеђивање суверенитета података и оперативне независности.

### Класификације оквира за постављање

Разумевање различитих приступа постављању помаже у избору праве стратегије за специфичне случајеве употребе:

- **Фокусирано на развој**: Поједностављено подешавање за експериментисање и прототипирање.
- **За предузећа**: Решeња спремна за производњу са могућностима интеграције у предузећима.
- **Мултиплатформско**: Универзална компатибилност са различитим оперативним системима и хардвером.

### Кључне предности локалног постављања SLM-а

Локално постављање SLM-а нуди неколико основних предности које га чине идеалним за апликације осетљиве на приватност и потребе предузећа:

**Приватност и безбедност**: Локална обрада осигурава да осетљиви подаци никада не напуштају инфраструктуру организације, омогућавајући усклађеност са GDPR, HIPAA и другим регулаторним захтевима. Постављања у изолованим окружењима су могућа за класификоване системе, док комплетни записи о активностима обезбеђују надзор над безбедношћу.

**Исплативост**: Елиминација модела наплате по токену значајно смањује оперативне трошкове. Мањи захтеви за пропусним опсегом и смањена зависност од облака пружају предвидљиве структуре трошкова за буџетирање у предузећима.

**Перформансе и поузданост**: Брже време извршавања без кашњења у мрежи омогућава апликације у реалном времену. Функционалност ван мреже обезбеђује континуирани рад без обзира на интернет конекцију, док оптимизација локалних ресурса пружа конзистентне перформансе.

## Ollama: Универзална платформа за локално постављање

### Основна архитектура и филозофија

Ollama је дизајнирана као универзална, програмерски прилагођена платформа која демократизује локално постављање LLM-а на различитим хардверским конфигурацијама и оперативним системима.

**Техничка основа**: Изграђена на робусном оквиру llama.cpp, Ollama користи ефикасан GGUF формат модела за оптималне перформансе. Мултиплатформска компатибилност обезбеђује конзистентно понашање на Windows, macOS и Linux окружењима, док интелигентно управљање ресурсима оптимизује употребу CPU-а, GPU-а и меморије.

**Филозофија дизајна**: Ollama даје приоритет једноставности без жртвовања функционалности, нудећи постављање без конфигурације за тренутну продуктивност. Платформа одржава широку компатибилност модела уз пружање конзистентних API-ја за различите архитектуре модела.

### Напредне функције и могућности

**Изврсност у управљању моделима**: Ollama пружа свеобухватно управљање животним циклусом модела уз аутоматско преузимање, кеширање и верзионисање. Платформа подржава обиман екосистем модела, укључујући Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral и специјализоване моделе за уграђивање.

**Прилагођавање кроз Modelfiles**: Напредни корисници могу креирати прилагођене конфигурације модела са специфичним параметрима, системским упутствима и модификацијама понашања. Ово омогућава оптимизацију за одређене домене и специјализоване захтеве апликација.

**Оптимизација перформанси**: Ollama аутоматски открива и користи доступно хардверско убрзање, укључујући NVIDIA CUDA, Apple Metal и OpenCL. Интелигентно управљање меморијом обезбеђује оптималну употребу ресурса на различитим хардверским конфигурацијама.

### Стратегије имплементације у производњи

**Инсталација и подешавање**: Ollama пружа поједностављену инсталацију на различитим платформама кроз изворне инсталере, менаџере пакета (WinGet, Homebrew, APT) и Docker контејнере за контејнеризована постављања.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Основне команде и операције**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Напредна конфигурација**: Modelfiles омогућавају софистицирано прилагођавање за захтеве предузећа:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Примери интеграције за програмере

**Интеграција Python API-ја**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**Интеграција JavaScript/TypeScript-а (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**Коришћење RESTful API-ја са cURL-ом**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Подешавање и оптимизација перформанси

**Конфигурација меморије и нити**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Избор квантовања за различит хардвер**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Платформа за AI на ивици за предузећа

### Архитектура за предузећа

Microsoft Foundry Local представља свеобухватно решење за предузећа, дизајнирано посебно за производна постављања AI-а на ивици са дубоком интеграцијом у Microsoft екосистем.

**Основа заснована на ONNX-у**: Изграђена на индустријском стандарду ONNX Runtime, Foundry Local пружа оптимизоване перформансе на различитим хардверским архитектурама. Платформа користи интеграцију Windows ML-а за оптимизацију на Windows окружењима уз одржавање мултиплатформске компатибилности.

**Изврсност у хардверском убрзању**: Foundry Local карактерише интелигентно откривање хардвера и оптимизација на CPU-има, GPU-има и NPU-има. Дубока сарадња са произвођачима хардвера (AMD, Intel, NVIDIA, Qualcomm) обезбеђује оптималне перформансе на хардверским конфигурацијама за предузећа.

### Напредно искуство за програмере

**Приступ кроз више интерфејса**: Foundry Local пружа свеобухватне интерфејсе за развој, укључујући моћан CLI за управљање моделима и постављање, SDK-ове за више језика (Python, NodeJS) за изворну интеграцију и RESTful API-је са компатибилношћу OpenAI-а за беспрекорну миграцију.

**Интеграција са Visual Studio-ом**: Платформа се беспрекорно интегрише са AI Toolkit-ом за VS Code, пружајући алате за конверзију модела, квантовање и оптимизацију унутар окружења за развој. Ова интеграција убрзава радне токове развоја и смањује сложеност постављања.

**Платформа за оптимизацију модела**: Интеграција Microsoft Olive-а омогућава софистициране радне токове за оптимизацију модела, укључујући динамичко квантовање, оптимизацију графа и подешавање специфично за хардвер. Могућности конверзије засноване на облаку кроз Azure ML пружају скалабилну оптимизацију за велике моделе.

### Стратегије имплементације у производњи

**Инсталација и конфигурација**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Операције управљања моделима**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Напредна конфигурација постављања**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Интеграција у екосистем предузећа

**Безбедност и усклађеност**: Foundry Local пружа безбедносне функције на нивоу предузећа, укључујући контролу приступа засновану на улогама, евиденцију активности, извештавање о усклађености и шифровану складиштење модела. Интеграција са Microsoft инфраструктуром за безбедност осигурава поштовање безбедносних политика предузећа.

**Уграђене AI услуге**: Платформа нуди готове AI могућности, укључујући Phi Silica за локалну обраду језика, AI Imaging за побољшање и анализу слика и специјализоване API-је за уобичајене AI задатке у предузећима.

## Упоредна анализа: Ollama vs Foundry Local

### Упоредна анализа техничке архитектуре

| **Аспект** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **Формат модела** | GGUF (преко llama.cpp) | ONNX (преко ONNX Runtime) |
| **Фокус платформе** | Универзална мултиплатформска | Оптимизација за Windows/предузећа |
| **Интеграција хардвера** | Општа подршка за GPU/CPU | Дубока интеграција Windows ML-а, подршка за NPU |
| **Оптимизација** | Квантовање преко llama.cpp | Microsoft Olive + ONNX Runtime |
| **Функције за предузећа** | Вођено заједницом | На нивоу предузећа са SLA-овима |

### Карактеристике перформанси

**Предности Ollama перформанси**:
- Изузетне перформансе CPU-а кроз оптимизацију llama.cpp-а.
- Конзистентно понашање на различитим платформама и хардверу.
- Ефикасна употреба меморије уз интелигентно учитавање модела.
- Брзо време покретања за сценарије развоја и тестирања.

**Предности Foundry Local перформанси**:
- Супериорна употреба NPU-а на модерном Windows хардверу.
- Оптимизовано GPU убрзање кроз партнерства са произвођачима.
- Надзор перформанси на нивоу предузећа и оптимизација.
- Скалабилне могућности постављања за производна окружења.

### Анализа искуства за програмере

**Искуство програмера са Ollama**:
- Минимални захтеви за подешавање уз тренутну продуктивност.
- Интуитивни интерфејс командне линије за све операције.
- Обимна подршка заједнице и документација.
- Флексибилно прилагођавање кроз Modelfiles.

**Искуство програмера са Foundry Local**:
- Свеобухватна интеграција IDE-а са екосистемом Visual Studio-а.
- Радни токови развоја за предузећа са функцијама за сарадњу тимова.
- Професионални канали подршке уз подршку Microsoft-а.
- Напредни алати за отклањање грешака и оптимизацију.

### Оптимизација случајева употребе

**Изаберите Ollama када**:
- Развијате мултиплатформске апликације које захтевају конзистентно понашање.
- Дајете приоритет транспарентности отвореног кода и доприносима заједнице.
- Радите са ограниченим ресурсима или буџетским ограничењима.
- Градите експерименталне или истраживачке апликације.
- Захтевате широку компатибилност модела са различитим архитектурама.

**Изаберите Foundry Local када**:
- Постављате апликације за предузећа са строгим захтевима за перформансе.
- Искористите оптимизације хардвера специфичне за Windows (NPU, Windows ML).
- Захтевате подршку за предузећа, SLA-ове и функције усклађености.
- Градите производне апликације са интеграцијом у Microsoft екосистем.
- Потребни су вам напредни алати за оптимизацију и професионални радни токови развоја.

## Напредне стратегије постављања

### Шаблони за контејнеризовано постављање

**Контејнеризација Ollama**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Постављање Foundry Local-а за предузећа**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Технике оптимизације перформанси

**Стратегије оптимизације Ollama**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Оптимизација Foundry Local-а**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Разматрања безбедности и усклађености

### Имплементација безбедности у предузећима

**Најбоље праксе за безбедност Ollama**:
- Изолација мреже уз правила заштитног зида и VPN приступ.
- Аутентификација кроз интеграцију реверзног проксија.
- Верификација интегритета модела и сигурна дистрибуција модела.
- Евиденција активности за приступ API-ју и операције модела.

**Безбедност Foundry Local-а за предузећа**:
- Уграђена контрола приступа заснована на улогама уз интеграцију Active Directory-а.
- Свеобухватни записи активности уз извештавање о усклађености.
- Шифрована складиштење модела и сигурно постављање модела.
- Интеграција са Microsoft инфраструктуром за безбедност.

### Захтеви за усклађеност и регулативу

Обе платформе подржавају усклађеност са регулативама кроз:
- Контролу резиденције података која осигурава локалну обраду.
- Евиденцију активности за захтеве извештавања о регулативама.
- Контролу приступа за руковање осетљивим подацима.
- Шифровање у мировању и током преноса ради заштите података.

## Најбоље праксе за постављање у производњи

---

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати ауторитативним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.