<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-01T21:50:37+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "sr"
}
-->
# –û–¥–µ—ô–∞–∫ 2: –û—Å–Ω–æ–≤–µ –ø–æ—Ä–æ–¥–∏—Ü–µ Qwen

–ü–æ—Ä–æ–¥–∏—Ü–∞ –º–æ–¥–µ–ª–∞ Qwen –ø—Ä–µ–¥—Å—Ç–∞–≤—ô–∞ —Å–≤–µ–æ–±—É—Ö–≤–∞—Ç–∞–Ω –ø—Ä–∏—Å—Ç—É–ø Alibaba Cloud-–∞ –≤–µ–ª–∏–∫–∏–º —ò–µ–∑–∏—á–∫–∏–º –º–æ–¥–µ–ª–∏–º–∞ –∏ –º—É–ª—Ç–∏–º–æ–¥–∞–ª–Ω–æ—ò –≤–µ—à—Ç–∞—á–∫–æ—ò –∏–Ω—Ç–µ–ª–∏–≥–µ–Ω—Ü–∏—ò–∏, –ø–æ–∫–∞–∑—É—ò—É—õ–∏ –¥–∞ –æ—Ç–≤–æ—Ä–µ–Ω–∏ –º–æ–¥–µ–ª–∏ –º–æ–≥—É –ø–æ—Å—Ç–∏—õ–∏ –∏–∑—É–∑–µ—Ç–Ω–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–µ —É–∑ –ø—Ä–∏—Å—Ç—É–ø–∞—á–Ω–æ—Å—Ç —É —Ä–∞–∑–ª–∏—á–∏—Ç–∏–º —Å—Ü–µ–Ω–∞—Ä–∏—ò–∏–º–∞ –ø—Ä–∏–º–µ–Ω–µ. –í–∞–∂–Ω–æ —ò–µ —Ä–∞–∑—É–º–µ—Ç–∏ –∫–∞–∫–æ –ø–æ—Ä–æ–¥–∏—Ü–∞ Qwen –æ–º–æ–≥—É—õ–∞–≤–∞ –º–æ—õ–Ω–µ AI —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–∞ —Ñ–ª–µ–∫—Å–∏–±–∏–ª–Ω–∏–º –æ–ø—Ü–∏—ò–∞–º–∞ –ø—Ä–∏–º–µ–Ω–µ, –∏—Å—Ç–æ–≤—Ä–µ–º–µ–Ω–æ –æ–¥—Ä–∂–∞–≤–∞—ò—É—õ–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–µ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–µ —É —Ä–∞–∑–Ω–æ–≤—Ä—Å–Ω–∏–º –∑–∞–¥–∞—Ü–∏–º–∞.

## –†–µ—Å—É—Ä—Å–∏ –∑–∞ –ø—Ä–æ–≥—Ä–∞–º–µ—Ä–µ

### –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—ò—É–º –º–æ–¥–µ–ª–∞ Hugging Face
–ò–∑–∞–±—Ä–∞–Ω–∏ –º–æ–¥–µ–ª–∏ –ø–æ—Ä–æ–¥–∏—Ü–µ Qwen –¥–æ—Å—Ç—É–ø–Ω–∏ —Å—É –ø—Ä–µ–∫–æ [Hugging Face](https://huggingface.co/models?search=qwen), –ø—Ä—É–∂–∞—ò—É—õ–∏ –ø—Ä–∏—Å—Ç—É–ø –Ω–µ–∫–∏–º –≤–∞—Ä–∏—ò–∞–Ω—Ç–∞–º–∞ –æ–≤–∏—Ö –º–æ–¥–µ–ª–∞. –ú–æ–∂–µ—Ç–µ –∏—Å—Ç—Ä–∞–∂–∏—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω–µ –≤–∞—Ä–∏—ò–∞–Ω—Ç–µ, –ø—Ä–∏–ª–∞–≥–æ–¥–∏—Ç–∏ –∏—Ö –∑–∞ —Å–≤–æ—ò–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–µ –ø–æ—Ç—Ä–µ–±–µ –∏ –ø—Ä–∏–º–µ–Ω–∏—Ç–∏ –∏—Ö –∫—Ä–æ–∑ —Ä–∞–∑–ª–∏—á–∏—Ç–µ –æ–∫–≤–∏—Ä–µ.

### –ê–ª–∞—Ç–∫–µ –∑–∞ –ª–æ–∫–∞–ª–Ω–∏ —Ä–∞–∑–≤–æ—ò
–ó–∞ –ª–æ–∫–∞–ª–Ω–∏ —Ä–∞–∑–≤–æ—ò –∏ —Ç–µ—Å—Ç–∏—Ä–∞—ö–µ, –º–æ–∂–µ—Ç–µ –∫–æ—Ä–∏—Å—Ç–∏—Ç–∏ [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) –∑–∞ –ø–æ–∫—Ä–µ—Ç–∞—ö–µ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö Qwen –º–æ–¥–µ–ª–∞ –Ω–∞ –≤–∞—à–µ–º —Ä–∞–∑–≤–æ—ò–Ω–æ–º —Ä–∞—á—É–Ω–∞—Ä—É —É–∑ –æ–ø—Ç–∏–º–∏–∑–æ–≤–∞–Ω–µ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–µ.

### –†–µ—Å—É—Ä—Å–∏ –∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—ò—É
- [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—ò–∞ –º–æ–¥–µ–ª–∞ Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—ò–∞ Qwen –º–æ–¥–µ–ª–∞ –∑–∞ –ø—Ä–∏–º–µ–Ω—É –Ω–∞ –∏–≤–∏—Ü–∏](https://github.com/microsoft/olive)

## –£–≤–æ–¥

–£ –æ–≤–æ–º —Ç—É—Ç–æ—Ä–∏—ò–∞–ª—É –∏—Å—Ç—Ä–∞–∂–∏—õ–µ–º–æ –ø–æ—Ä–æ–¥–∏—Ü—É –º–æ–¥–µ–ª–∞ Qwen –∫–æ–º–ø–∞–Ω–∏—ò–µ Alibaba –∏ —ö–µ–Ω–µ –æ—Å–Ω–æ–≤–Ω–µ –∫–æ–Ω—Ü–µ–ø—Ç–µ. –ü–æ–∫—Ä–∏—õ–µ–º–æ –µ–≤–æ–ª—É—Ü–∏—ò—É –ø–æ—Ä–æ–¥–∏—Ü–µ Qwen, –∏–Ω–æ–≤–∞—Ç–∏–≤–Ω–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—ò–µ —Ç—Ä–µ–Ω–∏–Ω–≥–∞ –∫–æ—ò–µ —á–∏–Ω–µ Qwen –º–æ–¥–µ–ª–µ –µ—Ñ–∏–∫–∞—Å–Ω–∏–º, –∫—ô—É—á–Ω–µ –≤–∞—Ä–∏—ò–∞–Ω—Ç–µ —É –ø–æ—Ä–æ–¥–∏—Ü–∏ –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω–µ –ø—Ä–∏–º–µ–Ω–µ —É —Ä–∞–∑–ª–∏—á–∏—Ç–∏–º —Å—Ü–µ–Ω–∞—Ä–∏—ò–∏–º–∞.

## –¶–∏—ô–µ–≤–∏ —É—á–µ—ö–∞

–î–æ –∫—Ä–∞—ò–∞ –æ–≤–æ–≥ —Ç—É—Ç–æ—Ä–∏—ò–∞–ª–∞, –º–æ—õ–∏ —õ–µ—Ç–µ:

- –†–∞–∑—É–º–µ—Ç–∏ —Ñ–∏–ª–æ–∑–æ—Ñ–∏—ò—É –¥–∏–∑–∞—ò–Ω–∞ –∏ –µ–≤–æ–ª—É—Ü–∏—ò—É –ø–æ—Ä–æ–¥–∏—Ü–µ –º–æ–¥–µ–ª–∞ Qwen –∫–æ–º–ø–∞–Ω–∏—ò–µ Alibaba
- –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–æ–≤–∞—Ç–∏ –∫—ô—É—á–Ω–µ –∏–Ω–æ–≤–∞—Ü–∏—ò–µ –∫–æ—ò–µ –æ–º–æ–≥—É—õ–∞–≤–∞—ò—É Qwen –º–æ–¥–µ–ª–∏–º–∞ –¥–∞ –ø–æ—Å—Ç–∏–≥–Ω—É –≤–∏—Å–æ–∫–µ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–µ —É —Ä–∞–∑–ª–∏—á–∏—Ç–∏–º –≤–µ–ª–∏—á–∏–Ω–∞–º–∞ –ø–∞—Ä–∞–º–µ—Ç–∞—Ä–∞
- –ü—Ä–µ–ø–æ–∑–Ω–∞—Ç–∏ –ø—Ä–µ–¥–Ω–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ—ö–∞ —Ä–∞–∑–ª–∏—á–∏—Ç–∏—Ö –≤–∞—Ä–∏—ò–∞–Ω—Ç–∏ Qwen –º–æ–¥–µ–ª–∞
- –ü—Ä–∏–º–µ–Ω–∏—Ç–∏ –∑–Ω–∞—ö–µ –æ Qwen –º–æ–¥–µ–ª–∏–º–∞ –∑–∞ –∏–∑–±–æ—Ä –æ–¥–≥–æ–≤–∞—Ä–∞—ò—É—õ–∏—Ö –≤–∞—Ä–∏—ò–∞–Ω—Ç–∏ –∑–∞ —Ä–µ–∞–ª–Ω–µ —Å—Ü–µ–Ω–∞—Ä–∏—ò–µ

## –†–∞–∑—É–º–µ–≤–∞—ö–µ —Å–∞–≤—Ä–µ–º–µ–Ω–æ–≥ –ø–µ—ò–∑–∞–∂–∞ AI –º–æ–¥–µ–ª–∞

–ü–µ—ò–∑–∞–∂ AI –º–æ–¥–µ–ª–∞ –∑–Ω–∞—á–∞—ò–Ω–æ —Å–µ —Ä–∞–∑–≤–∏–æ, —Å–∞ —Ä–∞–∑–ª–∏—á–∏—Ç–∏–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—ò–∞–º–∞ –∫–æ—ò–µ —Å–ª–µ–¥–µ —Ä–∞–∑–ª–∏—á–∏—Ç–µ –ø—Ä–∏—Å—Ç—É–ø–µ —Ä–∞–∑–≤–æ—ò—É —ò–µ–∑–∏—á–∫–∏—Ö –º–æ–¥–µ–ª–∞. –î–æ–∫ —Å–µ –Ω–µ–∫–µ —Ñ–æ–∫—É—Å–∏—Ä–∞—ò—É –Ω–∞ –≤–ª–∞—Å–Ω–∏—á–∫–µ –º–æ–¥–µ–ª–µ –∑–∞—Ç–≤–æ—Ä–µ–Ω–æ–≥ –∫–æ–¥–∞, –¥—Ä—É–≥–µ –Ω–∞–≥–ª–∞—à–∞–≤–∞—ò—É –ø—Ä–∏—Å—Ç—É–ø–∞—á–Ω–æ—Å—Ç –∏ —Ç—Ä–∞–Ω—Å–ø–∞—Ä–µ–Ω—Ç–Ω–æ—Å—Ç –æ—Ç–≤–æ—Ä–µ–Ω–æ–≥ –∫–æ–¥–∞. –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–∞–ª–Ω–∏ –ø—Ä–∏—Å—Ç—É–ø —É–∫—ô—É—á—É—ò–µ –∏–ª–∏ –º–∞—Å–∏–≤–Ω–µ –≤–ª–∞—Å–Ω–∏—á–∫–µ –º–æ–¥–µ–ª–µ –¥–æ—Å—Ç—É–ø–Ω–µ —Å–∞–º–æ –ø—Ä–µ–∫–æ API-—ò–∞ –∏–ª–∏ –º–æ–¥–µ–ª–µ –æ—Ç–≤–æ—Ä–µ–Ω–æ–≥ –∫–æ–¥–∞ –∫–æ—ò–∏ –º–æ–≥—É –∑–∞–æ—Å—Ç–∞—ò–∞—Ç–∏ —É —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏–º–∞.

–û–≤–∞—ò –ø–∞—Ä–∞–¥–∏–≥–º–∞ —Å—Ç–≤–∞—Ä–∞ –∏–∑–∞–∑–æ–≤–µ –∑–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—ò–µ –∫–æ—ò–µ —Ç—Ä–∞–∂–µ –º–æ—õ–Ω–µ AI —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —É–∑ –∑–∞–¥—Ä–∂–∞–≤–∞—ö–µ –∫–æ–Ω—Ç—Ä–æ–ª–µ –Ω–∞–¥ —Å–≤–æ—ò–∏–º –ø–æ–¥–∞—Ü–∏–º–∞, —Ç—Ä–æ—à–∫–æ–≤–∏–º–∞ –∏ —Ñ–ª–µ–∫—Å–∏–±–∏–ª–Ω–æ—à—õ—É –ø—Ä–∏–º–µ–Ω–µ. –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–∞–ª–Ω–∏ –ø—Ä–∏—Å—Ç—É–ø —á–µ—Å—Ç–æ –∑–∞—Ö—Ç–µ–≤–∞ –∏–∑–±–æ—Ä –∏–∑–º–µ—í—É –≤—Ä—Ö—É–Ω—Å–∫–∏—Ö –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–∏ –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω–∏—Ö —Ä–∞–∑–º–∞—Ç—Ä–∞—ö–∞ –ø—Ä–∏–º–µ–Ω–µ.

## –ò–∑–∞–∑–æ–≤ –ø—Ä–∏—Å—Ç—É–ø–∞—á–Ω–µ AI –∏–∑–≤—Ä—Å–Ω–æ—Å—Ç–∏

–ü–æ—Ç—Ä–µ–±–∞ –∑–∞ –≤–∏—Å–æ–∫–æ–∫–≤–∞–ª–∏—Ç–µ—Ç–Ω–æ–º, –ø—Ä–∏—Å—Ç—É–ø–∞—á–Ω–æ–º AI –ø–æ—Å—Ç–∞–ª–∞ —ò–µ —Å–≤–µ –≤–∞–∂–Ω–∏—ò–∞ —É —Ä–∞–∑–ª–∏—á–∏—Ç–∏–º —Å—Ü–µ–Ω–∞—Ä–∏—ò–∏–º–∞. –†–∞–∑–º–æ—Ç—Ä–∏—Ç–µ –ø—Ä–∏–º–µ–Ω–µ –∫–æ—ò–µ –∑–∞—Ö—Ç–µ–≤–∞—ò—É —Ñ–ª–µ–∫—Å–∏–±–∏–ª–Ω–µ –æ–ø—Ü–∏—ò–µ –ø—Ä–∏–º–µ–Ω–µ –∑–∞ —Ä–∞–∑–ª–∏—á–∏—Ç–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–µ –ø–æ—Ç—Ä–µ–±–µ, –∏—Å–ø–ª–∞—Ç–∏–≤–µ –∏–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—ò–µ –≥–¥–µ —Ç—Ä–æ—à–∫–æ–≤–∏ API-—ò–∞ –º–æ–≥—É –ø–æ—Å—Ç–∞—Ç–∏ –∑–Ω–∞—á–∞—ò–Ω–∏, –º—É–ª—Ç–∏—ò–µ–∑–∏—á–∫–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∑–∞ –≥–ª–æ–±–∞–ª–Ω–µ –ø—Ä–∏–º–µ–Ω–µ –∏–ª–∏ —Å–ø–µ—Ü–∏—ò–∞–ª–∏–∑–æ–≤–∞–Ω–æ –¥–æ–º–µ–Ω—Å–∫–æ –∑–Ω–∞—ö–µ —É –æ–±–ª–∞—Å—Ç–∏–º–∞ –∫–∞–æ —à—Ç–æ —Å—É –ø—Ä–æ–≥—Ä–∞–º–∏—Ä–∞—ö–µ –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞.

### –ö—ô—É—á–Ω–∏ –∑–∞—Ö—Ç–µ–≤–∏ –ø—Ä–∏–º–µ–Ω–µ

–°–∞–≤—Ä–µ–º–µ–Ω–µ AI –ø—Ä–∏–º–µ–Ω–µ —Å—É–æ—á–∞–≤–∞—ò—É —Å–µ —Å–∞ –Ω–µ–∫–æ–ª–∏–∫–æ –æ—Å–Ω–æ–≤–Ω–∏—Ö –∑–∞—Ö—Ç–µ–≤–∞ –∫–æ—ò–∏ –æ–≥—Ä–∞–Ω–∏—á–∞–≤–∞—ò—É –ø—Ä–∞–∫—Ç–∏—á–Ω—É –ø—Ä–∏–º–µ–Ω—ô–∏–≤–æ—Å—Ç:

- **–ü—Ä–∏—Å—Ç—É–ø–∞—á–Ω–æ—Å—Ç**: –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç –æ—Ç–≤–æ—Ä–µ–Ω–æ–≥ –∫–æ–¥–∞ –∑–∞ —Ç—Ä–∞–Ω—Å–ø–∞—Ä–µ–Ω—Ç–Ω–æ—Å—Ç –∏ –ø—Ä–∏–ª–∞–≥–æ—í–∞–≤–∞—ö–µ
- **–ò—Å–ø–ª–∞—Ç–∏–≤–æ—Å—Ç**: –†–∞–∑—É–º–Ω–∏ –∑–∞—Ö—Ç–µ–≤–∏ –∑–∞ —Ä–∞—á—É–Ω–∞—Ä—Å–∫–µ —Ä–µ—Å—É—Ä—Å–µ –∑–∞ —Ä–∞–∑–ª–∏—á–∏—Ç–µ –±—É—ü–µ—Ç–µ
- **–§–ª–µ–∫—Å–∏–±–∏–ª–Ω–æ—Å—Ç**: –í–∏—à–µ –≤–µ–ª–∏—á–∏–Ω–∞ –º–æ–¥–µ–ª–∞ –∑–∞ —Ä–∞–∑–ª–∏—á–∏—Ç–µ —Å—Ü–µ–Ω–∞—Ä–∏—ò–µ –ø—Ä–∏–º–µ–Ω–µ
- **–ì–ª–æ–±–∞–ª–Ω–∏ –¥–æ–º–µ—Ç**: –à–∞–∫–µ –º—É–ª—Ç–∏—ò–µ–∑–∏—á–∫–µ –∏ –º–µ—í—É–∫—É–ª—Ç—É—Ä–Ω–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏
- **–°–ø–µ—Ü–∏—ò–∞–ª–∏–∑–∞—Ü–∏—ò–∞**: –í–∞—Ä–∏—ò–∞–Ω—Ç–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–µ –∑–∞ –¥–æ–º–µ–Ω –∑–∞ –æ–¥—Ä–µ—í–µ–Ω–µ —Å–ª—É—á–∞—ò–µ–≤–µ —É–ø–æ—Ç—Ä–µ–±–µ

## –§–∏–ª–æ–∑–æ—Ñ–∏—ò–∞ –º–æ–¥–µ–ª–∞ Qwen

–ü–æ—Ä–æ–¥–∏—Ü–∞ –º–æ–¥–µ–ª–∞ Qwen –ø—Ä–µ–¥—Å—Ç–∞–≤—ô–∞ —Å–≤–µ–æ–±—É—Ö–≤–∞—Ç–∞–Ω –ø—Ä–∏—Å—Ç—É–ø —Ä–∞–∑–≤–æ—ò—É AI –º–æ–¥–µ–ª–∞, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ —Å—Ç–∞–≤—ô–∞—ò—É—õ–∏ –ø—Ä–∏—Å—Ç—É–ø–∞—á–Ω–æ—Å—Ç –æ—Ç–≤–æ—Ä–µ–Ω–æ–≥ –∫–æ–¥–∞, –º—É–ª—Ç–∏—ò–µ–∑–∏—á–∫–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—É –ø—Ä–∏–º–µ–Ω—É —É–∑ –æ–¥—Ä–∂–∞–≤–∞—ö–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∏—Ö –∫–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–∏. Qwen –º–æ–¥–µ–ª–∏ —Ç–æ –ø–æ—Å—Ç–∏–∂—É –∫—Ä–æ–∑ —Ä–∞–∑–Ω–æ–≤—Ä—Å–Ω–µ –≤–µ–ª–∏—á–∏–Ω–µ –º–æ–¥–µ–ª–∞, –≤–∏—Å–æ–∫–æ–∫–≤–∞–ª–∏—Ç–µ—Ç–Ω–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—ò–µ —Ç—Ä–µ–Ω–∏–Ω–≥–∞ –∏ —Å–ø–µ—Ü–∏—ò–∞–ª–∏–∑–æ–≤–∞–Ω–µ –≤–∞—Ä–∏—ò–∞–Ω—Ç–µ –∑–∞ —Ä–∞–∑–ª–∏—á–∏—Ç–µ –¥–æ–º–µ–Ω–µ.

–ü–æ—Ä–æ–¥–∏—Ü–∞ Qwen –æ–±—É—Ö–≤–∞—Ç–∞ —Ä–∞–∑–ª–∏—á–∏—Ç–µ –ø—Ä–∏—Å—Ç—É–ø–µ –¥–∏–∑–∞—ò–Ω–∏—Ä–∞–Ω–µ –¥–∞ –ø—Ä—É–∂–µ –æ–ø—Ü–∏—ò–µ —à–∏—Ä–æ–º —Å–ø–µ–∫—Ç—Ä–∞ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–∏ –∏ –µ—Ñ–∏–∫–∞—Å–Ω–æ—Å—Ç–∏, –æ–º–æ–≥—É—õ–∞–≤–∞—ò—É—õ–∏ –ø—Ä–∏–º–µ–Ω—É –æ–¥ –º–æ–±–∏–ª–Ω–∏—Ö —É—Ä–µ—í–∞—ò–∞ –¥–æ —Å–µ—Ä–≤–µ—Ä—Å–∫–∏—Ö —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥—É–∑–µ—õ–∞, –∏—Å—Ç–æ–≤—Ä–µ–º–µ–Ω–æ –ø—Ä—É–∂–∞—ò—É—õ–∏ –∑–Ω–∞—á–∞—ò–Ω–µ AI —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏. –¶–∏—ô —ò–µ –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–æ–≤–∞—Ç–∏ –ø—Ä–∏—Å—Ç—É–ø –≤–∏—Å–æ–∫–æ–∫–≤–∞–ª–∏—Ç–µ—Ç–Ω–æ—ò AI —É–∑ –ø—Ä—É–∂–∞—ö–µ —Ñ–ª–µ–∫—Å–∏–±–∏–ª–Ω–æ—Å—Ç–∏ —É –∏–∑–±–æ—Ä—É –ø—Ä–∏–º–µ–Ω–µ.

### –û—Å–Ω–æ–≤–Ω–∏ –ø—Ä–∏–Ω—Ü–∏–ø–∏ –¥–∏–∑–∞—ò–Ω–∞ Qwen –º–æ–¥–µ–ª–∞

Qwen –º–æ–¥–µ–ª–∏ —Å—É –∏–∑–≥—Ä–∞—í–µ–Ω–∏ –Ω–∞ –Ω–µ–∫–æ–ª–∏–∫–æ –æ—Å–Ω–æ–≤–Ω–∏—Ö –ø—Ä–∏–Ω—Ü–∏–ø–∞ –∫–æ—ò–∏ –∏—Ö —Ä–∞–∑–ª–∏–∫—É—ò—É –æ–¥ –¥—Ä—É–≥–∏—Ö –ø–æ—Ä–æ–¥–∏—Ü–∞ —ò–µ–∑–∏—á–∫–∏—Ö –º–æ–¥–µ–ª–∞:

- **–ü—Ä–≤–æ –æ—Ç–≤–æ—Ä–µ–Ω–∏ –∫–æ–¥**: –ü–æ—Ç–ø—É–Ω–∞ —Ç—Ä–∞–Ω—Å–ø–∞—Ä–µ–Ω—Ç–Ω–æ—Å—Ç –∏ –ø—Ä–∏—Å—Ç—É–ø–∞—á–Ω–æ—Å—Ç –∑–∞ –∏—Å—Ç—Ä–∞–∂–∏–≤–∞—ö–µ –∏ –∫–æ–º–µ—Ä—Ü–∏—ò–∞–ª–Ω—É —É–ø–æ—Ç—Ä–µ–±—É
- **–°–≤–µ–æ–±—É—Ö–≤–∞—Ç–∞–Ω —Ç—Ä–µ–Ω–∏–Ω–≥**: –¢—Ä–µ–Ω–∏–Ω–≥ –Ω–∞ –º–∞—Å–∏–≤–Ω–∏–º, —Ä–∞–∑–Ω–æ–≤—Ä—Å–Ω–∏–º —Å–∫—É–ø–æ–≤–∏–º–∞ –ø–æ–¥–∞—Ç–∞–∫–∞ –∫–æ—ò–∏ –ø–æ–∫—Ä–∏–≤–∞—ò—É –≤–∏—à–µ —ò–µ–∑–∏–∫–∞ –∏ –¥–æ–º–µ–Ω–∞
- **–°–∫–∞–ª–∞–±–∏–ª–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: –í–∏—à–µ –≤–µ–ª–∏—á–∏–Ω–∞ –º–æ–¥–µ–ª–∞ –∑–∞ —É—Å–∫–ª–∞—í–∏–≤–∞—ö–µ —Å–∞ —Ä–∞–∑–ª–∏—á–∏—Ç–∏–º —Ä–∞—á—É–Ω–∞—Ä—Å–∫–∏–º –∑–∞—Ö—Ç–µ–≤–∏–º–∞
- **–°–ø–µ—Ü–∏—ò–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –∏–∑–≤—Ä—Å–Ω–æ—Å—Ç**: –í–∞—Ä–∏—ò–∞–Ω—Ç–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–µ –∑–∞ –¥–æ–º–µ–Ω –æ–ø—Ç–∏–º–∏–∑–æ–≤–∞–Ω–µ –∑–∞ –æ–¥—Ä–µ—í–µ–Ω–µ –∑–∞–¥–∞—Ç–∫–µ

## –ö—ô—É—á–Ω–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—ò–µ –∫–æ—ò–µ –æ–º–æ–≥—É—õ–∞–≤–∞—ò—É –ø–æ—Ä–æ–¥–∏—Ü—É Qwen

### –ú–∞—Å–∏–≤–Ω–∏ —Ç—Ä–µ–Ω–∏–Ω–≥ —É –≤–µ–ª–∏–∫–æ–º –æ–±–∏–º—É

–à–µ–¥–∞–Ω –æ–¥ –¥–µ—Ñ–∏–Ω–∏—à—É—õ–∏—Ö –∞—Å–ø–µ–∫–∞—Ç–∞ –ø–æ—Ä–æ–¥–∏—Ü–µ Qwen —ò–µ –º–∞—Å–∏–≤–Ω–∏ –æ–±–∏–º –ø–æ–¥–∞—Ç–∞–∫–∞ –∑–∞ —Ç—Ä–µ–Ω–∏–Ω–≥ –∏ —Ä–∞—á—É–Ω–∞—Ä—Å–∫–∏—Ö —Ä–µ—Å—É—Ä—Å–∞ —É–ª–æ–∂–µ–Ω–∏—Ö —É —Ä–∞–∑–≤–æ—ò –º–æ–¥–µ–ª–∞. Qwen –º–æ–¥–µ–ª–∏ –∫–æ—Ä–∏—Å—Ç–µ –ø–∞–∂—ô–∏–≤–æ –æ–¥–∞–±—Ä–∞–Ω–µ, –º—É–ª—Ç–∏—ò–µ–∑–∏—á–∫–µ —Å–∫—É–ø–æ–≤–µ –ø–æ–¥–∞—Ç–∞–∫–∞ –∫–æ—ò–∏ –æ–±—É—Ö–≤–∞—Ç–∞—ò—É —Ç—Ä–∏–ª–∏–æ–Ω–µ —Ç–æ–∫–µ–Ω–∞, –¥–∏–∑–∞—ò–Ω–∏—Ä–∞–Ω–µ –¥–∞ –ø—Ä—É–∂–µ —Å–≤–µ–æ–±—É—Ö–≤–∞—Ç–Ω–æ –∑–Ω–∞—ö–µ –æ —Å–≤–µ—Ç—É –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–∞.

–û–≤–∞—ò –ø—Ä–∏—Å—Ç—É–ø —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—à–µ –∫–æ–º–±–∏–Ω–æ–≤–∞—ö–µ–º –≤–∏—Å–æ–∫–æ–∫–≤–∞–ª–∏—Ç–µ—Ç–Ω–æ–≥ –≤–µ–± —Å–∞–¥—Ä–∂–∞—ò–∞, –∞–∫–∞–¥–µ–º—Å–∫–µ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–µ, —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—ò—É–º–∞ –∫–æ–¥–∞ –∏ –º—É–ª—Ç–∏—ò–µ–∑–∏—á–∫–∏—Ö —Ä–µ—Å—É—Ä—Å–∞. –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—ò–∞ —Ç—Ä–µ–Ω–∏–Ω–≥–∞ –Ω–∞–≥–ª–∞—à–∞–≤–∞ –∏ —à–∏—Ä–∏–Ω—É –∑–Ω–∞—ö–∞ –∏ –¥—É–±–∏–Ω—É —Ä–∞–∑—É–º–µ–≤–∞—ö–∞ —É —Ä–∞–∑–ª–∏—á–∏—Ç–∏–º –¥–æ–º–µ–Ω–∏–º–∞ –∏ —ò–µ–∑–∏—Ü–∏–º–∞.

### –ù–∞–ø—Ä–µ–¥–Ω–æ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–µ –∏ —Ä–∞–∑–º–∏—à—ô–∞—ö–µ

–ù–µ–¥–∞–≤–Ω–∏ Qwen –º–æ–¥–µ–ª–∏ —É–∫—ô—É—á—É—ò—É —Å–æ—Ñ–∏—Å—Ç–∏—Ü–∏—Ä–∞–Ω–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–∞ –∫–æ—ò–µ –æ–º–æ–≥—É—õ–∞–≤–∞—ò—É —Å–ª–æ–∂–µ–Ω–æ —Ä–µ—à–∞–≤–∞—ö–µ –ø—Ä–æ–±–ª–µ–º–∞ —É –≤–∏—à–µ –∫–æ—Ä–∞–∫–∞:

**–ú–æ–¥ —Ä–∞–∑–º–∏—à—ô–∞—ö–∞ (Qwen3)**: –ú–æ–¥–µ–ª–∏ –º–æ–≥—É –¥–∞ —Å–µ —É–∫—ô—É—á–µ —É –¥–µ—Ç–∞—ô–Ω–æ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–µ –∫–æ—Ä–∞–∫ –ø–æ –∫–æ—Ä–∞–∫ –ø—Ä–µ –Ω–µ–≥–æ —à—Ç–æ –ø—Ä—É–∂–µ –∫–æ–Ω–∞—á–Ω–µ –æ–¥–≥–æ–≤–æ—Ä–µ, —Å–ª–∏—á–Ω–æ —ô—É–¥—Å–∫–∏–º –ø—Ä–∏—Å—Ç—É–ø–∏–º–∞ —Ä–µ—à–∞–≤–∞—ö—É –ø—Ä–æ–±–ª–µ–º–∞.

**–î–≤–æ—Å—Ç—Ä—É–∫–∏ –º–æ–¥ —Ä–∞–¥–∞**: –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç –ø—Ä–µ–±–∞—Ü–∏–≤–∞—ö–∞ –∏–∑–º–µ—í—É –±—Ä–∑–æ–≥ –æ–¥–≥–æ–≤–æ—Ä–∞ –∑–∞ —ò–µ–¥–Ω–æ—Å—Ç–∞–≤–Ω–µ —É–ø–∏—Ç–µ –∏ –¥—É–±—ô–µ–≥ —Ä–∞–∑–º–∏—à—ô–∞—ö–∞ –∑–∞ —Å–ª–æ–∂–µ–Ω–µ –ø—Ä–æ–±–ª–µ–º–µ.

**–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—ò–∞ –ª–∞–Ω—Ü–∞ –º–∏—Å–ª–∏**: –ü—Ä–∏—Ä–æ–¥–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—ò–∞ –∫–æ—Ä–∞–∫–∞ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–∞ –∫–æ—ò–∏ –ø–æ–±–æ—ô—à–∞–≤–∞—ò—É —Ç—Ä–∞–Ω—Å–ø–∞—Ä–µ–Ω—Ç–Ω–æ—Å—Ç –∏ —Ç–∞—á–Ω–æ—Å—Ç —É —Å–ª–æ–∂–µ–Ω–∏–º –∑–∞–¥–∞—Ü–∏–º–∞.

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ–Ω—Å–∫–µ –∏–Ω–æ–≤–∞—Ü–∏—ò–µ

–ü–æ—Ä–æ–¥–∏—Ü–∞ Qwen —É–∫—ô—É—á—É—ò–µ –Ω–µ–∫–æ–ª–∏–∫–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ–Ω—Å–∫–∏—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—ò–∞ –¥–∏–∑–∞—ò–Ω–∏—Ä–∞–Ω–∏—Ö –∑–∞ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–µ –∏ –µ—Ñ–∏–∫–∞—Å–Ω–æ—Å—Ç:

**–°–∫–∞–ª–∞–±–∏–ª–Ω–∏ –¥–∏–∑–∞—ò–Ω**: –ö–æ–Ω–∑–∏—Å—Ç–µ–Ω—Ç–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —à–∏—Ä–æ–º –≤–µ–ª–∏—á–∏–Ω–∞ –º–æ–¥–µ–ª–∞ –∫–æ—ò–∞ –æ–º–æ–≥—É—õ–∞–≤–∞ –ª–∞–∫–æ —Å–∫–∞–ª–∏—Ä–∞—ö–µ –∏ –ø–æ—Ä–µ—í–µ—ö–µ.

**–ú—É–ª—Ç–∏–º–æ–¥–∞–ª–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—ò–∞**: –ë–µ—Å–ø—Ä–µ–∫–æ—Ä–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—ò–∞ –æ–±—Ä–∞–¥–µ —Ç–µ–∫—Å—Ç–∞, –≤–∏–∑–∏—ò–µ –∏ –∞—É–¥–∏–æ —Å–∞–¥—Ä–∂–∞—ò–∞ —É —É—ò–µ–¥–∏—ö–µ–Ω–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ.

**–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—ò–∞ –ø—Ä–∏–º–µ–Ω–µ**: –í–∏—à–µ –æ–ø—Ü–∏—ò–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞—ö–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∞ –ø—Ä–∏–º–µ–Ω–µ –∑–∞ —Ä–∞–∑–ª–∏—á–∏—Ç–µ —Ö–∞—Ä–¥–≤–µ—Ä—Å–∫–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—ò–µ.

## –í–µ–ª–∏—á–∏–Ω–∞ –º–æ–¥–µ–ª–∞ –∏ –æ–ø—Ü–∏—ò–µ –ø—Ä–∏–º–µ–Ω–µ

–°–∞–≤—Ä–µ–º–µ–Ω–∞ –æ–∫—Ä—É–∂–µ—ö–∞ –ø—Ä–∏–º–µ–Ω–µ –∏–º–∞—ò—É –∫–æ—Ä–∏—Å—Ç–∏ –æ–¥ —Ñ–ª–µ–∫—Å–∏–±–∏–ª–Ω–æ—Å—Ç–∏ Qwen –º–æ–¥–µ–ª–∞ —É —Ä–∞–∑–ª–∏—á–∏—Ç–∏–º —Ä–∞—á—É–Ω–∞—Ä—Å–∫–∏–º –∑–∞—Ö—Ç–µ–≤–∏–º–∞:

### –ú–∞–ª–∏ –º–æ–¥–µ–ª–∏ (0.5B-3B)

Qwen –ø—Ä—É–∂–∞ –µ—Ñ–∏–∫–∞—Å–Ω–µ –º–∞–ª–µ –º–æ–¥–µ–ª–µ –ø–æ–≥–æ–¥–Ω–µ –∑–∞ –ø—Ä–∏–º–µ–Ω—É –Ω–∞ –∏–≤–∏—Ü–∏, –º–æ–±–∏–ª–Ω–µ –∞–ø–ª–∏–∫–∞—Ü–∏—ò–µ –∏ –æ–∫—Ä—É–∂–µ—ö–∞ —Å–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–º —Ä–µ—Å—É—Ä—Å–∏–º–∞, —É–∑ –æ–¥—Ä–∂–∞–≤–∞—ö–µ –∏–º–ø—Ä–µ—Å–∏–≤–Ω–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.

### –°—Ä–µ–¥—ö–∏ –º–æ–¥–µ–ª–∏ (7B-32B)

–ú–æ–¥–µ–ª–∏ —Å—Ä–µ–¥—ö–µ–≥ –æ–ø—Å–µ–≥–∞ –Ω—É–¥–µ –ø–æ–±–æ—ô—à–∞–Ω–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∑–∞ –ø—Ä–æ—Ñ–µ—Å–∏–æ–Ω–∞–ª–Ω–µ –ø—Ä–∏–º–µ–Ω–µ, –ø—Ä—É–∂–∞—ò—É—õ–∏ –æ–¥–ª–∏—á–∞–Ω –±–∞–ª–∞–Ω—Å –∏–∑–º–µ—í—É –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–∏ –∏ —Ä–∞—á—É–Ω–∞—Ä—Å–∫–∏—Ö –∑–∞—Ö—Ç–µ–≤–∞.

### –í–µ–ª–∏–∫–∏ –º–æ–¥–µ–ª–∏ (72B+)

–ú–æ–¥–µ–ª–∏ –ø—É–Ω–µ –≤–µ–ª–∏—á–∏–Ω–µ –ø—Ä—É–∂–∞—ò—É –≤—Ä—Ö—É–Ω—Å–∫–µ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–µ –∑–∞ –∑–∞—Ö—Ç–µ–≤–Ω–µ –ø—Ä–∏–º–µ–Ω–µ, –∏—Å—Ç—Ä–∞–∂–∏–≤–∞—ö–µ –∏ –ø—Ä–∏–º–µ–Ω–µ —É –ø—Ä–µ–¥—É–∑–µ—õ–∏–º–∞ –∫–æ—ò–µ –∑–∞—Ö—Ç–µ–≤–∞—ò—É –º–∞–∫—Å–∏–º–∞–ª–Ω–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.

## –ü—Ä–µ–¥–Ω–æ—Å—Ç–∏ –ø–æ—Ä–æ–¥–∏—Ü–µ Qwen –º–æ–¥–µ–ª–∞

### –ü—Ä–∏—Å—Ç—É–ø–∞—á–Ω–æ—Å—Ç –æ—Ç–≤–æ—Ä–µ–Ω–æ–≥ –∫–æ–¥–∞

Qwen –º–æ–¥–µ–ª–∏ –ø—Ä—É–∂–∞—ò—É –ø–æ—Ç–ø—É–Ω—É —Ç—Ä–∞–Ω—Å–ø–∞—Ä–µ–Ω—Ç–Ω–æ—Å—Ç –∏ –º–æ–≥—É—õ–Ω–æ—Å—Ç–∏ –ø—Ä–∏–ª–∞–≥–æ—í–∞–≤–∞—ö–∞, –æ–º–æ–≥—É—õ–∞–≤–∞—ò—É—õ–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—ò–∞–º–∞ –¥–∞ —Ä–∞–∑—É–º–µ—ò—É, –º–æ–¥–∏—Ñ–∏–∫—É—ò—É –∏ –ø—Ä–∏–ª–∞–≥–æ–¥–µ –º–æ–¥–µ–ª–µ —Å–≤–æ—ò–∏–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∏–º –ø–æ—Ç—Ä–µ–±–∞–º–∞ –±–µ–∑ –∑–∞–≤–∏—Å–Ω–æ—Å—Ç–∏ –æ–¥ –¥–æ–±–∞–≤—ô–∞—á–∞.

### –§–ª–µ–∫—Å–∏–±–∏–ª–Ω–æ—Å—Ç –ø—Ä–∏–º–µ–Ω–µ

–û–ø—Å–µ–≥ –≤–µ–ª–∏—á–∏–Ω–∞ –º–æ–¥–µ–ª–∞ –æ–º–æ–≥—É—õ–∞–≤–∞ –ø—Ä–∏–º–µ–Ω—É —É —Ä–∞–∑–Ω–æ–≤—Ä—Å–Ω–∏–º —Ö–∞—Ä–¥–≤–µ—Ä—Å–∫–∏–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—ò–∞–º–∞, –æ–¥ –º–æ–±–∏–ª–Ω–∏—Ö —É—Ä–µ—í–∞—ò–∞ –¥–æ –≤—Ä—Ö—É–Ω—Å–∫–∏—Ö —Å–µ—Ä–≤–µ—Ä–∞, –ø—Ä—É–∂–∞—ò—É—õ–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—ò–∞–º–∞ —Ñ–ª–µ–∫—Å–∏–±–∏–ª–Ω–æ—Å—Ç —É –∏–∑–±–æ—Ä—É AI –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–µ.

### –ú—É–ª—Ç–∏—ò–µ–∑–∏—á–∫–∞ –∏–∑–≤—Ä—Å–Ω–æ—Å—Ç

Qwen –º–æ–¥–µ–ª–∏ —Å–µ –∏—Å—Ç–∏—á—É —É –º—É–ª—Ç–∏—ò–µ–∑–∏—á–∫–æ–º —Ä–∞–∑—É–º–µ–≤–∞—ö—É –∏ –≥–µ–Ω–µ—Ä–∏—Å–∞—ö—É, –ø–æ–¥—Ä–∂–∞–≤–∞—ò—É—õ–∏ –¥–µ—Å–µ—Ç–∏–Ω–µ —ò–µ–∑–∏–∫–∞ —Å–∞ –ø–æ—Å–µ–±–Ω–æ–º —Å–Ω–∞–≥–æ–º —É –µ–Ω–≥–ª–µ—Å–∫–æ–º –∏ –∫–∏–Ω–µ—Å–∫–æ–º, —á–∏–Ω–µ—õ–∏ –∏—Ö –ø–æ–≥–æ–¥–Ω–∏–º –∑–∞ –≥–ª–æ–±–∞–ª–Ω–µ –ø—Ä–∏–º–µ–Ω–µ.

### –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–µ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–µ

Qwen –º–æ–¥–µ–ª–∏ –¥–æ—Å–ª–µ–¥–Ω–æ –ø–æ—Å—Ç–∏–∂—É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–µ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤–∏–º–∞ —É–∑ –ø—Ä—É–∂–∞—ö–µ –ø—Ä–∏—Å—Ç—É–ø–∞—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–æ—Ä–µ–Ω–æ–≥ –∫–æ–¥–∞, –ø–æ–∫–∞–∑—É—ò—É—õ–∏ –¥–∞ –æ—Ç–≤–æ—Ä–µ–Ω–∏ –º–æ–¥–µ–ª–∏ –º–æ–≥—É –ø–∞—Ä–∏—Ä–∞—Ç–∏ –≤–ª–∞—Å–Ω–∏—á–∫–∏–º –∞–ª—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞–º–∞.

### –°–ø–µ—Ü–∏—ò–∞–ª–∏–∑–æ–≤–∞–Ω–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏

–í–∞—Ä–∏—ò–∞–Ω—Ç–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–µ –∑–∞ –¥–æ–º–µ–Ω, –∫–∞–æ —à—Ç–æ —Å—É Qwen-Coder –∏ Qwen-Math, –ø—Ä—É–∂–∞—ò—É —Å–ø–µ—Ü–∏—ò–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∑–Ω–∞—ö–µ —É–∑ –æ–¥—Ä–∂–∞–≤–∞—ö–µ –æ–ø—à—Ç–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞–∑—É–º–µ–≤–∞—ö–∞ —ò–µ–∑–∏–∫–∞.

## –ü—Ä–∞–∫—Ç–∏—á–Ω–∏ –ø—Ä–∏–º–µ—Ä–∏ –∏ —Å–ª—É—á–∞—ò–µ–≤–∏ —É–ø–æ—Ç—Ä–µ–±–µ

–ü—Ä–µ –Ω–µ–≥–æ —à—Ç–æ —Å–µ —É–ø—É—Å—Ç–∏–º–æ —É —Ç–µ—Ö–Ω–∏—á–∫–µ –¥–µ—Ç–∞—ô–µ, –∏—Å—Ç—Ä–∞–∂–∏–º–æ –Ω–µ–∫–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–µ –ø—Ä–∏–º–µ—Ä–µ —à—Ç–∞ Qwen –º–æ–¥–µ–ª–∏ –º–æ–≥—É –¥–∞ –ø–æ—Å—Ç–∏–≥–Ω—É:

### –ü—Ä–∏–º–µ—Ä –º–∞—Ç–µ–º–∞—Ç–∏—á–∫–æ–≥ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–∞

Qwen-Math —Å–µ –∏—Å—Ç–∏—á–µ —É —Ä–µ—à–∞–≤–∞—ö—É –º–∞—Ç–µ–º–∞—Ç–∏—á–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º–∞ –∫–æ—Ä–∞–∫ –ø–æ –∫–æ—Ä–∞–∫. –ù–∞ –ø—Ä–∏–º–µ—Ä, –∫–∞–¥–∞ —Å–µ –∑–∞—Ç—Ä–∞–∂–∏ —Ä–µ—à–µ—ö–µ —Å–ª–æ–∂–µ–Ω–æ–≥ –ø—Ä–æ–±–ª–µ–º–∞ –∏–∑ –∫–∞–ª–∫—É–ª—É—Å–∞:

```
User: Find the derivative of f(x) = x¬≥ + 2x¬≤ - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x¬≥ + 2x¬≤ - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(x‚Åø) = n¬∑x‚Åø‚Åª¬π

Breaking down each term:
- d/dx(x¬≥) = 3x¬≤
- d/dx(2x¬≤) = 2 ¬∑ 2x¬π = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x¬≤ + 4x - 5
```

### –ü—Ä–∏–º–µ—Ä –º—É–ª—Ç–∏—ò–µ–∑–∏—á–∫–µ –ø–æ–¥—Ä—à–∫–µ

Qwen –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—É—ò—É —ò–∞–∫–µ –º—É–ª—Ç–∏—ò–µ–∑–∏—á–∫–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —É —Ä–∞–∑–ª–∏—á–∏—Ç–∏–º —ò–µ–∑–∏—Ü–∏–º–∞:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**‰∏≠Êñá:**
‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâÊòØÊåáÊú∫Âô®ÔºàÁâπÂà´ÊòØËÆ°ÁÆóÊú∫Á≥ªÁªüÔºâÂØπ‰∫∫Á±ªÊô∫ËÉΩËøáÁ®ãÁöÑÊ®°Êãü„ÄÇËøô‰∫õËøáÁ®ãÂåÖÊã¨Â≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥ÈóÆÈ¢ò„ÄÅÊÑüÁü•ÂíåËØ≠Ë®ÄÁêÜËß£„ÄÇ‰∫∫Â∑•Êô∫ËÉΩ‰ΩøÊú∫Âô®ËÉΩÂ§üÊâßË°åÈÄöÂ∏∏ÈúÄË¶Å‰∫∫Á±ªËÆ§Áü•ËÉΩÂäõÁöÑ‰ªªÂä°„ÄÇ
```

### –ü—Ä–∏–º–µ—Ä –º—É–ª—Ç–∏–º–æ–¥–∞–ª–Ω–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏

Qwen-VL –º–æ–∂–µ –∏—Å—Ç–æ–≤—Ä–µ–º–µ–Ω–æ –æ–±—Ä–∞—í–∏–≤–∞—Ç–∏ —Ç–µ–∫—Å—Ç –∏ —Å–ª–∏–∫–µ:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### –ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∏—Å–∞—ö–∞ –∫–æ–¥–∞

Qwen-Coder —Å–µ –∏—Å—Ç–∏—á–µ —É –≥–µ–Ω–µ—Ä–∏—Å–∞—ö—É –∏ –æ–±—ò–∞—à—ö–∞–≤–∞—ö—É –∫–æ–¥–∞ —É —Ä–∞–∑–ª–∏—á–∏—Ç–∏–º –ø—Ä–æ–≥—Ä–∞–º—Å–∫–∏–º —ò–µ–∑–∏—Ü–∏–º–∞:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

–û–≤–∞ –∏–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—ò–∞ –ø—Ä–∞—Ç–∏ –Ω–∞—ò–±–æ—ô–µ –ø—Ä–∞–∫—Å–µ —Å–∞ —ò–∞—Å–Ω–∏–º –∏–º–µ–Ω–∏–º–∞ –ø—Ä–æ–º–µ–Ω—ô–∏–≤–∏—Ö, —Å–≤–µ–æ–±—É—Ö–≤–∞—Ç–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—ò–æ–º –∏ –µ—Ñ–∏–∫–∞—Å–Ω–æ–º –ª–æ–≥–∏–∫–æ–º.
```

### Edge Deployment Example

Qwen models can be deployed on various edge devices with optimized configurations:

```
# –ü—Ä–∏–º–µ—Ä –ø—Ä–∏–º–µ–Ω–µ –Ω–∞ –º–æ–±–∏–ª–Ω–æ–º —É—Ä–µ—í–∞—ò—É —É–∑ –∫–≤–∞–Ω—Ç–æ–≤–∞—ö–µ
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# –£—á–∏—Ç–∞–≤–∞—ö–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–æ–≥ –º–æ–¥–µ–ª–∞ –∑–∞ –º–æ–±–∏–ª–Ω—É –ø—Ä–∏–º–µ–Ω—É

```
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## –ï–≤–æ–ª—É—Ü–∏—ò–∞ –ø–æ—Ä–æ–¥–∏—Ü–µ Qwen

### Qwen 1.0 –∏ 1.5: –û—Å–Ω–æ–≤–Ω–∏ –º–æ–¥–µ–ª–∏

–†–∞–Ω–∏ Qwen –º–æ–¥–µ–ª–∏ —É—Å–ø–æ—Å—Ç–∞–≤–∏–ª–∏ —Å—É –æ—Å–Ω–æ–≤–Ω–µ –ø—Ä–∏–Ω—Ü–∏–ø–µ —Å–≤–µ–æ–±—É—Ö–≤–∞—Ç–Ω–æ–≥ —Ç—Ä–µ–Ω–∏–Ω–≥–∞ –∏ –ø—Ä–∏—Å—Ç—É–ø–∞—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–æ—Ä–µ–Ω–æ–≥ –∫–æ–¥–∞:

- **Qwen-7B (7B –ø–∞—Ä–∞–º–µ—Ç–∞—Ä–∞)**: –ü–æ—á–µ—Ç–Ω–æ –∏–∑–¥–∞—ö–µ —Å–∞ —Ñ–æ–∫—É—Å–æ–º –Ω–∞ —Ä–∞–∑—É–º–µ–≤–∞—ö–µ –∫–∏–Ω–µ—Å–∫–æ–≥ –∏ –µ–Ω–≥–ª–µ—Å–∫–æ–≥ —ò–µ–∑–∏–∫–∞
- **Qwen-14B (14B –ø–∞—Ä–∞–º–µ—Ç–∞—Ä–∞)**: –ü–æ–±–æ—ô—à–∞–Ω–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–∞ —É–Ω–∞–ø—Ä–µ—í–µ–Ω–∏–º —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–µ–º –∏ –∑–Ω–∞—ö–µ–º
- **Qwen-72B (72B –ø–∞—Ä–∞–º–µ—Ç–∞—Ä–∞)**: –ú–æ–¥–µ–ª –≤–µ–ª–∏–∫–æ–≥ –æ–±–∏–º–∞ –∫–æ—ò–∏ –ø—Ä—É–∂–∞ –≤—Ä—Ö—É–Ω—Å–∫–µ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–µ
- **–°–µ—Ä–∏—ò–∞ Qwen1.5**: –ü—Ä–æ—à–∏—Ä–µ–Ω–∞ –Ω–∞ –≤–∏—à–µ –≤–µ–ª–∏—á–∏–Ω–∞ (0.5B –¥–æ 110B) —Å–∞ –ø–æ–±–æ—ô—à–∞–Ω–∏–º —Ä—É–∫–æ–≤–∞—ö–µ–º –¥—É–≥–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º

### –ü–æ—Ä–æ–¥–∏—Ü–∞ Qwen2: –ú—É–ª—Ç–∏–º–æ–¥–∞–ª–Ω–∞ –µ–∫—Å–ø–∞–Ω–∑–∏—ò–∞

–°–µ—Ä–∏—ò–∞ Qwen2 –æ–∑–Ω–∞—á–∏–ª–∞ —ò–µ –∑–Ω–∞—á–∞—ò–∞–Ω –Ω–∞–ø—Ä–µ–¥–∞–∫ —É —ò–µ–∑–∏—á–∫–∏–º –∏ –º—É–ª—Ç–∏–º–æ–¥–∞–ª–Ω–∏–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏–º–∞:

- **Qwen2-0.5B –¥–æ 72B**: –°–≤–µ–æ–±—É—Ö–≤–∞—Ç–∞–Ω –æ–ø—Å–µ–≥ —ò–µ–∑–∏—á–∫–∏—Ö –º–æ–¥–µ–ª–∞ –∑–∞ —Ä–∞–∑–ª–∏—á–∏—Ç–µ –ø–æ—Ç—Ä–µ–±–µ –ø—Ä–∏–º–µ–Ω–µ
- **Qwen2-57B-A14B (MoE)**: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–µ—à–∞–≤–∏–Ω–µ –µ–∫—Å–ø–µ—Ä–∞—Ç–∞ –∑–∞ –µ—Ñ–∏–∫–∞—Å–Ω–æ –∫–æ—Ä–∏—à—õ–µ—ö–µ –ø–∞—Ä–∞–º–µ—Ç–∞—Ä–∞
- **Qwen2-VL**: –ù–∞–ø—Ä–µ–¥–Ω–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–∑–∏—ò–µ –∏ —ò–µ–∑–∏–∫–∞ –∑–∞ —Ä–∞–∑—É–º–µ–≤–∞—ö–µ —Å–ª–∏–∫–∞
- **Qwen2-Audio**: –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–¥–µ –∏ —Ä–∞–∑—É–º–µ–≤–∞—ö–∞ –∞—É–¥–∏–æ —Å–∞–¥—Ä–∂–∞—ò–∞
- **Qwen2-Math**: –°–ø–µ—Ü–∏—ò–∞–ª–∏–∑–æ–≤–∞–Ω–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–∫–æ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–µ –∏ —Ä–µ—à–∞–≤–∞—ö–µ –ø—Ä–æ–±–ª–µ–º–∞

### –ü–æ—Ä–æ–¥–∏—Ü–∞ Qwen2.5: –ü–æ–±–æ—ô—à–∞–Ω–µ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–µ

–°–µ—Ä–∏—ò–∞ Qwen2.5 –¥–æ–Ω–µ–ª–∞ —ò–µ –∑–Ω–∞—á–∞—ò–Ω–∞ –ø–æ–±–æ—ô—à–∞—ö–∞ —É —Å–≤–∏–º –¥–∏–º–µ–Ω–∑–∏—ò–∞–º–∞:

- **–ü—Ä–æ—à–∏—Ä–µ–Ω —Ç—Ä–µ–Ω–∏–Ω–≥**: 18 —Ç—Ä–∏–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–∞ —Ç—Ä–µ–Ω–∏–Ω–≥ –ø–æ–¥–∞—Ç–∞–∫–∞ –∑–∞ –ø–æ–±–æ—ô—à–∞–Ω–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏
- **–ü—Ä–æ—à–∏—Ä–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç**: –î–æ 128K —Ç–æ–∫–µ–Ω–∞ –¥—É–∂–∏–Ω–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å–∞ Turbo –≤–∞—Ä–∏—ò–∞–Ω—Ç–æ–º –∫–æ—ò–∞ –ø–æ–¥—Ä–∂–∞–≤–∞ 1M —Ç–æ–∫–µ–Ω–∞
- **–ü–æ–±–æ—ô—à–∞–Ω–∞ —Å–ø–µ—Ü–∏—ò–∞–ª–∏–∑–∞—Ü–∏—ò–∞**: –£–Ω–∞–ø—Ä–µ—í–µ–Ω–µ Qwen2.5-Coder –∏ Qwen2.5-Math –≤–∞—Ä–∏—ò–∞–Ω—Ç–µ
- **–ë–æ—ô–∞ –º—É–ª—Ç–∏—ò–µ–∑–∏—á–∫–∞ –ø–æ–¥—Ä—à–∫–∞**: –ü–æ–±–æ—ô—à–∞–Ω–µ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–µ —É 27+ —ò–µ–∑–∏–∫–∞

### –ü–æ—Ä–æ–¥–∏—Ü–∞ Qwen3: –ù–∞–ø—Ä–µ–¥–Ω–æ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–µ

–ù–∞—ò–Ω–æ–≤–∏—ò–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—ò–∞ –ø–æ–º–µ—Ä–∞ –≥—Ä–∞–Ω–∏—Ü–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–∞ –∏ —Ä–∞–∑–º–∏—à—ô–∞—ö–∞:

- **Qwen3-235B-A22B**: –í—Ä—Ö—É–Ω—Å–∫–∏ –º–æ–¥–µ–ª –º–µ—à–∞–≤–∏–Ω–µ –µ–∫—Å–ø–µ—Ä–∞—Ç–∞ —Å–∞ —É–∫—É–ø–Ω–æ 235B –ø–∞—Ä–∞–º–µ—Ç–∞—Ä–∞
- **Qwen3-30B-A3B**: –ï—Ñ–∏–∫–∞—Å–∞–Ω MoE –º–æ–¥–µ–ª —Å–∞ —Å–Ω–∞–∂–Ω–∏–º –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–∞–º–∞ –ø–æ –∞–∫—Ç–∏–≤–Ω–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—É
- **–ì—É—Å—Ç–∏ –º–æ–¥–µ–ª–∏**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B –∑–∞ —Ä–∞–∑–ª–∏—á–∏—Ç–µ —Å—Ü–µ–Ω–∞—Ä–∏—ò–µ –ø—Ä–∏–º–µ–Ω–µ
- **–ú–æ–¥ —Ä–∞–∑–º–∏—à—ô–∞—ö–∞**: –•–∏–±—Ä–∏–¥–Ω–∏ –ø—Ä–∏—Å—Ç—É–ø —Ä–µ–∑–æ–Ω–æ–≤–∞—ö—É –∫–æ—ò–∏ –ø–æ–¥—Ä–∂–∞–≤–∞ –∏ –±—Ä–∑–µ –æ–¥–≥–æ–≤–æ—Ä–µ –∏ –¥—É–±–æ–∫–æ —Ä–∞–∑–º–∏—à—ô–∞—ö–µ
- **–ú—É–ª—Ç–∏—ò–µ–∑–∏—á–∫–∞ –∏–∑–≤—Ä—Å–Ω–æ—Å—Ç**: –ü–æ–¥—Ä—à–∫–∞ –∑–∞ 119 —ò–µ–∑–∏–∫–∞ –∏ –¥–∏—ò–∞–ª–µ–∫–∞—Ç–∞
- **–£–Ω–∞–ø—Ä–µ—í–µ–Ω —Ç—Ä–µ–Ω–∏–Ω–≥**: 36 —Ç—Ä–∏–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–∞ —Ä–∞–∑–Ω–æ–≤—Ä—Å–Ω–∏—Ö, –≤–∏—Å–æ–∫–æ–∫–≤–∞–ª–∏—Ç–µ—Ç–Ω–∏—Ö —Ç—Ä–µ–Ω–∏–Ω–≥ –ø–æ–¥–∞—Ç–∞–∫–∞

## –ü—Ä–∏–º–µ–Ω–µ Qwen –º–æ–¥–µ–ª–∞

### –ü—Ä–∏–º–µ–Ω–µ —É –ø—Ä–µ–¥—É–∑–µ—õ–∏–º–∞

–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—ò–µ –∫–æ—Ä–∏—Å—Ç–µ Qwen –º–æ–¥–µ–ª–µ –∑–∞ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω–∞—Ç–∞, –∞—É—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—ò—É –∫–æ—Ä–∏—Å–Ω–∏—á–∫–µ –ø–æ–¥—Ä—à–∫–µ, –ø–æ–º–æ—õ —É –≥–µ–Ω–µ—Ä–∏—Å–∞—ö—É –∫–æ–¥–∞ –∏ –∞–ø–ª–∏–∫–∞—Ü–∏—ò–µ –ø–æ—Å–ª–æ–≤–Ω–µ –∏–Ω—Ç–µ–ª–∏–≥–µ–Ω—Ü–∏—ò–µ. –ü—Ä–∏—Ä–æ–¥–∞ –æ—Ç–≤–æ—Ä–µ–Ω–æ–≥ –∫–æ–¥–∞ –æ–º–æ–≥—É—õ–∞–≤–∞ –ø—Ä–∏–ª–∞–≥–æ
- Qwen3-235B-A22B –ø–æ—Å—Ç–∏–∂–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–µ —É –µ–≤–∞–ª—É–∞—Ü–∏—ò–∞–º–∞ –∫–æ–¥–∏—Ä–∞—ö–∞, –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –æ–ø—à—Ç–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —É –ø–æ—Ä–µ—í–µ—ö—É —Å–∞ –¥—Ä—É–≥–∏–º –≤—Ä—Ö—É–Ω—Å–∫–∏–º –º–æ–¥–µ–ª–∏–º–∞ –∫–∞–æ —à—Ç–æ —Å—É DeepSeek-R1, o1, o3-mini, Grok-3 –∏ Gemini-2.5-Pro  
- Qwen3-30B-A3B –Ω–∞–¥–º–∞—à—É—ò–µ QwQ-32B —Å–∞ 10 –ø—É—Ç–∞ –≤–∏—à–µ –∞–∫—Ç–∏–≤–∏—Ä–∞–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç–∞—Ä–∞  
- Qwen3-4B –º–æ–∂–µ –¥–∞ —Å–µ —Ç–∞–∫–º–∏—á–∏ —Å–∞ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–∞–º–∞ Qwen2.5-72B-Instruct  

**–î–æ—Å—Ç–∏–≥–Ω—É—õ–∞ —É –µ—Ñ–∏–∫–∞—Å–Ω–æ—Å—Ç–∏:**  
- –û—Å–Ω–æ–≤–Ω–∏ –º–æ–¥–µ–ª–∏ Qwen3-MoE –ø–æ—Å—Ç–∏–∂—É —Å–ª–∏—á–Ω–µ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–µ –∫–∞–æ –≥—É—Å—Ç–æ –±–∞–∑–Ω–∏ –º–æ–¥–µ–ª–∏ Qwen2.5, –∫–æ—Ä–∏—Å—Ç–µ—õ–∏ —Å–∞–º–æ 10% –∞–∫—Ç–∏–≤–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç–∞—Ä–∞  
- –ó–Ω–∞—á–∞—ò–Ω–∞ —É—à—Ç–µ–¥–∞ —Ç—Ä–æ—à–∫–æ–≤–∞ —É —Ç—Ä–µ–Ω–∏–Ω–≥—É –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—ò–∏ —É –ø–æ—Ä–µ—í–µ—ö—É —Å–∞ –≥—É—Å—Ç–∏–º –º–æ–¥–µ–ª–∏–º–∞  

**–ú—É–ª—Ç–∏—ò–µ–∑–∏—á–Ω–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏:**  
- –ú–æ–¥–µ–ª–∏ Qwen3 –ø–æ–¥—Ä–∂–∞–≤–∞—ò—É 119 —ò–µ–∑–∏–∫–∞ –∏ –¥–∏—ò–∞–ª–µ–∫–∞—Ç–∞  
- –à–∞–∫–µ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–µ —É —Ä–∞–∑–ª–∏—á–∏—Ç–∏–º —ò–µ–∑–∏—á–∫–∏–º –∏ –∫—É–ª—Ç—É—Ä–Ω–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏–º–∞  

**–û–±–∏–º —Ç—Ä–µ–Ω–∏–Ω–≥–∞:**  
- Qwen3 –∫–æ—Ä–∏—Å—Ç–∏ —Å–∫–æ—Ä–æ –¥—É–ø–ª–æ –≤–∏—à–µ, —Å–∞ –ø—Ä–∏–±–ª–∏–∂–Ω–æ 36 —Ç—Ä–∏–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–∞ –∫–æ—ò–∏ –ø–æ–∫—Ä–∏–≤–∞—ò—É 119 —ò–µ–∑–∏–∫–∞ –∏ –¥–∏—ò–∞–ª–µ–∫–∞—Ç–∞ —É –ø–æ—Ä–µ—í–µ—ö—É —Å–∞ Qwen2.5 –∫–æ—ò–∏ –∏–º–∞ 18 —Ç—Ä–∏–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–∞  

### –ú–∞—Ç—Ä–∏—Ü–∞ –ø–æ—Ä–µ—í–µ—ö–∞ –º–æ–¥–µ–ª–∞  

| –°–µ—Ä–∏—ò–∞ –º–æ–¥–µ–ª–∞ | –û–ø—Å–µ–≥ –ø–∞—Ä–∞–º–µ—Ç–∞—Ä–∞ | –î—É–∂–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ | –ö—ô—É—á–Ω–µ –ø—Ä–µ–¥–Ω–æ—Å—Ç–∏ | –ù–∞—ò–±–æ—ô–µ –ø—Ä–∏–º–µ–Ω–µ |  
|---------------|------------------|------------------|------------------|-----------------|  
| **Qwen2.5** | 0.5B-72B | 32K-128K | –£—Ä–∞–≤–Ω–æ—Ç–µ–∂–µ–Ω–µ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–µ, –º—É–ª—Ç–∏—ò–µ–∑–∏—á–Ω–æ—Å—Ç | –û–ø—à—Ç–µ –∞–ø–ª–∏–∫–∞—Ü–∏—ò–µ, –ø—Ä–æ–¥—É–∫—Ü–∏–æ–Ω–∞ —É–ø–æ—Ç—Ä–µ–±–∞ |  
| **Qwen2.5-Coder** | 1.5B-32B | 128K | –ì–µ–Ω–µ—Ä–∞—Ü–∏—ò–∞ –∫–æ–¥–∞, –ø—Ä–æ–≥—Ä–∞–º–∏—Ä–∞—ö–µ | –†–∞–∑–≤–æ—ò —Å–æ—Ñ—Ç–≤–µ—Ä–∞, –ø–æ–º–æ—õ —É –∫–æ–¥–∏—Ä–∞—ö—É |  
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | –ú–∞—Ç–µ–º–∞—Ç–∏—á–∫–æ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–µ | –ï–¥—É–∫–∞—Ç–∏–≤–Ω–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ, –°–¢–ï–ú –∞–ø–ª–∏–∫–∞—Ü–∏—ò–µ |  
| **Qwen2.5-VL** | –†–∞–∑–ª–∏—á–∏—Ç–æ | –ü—Ä–æ–º–µ–Ω—ô–∏–≤–æ | –†–∞–∑—É–º–µ–≤–∞—ö–µ –≤–∏–∑–∏—ò–µ –∏ —ò–µ–∑–∏–∫–∞ | –ú—É–ª—Ç–∏–º–æ–¥–∞–ª–Ω–µ –∞–ø–ª–∏–∫–∞—Ü–∏—ò–µ, –∞–Ω–∞–ª–∏–∑–∞ —Å–ª–∏–∫–∞ |  
| **Qwen3** | 0.6B-235B | –ü—Ä–æ–º–µ–Ω—ô–∏–≤–æ | –ù–∞–ø—Ä–µ–¥–Ω–æ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–µ, –º–æ–¥ —Ä–∞–∑–º–∏—à—ô–∞—ö–∞ | –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–µ, –∏—Å—Ç—Ä–∞–∂–∏–≤–∞—á–∫–µ –∞–ø–ª–∏–∫–∞—Ü–∏—ò–µ |  
| **Qwen3 MoE** | 30B-235B —É–∫—É–ø–Ω–æ | –ü—Ä–æ–º–µ–Ω—ô–∏–≤–æ | –ï—Ñ–∏–∫–∞—Å–Ω–µ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–µ –≤–µ–ª–∏–∫–æ–≥ –æ–±–∏–º–∞ | –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–µ –∞–ø–ª–∏–∫–∞—Ü–∏—ò–µ, –ø–æ—Ç—Ä–µ–±–µ –∑–∞ –≤–∏—Å–æ–∫–∏–º –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–∞–º–∞ |  

## –í–æ–¥–∏—á –∑–∞ –∏–∑–±–æ—Ä –º–æ–¥–µ–ª–∞  

### –ó–∞ –æ—Å–Ω–æ–≤–Ω–µ –∞–ø–ª–∏–∫–∞—Ü–∏—ò–µ  
- **Qwen2.5-0.5B/1.5B**: –ú–æ–±–∏–ª–Ω–µ –∞–ø–ª–∏–∫–∞—Ü–∏—ò–µ, —É—Ä–µ—í–∞—ò–∏ –Ω–∞ –∏–≤–∏—Ü–∏, –∞–ø–ª–∏–∫–∞—Ü–∏—ò–µ —É —Ä–µ–∞–ª–Ω–æ–º –≤—Ä–µ–º–µ–Ω—É  
- **Qwen2.5-3B/7B**: –û–ø—à—Ç–∏ —á–µ—Ç-–±–æ—Ç–æ–≤–∏, –≥–µ–Ω–µ—Ä–∏—Å–∞—ö–µ —Å–∞–¥—Ä–∂–∞—ò–∞, —Å–∏—Å—Ç–µ–º–∏ –ø–∏—Ç–∞—ö–∞ –∏ –æ–¥–≥–æ–≤–æ—Ä–∞  

### –ó–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–∫–µ –∏ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–µ –∑–∞–¥–∞—Ç–∫–µ  
- **Qwen2.5-Math**: –†–µ—à–∞–≤–∞—ö–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º–∞ –∏ –°–¢–ï–ú –µ–¥—É–∫–∞—Ü–∏—ò–∞  
- **Qwen3 —Å–∞ –º–æ–¥–æ–º —Ä–∞–∑–º–∏—à—ô–∞—ö–∞**: –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–µ –∫–æ—ò–µ –∑–∞—Ö—Ç–µ–≤–∞ –∞–Ω–∞–ª–∏–∑—É –∫–æ—Ä–∞–∫ –ø–æ –∫–æ—Ä–∞–∫  

### –ó–∞ –ø—Ä–æ–≥—Ä–∞–º–∏—Ä–∞—ö–µ –∏ —Ä–∞–∑–≤–æ—ò  
- **Qwen2.5-Coder**: –ì–µ–Ω–µ—Ä–∞—Ü–∏—ò–∞ –∫–æ–¥–∞, –¥–µ–±–∞–≥–æ–≤–∞—ö–µ, –ø–æ–º–æ—õ —É –ø—Ä–æ–≥—Ä–∞–º–∏—Ä–∞—ö—É  
- **Qwen3**: –ù–∞–ø—Ä–µ–¥–Ω–∏ –∑–∞–¥–∞—Ü–∏ –ø—Ä–æ–≥—Ä–∞–º–∏—Ä–∞—ö–∞ —Å–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏–º–∞ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–∞  

### –ó–∞ –º—É–ª—Ç–∏–º–æ–¥–∞–ª–Ω–µ –∞–ø–ª–∏–∫–∞—Ü–∏—ò–µ  
- **Qwen2.5-VL**: –†–∞–∑—É–º–µ–≤–∞—ö–µ —Å–ª–∏–∫–∞, –≤–∏–∑—É–µ–ª–Ω–æ –ø–∏—Ç–∞—ö–µ –∏ –æ–¥–≥–æ–≤–æ—Ä  
- **Qwen-Audio**: –û–±—Ä–∞–¥–∞ –∑–≤—É–∫–∞ –∏ —Ä–∞–∑—É–º–µ–≤–∞—ö–µ –≥–æ–≤–æ—Ä–∞  

### –ó–∞ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—É —É–ø–æ—Ç—Ä–µ–±—É  
- **Qwen2.5-32B/72B**: –†–∞–∑—É–º–µ–≤–∞—ö–µ —ò–µ–∑–∏–∫–∞ –≤–∏—Å–æ–∫–∏—Ö –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–∏  
- **Qwen3-235B-A22B**: –ú–∞–∫—Å–∏–º–∞–ª–Ω–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∑–∞ –∑–∞—Ö—Ç–µ–≤–Ω–µ –∞–ø–ª–∏–∫–∞—Ü–∏—ò–µ  

## –ü–ª–∞—Ç—Ñ–æ—Ä–º–µ –∑–∞ –ø—Ä–∏–º–µ–Ω—É –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç  

### –û–±–ª–∞—á–Ω–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ  
- **Hugging Face Hub**: –°–≤–µ–æ–±—É—Ö–≤–∞—Ç–∞–Ω —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—ò—É–º –º–æ–¥–µ–ª–∞ —Å–∞ –ø–æ–¥—Ä—à–∫–æ–º –∑–∞—ò–µ–¥–Ω–∏—Ü–µ  
- **ModelScope**: –ê–ª–∏–±–∞–±–∏–Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –º–æ–¥–µ–ª–∞ —Å–∞ –∞–ª–∞—Ç–∏–º–∞ –∑–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—ò—É  
- **–†–∞–∑–Ω–∏ –æ–±–ª–∞—á–Ω–∏ –ø—Ä–æ–≤–∞—ò–¥–µ—Ä–∏**: –ü–æ–¥—Ä—à–∫–∞ –∫—Ä–æ–∑ —Å—Ç–∞–Ω–¥–∞—Ä–¥–Ω–µ ML –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ  

### –û–∫–≤–∏—Ä–∏ –∑–∞ –ª–æ–∫–∞–ª–Ω–∏ —Ä–∞–∑–≤–æ—ò  
- **Transformers**: –°—Ç–∞–Ω–¥–∞—Ä–¥–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—ò–∞ Hugging Face-–∞ –∑–∞ –ª–∞–∫—É –ø—Ä–∏–º–µ–Ω—É  
- **vLLM**: –°–µ—Ä–≤–µ—Ä –≤–∏—Å–æ–∫–∏—Ö –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–∏ –∑–∞ –ø—Ä–æ–¥—É–∫—Ü–∏–æ–Ω–∞ –æ–∫—Ä—É–∂–µ—ö–∞  
- **Ollama**: –ü–æ—ò–µ–¥–Ω–æ—Å—Ç–∞–≤—ô–µ–Ω–∞ –ª–æ–∫–∞–ª–Ω–∞ –ø—Ä–∏–º–µ–Ω–∞ –∏ —É–ø—Ä–∞–≤—ô–∞—ö–µ  
- **ONNX Runtime**: –ö—Ä–æ—Å-–ø–ª–∞—Ç—Ñ–æ—Ä–º—Å–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—ò–∞ –∑–∞ —Ä–∞–∑–ª–∏—á–∏—Ç —Ö–∞—Ä–¥–≤–µ—Ä  
- **llama.cpp**: –ï—Ñ–∏–∫–∞—Å–Ω–∞ C++ –∏–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—ò–∞ –∑–∞ —Ä–∞–∑–ª–∏—á–∏—Ç–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ  

### –†–µ—Å—É—Ä—Å–∏ –∑–∞ —É—á–µ—ö–µ  
- **Qwen –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—ò–∞**: –ó–≤–∞–Ω–∏—á–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—ò–∞ –∏ –∫–∞—Ä—Ç–∏—Ü–µ –º–æ–¥–µ–ª–∞  
- **Hugging Face Model Hub**: –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏ –¥–µ–º–æ –∏ –ø—Ä–∏–º–µ—Ä–∏ –∑–∞—ò–µ–¥–Ω–∏—Ü–µ  
- **–ò—Å—Ç—Ä–∞–∂–∏–≤–∞—á–∫–∏ —Ä–∞–¥–æ–≤–∏**: –¢–µ—Ö–Ω–∏—á–∫–∏ —Ä–∞–¥–æ–≤–∏ –Ω–∞ arxiv-—É –∑–∞ –¥—É–±—ô–µ —Ä–∞–∑—É–º–µ–≤–∞—ö–µ  
- **–§–æ—Ä—É–º–∏ –∑–∞—ò–µ–¥–Ω–∏—Ü–µ**: –ê–∫—Ç–∏–≤–Ω–∞ –ø–æ–¥—Ä—à–∫–∞ –∑–∞—ò–µ–¥–Ω–∏—Ü–µ –∏ –¥–∏—Å–∫—É—Å–∏—ò–µ  

### –ö–∞–∫–æ –∑–∞–ø–æ—á–µ—Ç–∏ —Å–∞ Qwen –º–æ–¥–µ–ª–∏–º–∞  

#### –ü–ª–∞—Ç—Ñ–æ—Ä–º–µ –∑–∞ —Ä–∞–∑–≤–æ—ò  
1. **Hugging Face Transformers**: –ó–∞–ø–æ—á–Ω–∏—Ç–µ —Å–∞ —Å—Ç–∞–Ω–¥–∞—Ä–¥–Ω–æ–º Python –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—ò–æ–º  
2. **ModelScope**: –ò—Å—Ç—Ä–∞–∂–∏—Ç–µ –ê–ª–∏–±–∞–±–∏–Ω–µ –∞–ª–∞—Ç–µ –∑–∞ –æ–ø—Ç–∏–º–∏–∑–æ–≤–∞–Ω—É –ø—Ä–∏–º–µ–Ω—É  
3. **–õ–æ–∫–∞–ª–Ω–∞ –ø—Ä–∏–º–µ–Ω–∞**: –ö–æ—Ä–∏—Å—Ç–∏—Ç–µ Ollama –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–Ω–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ –∑–∞ –ª–æ–∫–∞–ª–Ω–æ —Ç–µ—Å—Ç–∏—Ä–∞—ö–µ  

#### –ü—É—Ç —É—á–µ—ö–∞  
1. **–†–∞–∑—É–º–µ–≤–∞—ö–µ –æ—Å–Ω–æ–≤–Ω–∏—Ö –∫–æ–Ω—Ü–µ–ø–∞—Ç–∞**: –ü—Ä–æ—É—á–∏—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ Qwen –ø–æ—Ä–æ–¥–∏—Ü–µ  
2. **–ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Å–∞—ö–µ —Å–∞ –≤–∞—Ä–∏—ò–∞–Ω—Ç–∞–º–∞**: –ò—Å–ø—Ä–æ–±–∞—ò—Ç–µ —Ä–∞–∑–ª–∏—á–∏—Ç–µ –≤–µ–ª–∏—á–∏–Ω–µ –º–æ–¥–µ–ª–∞ –¥–∞ —Ä–∞–∑—É–º–µ—Ç–µ –∫–æ–º–ø—Ä–æ–º–∏—Å–µ —É –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–∞–º–∞  
3. **–ü—Ä–∞–∫—Ç–∏—á–Ω–∞ –∏–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—ò–∞**: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ –º–æ–¥–µ–ª–µ —É —Ä–∞–∑–≤–æ—ò–Ω–∏–º –æ–∫—Ä—É–∂–µ—ö–∏–º–∞  
4. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—ò–∞ –ø—Ä–∏–º–µ–Ω–µ**: –§–∏–Ω–æ –ø–æ–¥–µ—Å–∏—Ç–µ –∑–∞ –ø—Ä–æ–¥—É–∫—Ü–∏–æ–Ω–µ —Å–ª—É—á–∞—ò–µ–≤–µ  

#### –ù–∞—ò–±–æ—ô–µ –ø—Ä–∞–∫—Å–µ  
- **–ó–∞–ø–æ—á–Ω–∏—Ç–µ —Å–∞ –º–∞—ö–∏–º –º–æ–¥–µ–ª–∏–º–∞**: –ü–æ—á–Ω–∏—Ç–µ —Å–∞ –º–∞—ö–∏–º –º–æ–¥–µ–ª–∏–º–∞ (1.5B-7B) –∑–∞ –ø–æ—á–µ—Ç–Ω–∏ —Ä–∞–∑–≤–æ—ò  
- **–ö–æ—Ä–∏—Å—Ç–∏—Ç–µ —à–∞–±–ª–æ–Ω–µ –∑–∞ —á–µ—Ç**: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ –ø—Ä–∞–≤–∏–ª–Ω–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–∞—ö–µ –∑–∞ –æ–ø—Ç–∏–º–∞–ª–Ω–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–µ  
- **–ü—Ä–∞—Ç–∏—Ç–µ —Ä–µ—Å—É—Ä—Å–µ**: –ü—Ä–∞—Ç–∏—Ç–µ —É–ø–æ—Ç—Ä–µ–±—É –º–µ–º–æ—Ä–∏—ò–µ –∏ –±—Ä–∑–∏–Ω—É –∏–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—ò–µ  
- **–†–∞–∑–º–æ—Ç—Ä–∏—Ç–µ —Å–ø–µ—Ü–∏—ò–∞–ª–∏–∑–∞—Ü–∏—ò—É**: –ò–∑–∞–±–µ—Ä–∏—Ç–µ –≤–∞—Ä–∏—ò–∞–Ω—Ç–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–µ –∑–∞ –¥–æ–º–µ–Ω –∫–∞–¥–∞ —ò–µ —Ç–æ –ø—Ä–∏–∫–ª–∞–¥–Ω–æ  

## –ù–∞–ø—Ä–µ–¥–Ω–∏ –æ–±—Ä–∞—Å—Ü–∏ —É–ø–æ—Ç—Ä–µ–±–µ  

### –ü—Ä–∏–º–µ—Ä–∏ —Ñ–∏–Ω–æ –ø–æ–¥–µ—à–∞–≤–∞—ö–∞  

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```
  
### –°–ø–µ—Ü–∏—ò–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∏–Ω–∂–µ—ö–µ—Ä—Å—Ç–≤–æ —É–ø–∏—Ç–∞  

**–ó–∞ –∑–∞–¥–∞—Ç–∫–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–∞:**  
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```
  
**–ó–∞ –≥–µ–Ω–µ—Ä–∏—Å–∞—ö–µ –∫–æ–¥–∞ —Å–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º:**  
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```
  
### –ú—É–ª—Ç–∏—ò–µ–∑–∏—á–Ω–µ –∞–ø–ª–∏–∫–∞—Ü–∏—ò–µ  

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (‰∏≠Êñá)",
        "es": "Spanish (Espa√±ol)",
        "fr": "French (Fran√ßais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (Êó•Êú¨Ë™û)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```
  
### üîß –û–±—Ä–∞—Å—Ü–∏ –ø—Ä–æ–¥—É–∫—Ü–∏–æ–Ω–µ –ø—Ä–∏–º–µ–Ω–µ  

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```
  

## –°—Ç—Ä–∞—Ç–µ–≥–∏—ò–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—ò–µ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–∏  

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—ò–∞ –º–µ–º–æ—Ä–∏—ò–µ  

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```
  
### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—ò–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—ò–µ  

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```
  

## –ù–∞—ò–±–æ—ô–µ –ø—Ä–∞–∫—Å–µ –∏ —Å–º–µ—Ä–Ω–∏—Ü–µ  

### –ë–µ–∑–±–µ–¥–Ω–æ—Å—Ç –∏ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç  

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```
  
### –ü—Ä–∞—õ–µ—ö–µ –∏ –µ–≤–∞–ª—É–∞—Ü–∏—ò–∞  

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```
  

## –ó–∞–∫—ô—É—á–∞–∫  

–ü–æ—Ä–æ–¥–∏—Ü–∞ Qwen –º–æ–¥–µ–ª–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤—ô–∞ —Å–≤–µ–æ–±—É—Ö–≤–∞—Ç–∞–Ω –ø—Ä–∏—Å—Ç—É–ø –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü–∏—ò–∏ AI —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—ò–µ —É–∑ –æ–¥—Ä–∂–∞–≤–∞—ö–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∏—Ö –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–∏ —É —Ä–∞–∑–ª–∏—á–∏—Ç–∏–º –∞–ø–ª–∏–∫–∞—Ü–∏—ò–∞–º–∞. –ö—Ä–æ–∑ –ø–æ—Å–≤–µ—õ–µ–Ω–æ—Å—Ç –æ—Ç–≤–æ—Ä–µ–Ω–æ–º –ø—Ä–∏—Å—Ç—É–ø—É, –º—É–ª—Ç–∏—ò–µ–∑–∏—á–Ω–∏–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏–º–∞ –∏ —Ñ–ª–µ–∫—Å–∏–±–∏–ª–Ω–∏–º –æ–ø—Ü–∏—ò–∞–º–∞ –ø—Ä–∏–º–µ–Ω–µ, Qwen –æ–º–æ–≥—É—õ–∞–≤–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—ò–∞–º–∞ –∏ –ø—Ä–æ–≥—Ä–∞–º–µ—Ä–∏–º–∞ –¥–∞ –∏—Å–∫–æ—Ä–∏—Å—Ç–µ –º–æ—õ–Ω–µ AI —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–µ–∑ –æ–±–∑–∏—Ä–∞ –Ω–∞ —Ä–µ—Å—É—Ä—Å–µ –∏–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–µ –∑–∞—Ö—Ç–µ–≤–µ.  

### –ö—ô—É—á–Ω–∏ –∑–∞–∫—ô—É—á—Ü–∏  

**–ò–∑–≤—Ä—Å–Ω–æ—Å—Ç –æ—Ç–≤–æ—Ä–µ–Ω–æ–≥ –∫–æ–¥–∞**: Qwen –ø–æ–∫–∞–∑—É—ò–µ –¥–∞ –º–æ–¥–µ–ª–∏ –æ—Ç–≤–æ—Ä–µ–Ω–æ–≥ –∫–æ–¥–∞ –º–æ–≥—É –ø–æ—Å—Ç–∏—õ–∏ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–µ —Å–∞ –≤–ª–∞—Å–Ω–∏—á–∫–∏–º –∞–ª—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞–º–∞ —É–∑ –ø—Ä—É–∂–∞—ö–µ —Ç—Ä–∞–Ω—Å–ø–∞—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏, –ø—Ä–∏–ª–∞–≥–æ—í–∞–≤–∞—ö–∞ –∏ –∫–æ–Ω—Ç—Ä–æ–ª–µ.  

**–°–∫–∞–ª–∞–±–∏–ª–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: –û–ø—Å–µ–≥ –æ–¥ 0.5B –¥–æ 235B –ø–∞—Ä–∞–º–µ—Ç–∞—Ä–∞ –æ–º–æ–≥—É—õ–∞–≤–∞ –ø—Ä–∏–º–µ–Ω—É —É —Ü–µ–ª–æ–º —Å–ø–µ–∫—Ç—Ä—É —Ä–∞—á—É–Ω–∞—Ä—Å–∫–∏—Ö –æ–∫—Ä—É–∂–µ—ö–∞, –æ–¥ –º–æ–±–∏–ª–Ω–∏—Ö —É—Ä–µ—í–∞—ò–∞ –¥–æ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞.  

**–°–ø–µ—Ü–∏—ò–∞–ª–∏–∑–æ–≤–∞–Ω–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏**: –í–∞—Ä–∏—ò–∞–Ω—Ç–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–µ –∑–∞ –¥–æ–º–µ–Ω –∫–∞–æ —à—Ç–æ —Å—É Qwen-Coder, Qwen-Math –∏ Qwen-VL –ø—Ä—É–∂–∞—ò—É —Å–ø–µ—Ü–∏—ò–∞–ª–∏–∑–æ–≤–∞–Ω—É –µ–∫—Å–ø–µ—Ä—Ç–∏–∑—É —É–∑ –æ–¥—Ä–∂–∞–≤–∞—ö–µ –æ–ø—à—Ç–µ–≥ —Ä–∞–∑—É–º–µ–≤–∞—ö–∞ —ò–µ–∑–∏–∫–∞.  

**–ì–ª–æ–±–∞–ª–Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç**: –à–∞–∫–∞ –º—É–ª—Ç–∏—ò–µ–∑–∏—á–Ω–∞ –ø–æ–¥—Ä—à–∫–∞ –∑–∞ 119+ —ò–µ–∑–∏–∫–∞ —á–∏–Ω–∏ Qwen –ø–æ–≥–æ–¥–Ω–∏–º –∑–∞ –º–µ—í—É–Ω–∞—Ä–æ–¥–Ω–µ –∞–ø–ª–∏–∫–∞—Ü–∏—ò–µ –∏ —Ä–∞–∑–Ω–æ–≤—Ä—Å–Ω–µ –∫–æ—Ä–∏—Å–Ω–∏—á–∫–µ –±–∞–∑–µ.  

**–ö–æ–Ω—Ç–∏–Ω—É–∏—Ä–∞–Ω–∞ –∏–Ω–æ–≤–∞—Ü–∏—ò–∞**: –ï–≤–æ–ª—É—Ü–∏—ò–∞ –æ–¥ Qwen 1.0 –¥–æ Qwen3 –ø–æ–∫–∞–∑—É—ò–µ –∫–æ–Ω–∑–∏—Å—Ç–µ–Ω—Ç–Ω–æ –ø–æ–±–æ—ô—à–∞—ö–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏, –µ—Ñ–∏–∫–∞—Å–Ω–æ—Å—Ç–∏ –∏ –æ–ø—Ü–∏—ò–∞ –ø—Ä–∏–º–µ–Ω–µ.  

### –ü–æ–≥–ª–µ–¥ —É –±—É–¥—É—õ–Ω–æ—Å—Ç  

–ö–∞–∫–æ —Å–µ –ø–æ—Ä–æ–¥–∏—Ü–∞ Qwen –º–æ–¥–µ–ª–∞ –Ω–∞—Å—Ç–∞–≤—ô–∞ —Ä–∞–∑–≤–∏—ò–∞—Ç–∏, –º–æ–∂–µ–º–æ –æ—á–µ–∫–∏–≤–∞—Ç–∏:  
- **–ü–æ–±–æ—ô—à–∞–Ω—É –µ—Ñ–∏–∫–∞—Å–Ω–æ—Å—Ç**: –ö–æ–Ω—Ç–∏–Ω—É–∏—Ä–∞–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—ò–∞ –∑–∞ –±–æ—ô–µ –æ–¥–Ω–æ—Å–µ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–∏-–ø–∞—Ä–∞–º–µ—Ç–∞—Ä–∞  
- **–ü—Ä–æ—à–∏—Ä–µ–Ω–µ –º—É–ª—Ç–∏–º–æ–¥–∞–ª–Ω–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏**: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—ò—É —Å–æ—Ñ–∏—Å—Ç–∏—Ü–∏—Ä–∞–Ω–∏—ò–µ –æ–±—Ä–∞–¥–µ –≤–∏–∑–∏—ò–µ, –∑–≤—É–∫–∞ –∏ —Ç–µ–∫—Å—Ç–∞  
- **–£–Ω–∞–ø—Ä–µ—í–µ–Ω–æ —Ä–µ–∑–æ–Ω–æ–≤–∞—ö–µ**: –ù–∞–ø—Ä–µ–¥–Ω–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∏ —Ä–∞–∑–º–∏—à—ô–∞—ö–∞ –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–µ—à–∞–≤–∞—ö–∞ –ø—Ä–æ–±–ª–µ–º–∞ —É –≤–∏—à–µ –∫–æ—Ä–∞–∫–∞  
- **–ë–æ—ô–µ –∞–ª–∞—Ç–µ –∑–∞ –ø—Ä–∏–º–µ–Ω—É**: –ü–æ–±–æ—ô—à–∞–Ω–µ –æ–∫–≤–∏—Ä–µ –∏ –∞–ª–∞—Ç–µ –∑–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—ò—É –∑–∞ —Ä–∞–∑–ª–∏—á–∏—Ç–µ —Å—Ü–µ–Ω–∞—Ä–∏—ò–µ –ø—Ä–∏–º–µ–Ω–µ  
- **–†–∞—Å—Ç –∑–∞—ò–µ–¥–Ω–∏—Ü–µ**: –ü—Ä–æ—à–∏—Ä–µ–Ω –µ–∫–æ—Å–∏—Å—Ç–µ–º –∞–ª–∞—Ç–∞, –∞–ø–ª–∏–∫–∞—Ü–∏—ò–∞ –∏ –¥–æ–ø—Ä–∏–Ω–æ—Å–∞ –∑–∞—ò–µ–¥–Ω–∏—Ü–µ  

### –°–ª–µ–¥–µ—õ–∏ –∫–æ—Ä–∞—Ü–∏  

–ë–µ–∑ –æ–±–∑–∏—Ä–∞ –¥–∞ –ª–∏ –≥—Ä–∞–¥–∏—Ç–µ —á–µ—Ç-–±–æ—Ç, —Ä–∞–∑–≤–∏—ò–∞—Ç–µ –µ–¥—É–∫–∞—Ç–∏–≤–Ω–µ –∞–ª–∞—Ç–µ, –∫—Ä–µ–∏—Ä–∞—Ç–µ –∞—Å–∏—Å—Ç–µ–Ω—Ç–µ –∑–∞ –∫–æ–¥–∏—Ä–∞—ö–µ –∏–ª–∏ —Ä–∞–¥–∏—Ç–µ –Ω–∞ –º—É–ª—Ç–∏—ò–µ–∑–∏—á–Ω–∏–º –∞–ø–ª–∏–∫–∞—Ü–∏—ò–∞–º–∞, –ø–æ—Ä–æ–¥–∏—Ü–∞ Qwen –ø—Ä—É–∂–∞ —Å–∫–∞–ª–∞–±–∏–ª–Ω–∞ —Ä–µ—à–µ—ö–∞ —Å–∞ —ò–∞–∫–æ–º –ø–æ–¥—Ä—à–∫–æ–º –∑–∞—ò–µ–¥–Ω–∏—Ü–µ –∏ —Å–≤–µ–æ–±—É—Ö–≤–∞—Ç–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—ò–æ–º.  

–ó–∞ –Ω–∞—ò–Ω–æ–≤–∏—ò–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—ò–µ, –∏–∑–¥–∞—ö–∞ –º–æ–¥–µ–ª–∞ –∏ –¥–µ—Ç–∞—ô–Ω—É —Ç–µ—Ö–Ω–∏—á–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—ò—É, –ø–æ—Å–µ—Ç–∏—Ç–µ –∑–≤–∞–Ω–∏—á–Ω–µ Qwen —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—ò—É–º–µ –Ω–∞ Hugging Face-—É –∏ –∏—Å—Ç—Ä–∞–∂–∏—Ç–µ –∞–∫—Ç–∏–≤–Ω–µ –¥–∏—Å–∫—É—Å–∏—ò–µ –∏ –ø—Ä–∏–º–µ—Ä–µ –∑–∞—ò–µ–¥–Ω–∏—Ü–µ.  

–ë—É–¥—É—õ–Ω–æ—Å—Ç —Ä–∞–∑–≤–æ—ò–∞ AI –ª–µ–∂–∏ —É –¥–æ—Å—Ç—É–ø–Ω–∏–º, —Ç—Ä–∞–Ω—Å–ø–∞—Ä–µ–Ω—Ç–Ω–∏–º –∏ –º–æ—õ–Ω–∏–º –∞–ª–∞—Ç–∏–º–∞ –∫–æ—ò–∏ –æ–º–æ–≥—É—õ–∞–≤–∞—ò—É –∏–Ω–æ–≤–∞—Ü–∏—ò–µ —É —Å–≤–∏–º —Å–µ–∫—Ç–æ—Ä–∏–º–∞ –∏ —Ä–∞–∑–º–µ—Ä–∞–º–∞. –ü–æ—Ä–æ–¥–∏—Ü–∞ Qwen –º–æ–¥–µ–ª–∞ –æ–ª–∏—á–∞–≤–∞ –æ–≤—É –≤–∏–∑–∏—ò—É, –ø—Ä—É–∂–∞—ò—É—õ–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—ò–∞–º–∞ –∏ –ø—Ä–æ–≥—Ä–∞–º–µ—Ä–∏–º–∞ –æ—Å–Ω–æ–≤—É –∑–∞ –∏–∑–≥—Ä–∞–¥—ö—É —Å–ª–µ–¥–µ—õ–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—ò–µ –∞–ø–ª–∏–∫–∞—Ü–∏—ò–∞ –∑–∞—Å–Ω–æ–≤–∞–Ω–∏—Ö –Ω–∞ AI.  

## –î–æ–¥–∞—Ç–Ω–∏ —Ä–µ—Å—É—Ä—Å–∏  

- **–ó–≤–∞–Ω–∏—á–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—ò–∞**: [Qwen –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—ò–∞](https://qwen.readthedocs.io/)  
- **Model Hub**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)  
- **–¢–µ—Ö–Ω–∏—á–∫–∏ —Ä–∞–¥–æ–≤–∏**: [Qwen –∏—Å—Ç—Ä–∞–∂–∏–≤–∞—á–∫–µ –ø—É–±–ª–∏–∫–∞—Ü–∏—ò–µ](https://arxiv.org/search/?query=Qwen&searchtype=all)  
- **–ó–∞—ò–µ–¥–Ω–∏—Ü–∞**: [GitHub –¥–∏—Å–∫—É—Å–∏—ò–µ –∏ –ø—Ä–æ–±–ª–µ–º–∏](https://github.com/QwenLM/)  
- **ModelScope –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)  

## –ò—Å—Ö–æ–¥–∏ —É—á–µ—ö–∞  

–ù–∞–∫–æ–Ω –∑–∞–≤—Ä—à–µ—Ç–∫–∞ –æ–≤–æ–≥ –º–æ–¥—É–ª–∞, –º–æ—õ–∏ —õ–µ—Ç–µ –¥–∞:  
1. –û–±—ò–∞—Å–Ω–∏—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ–Ω—Å–∫–µ –ø—Ä–µ–¥–Ω–æ—Å—Ç–∏ –ø–æ—Ä–æ–¥–∏—Ü–µ Qwen –º–æ–¥–µ–ª–∞ –∏ —ö–µ–Ω –ø—Ä–∏—Å—Ç—É–ø –æ—Ç–≤–æ—Ä–µ–Ω–æ–º –∫–æ–¥—É  
2. –ò–∑–∞–±–µ—Ä–µ—Ç–µ –æ–¥–≥–æ–≤–∞—Ä–∞—ò—É—õ—É –≤–∞—Ä–∏—ò–∞–Ω—Ç—É Qwen –º–æ–¥–µ–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤—É —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∏—Ö –∑–∞—Ö—Ç–µ–≤–∞ –∞–ø–ª–∏–∫–∞—Ü–∏—ò–µ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ—ö–∞ —Ä–µ—Å—É—Ä—Å–∞  
3. –ü—Ä–∏–º–µ–Ω–∏—Ç–µ Qwen –º–æ–¥–µ–ª–µ —É —Ä–∞–∑–ª–∏—á–∏—Ç–∏–º —Å—Ü–µ–Ω–∞—Ä–∏—ò–∏–º–∞ –ø—Ä–∏–º–µ–Ω–µ —Å–∞ –æ–ø—Ç–∏–º–∏–∑–æ–≤–∞–Ω–∏–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—ò–∞–º–∞  
4. –ü—Ä–∏–º–µ–Ω–∏—Ç–µ —Ç–µ—Ö–Ω–∏–∫–µ –∫–≤–∞–Ω—Ç–æ–≤–∞—ö–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—ò–µ –∑–∞ –ø–æ–±–æ—ô—à–∞—ö–µ –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–∏ Qwen –º–æ–¥–µ–ª–∞  
5. –ü—Ä–æ—Ü–µ–Ω–∏—Ç–µ –∫–æ–º–ø—Ä–æ–º–∏—Å–µ –∏–∑–º–µ—í—É –≤–µ–ª–∏—á–∏–Ω–µ –º–æ–¥–µ–ª–∞, –ø–µ—Ä—Ñ–æ—Ä–º–∞–Ω—Å–∏ –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —É –æ–∫–≤–∏—Ä—É –ø–æ—Ä–æ–¥–∏—Ü–µ Qwen  

## –®—Ç–∞ —Å–ª–µ–¥–∏  

- [03: –û—Å–Ω–æ–≤–µ –ø–æ—Ä–æ–¥–∏—Ü–µ Gemma](03.GemmaFamily.md)  

---

**–û–¥—Ä–∏—Ü–∞—ö–µ –æ–¥ –æ–¥–≥–æ–≤–æ—Ä–Ω–æ—Å—Ç–∏**:  
–û–≤–∞—ò –¥–æ–∫—É–º–µ–Ω—Ç —ò–µ –ø—Ä–µ–≤–µ–¥–µ–Ω –ø–æ–º–æ—õ—É —É—Å–ª—É–≥–µ –∑–∞ –ø—Ä–µ–≤–æ—í–µ—ö–µ —É–∑ –ø–æ–º–æ—õ –≤–µ—à—Ç–∞—á–∫–µ –∏–Ω—Ç–µ–ª–∏–≥–µ–Ω—Ü–∏—ò–µ [Co-op Translator](https://github.com/Azure/co-op-translator). –ò–∞–∫–æ —Å–µ —Ç—Ä—É–¥–∏–º–æ –¥–∞ –æ–±–µ–∑–±–µ–¥–∏–º–æ —Ç–∞—á–Ω–æ—Å—Ç, –º–æ–ª–∏–º–æ –≤–∞—Å –¥–∞ –∏–º–∞—Ç–µ —É –≤–∏–¥—É –¥–∞ –∞—É—Ç–æ–º–∞—Ç—Å–∫–∏ –ø—Ä–µ–≤–æ–¥–∏ –º–æ–≥—É —Å–∞–¥—Ä–∂–∞—Ç–∏ –≥—Ä–µ—à–∫–µ –∏–ª–∏ –Ω–µ—Ç–∞—á–Ω–æ—Å—Ç–∏. –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —ö–µ–≥–æ–≤–æ–º –∏–∑–≤–æ—Ä–Ω–æ–º —ò–µ–∑–∏–∫—É —Ç—Ä–µ–±–∞ —Å–º–∞—Ç—Ä–∞—Ç–∏ –∞—É—Ç–æ—Ä–∏—Ç–∞—Ç–∏–≤–Ω–∏–º –∏–∑–≤–æ—Ä–æ–º. –ó–∞ –∫—Ä–∏—Ç–∏—á–Ω–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—ò–µ –ø—Ä–µ–ø–æ—Ä—É—á—É—ò–µ —Å–µ –ø—Ä–æ—Ñ–µ—Å–∏–æ–Ω–∞–ª–Ω–∏ –ø—Ä–µ–≤–æ–¥ –æ–¥ —Å—Ç—Ä–∞–Ω–µ —ô—É–¥–∏. –ù–µ –ø—Ä–µ—É–∑–∏–º–∞–º–æ –æ–¥–≥–æ–≤–æ—Ä–Ω–æ—Å—Ç –∑–∞ –±–∏–ª–æ –∫–∞–∫–≤–∞ –ø–æ–≥—Ä–µ—à–Ω–∞ —Ç—É–º–∞—á–µ—ö–∞ –∏–ª–∏ –Ω–µ—Å–ø–æ—Ä–∞–∑—É–º–µ –∫–æ—ò–∏ –º–æ–≥—É –Ω–∞—Å—Ç–∞—Ç–∏ —É—Å–ª–µ–¥ –∫–æ—Ä–∏—à—õ–µ—ö–∞ –æ–≤–æ–≥ –ø—Ä–µ–≤–æ–¥–∞.