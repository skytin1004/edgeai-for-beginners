<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7c8a73e11384e3462674273498d0f9a6",
  "translation_date": "2025-10-08T12:04:45+00:00",
  "source_file": "Workshop/Session01-GettingStartedFoundryLocal.md",
  "language_code": "uk"
}
-->
# Сесія 1: Початок роботи з Foundry Local

## Анотація

Розпочніть свою подорож із Foundry Local, встановивши та налаштувавши його на Windows 11. Дізнайтеся, як налаштувати CLI, увімкнути апаратне прискорення та кешувати моделі для швидкого локального інференсу. У цій практичній сесії ви дізнаєтеся, як запускати моделі, такі як Phi, Qwen, DeepSeek і GPT-OSS-20B, використовуючи відтворювані команди CLI.

## Навчальні цілі

До кінця цієї сесії ви зможете:

- **Встановити та налаштувати**: Налаштувати Foundry Local на Windows 11 з оптимальними параметрами продуктивності
- **Опанувати роботу з CLI**: Використовувати Foundry Local CLI для управління моделями та їх розгортання
- **Увімкнути апаратне прискорення**: Налаштувати прискорення GPU за допомогою ONNXRuntime або WebGPU
- **Розгортати кілька моделей**: Локально запускати моделі phi-4, GPT-OSS-20B, Qwen і DeepSeek
- **Створити свій перший додаток**: Адаптувати існуючі приклади для використання Foundry Local Python SDK

# Тестування моделі (неінтерактивний одиночний запит)
foundry model run phi-4-mini --prompt "Привіт, представ себе"

- Windows 11 (22H2 або пізніша версія)
# Список доступних моделей каталогу (завантажені моделі з'являються після запуску)
foundry model list
## NOTE: Наразі немає спеціального прапорця `--running`; щоб побачити, які моделі завантажені, ініціюйте чат або перевірте журнали сервісу.
- Встановлений Python 3.10+
- Visual Studio Code з розширенням Python
- Адміністративні права для встановлення

### (Необов'язково) Змінні середовища

Створіть `.env` (або встановіть у shell), щоб зробити скрипти портативними:
# Порівняння відповідей (неінтерактивно)
foundry model run gpt-oss-20b --prompt "Поясни, що таке edge AI простими словами"
| Змінна | Призначення | Приклад |
|--------|-------------|---------|
| `FOUNDRY_LOCAL_ALIAS` | Бажаний псевдонім моделі (каталог автоматично вибирає найкращий варіант) | `phi-3.5-mini` |
| `FOUNDRY_LOCAL_ENDPOINT` | Перевизначення кінцевої точки (інакше автоматично з менеджера) | `http://localhost:5273/v1` |
| `FOUNDRY_LOCAL_STREAM` | Увімкнення демонстрації потокової передачі | `true` |

> Якщо `FOUNDRY_LOCAL_ENDPOINT=auto` (або не встановлено), ми отримуємо його з менеджера SDK.

## Демонстраційний процес (30 хвилин)

### 1. Встановлення Foundry Local та перевірка налаштування CLI (10 хвилин)

# Список кешованих моделей
foundry cache list

```powershell
# Install via winget (recommended)
winget install Microsoft.FoundryLocal

# Or download from Microsoft Learn
# https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install
```

**macOS (Попередній перегляд / Якщо підтримується)**

Якщо доступний нативний пакет для macOS (перевірте офіційну документацію для останньої версії):

```bash
# Homebrew (if/when available)
brew update
brew install foundry-local  # hypothetical formula name

# Or manual download (tarball)
curl -L -o foundry-local.tar.gz "https://download.microsoft.com/foundry-local/latest/macos/foundry-local.tar.gz"
tar -xzf foundry-local.tar.gz
sudo ./install.sh
```

Якщо нативні бінарні файли для macOS ще недоступні, ви все одно можете: 
1. Використовувати Windows 11 ARM/Intel VM (Parallels / UTM) і слідувати інструкціям для Windows. 
2. Запускати моделі через контейнер (якщо опубліковано образ контейнера) і встановити `FOUNDRY_LOCAL_ENDPOINT` на відкритий порт. 

**Створення віртуального середовища Python (кросплатформне)**

Windows PowerShell:
```powershell
py -m venv .venv
 .\.venv\Scripts\Activate.ps1
```

macOS / Linux:
```bash
python3 -m venv .venv
source .venv/bin/activate
```

Оновіть pip і встановіть основні залежності:
```bash
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

#### Крок 1.2: Перевірка встановлення

```powershell
# Check version
foundry --version

# Initialize configuration
foundry init

# View available commands
foundry --help
```

#### Крок 1.3: Налаштування середовища

```powershell
# Set up Python environment for Module08
cd Module08
py -m venv .venv
.\.venv\Scripts\activate

# Install Foundry Local Python SDK and dependencies
pip install foundry-local-sdk openai requests
```

### Ініціалізація SDK (рекомендовано)

Замість ручного запуску сервісу та моделей, **Foundry Local Python SDK** може автоматизувати весь процес:

```python
from foundry_local import FoundryLocalManager
from openai import OpenAI
import os

alias = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-3.5-mini")

# Bootstraps service + downloads + loads most suitable variant for hardware
manager = FoundryLocalManager(alias)

print("Service running:", manager.is_service_running())
print("Endpoint:", manager.endpoint)
print("Cached models:", manager.list_cached_models())

client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key or "not-needed")

resp = client.chat.completions.create(
    model=manager.get_model_info(alias).id,
    messages=[
        {"role": "system", "content": "You are a helpful local assistant."},
        {"role": "user", "content": "Hello"}
    ],
    max_tokens=120,
    temperature=0.5
)
print(resp.choices[0].message.content)
```

Якщо ви віддаєте перевагу явному контролю, ви все ще можете використовувати CLI + клієнт OpenAI, як показано далі.

### 2. Увімкнення апаратного прискорення (5 хвилин)

#### Крок 2.1: Перевірка апаратних можливостей

```powershell
# Check available compute providers
foundry system info

# List GPU capabilities
foundry system gpu-info
```

#### Крок 2.2: Налаштування апаратного прискорення

```powershell
# Enable ONNX Runtime GPU (if NVIDIA GPU available)
foundry config set compute.onnx.enable_gpu true

# Enable WebGPU for broader hardware support
foundry config set compute.webgpu.enabled true

# Verify configuration
foundry config list
```

### 3. Локальний запуск моделей через CLI (10 хвилин)

#### Крок 3.1: Розгортання моделі Phi-4

```powershell
# Download and run phi-4-mini
foundry model run phi-4-mini

# Test the model (one-shot prompt)
foundry model run phi-4-mini --prompt "Hello, introduce yourself"

# NOTE: There is no `--running` flag; use `foundry model list` and recent activity to infer loaded models.
```

#### Крок 3.2: Розгортання GPT-OSS-20B

```powershell
# Download and run GPT-OSS-20B
foundry model run gpt-oss-20b

# Compare responses (one-shot prompt)
foundry model run gpt-oss-20b --prompt "Explain edge AI in simple terms"
```

#### Крок 3.3: Завантаження додаткових моделей

```powershell
# Download Qwen model family
foundry model download qwen2.5-0.5b
foundry model download qwen2.5-7b

# Download DeepSeek models
foundry model download deepseek-coder-1.3b

# List cached models
foundry cache list
```

### 4. Стартовий проєкт: Адаптація 01-run-phi для Foundry Local (5 хвилин)

#### Крок 4.1: Створення базового чату-додатку

Створіть `samples/01-foundry-quickstart/chat_quickstart.py` (оновлено для використання менеджера, якщо доступний):

```python
#!/usr/bin/env python3
"""
Foundry Local Chat Quickstart
Demo: Basic chat interaction using Foundry Local Python SDK
Reference: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-sdk?pivots=programming-language-python
"""

import os, sys
from openai import OpenAI
try:
    from foundry_local import FoundryLocalManager  # control-plane SDK
except ImportError:
    FoundryLocalManager = None

def main():
    """Main chat function using Foundry Local SDK"""
    
    # Preferred: bootstrap via SDK manager (auto start + download + load)
    alias = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-3.5-mini")
    if FoundryLocalManager:
        manager = FoundryLocalManager(alias)
        endpoint = manager.endpoint
        model_id = manager.get_model_info(alias).id
        api_key = manager.api_key or "not-needed"
    else:
        # Fallback: assume default endpoint & alias already running via CLI
        endpoint = os.getenv("FOUNDRY_LOCAL_ENDPOINT", "http://localhost:5273/v1")
        model_id = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-4-mini")
        api_key = "not-needed"

    client = OpenAI(base_url=endpoint, api_key=api_key)
    
    # Get user input
    if len(sys.argv) > 1:
        user_message = " ".join(sys.argv[1:])
    else:
        user_message = input("Enter your message: ")
    
    try:
        # Make chat completion request
        response = client.chat.completions.create(
            model=model_id,
            messages=[
                {"role": "system", "content": "You are a helpful AI assistant powered by Microsoft Foundry Local."},
                {"role": "user", "content": user_message}
            ],
            max_tokens=500,
            temperature=0.7
        )
        
        # Display response
        print(f"\nModel: {response.model}")
        print(f"Response: {response.choices[0].message.content}")
        print(f"Tokens used: {response.usage.total_tokens if response.usage else 'N/A'}")
        
    except Exception as e:
        print(f"Error: {e}")
        print("\nTroubleshooting:")
    print("1. Ensure Foundry Local is running: foundry status")
    print("2. List models: foundry model list")
    print(f"3. Start model if needed: foundry model run {model_id}")
    print("4. Or let SDK bootstrap: pip install foundry-local-sdk")

if __name__ == "__main__":
    main()
```

#### Крок 4.2: Тестування додатку

```powershell
# Ensure phi-4-mini is running
foundry model run phi-4-mini

# Run the quickstart app
python samples/01-foundry-quickstart/chat_quickstart.py "What is Microsoft Foundry Local?"

# Try interactive mode
python samples/01-foundry-quickstart/chat_quickstart.py
```

## Основні концепції

### 1. Архітектура Foundry Local

- **Локальний інференс-двигун**: Запускає моделі повністю на вашому пристрої
- **Сумісність з OpenAI SDK**: Безшовна інтеграція з існуючим кодом OpenAI
- **Управління моделями**: Завантаження, кешування та ефективний запуск кількох моделей
- **Оптимізація апаратного забезпечення**: Використання прискорення GPU, NPU та CPU

### 2. Довідник команд CLI

```powershell
# Core Commands
foundry --version              # Check installation
# Model Management
foundry model list             # List available models
foundry model unload <name>    # Unload from memory

foundry config list            # Current configuration
```

### 3. Інтеграція Python SDK

```python
# Basic client setup
from foundry_local import FoundryLocalManager
from openai import OpenAI
import os

alias = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-3.5-mini")
manager = FoundryLocalManager(alias)
client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key or "not-needed")

response = client.chat.completions.create(
    model=manager.get_model_info(alias).id,
    messages=[{"role": "user", "content": "Hello!"}],
    max_tokens=50
)
print(response.choices[0].message.content)

# Streaming example
stream = client.chat.completions.create(
    model=manager.get_model_info(alias).id,
    messages=[{"role": "user", "content": "Give a 1-sentence definition of edge AI."}],
    stream=True,
    max_tokens=60,
    temperature=0.4
)
for chunk in stream:
    delta = chunk.choices[0].delta
    if delta and delta.content:
        print(delta.content, end="", flush=True)
print()
```

## Вирішення поширених проблем

### Проблема 1: "Команда Foundry не знайдена"

**Рішення:**
```powershell
# Restart PowerShell after installation
# Or manually add to PATH
$env:PATH += ";C:\Program Files\Microsoft\FoundryLocal"
```

### Проблема 2: "Не вдалося завантажити модель"

**Рішення:**
```powershell
# Check available memory
foundry system info

# Try smaller model first
foundry model run phi-4-mini

# Check disk space for model cache
dir "$env:USERPROFILE\.foundry\models"
```

### Проблема 3: "З'єднання відхилено на localhost:5273"

**Рішення:**
```powershell
# Check if service is running
foundry status

# Start service if needed
foundry service start

# Check for port conflicts
netstat -an | findstr 5273
```

## Поради щодо оптимізації продуктивності

### 1. Стратегія вибору моделі

- **Phi-4-mini**: Найкраще підходить для загальних завдань, низьке споживання пам'яті
- **Qwen2.5-0.5b**: Найшвидший інференс, мінімальні ресурси
- **GPT-OSS-20B**: Найвища якість, потребує більше ресурсів
- **DeepSeek-Coder**: Оптимізовано для програмування

### 2. Оптимізація апаратного забезпечення

```powershell
# Enable all acceleration options
foundry config set compute.onnx.enable_gpu true
foundry config set compute.webgpu.enabled true
foundry config set compute.cpu.threads auto

# Optimize memory usage
foundry config set model.cache.max_size 10GB
foundry config set model.preload false
```

### 3. Моніторинг продуктивності

```powershell
# Performance & latency measurement
# Use the Python benchmark script (Session 3) instead of legacy 'model stats' or 'model benchmark' commands.
# Example:
set BENCH_MODELS=phi-4-mini,qwen2.5-0.5b
python Workshop\samples\session03\benchmark_oss_models.py

# Re-run after enabling GPU acceleration to compare:
foundry config set compute.onnx.enable_gpu true
python Workshop\samples\session03\benchmark_oss_models.py
```

### Додаткові покращення

| Покращення | Що | Як |
|------------|-----|----|
| Спільні утиліти | Уникнення дублювання клієнтської/ініціалізаційної логіки | Використовуйте `Workshop/samples/workshop_utils.py` (`get_client`, `chat_once`) |
| Видимість використання токенів | Навчання економічному мисленню | Встановіть `SHOW_USAGE=1`, щоб виводити токени запиту/відповіді/загальні |
| Детерміновані порівняння | Стабільне тестування продуктивності та перевірка регресій | Використовуйте `temperature=0`, `top_p=1`, однаковий текст запиту |
| Затримка першого токена | Метрика сприйняття швидкості відповіді | Адаптуйте скрипт тестування з потоковою передачею (`BENCH_STREAM=1`) |
| Повтор при тимчасових помилках | Стійкі демонстрації при холодному старті | `RETRY_ON_FAIL=1` (за замовчуванням) та налаштуйте `RETRY_BACKOFF` |
| Тестування працездатності | Швидка перевірка основних функцій | Запустіть `python Workshop/tests/smoke.py` перед воркшопом |
| Профілі псевдонімів моделей | Швидке перемикання набору моделей між машинами | Підтримуйте `.env` з `FOUNDRY_LOCAL_ALIAS`, `SLM_ALIAS`, `LLM_ALIAS` |
| Ефективність кешування | Уникнення повторного прогріву при багатозразковому запуску | Утиліти кешування менеджерів; повторне використання між скриптами/ноутбуками |
| Прогрів першого запуску | Зменшення пікових затримок p95 | Виконайте невеликий запит після створення `FoundryLocalManager` |

Приклад детермінованого прогріву (PowerShell):

```powershell
set FOUNDRY_LOCAL_ALIAS=phi-4-mini
set SHOW_USAGE=1
python Workshop\samples\session01\chat_bootstrap.py "List two privacy benefits of local inference." | Out-Null
python Workshop\samples\session01\chat_bootstrap.py "List two privacy benefits of local inference."
```

Ви повинні побачити схожий результат і однакову кількість токенів при другому запуску, що підтверджує детермінізм.

## Наступні кроки

Після завершення цієї сесії:

1. **Дослідіть Сесію 2**: Створення AI-рішень з Azure AI Foundry RAG
2. **Спробуйте різні моделі**: Експериментуйте з Qwen, DeepSeek та іншими сімействами моделей
3. **Оптимізуйте продуктивність**: Налаштуйте параметри для вашого конкретного апаратного забезпечення
4. **Створюйте власні додатки**: Використовуйте Foundry Local SDK у своїх проєктах

## Додаткові ресурси

### Документація
- [Довідник Foundry Local Python SDK](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-sdk?pivots=programming-language-python)
- [Посібник з встановлення Foundry Local](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)
- [Каталог моделей](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/models)

### Зразки коду
- [Зразок Module08 01](./samples/01/README.md) - Швидкий старт REST Chat
- [Зразок Module08 02](./samples/02/README.md) - Інтеграція з OpenAI SDK
- [Зразок Module08 03](./samples/03/README.md) - Відкриття моделей та тестування продуктивності

### Спільнота
- [Обговорення на GitHub Foundry Local](https://github.com/microsoft/Foundry-Local/discussions)
- [Спільнота Azure AI](https://techcommunity.microsoft.com/category/artificialintelligence)

---

**Тривалість сесії**: 30 хвилин практики + 15 хвилин запитань та відповідей  
**Рівень складності**: Початковий  
**Попередні вимоги**: Windows 11, Python 3.10+, доступ адміністратора  

## Зразковий сценарій та відповідність воркшопу

| Сценарій воркшопу / Ноутбук | Сценарій | Мета | Приклад вхідних даних | Необхідний набір даних |
|-----------------------------|----------|------|-----------------------|------------------------|
| `samples/session01/chat_bootstrap.py` / `notebooks/session01_chat_bootstrap.ipynb` | Внутрішня IT-команда оцінює інференс на пристрої для порталу оцінки конфіденційності | Довести, що локальний SLM відповідає з затримкою менше секунди на стандартні запити | "Назвіть дві переваги локального інференсу." | Немає (одиночний запит) |
| Код адаптації швидкого старту | Розробник переносить існуючий скрипт OpenAI на Foundry Local | Показати сумісність "з коробки" | "Назвіть дві переваги локального інференсу." | Тільки вбудований запит |

### Опис сценарію
Команда з безпеки та відповідності повинна перевірити, чи можна обробляти конфіденційні прототипні дані локально. Вони запускають скрипт ініціалізації з кількома запитами (конфіденційність, затримка, вартість), використовуючи детермінований режим temperature=0, щоб зафіксувати базові результати для подальшого порівняння (тестування продуктивності в Сесії 3 та порівняння SLM і LLM у Сесії 4).

### Мінімальний набір запитів у форматі JSON (необов'язково)
```json
[
    "List two benefits of local inference.",
    "Summarize why keeping data on device improves privacy.",
    "Give one trade‑off when choosing an SLM over a large model."
]
```

Використовуйте цей список для створення відтворюваного циклу оцінки або для підготовки майбутнього тестового середовища для перевірки регресій.

---

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, звертаємо вашу увагу, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ мовою оригіналу слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.