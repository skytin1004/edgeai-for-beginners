<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-18T22:29:56+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "uk"
}
-->
# –†–æ–∑–¥—ñ–ª 2: –û—Å–Ω–æ–≤–∏ —Å—ñ–º–µ–π—Å—Ç–≤–∞ Qwen

–°—ñ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Qwen –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î –∫–æ–º–ø–ª–µ–∫—Å–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥ Alibaba Cloud –¥–æ –≤–µ–ª–∏–∫–∏—Ö –º–æ–≤–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π —Ç–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—ó AI, –¥–µ–º–æ–Ω—Å—Ç—Ä—É—é—á–∏, —â–æ –≤—ñ–¥–∫—Ä–∏—Ç—ñ –º–æ–¥–µ–ª—ñ –º–æ–∂—É—Ç—å –¥–æ—Å—è–≥–∞—Ç–∏ –≤–∏–¥–∞—Ç–Ω–æ—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ, –∑–∞–ª–∏—à–∞—é—á–∏—Å—å –¥–æ—Å—Ç—É–ø–Ω–∏–º–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è. –í–∞–∂–ª–∏–≤–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏, —è–∫ —Å—ñ–º–µ–π—Å—Ç–≤–æ Qwen –∑–∞–±–µ–∑–ø–µ—á—É—î –ø–æ—Ç—É–∂–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ AI –∑ –≥–Ω—É—á–∫–∏–º–∏ –≤–∞—Ä—ñ–∞–Ω—Ç–∞–º–∏ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø—Ä–æ–º–æ–∂–Ω—ñ—Å—Ç—å —É –≤–∏–∫–æ–Ω–∞–Ω–Ω—ñ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–∏—Ö –∑–∞–≤–¥–∞–Ω—å.

## –†–µ—Å—É—Ä—Å–∏ –¥–ª—è —Ä–æ–∑—Ä–æ–±–Ω–∏–∫—ñ–≤

### –†–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ–π –º–æ–¥–µ–ª–µ–π Hugging Face
–í–∏–±—Ä–∞–Ω—ñ –º–æ–¥–µ–ª—ñ —Å—ñ–º–µ–π—Å—Ç–≤–∞ Qwen –¥–æ—Å—Ç—É–ø–Ω—ñ —á–µ—Ä–µ–∑ [Hugging Face](https://huggingface.co/models?search=qwen), —â–æ –∑–∞–±–µ–∑–ø–µ—á—É—î –¥–æ—Å—Ç—É–ø –¥–æ –¥–µ—è–∫–∏—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ —Ü–∏—Ö –º–æ–¥–µ–ª–µ–π. –í–∏ –º–æ–∂–µ—Ç–µ –¥–æ—Å–ª—ñ–¥–∂—É–≤–∞—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏, –Ω–∞–ª–∞—à—Ç–æ–≤—É–≤–∞—Ç–∏ —ó—Ö –¥–ª—è —Å–≤–æ—ó—Ö –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –ø–æ—Ç—Ä–µ–± —ñ —Ä–æ–∑–≥–æ—Ä—Ç–∞—Ç–∏ —á–µ—Ä–µ–∑ —Ä—ñ–∑–Ω—ñ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏.

### –Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ—ó —Ä–æ–∑—Ä–æ–±–∫–∏
–î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ—ó —Ä–æ–∑—Ä–æ–±–∫–∏ —Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ [Microsoft Foundry Local](https://github.com/microsoft/foundry-local), —â–æ–± –∑–∞–ø—É—Å–∫–∞—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω—ñ –º–æ–¥–µ–ª—ñ Qwen –Ω–∞ –≤–∞—à—ñ–π –º–∞—à–∏–Ω—ñ —Ä–æ–∑—Ä–æ–±–∫–∏ –∑ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ—é –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—é.

### –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è
- [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π Qwen –¥–ª—è —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è –Ω–∞ –ø–µ—Ä–∏—Ñ–µ—Ä—ñ—ó](https://github.com/microsoft/olive)

## –í—Å—Ç—É–ø

–£ —Ü—å–æ–º—É –Ω–∞–≤—á–∞–ª—å–Ω–æ–º—É –ø–æ—Å—ñ–±–Ω–∏–∫—É –º–∏ –¥–æ—Å–ª—ñ–¥–∏–º–æ —Å—ñ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Qwen –≤—ñ–¥ Alibaba —Ç–∞ –π–æ–≥–æ –æ—Å–Ω–æ–≤–Ω—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó. –ú–∏ —Ä–æ–∑–≥–ª—è–Ω–µ–º–æ –µ–≤–æ–ª—é—Ü—ñ—é —Å—ñ–º–µ–π—Å—Ç–≤–∞ Qwen, —ñ–Ω–Ω–æ–≤–∞—Ü—ñ–π–Ω—ñ –º–µ—Ç–æ–¥–∏ –Ω–∞–≤—á–∞–Ω–Ω—è, —è–∫—ñ —Ä–æ–±–ª—è—Ç—å –º–æ–¥–µ–ª—ñ Qwen –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∏–º–∏, –∫–ª—é—á–æ–≤—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ –≤ —Å—ñ–º–µ–π—Å—Ç–≤—ñ —Ç–∞ –ø—Ä–∞–∫—Ç–∏—á–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –≤ —Ä—ñ–∑–Ω–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—è—Ö.

## –¶—ñ–ª—ñ –Ω–∞–≤—á–∞–Ω–Ω—è

–î–æ –∫—ñ–Ω—Ü—è —Ü—å–æ–≥–æ –Ω–∞–≤—á–∞–ª—å–Ω–æ–≥–æ –ø–æ—Å—ñ–±–Ω–∏–∫–∞ –≤–∏ –∑–º–æ–∂–µ—Ç–µ:

- –ó—Ä–æ–∑—É–º—ñ—Ç–∏ —Ñ—ñ–ª–æ—Å–æ—Ñ—ñ—é –¥–∏–∑–∞–π–Ω—É —Ç–∞ –µ–≤–æ–ª—é—Ü—ñ—é —Å—ñ–º–µ–π—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π Qwen –≤—ñ–¥ Alibaba
- –í–∏–∑–Ω–∞—á–∏—Ç–∏ –∫–ª—é—á–æ–≤—ñ —ñ–Ω–Ω–æ–≤–∞—Ü—ñ—ó, —è–∫—ñ –¥–æ–∑–≤–æ–ª—è—é—Ç—å –º–æ–¥–µ–ª—è–º Qwen –¥–æ—Å—è–≥–∞—Ç–∏ –≤–∏—Å–æ–∫–æ—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –ø—Ä–∏ —Ä—ñ–∑–Ω–∏—Ö —Ä–æ–∑–º—ñ—Ä–∞—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- –†–æ–∑–ø—ñ–∑–Ω–∞—Ç–∏ –ø–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –æ–±–º–µ–∂–µ–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ –º–æ–¥–µ–ª–µ–π Qwen
- –ó–∞—Å—Ç–æ—Å—É–≤–∞—Ç–∏ –∑–Ω–∞–Ω–Ω—è –ø—Ä–æ –º–æ–¥–µ–ª—ñ Qwen –¥–ª—è –≤–∏–±–æ—Ä—É –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ –¥–ª—è —Ä–µ–∞–ª—å–Ω–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤

## –†–æ–∑—É–º—ñ–Ω–Ω—è —Å—É—á–∞—Å–Ω–æ–≥–æ –ª–∞–Ω–¥—à–∞—Ñ—Ç—É AI –º–æ–¥–µ–ª–µ–π

–õ–∞–Ω–¥—à–∞—Ñ—Ç AI –∑–Ω–∞—á–Ω–æ –µ–≤–æ–ª—é—Ü—ñ–æ–Ω—É–≤–∞–≤, —ñ —Ä—ñ–∑–Ω—ñ –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—ó –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å —Ä—ñ–∑–Ω—ñ –ø—ñ–¥—Ö–æ–¥–∏ –¥–æ —Ä–æ–∑—Ä–æ–±–∫–∏ –º–æ–≤–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π. –î–µ—è–∫—ñ –∑–æ—Å–µ—Ä–µ–¥–∂—É—é—Ç—å—Å—è –Ω–∞ –∑–∞–∫—Ä–∏—Ç–∏—Ö –ø—Ä–æ–ø—Ä—ñ—î—Ç–∞—Ä–Ω–∏—Ö –º–æ–¥–µ–ª—è—Ö, —ñ–Ω—à—ñ –Ω–∞–≥–æ–ª–æ—à—É—é—Ç—å –Ω–∞ –≤—ñ–¥–∫—Ä–∏—Ç–æ—Å—Ç—ñ —Ç–∞ –ø—Ä–æ–∑–æ—Ä–æ—Å—Ç—ñ. –¢—Ä–∞–¥–∏—Ü—ñ–π–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥ –ø–µ—Ä–µ–¥–±–∞—á–∞—î –∞–±–æ –º–∞—Å–∏–≤–Ω—ñ –ø—Ä–æ–ø—Ä—ñ—î—Ç–∞—Ä–Ω—ñ –º–æ–¥–µ–ª—ñ, –¥–æ—Å—Ç—É–ø–Ω—ñ –ª–∏—à–µ —á–µ—Ä–µ–∑ API, –∞–±–æ –≤—ñ–¥–∫—Ä–∏—Ç—ñ –º–æ–¥–µ–ª—ñ, —è–∫—ñ –º–æ–∂—É—Ç—å –ø–æ—Å—Ç—É–ø–∞—Ç–∏—Å—è –≤ –º–æ–∂–ª–∏–≤–æ—Å—Ç—è—Ö.

–¶—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ —Å—Ç–≤–æ—Ä—é—î –≤–∏–∫–ª–∏–∫–∏ –¥–ª—è –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ–π, —è–∫—ñ –ø—Ä–∞–≥–Ω—É—Ç—å –ø–æ—Ç—É–∂–Ω–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π AI, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ —Å–≤–æ—ó–º–∏ –¥–∞–Ω–∏–º–∏, –≤–∏—Ç—Ä–∞—Ç–∞–º–∏ —Ç–∞ –≥–Ω—É—á–∫—ñ—Å—Ç—é —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è. –¢—Ä–∞–¥–∏—Ü—ñ–π–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥ —á–∞—Å—Ç–æ –≤–∏–º–∞–≥–∞—î –≤–∏–±–æ—Ä—É –º—ñ–∂ –ø–µ—Ä–µ–¥–æ–≤–æ—é –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—é —Ç–∞ –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–º–∏ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è–º–∏ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è.

## –í–∏–∫–ª–∏–∫ –¥–æ—Å—Ç—É–ø–Ω–æ—ó –¥–æ—Å–∫–æ–Ω–∞–ª–æ—Å—Ç—ñ AI

–ü–æ—Ç—Ä–µ–±–∞ —É –≤–∏—Å–æ–∫–æ—è–∫—ñ—Å–Ω–æ–º—É, –¥–æ—Å—Ç—É–ø–Ω–æ–º—É AI —Å—Ç–∞—î –¥–µ–¥–∞–ª—ñ –≤–∞–∂–ª–∏–≤—ñ—à–æ—é –≤ —Ä—ñ–∑–Ω–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—è—Ö. –†–æ–∑–≥–ª—è–Ω–µ–º–æ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è, —è–∫—ñ –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –≥–Ω—É—á–∫–∏—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ–π–Ω–∏—Ö –ø–æ—Ç—Ä–µ–±, –µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∏—Ö —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ–π, –¥–µ –≤–∏—Ç—Ä–∞—Ç–∏ –Ω–∞ API –º–æ–∂—É—Ç—å –±—É—Ç–∏ –∑–Ω–∞—á–Ω–∏–º–∏, –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω—å –∞–±–æ —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ—ó –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∏ –≤ —Ç–∞–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö, —è–∫ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è —Ç–∞ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞.

### –û—Å–Ω–æ–≤–Ω—ñ –≤–∏–º–æ–≥–∏ –¥–æ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è

–°—É—á–∞—Å–Ω—ñ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è AI —Å—Ç–∏–∫–∞—é—Ç—å—Å—è –∑ –∫—ñ–ª—å–∫–æ–º–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏–º–∏ –≤–∏–º–æ–≥–∞–º–∏, —è–∫—ñ –æ–±–º–µ–∂—É—é—Ç—å –ø—Ä–∞–∫—Ç–∏—á–Ω—É –∑–∞—Å—Ç–æ—Å–æ–≤–Ω—ñ—Å—Ç—å:

- **–î–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å**: –í—ñ–¥–∫—Ä–∏—Ç–∏–π –¥–æ—Å—Ç—É–ø –¥–ª—è –ø—Ä–æ–∑–æ—Ä–æ—Å—Ç—ñ —Ç–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
- **–ï–∫–æ–Ω–æ–º—ñ—á–Ω–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å**: –†–æ–∑—É–º–Ω—ñ –≤–∏–º–æ–≥–∏ –¥–æ –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å—ñ–≤ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –±—é–¥–∂–µ—Ç—ñ–≤
- **–ì–Ω—É—á–∫—ñ—Å—Ç—å**: –†—ñ–∑–Ω—ñ —Ä–æ–∑–º—ñ—Ä–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è
- **–ì–ª–æ–±–∞–ª—å–Ω–µ –æ—Ö–æ–ø–ª–µ–Ω–Ω—è**: –°–∏–ª—å–Ω—ñ –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω—ñ —Ç–∞ –º—ñ–∂–∫—É–ª—å—Ç—É—Ä–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ
- **–°–ø–µ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è**: –í–∞—Ä—ñ–∞–Ω—Ç–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –¥–æ–º–µ–Ω—ñ–≤ —ñ –∑–∞–≤–¥–∞–Ω—å

## –§—ñ–ª–æ—Å–æ—Ñ—ñ—è –º–æ–¥–µ–ª–µ–π Qwen

–°—ñ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Qwen –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î –∫–æ–º–ø–ª–µ–∫—Å–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥ –¥–æ —Ä–æ–∑—Ä–æ–±–∫–∏ AI –º–æ–¥–µ–ª–µ–π, —è–∫–∏–π —Å—Ç–∞–≤–∏—Ç—å —É –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç –≤—ñ–¥–∫—Ä–∏—Ç—ñ—Å—Ç—å, –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω—ñ—Å—Ç—å —ñ –ø—Ä–∞–∫—Ç–∏—á–Ω–µ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø—Ä–æ–º–æ–∂–Ω—ñ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ. –ú–æ–¥–µ–ª—ñ Qwen –¥–æ—Å—è–≥–∞—é—Ç—å —Ü—å–æ–≥–æ –∑–∞–≤–¥—è–∫–∏ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–∏–º —Ä–æ–∑–º—ñ—Ä–∞–º –º–æ–¥–µ–ª–µ–π, –≤–∏—Å–æ–∫–æ—è–∫—ñ—Å–Ω–∏–º –º–µ—Ç–æ–¥–∞–º –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–º –≤–∞—Ä—ñ–∞–Ω—Ç–∞–º –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –¥–æ–º–µ–Ω—ñ–≤.

–°—ñ–º–µ–π—Å—Ç–≤–æ Qwen –æ—Ö–æ–ø–ª—é—î —Ä—ñ–∑–Ω—ñ –ø—ñ–¥—Ö–æ–¥–∏, —è–∫—ñ –∑–∞–±–µ–∑–ø–µ—á—É—é—Ç—å –≤–∞—Ä—ñ–∞–Ω—Ç–∏ –Ω–∞ —Å–ø–µ–∫—Ç—Ä—ñ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ —Ç–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ, –¥–æ–∑–≤–æ–ª—è—é—á–∏ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è –≤—ñ–¥ –º–æ–±—ñ–ª—å–Ω–∏—Ö –ø—Ä–∏—Å—Ç—Ä–æ—ó–≤ –¥–æ —Å–µ—Ä–≤–µ—Ä—ñ–≤ –ø—ñ–¥–ø—Ä–∏—î–º—Å—Ç–≤, –∑–∞–±–µ–∑–ø–µ—á—É—é—á–∏ –∑–Ω–∞—á—É—â—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ AI. –ú–µ—Ç–∞ ‚Äî –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑—É–≤–∞—Ç–∏ –¥–æ—Å—Ç—É–ø –¥–æ –≤–∏—Å–æ–∫–æ—è–∫—ñ—Å–Ω–æ–≥–æ AI, –∑–∞–±–µ–∑–ø–µ—á—É—é—á–∏ –≥–Ω—É—á–∫—ñ—Å—Ç—å —É –≤–∏–±–æ—Ä—ñ –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è.

### –û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏ –¥–∏–∑–∞–π–Ω—É Qwen

–ú–æ–¥–µ–ª—ñ Qwen –ø–æ–±—É–¥–æ–≤–∞–Ω—ñ –Ω–∞ –∫—ñ–ª—å–∫–æ—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö, —è–∫—ñ –≤—ñ–¥—Ä—ñ–∑–Ω—è—é—Ç—å —ó—Ö –≤—ñ–¥ —ñ–Ω—à–∏—Ö —Å—ñ–º–µ–π—Å—Ç–≤ –º–æ–≤–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π:

- **–ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç –≤—ñ–¥–∫—Ä–∏—Ç–æ—Å—Ç—ñ**: –ü–æ–≤–Ω–∞ –ø—Ä–æ–∑–æ—Ä—ñ—Å—Ç—å —ñ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å –¥–ª—è –¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å —Ç–∞ –∫–æ–º–µ—Ä—Ü—ñ–π–Ω–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
- **–ö–æ–º–ø–ª–µ–∫—Å–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è**: –ù–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –º–∞—Å–∏–≤–Ω–∏—Ö, —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–∏—Ö, —â–æ –æ—Ö–æ–ø–ª—é—é—Ç—å –∫—ñ–ª—å–∫–∞ –º–æ–≤ —ñ –¥–æ–º–µ–Ω—ñ–≤
- **–ú–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞**: –†—ñ–∑–Ω—ñ —Ä–æ–∑–º—ñ—Ä–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ —Ä—ñ–∑–Ω–∏–º –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∏–º –≤–∏–º–æ–≥–∞–º
- **–°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ –¥–æ—Å–∫–æ–Ω–∞–ª—ñ—Å—Ç—å**: –í–∞—Ä—ñ–∞–Ω—Ç–∏, –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –∑–∞–≤–¥–∞–Ω—å

## –ö–ª—é—á–æ–≤—ñ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó, —â–æ –∑–∞–±–µ–∑–ø–µ—á—É—é—Ç—å —Å—ñ–º–µ–π—Å—Ç–≤–æ Qwen

### –ú–∞—Å—à—Ç–∞–±–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è

–û–¥–Ω–∏–º —ñ–∑ –≤–∏–∑–Ω–∞—á–∞–ª—å–Ω–∏—Ö –∞—Å–ø–µ–∫—Ç—ñ–≤ —Å—ñ–º–µ–π—Å—Ç–≤–∞ Qwen —î –º–∞—Å—à—Ç–∞–± –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö —Ç–∞ –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å—ñ–≤, –≤–∫–ª–∞–¥–µ–Ω–∏—Ö —É —Ä–æ–∑—Ä–æ–±–∫—É –º–æ–¥–µ–ª–µ–π. –ú–æ–¥–µ–ª—ñ Qwen –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å —Ä–µ—Ç–µ–ª—å–Ω–æ –ø—ñ–¥—ñ–±—Ä–∞–Ω—ñ –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω—ñ –Ω–∞–±–æ—Ä–∏ –¥–∞–Ω–∏—Ö, —â–æ –æ—Ö–æ–ø–ª—é—é—Ç—å —Ç—Ä–∏–ª—å–π–æ–Ω–∏ —Ç–æ–∫–µ–Ω—ñ–≤, —Å—Ç–≤–æ—Ä–µ–Ω—ñ –¥–ª—è –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—è –≤—Å–µ–±—ñ—á–Ω–∏—Ö –∑–Ω–∞–Ω—å –ø—Ä–æ —Å–≤—ñ—Ç —Ç–∞ –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –ª–æ–≥—ñ—á–Ω–æ–≥–æ –º–∏—Å–ª–µ–Ω–Ω—è.

–¶–µ–π –ø—ñ–¥—Ö—ñ–¥ –ø–æ—î–¥–Ω—É—î –≤–∏—Å–æ–∫–æ—è–∫—ñ—Å–Ω–∏–π –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç, –∞–∫–∞–¥–µ–º—ñ—á–Ω—É –ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä—É, —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—ó –∫–æ–¥—É —Ç–∞ –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω—ñ —Ä–µ—Å—É—Ä—Å–∏. –ú–µ—Ç–æ–¥–æ–ª–æ–≥—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞–≥–æ–ª–æ—à—É—î —è–∫ –Ω–∞ —à–∏—Ä–æ—Ç—ñ –∑–Ω–∞–Ω—å, —Ç–∞–∫ —ñ –Ω–∞ –≥–ª–∏–±–∏–Ω—ñ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –≤ —Ä—ñ–∑–Ω–∏—Ö –¥–æ–º–µ–Ω–∞—Ö —ñ –º–æ–≤–∞—Ö.

### –†–æ–∑—à–∏—Ä–µ–Ω–µ –ª–æ–≥—ñ—á–Ω–µ –º–∏—Å–ª–µ–Ω–Ω—è

–û—Å—Ç–∞–Ω–Ω—ñ –º–æ–¥–µ–ª—ñ Qwen –≤–∫–ª—é—á–∞—é—Ç—å —Å–∫–ª–∞–¥–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –ª–æ–≥—ñ—á–Ω–æ–≥–æ –º–∏—Å–ª–µ–Ω–Ω—è, —è–∫—ñ –¥–æ–∑–≤–æ–ª—è—é—Ç—å –≤–∏—Ä—ñ—à—É–≤–∞—Ç–∏ –±–∞–≥–∞—Ç–æ–∫—Ä–æ–∫–æ–≤—ñ –ø—Ä–æ–±–ª–µ–º–∏:

**–†–µ–∂–∏–º –º–∏—Å–ª–µ–Ω–Ω—è (Qwen3)**: –ú–æ–¥–µ–ª—ñ –º–æ–∂—É—Ç—å –≤–∏–∫–æ–Ω—É–≤–∞—Ç–∏ –¥–µ—Ç–∞–ª—å–Ω–µ –ø–æ–∫—Ä–æ–∫–æ–≤–µ –ª–æ–≥—ñ—á–Ω–µ –º–∏—Å–ª–µ–Ω–Ω—è –ø–µ—Ä–µ–¥ –Ω–∞–¥–∞–Ω–Ω—è–º –æ—Å—Ç–∞—Ç–æ—á–Ω–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π, –ø–æ–¥—ñ–±–Ω–æ –¥–æ –ª—é–¥—Å—å–∫–æ–≥–æ –ø—ñ–¥—Ö–æ–¥—É –¥–æ –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –ø—Ä–æ–±–ª–µ–º.

**–î–≤–æ—Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π —Ä–µ–∂–∏–º**: –ó–¥–∞—Ç–Ω—ñ—Å—Ç—å –ø–µ—Ä–µ–º–∏–∫–∞—Ç–∏—Å—è –º—ñ–∂ —à–≤–∏–¥–∫–∏–º —Ä–µ–∂–∏–º–æ–º –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –¥–ª—è –ø—Ä–æ—Å—Ç–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤ —ñ –≥–ª–∏–±–æ–∫–∏–º —Ä–µ–∂–∏–º–æ–º –º–∏—Å–ª–µ–Ω–Ω—è –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –ø—Ä–æ–±–ª–µ–º.

**–Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –ª–∞–Ω—Ü—é–≥–∞ –¥—É–º–æ–∫**: –ü—Ä–∏—Ä–æ–¥–Ω–µ –≤–∫–ª—é—á–µ–Ω–Ω—è –∫—Ä–æ–∫—ñ–≤ –ª–æ–≥—ñ—á–Ω–æ–≥–æ –º–∏—Å–ª–µ–Ω–Ω—è, —â–æ –ø–æ–∫—Ä–∞—â—É—î –ø—Ä–æ–∑–æ—Ä—ñ—Å—Ç—å —ñ —Ç–æ—á–Ω—ñ—Å—Ç—å —É —Å–∫–ª–∞–¥–Ω–∏—Ö –∑–∞–≤–¥–∞–Ω–Ω—è—Ö.

### –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ —ñ–Ω–Ω–æ–≤–∞—Ü—ñ—ó

–°—ñ–º–µ–π—Å—Ç–≤–æ Qwen –≤–∫–ª—é—á–∞—î –∫—ñ–ª—å–∫–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω–∏—Ö –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ–π, —Ä–æ–∑—Ä–æ–±–ª–µ–Ω–∏—Ö –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ —Ç–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ:

**–ú–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–∏–π –¥–∏–∑–∞–π–Ω**: –£–∑–≥–æ–¥–∂–µ–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ä–æ–∑–º—ñ—Ä—ñ–≤ –º–æ–¥–µ–ª–µ–π, —â–æ –¥–æ–∑–≤–æ–ª—è—î –ª–µ–≥–∫–æ –º–∞—Å—à—Ç–∞–±—É–≤–∞—Ç–∏ —Ç–∞ –ø–æ—Ä—ñ–≤–Ω—é–≤–∞—Ç–∏.

**–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è**: –ë–µ–∑—à–æ–≤–Ω–∞ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É, –∑–æ–±—Ä–∞–∂–µ–Ω—å —ñ –∞—É–¥—ñ–æ –≤ —î–¥–∏–Ω—ñ–π –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—ñ.

**–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è**: –†—ñ–∑–Ω—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ –∫–≤–∞–Ω—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ —Ñ–æ—Ä–º–∞—Ç–∏ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –∞–ø–∞—Ä–∞—Ç–Ω–∏—Ö –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π.

## –†–æ–∑–º—ñ—Ä–∏ –º–æ–¥–µ–ª–µ–π —Ç–∞ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è

–°—É—á–∞—Å–Ω—ñ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è –æ—Ç—Ä–∏–º—É—é—Ç—å –≤–∏–≥–æ–¥—É –≤—ñ–¥ –≥–Ω—É—á–∫–æ—Å—Ç—ñ –º–æ–¥–µ–ª–µ–π Qwen –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∏—Ö –≤–∏–º–æ–≥:

### –ú–∞–ª—ñ –º–æ–¥–µ–ª—ñ (0.5B-3B)

Qwen –ø—Ä–æ–ø–æ–Ω—É—î –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ –º–∞–ª—ñ –º–æ–¥–µ–ª—ñ, —è–∫—ñ –ø—ñ–¥—Ö–æ–¥—è—Ç—å –¥–ª—è —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è –Ω–∞ –ø–µ—Ä–∏—Ñ–µ—Ä—ñ—ó, –º–æ–±—ñ–ª—å–Ω–∏—Ö –¥–æ–¥–∞—Ç–∫—ñ–≤ —Ç–∞ —Å–µ—Ä–µ–¥–æ–≤–∏—â —ñ–∑ –æ–±–º–µ–∂–µ–Ω–∏–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ –≤—Ä–∞–∂–∞—é—á—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ.

### –°–µ—Ä–µ–¥–Ω—ñ –º–æ–¥–µ–ª—ñ (7B-32B)

–ú–æ–¥–µ–ª—ñ —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ –¥—ñ–∞–ø–∞–∑–æ–Ω—É –ø—Ä–æ–ø–æ–Ω—É—é—Ç—å —Ä–æ–∑—à–∏—Ä–µ–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–∏—Ö –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω—å, –∑–∞–±–µ–∑–ø–µ—á—É—é—á–∏ –≤—ñ–¥–º—ñ–Ω–Ω–∏–π –±–∞–ª–∞–Ω—Å –º—ñ–∂ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—é —Ç–∞ –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∏–º–∏ –≤–∏–º–æ–≥–∞–º–∏.

### –í–µ–ª–∏–∫—ñ –º–æ–¥–µ–ª—ñ (72B+)

–ú–æ–¥–µ–ª—ñ –ø–æ–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±—É –∑–∞–±–µ–∑–ø–µ—á—É—é—Ç—å –ø–µ—Ä–µ–¥–æ–≤—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –¥–ª—è –≤–∏–º–æ–≥–ª–∏–≤–∏—Ö –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω—å, –¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å —Ç–∞ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω—å –Ω–∞ —Ä—ñ–≤–Ω—ñ –ø—ñ–¥–ø—Ä–∏—î–º—Å—Ç–≤, —è–∫—ñ –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó –ø–æ—Ç—É–∂–Ω–æ—Å—Ç—ñ.

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Å—ñ–º–µ–π—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π Qwen

### –í—ñ–¥–∫—Ä–∏—Ç–∏–π –¥–æ—Å—Ç—É–ø

–ú–æ–¥–µ–ª—ñ Qwen –∑–∞–±–µ–∑–ø–µ—á—É—é—Ç—å –ø–æ–≤–Ω—É –ø—Ä–æ–∑–æ—Ä—ñ—Å—Ç—å —ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è, –¥–æ–∑–≤–æ–ª—è—é—á–∏ –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—è–º —Ä–æ–∑—É–º—ñ—Ç–∏, –∑–º—ñ–Ω—é–≤–∞—Ç–∏ —Ç–∞ –∞–¥–∞–ø—Ç—É–≤–∞—Ç–∏ –º–æ–¥–µ–ª—ñ –¥–æ —Å–≤–æ—ó—Ö –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –ø–æ—Ç—Ä–µ–± –±–µ–∑ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤—ñ–¥ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞.

### –ì–Ω—É—á–∫—ñ—Å—Ç—å —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è

–†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å —Ä–æ–∑–º—ñ—Ä—ñ–≤ –º–æ–¥–µ–ª–µ–π –¥–æ–∑–≤–æ–ª—è—î —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –∞–ø–∞—Ä–∞—Ç–Ω–∏—Ö –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è—Ö, –≤—ñ–¥ –º–æ–±—ñ–ª—å–Ω–∏—Ö –ø—Ä–∏—Å—Ç—Ä–æ—ó–≤ –¥–æ –≤–∏—Å–æ–∫–æ–ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–∏—Ö —Å–µ—Ä–≤–µ—Ä—ñ–≤, –∑–∞–±–µ–∑–ø–µ—á—É—é—á–∏ –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—è–º –≥–Ω—É—á–∫—ñ—Å—Ç—å —É –≤–∏–±–æ—Ä—ñ AI —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∏.

### –ë–∞–≥–∞—Ç–æ–º–æ–≤–Ω–∞ –¥–æ—Å–∫–æ–Ω–∞–ª—ñ—Å—Ç—å

–ú–æ–¥–µ–ª—ñ Qwen –≤—ñ–¥–∑–Ω–∞—á–∞—é—Ç—å—Å—è –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω–∏–º —Ä–æ–∑—É–º—ñ–Ω–Ω—è–º —ñ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—î—é, –ø—ñ–¥—Ç—Ä–∏–º—É—é—á–∏ –¥–µ—Å—è—Ç–∫–∏ –º–æ–≤ —ñ–∑ –æ—Å–æ–±–ª–∏–≤–æ—é —Å–∏–ª–æ—é –≤ –∞–Ω–≥–ª—ñ–π—Å—å–∫—ñ–π —Ç–∞ –∫–∏—Ç–∞–π—Å—å–∫—ñ–π, —â–æ —Ä–æ–±–∏—Ç—å —ó—Ö –ø—Ä–∏–¥–∞—Ç–Ω–∏–º–∏ –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω—å.

### –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å

–ú–æ–¥–µ–ª—ñ Qwen —Å—Ç–∞–±—ñ–ª—å–Ω–æ –¥–æ—Å—è–≥–∞—é—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É —Ç–µ—Å—Ç–∞—Ö, –∑–∞–±–µ–∑–ø–µ—á—É—é—á–∏ –≤—ñ–¥–∫—Ä–∏—Ç–∏–π –¥–æ—Å—Ç—É–ø, –¥–µ–º–æ–Ω—Å—Ç—Ä—É—é—á–∏, —â–æ –≤—ñ–¥–∫—Ä–∏—Ç—ñ –º–æ–¥–µ–ª—ñ –º–æ–∂—É—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏ –ø—Ä–æ–ø—Ä—ñ—î—Ç–∞—Ä–Ω–∏–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞–º.

### –°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ

–í–∞—Ä—ñ–∞–Ω—Ç–∏, —Ç–∞–∫—ñ —è–∫ Qwen-Coder —ñ Qwen-Math, –∑–∞–±–µ–∑–ø–µ—á—É—é—Ç—å —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—É –µ–∫—Å–ø–µ—Ä—Ç–∏–∑—É, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ –∑–∞–≥–∞–ª—å–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –º–æ–≤–∏.

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ —Ç–∞ —Å—Ü–µ–Ω–∞—Ä—ñ—ó –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

–ü–µ—Ä–µ–¥ —Ç–∏–º —è–∫ –∑–∞–≥–ª–∏–±–∏—Ç–∏—Å—è –≤ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ –¥–µ—Ç–∞–ª—ñ, —Ä–æ–∑–≥–ª—è–Ω–µ–º–æ –∫—ñ–ª—å–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —Ç–æ–≥–æ, —â–æ –º–æ–∂—É—Ç—å –¥–æ—Å—è–≥—Ç–∏ –º–æ–¥–µ–ª—ñ Qwen:

### –ü—Ä–∏–∫–ª–∞–¥ –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–æ–≥–æ –º–∏—Å–ª–µ–Ω–Ω—è

Qwen-Math –≤—ñ–¥–∑–Ω–∞—á–∞—î—Ç—å—Å—è –ø–æ–∫—Ä–æ–∫–æ–≤–∏–º –≤–∏—Ä—ñ—à–µ–Ω–Ω—è–º –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∏—Ö –∑–∞–¥–∞—á. –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –ø—Ä–∏ –≤–∏—Ä—ñ—à–µ–Ω–Ω—ñ —Å–∫–ª–∞–¥–Ω–æ—ó –∑–∞–¥–∞—á—ñ –∑ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è:

```
User: Find the derivative of f(x) = x¬≥ + 2x¬≤ - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x¬≥ + 2x¬≤ - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(x‚Åø) = n¬∑x‚Åø‚Åª¬π

Breaking down each term:
- d/dx(x¬≥) = 3x¬≤
- d/dx(2x¬≤) = 2 ¬∑ 2x¬π = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x¬≤ + 4x - 5
```

### –ü—Ä–∏–∫–ª–∞–¥ –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω–æ—ó –ø—ñ–¥—Ç—Ä–∏–º–∫–∏

–ú–æ–¥–µ–ª—ñ Qwen –¥–µ–º–æ–Ω—Å—Ç—Ä—É—é—Ç—å —Å–∏–ª—å–Ω—ñ –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –≤ —Ä—ñ–∑–Ω–∏—Ö –º–æ–≤–∞—Ö:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**‰∏≠Êñá:**
‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâÊòØÊåáÊú∫Âô®ÔºàÁâπÂà´ÊòØËÆ°ÁÆóÊú∫Á≥ªÁªüÔºâÂØπ‰∫∫Á±ªÊô∫ËÉΩËøáÁ®ãÁöÑÊ®°Êãü„ÄÇËøô‰∫õËøáÁ®ãÂåÖÊã¨Â≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥ÈóÆÈ¢ò„ÄÅÊÑüÁü•ÂíåËØ≠Ë®ÄÁêÜËß£„ÄÇ‰∫∫Â∑•Êô∫ËÉΩ‰ΩøÊú∫Âô®ËÉΩÂ§üÊâßË°åÈÄöÂ∏∏ÈúÄË¶Å‰∫∫Á±ªËÆ§Áü•ËÉΩÂäõÁöÑ‰ªªÂä°„ÄÇ
```

### –ü—Ä–∏–∫–ª–∞–¥ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π

Qwen-VL –º–æ–∂–µ –æ–¥–Ω–æ—á–∞—Å–Ω–æ –æ–±—Ä–æ–±–ª—è—Ç–∏ —Ç–µ–∫—Å—Ç —ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### –ü—Ä–∏–∫–ª–∞–¥ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–¥—É

Qwen-Coder –≤—ñ–¥–∑–Ω–∞—á–∞—î—Ç—å—Å—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—î—é —Ç–∞ –ø–æ—è—Å–Ω–µ–Ω–Ω—è–º –∫–æ–¥—É –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –º–æ–≤ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    –í–∏–∫–æ–Ω–∞—Ç–∏ –±—ñ–Ω–∞—Ä–Ω–∏–π –ø–æ—à—É–∫ —É –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–æ–º—É –º–∞—Å–∏–≤—ñ –¥–ª—è –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è —Ü—ñ–ª—å–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è.
    
    –ê—Ä–≥—É–º–µ–Ω—Ç–∏:
        arr (list): –í—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–∏–π —Å–ø–∏—Å–æ–∫ –ø–æ—Ä—ñ–≤–Ω—é–≤–∞–Ω–∏—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤
        target: –ó–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è –ø–æ—à—É–∫—É
        
    –ü–æ–≤–µ—Ä—Ç–∞—î:
        int: –Ü–Ω–¥–µ–∫—Å —Ü—ñ–ª—å–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è, —è–∫—â–æ –∑–Ω–∞–π–¥–µ–Ω–æ, -1, —è–∫—â–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ
        
    –ß–∞—Å–æ–≤–∞ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å: O(log n)
    –ü—Ä–æ—Å—Ç–æ—Ä–æ–≤–∞ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å: O(1)
    """
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª—ñ–≤–æ–≥–æ —Ç–∞ –ø—Ä–∞–≤–æ–≥–æ –ø–æ–∫–∞–∂—á–∏–∫—ñ–≤
    left, right = 0, len(arr) - 1
    
    # –ü—Ä–æ–¥–æ–≤–∂—É–≤–∞—Ç–∏ –ø–æ—à—É–∫, –ø–æ–∫–∏ –ø—Ä–æ—Å—Ç—ñ—Ä –ø–æ—à—É–∫—É —î –¥—ñ–π—Å–Ω–∏–º
    while left <= right:
        # –û–±—á–∏—Å–ª–∏—Ç–∏ —Å–µ—Ä–µ–¥–Ω—ñ–π —ñ–Ω–¥–µ–∫—Å, —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –ø–µ—Ä–µ–ø–æ–≤–Ω–µ–Ω–Ω—è —Ü—ñ–ª–æ–≥–æ —á–∏—Å–ª–∞
        mid = left + (right - left) // 2
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏, —á–∏ –∑–Ω–∞–π–¥–µ–Ω–æ —Ü—ñ–ª—å–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è
        if arr[mid] == target:
            return mid
        
        # –Ø–∫—â–æ —Ü—ñ–ª—å–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è –º–µ–Ω—à–µ, —à—É–∫–∞—Ç–∏ –≤ –ª—ñ–≤—ñ–π –ø–æ–ª–æ–≤–∏–Ω—ñ
        elif arr[mid] > target:
            right = mid - 1
        
        # –Ø–∫—â–æ —Ü—ñ–ª—å–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è –±—ñ–ª—å—à–µ, —à—É–∫–∞—Ç–∏ –≤ –ø—Ä–∞–≤—ñ–π –ø–æ–ª–æ–≤–∏–Ω—ñ
        else:
            left = mid + 1
    
    # –¶—ñ–ª—å–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ
    return -1

# –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"–Ü–Ω–¥–µ–∫—Å 7: {result}")  # –í–∏—Ö—ñ–¥: –Ü–Ω–¥–µ–∫—Å 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### –ü—Ä–∏–∫–ª–∞–¥ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è –Ω–∞ –ø–µ—Ä–∏—Ñ–µ—Ä—ñ—ó

–ú–æ–¥–µ–ª—ñ Qwen –º–æ–∂—É—Ç—å –±—É—Ç–∏ —Ä–æ–∑–≥–æ—Ä–Ω—É—Ç—ñ –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –ø–µ—Ä–∏—Ñ–µ—Ä—ñ–π–Ω–∏—Ö –ø—Ä–∏—Å—Ç—Ä–æ—è—Ö –∑ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–º–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è–º–∏:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## –ï–≤–æ–ª—é—Ü—ñ—è —Å—ñ–º–µ–π—Å—Ç–≤–∞ Qwen

### Qwen 1.0 —Ç–∞ 1.5: –ë–∞–∑–æ–≤—ñ –º–æ–¥–µ–ª—ñ

–†–∞–Ω–Ω—ñ –º–æ–¥–µ–ª—ñ Qwen –∑–∞–∫–ª–∞–ª–∏ –æ—Å–Ω–æ–≤–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –≤—ñ–¥–∫—Ä–∏—Ç–æ–≥–æ –¥–æ—Å—Ç—É–ø—É:

- **Qwen-7B (7B –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤)**: –ü–æ—á–∞—Ç–∫–æ–≤–∏–π –≤–∏–ø—É—Å–∫ —ñ–∑ —Ñ–æ–∫—É—Å–æ–º –Ω–∞ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –∫–∏—Ç–∞–π—Å—å–∫–æ—ó —Ç–∞ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –º–æ–≤
- **Qwen-14B (14B –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤)**: –ü–æ–∫—Ä–∞—â–µ–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–∏–º –ª–æ–≥—ñ—á–Ω–∏–º –º–∏—Å–ª–µ–Ω–Ω—è–º —Ç–∞ –∑–Ω–∞–Ω–Ω—è–º–∏
- **Qwen-72B (72B –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤)**: –ú–∞—Å—à—Ç–∞–±–Ω–∞ –º–æ–¥–µ–ª—å, —â–æ –∑–∞–±–µ–∑–ø–µ—á—É—î –ø–µ—Ä–µ–¥–æ–≤—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
- **–°–µ—Ä—ñ—è Qwen1.5**: –†–æ–∑—à–∏—Ä–µ–Ω–∞ –¥–æ –∫—ñ–ª—å–∫–æ—Ö —Ä–æ–∑–º—ñ—Ä—ñ–≤ (0.5B –¥–æ 110B) –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–æ—é –æ–±—Ä–æ–±–∫–æ—é –¥–æ–≤–≥–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ–≤

### –°—ñ–º–µ–π—Å—Ç–≤–æ Qwen2: –†–æ–∑—à–∏—Ä–µ–Ω–Ω—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—ñ

–°–µ—Ä—ñ—è Qwen2 –≤—ñ–¥–∑–Ω–∞—á–∏–ª–∞ –∑–Ω–∞—á–Ω–∏–π –ø—Ä–æ–≥—Ä–µ—Å —è–∫ —É –º–æ–≤–Ω–∏—Ö, —Ç–∞–∫ —ñ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç—è—Ö:

- **Qwen2-0.5B –¥–æ 72B**: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∏–π –¥—ñ–∞–ø–∞–∑–æ–Ω –º–æ–≤–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –ø–æ—Ç—Ä–µ–± —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è
- **Qwen2-57B-A14B (MoE)**: –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ "—Å—É–º—ñ—à –µ–∫—Å–ø–µ—Ä—Ç—ñ–≤" –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- **Qwen2-VL**: –†–æ–∑—à–∏—Ä–µ–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å
- **Qwen2-Audio**: –û–±—Ä–æ–±–∫–∞ —Ç–∞ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –∞—É–¥—ñ–æ
- **Qwen2-Math**: –°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–µ –º–∏—Å–ª–µ–Ω–Ω—è —Ç–∞ –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –∑–∞–¥–∞—á

### –°—ñ–º–µ–π—Å—Ç–≤–æ Qwen2.5: –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å

–°–µ—Ä—ñ—è Qwen2.5 –ø—Ä–∏–Ω–µ—Å–ª–∞ –∑–Ω–∞—á–Ω—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –≤ —É—Å—ñ—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö:

- **–†–æ–∑—à–∏—Ä–µ–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è**: 18 —Ç—Ä–∏–ª—å–π–æ–Ω—ñ–≤ —Ç–æ–∫–µ–Ω—ñ–≤ –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π
- **–†–æ–∑—à–∏—Ä–µ–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç**: –î–æ 128K —Ç–æ–∫–µ–Ω—ñ–≤ –¥–æ–≤–∂–∏–Ω–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É, –∑ –≤–∞—Ä—ñ–∞–Ω—Ç–æ–º Turbo, —â–æ –ø—ñ–¥—Ç—Ä–∏–º—É—î 1M —Ç–æ–∫–µ–Ω—ñ–≤
- **–ü–æ–∫—Ä–∞—â–µ–Ω–∞ —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è**: –ü–æ–∫—Ä–∞—â–µ–Ω—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ Qwen2.5-Coder —Ç–∞ Qwen2.5-Math
- **–ö—Ä–∞—â–∞ –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞**: –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å —É 27+ –º–æ–≤–∞—Ö

### –°—ñ–º–µ–π—Å—Ç–≤–æ Qwen3: –†–æ–∑—à–∏—Ä–µ–Ω–µ –ª–æ–≥—ñ—á–Ω–µ –º–∏—Å–ª–µ–Ω–Ω—è

–û—Å—Ç–∞–Ω–Ω—î –ø–æ–∫–æ–ª—ñ–Ω–Ω—è —Ä–æ–∑—à–∏—Ä—é—î –º–µ–∂—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –ª–æ–≥—ñ—á–Ω–æ–≥–æ –º–∏—Å–ª–µ–Ω–Ω—è:

- **Qwen3-235B-A22B**: –§–ª–∞–≥–º–∞–Ω—Å—å–∫–∞ –º–æ–¥–µ–ª—å "—Å—É–º—ñ—à –µ–∫—Å–ø–µ—Ä—Ç—ñ–≤" —ñ–∑ –∑–∞–≥–∞–ª—å–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ 235B
- **Qwen3-30B-A3B**: –ï—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –º–æ–¥–µ–ª—å MoE —ñ–∑ —Å–∏–ª—å–Ω–æ—é –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—é –Ω–∞ –∞–∫—Ç–∏–≤–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä
- **–©—ñ–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è
- **–†–µ–∂–∏–º –º–∏—Å–ª–µ–Ω–Ω—è**: –ì—ñ–±—Ä–∏–¥–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥ –¥–æ –ª–æ–≥—ñ—á–Ω–æ–≥–æ –º–∏—Å–ª–µ–Ω–Ω—è, —â–æ –ø—ñ–¥—Ç—Ä–∏–º—É—î —è–∫ —à–≤–∏–¥–∫—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ, —Ç–∞–∫ —ñ –≥–ª–∏–±–æ–∫–µ –º–∏—Å–ª–µ–Ω–Ω—è
- **–ë–∞–≥–∞—Ç–æ–º–æ–≤–Ω–∞ –¥–æ—Å–∫–æ–Ω–∞–ª—ñ—Å—Ç—å**: –ü—ñ–¥—Ç—Ä–∏–º
–û—Å—å —è–∫ –ø–æ—á–∞—Ç–∏ —Ä–æ–±–æ—Ç—É –∑ –º–æ–¥–µ–ª—è–º–∏ Qwen –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ Hugging Face Transformers:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π Qwen2.5

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### –°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π

**–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–¥—É –∑ Qwen-Coder:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**–†–æ–∑–≤'—è–∑–∞–Ω–Ω—è –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∏—Ö –∑–∞–¥–∞—á:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = x¬≥ + 2x¬≤ - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**–ó–∞–≤–¥–∞–Ω–Ω—è –∑ —Ä–æ–∑—É–º—ñ–Ω–Ω—è–º –∑–æ–±—Ä–∞–∂–µ–Ω—å —ñ —Ç–µ–∫—Å—Ç—É:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### –†–µ–∂–∏–º –º–∏—Å–ª–µ–Ω–Ω—è (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### üì± –†–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è –Ω–∞ –º–æ–±—ñ–ª—å–Ω–∏—Ö –ø—Ä–∏—Å—Ç—Ä–æ—è—Ö —Ç–∞ –ø–µ—Ä–∏—Ñ–µ—Ä—ñ–π–Ω–∏—Ö –ø—Ä–∏—Å—Ç—Ä–æ—è—Ö

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### –ü—Ä–∏–∫–ª–∞–¥ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è —á–µ—Ä–µ–∑ API

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## –û—Ü—ñ–Ω–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ —Ç–∞ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è

–°—ñ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Qwen –¥–æ—Å—è–≥–ª–æ –≤–∏–¥–∞—Ç–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É —Ä—ñ–∑–Ω–∏—Ö —Ç–µ—Å—Ç–∞—Ö, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å —É –≤—ñ–¥–∫—Ä–∏—Ç–æ–º—É –¥–æ—Å—Ç—É–ø—ñ:

### –û—Å–Ω–æ–≤–Ω—ñ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ

**–í–∏–¥–∞—Ç–Ω—ñ –∑–¥—ñ–±–Ω–æ—Å—Ç—ñ –¥–æ –ª–æ–≥—ñ—á–Ω–æ–≥–æ –º–∏—Å–ª–µ–Ω–Ω—è:**
- Qwen3-235B-A22B –¥–µ–º–æ–Ω—Å—Ç—Ä—É—î –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤ —Ç–µ—Å—Ç–∞—Ö –Ω–∞ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è, –º–∞—Ç–µ–º–∞—Ç–∏–∫—É —Ç–∞ –∑–∞–≥–∞–ª—å–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –ø–æ—Ä—ñ–≤–Ω—è–Ω–æ –∑ —ñ–Ω—à–∏–º–∏ –ø—Ä–æ–≤—ñ–¥–Ω–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏, —Ç–∞–∫–∏–º–∏ —è–∫ DeepSeek-R1, o1, o3-mini, Grok-3 —ñ Gemini-2.5-Pro
- Qwen3-30B-A3B –ø–µ—Ä–µ–≤–µ—Ä—à—É—î QwQ-32B —ñ–∑ 10-–∫—Ä–∞—Ç–Ω–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –∞–∫—Ç–∏–≤–æ–≤–∞–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- Qwen3-4B –º–æ–∂–µ –∫–æ–Ω–∫—É—Ä—É–≤–∞—Ç–∏ –∑ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—é Qwen2.5-72B-Instruct

**–î–æ—Å—è–≥–Ω–µ–Ω–Ω—è –≤ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ:**
- –ë–∞–∑–æ–≤—ñ –º–æ–¥–µ–ª—ñ Qwen3-MoE –¥–æ—Å—è–≥–∞—é—Ç—å –∞–Ω–∞–ª–æ–≥—ñ—á–Ω–æ—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ, —è–∫ —ñ —â—ñ–ª—å–Ω—ñ –±–∞–∑–æ–≤—ñ –º–æ–¥–µ–ª—ñ Qwen2.5, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –ª–∏—à–µ 10% –∞–∫—Ç–∏–≤–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- –ó–Ω–∞—á–Ω–∞ –µ–∫–æ–Ω–æ–º—ñ—è –≤–∏—Ç—Ä–∞—Ç —è–∫ –Ω–∞ –Ω–∞–≤—á–∞–Ω–Ω—è, —Ç–∞–∫ —ñ –Ω–∞ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—é –ø–æ—Ä—ñ–≤–Ω—è–Ω–æ –∑ —â—ñ–ª—å–Ω–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏

**–ú–æ–≤–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ:**
- –ú–æ–¥–µ–ª—ñ Qwen3 –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å 119 –º–æ–≤ —ñ –¥—ñ–∞–ª–µ–∫—Ç—ñ–≤
- –í–∏—Å–æ–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å —É —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–∏—Ö –º–æ–≤–Ω–∏—Ö —ñ –∫—É–ª—å—Ç—É—Ä–Ω–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö

**–ú–∞—Å—à—Ç–∞–± –Ω–∞–≤—á–∞–Ω–Ω—è:**
- Qwen3 –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –º–∞–π–∂–µ –≤–¥–≤—ñ—á—ñ –±—ñ–ª—å—à–µ –¥–∞–Ω–∏—Ö, –ø—Ä–∏–±–ª–∏–∑–Ω–æ 36 —Ç—Ä–∏–ª—å–π–æ–Ω—ñ–≤ —Ç–æ–∫–µ–Ω—ñ–≤, –æ—Ö–æ–ø–ª—é—é—á–∏ 119 –º–æ–≤ —ñ –¥—ñ–∞–ª–µ–∫—Ç—ñ–≤, –ø–æ—Ä—ñ–≤–Ω—è–Ω–æ –∑ 18 —Ç—Ä–∏–ª—å–π–æ–Ω–∞–º–∏ —Ç–æ–∫–µ–Ω—ñ–≤ —É Qwen2.5

### –ú–∞—Ç—Ä–∏—Ü—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π

| –°–µ—Ä—ñ—è –º–æ–¥–µ–ª–µ–π | –î—ñ–∞–ø–∞–∑–æ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ | –î–æ–≤–∂–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É | –û—Å–Ω–æ–≤–Ω—ñ —Å–∏–ª—å–Ω—ñ —Å—Ç–æ—Ä–æ–Ω–∏ | –ù–∞–π–∫—Ä–∞—â—ñ –≤–∏–ø–∞–¥–∫–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è |
|---------------|---------------------|-------------------|------------------------|-------------------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | –ó–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å, –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω—ñ—Å—Ç—å | –ó–∞–≥–∞–ª—å–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è, —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è –≤ –≤–∏—Ä–æ–±–Ω–∏—Ü—Ç–≤—ñ |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–¥—É, –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è | –†–æ–∑—Ä–æ–±–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–Ω–æ–≥–æ –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—è, –¥–æ–ø–æ–º–æ–≥–∞ –≤ –∫–æ–¥—É–≤–∞–Ω–Ω—ñ |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | –ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–µ –º–∏—Å–ª–µ–Ω–Ω—è | –û—Å–≤—ñ—Ç–Ω—ñ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏, –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –≤ STEM |
| **Qwen2.5-VL** | –†—ñ–∑–Ω—ñ | –ó–º—ñ–Ω–Ω–∞ | –†–æ–∑—É–º—ñ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å —ñ —Ç–µ–∫—Å—Ç—É | –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è, –∞–Ω–∞–ª—ñ–∑ –∑–æ–±—Ä–∞–∂–µ–Ω—å |
| **Qwen3** | 0.6B-235B | –ó–º—ñ–Ω–Ω–∞ | –†–æ–∑—à–∏—Ä–µ–Ω–µ –º–∏—Å–ª–µ–Ω–Ω—è, —Ä–µ–∂–∏–º –º–∏—Å–ª–µ–Ω–Ω—è | –°–∫–ª–∞–¥–Ω–µ –º–∏—Å–ª–µ–Ω–Ω—è, –¥–æ—Å–ª—ñ–¥–Ω–∏—Ü—å–∫—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è |
| **Qwen3 MoE** | 30B-235B –∑–∞–≥–∞–ª–æ–º | –ó–º—ñ–Ω–Ω–∞ | –ï—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å —É –≤–µ–ª–∏–∫–æ–º—É –º–∞—Å—à—Ç–∞–±—ñ | –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è, –ø–æ—Ç—Ä–µ–±–∏ —É –≤–∏—Å–æ–∫—ñ–π –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ |

## –ü–æ—Å—ñ–±–Ω–∏–∫ –∑ –≤–∏–±–æ—Ä—É –º–æ–¥–µ–ª—ñ

### –î–ª—è –±–∞–∑–æ–≤–∏—Ö –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω—å
- **Qwen2.5-0.5B/1.5B**: –ú–æ–±—ñ–ª—å–Ω—ñ –¥–æ–¥–∞—Ç–∫–∏, –ø–µ—Ä–∏—Ñ–µ—Ä—ñ–π–Ω—ñ –ø—Ä–∏—Å—Ç—Ä–æ—ó, –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ
- **Qwen2.5-3B/7B**: –ó–∞–≥–∞–ª—å–Ω—ñ —á–∞—Ç-–±–æ—Ç–∏, –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–Ω—Ç–µ–Ω—Ç—É, —Å–∏—Å—Ç–µ–º–∏ –∑–∞–ø–∏—Ç–∞–Ω—å —ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π

### –î–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∏—Ö —ñ –ª–æ–≥—ñ—á–Ω–∏—Ö –∑–∞–¥–∞—á
- **Qwen2.5-Math**: –†–æ–∑–≤'—è–∑–∞–Ω–Ω—è –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∏—Ö –∑–∞–¥–∞—á —ñ –æ—Å–≤—ñ—Ç–∞ –≤ STEM
- **Qwen3 –∑ —Ä–µ–∂–∏–º–æ–º –º–∏—Å–ª–µ–Ω–Ω—è**: –°–∫–ª–∞–¥–Ω–µ –º–∏—Å–ª–µ–Ω–Ω—è, —â–æ –ø–æ—Ç—Ä–µ–±—É—î –ø–æ–∫—Ä–æ–∫–æ–≤–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É

### –î–ª—è –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è —Ç–∞ —Ä–æ–∑—Ä–æ–±–∫–∏
- **Qwen2.5-Coder**: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–¥—É, –Ω–∞–ª–∞–≥–æ–¥–∂–µ–Ω–Ω—è, –¥–æ–ø–æ–º–æ–≥–∞ –≤ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—ñ
- **Qwen3**: –†–æ–∑—à–∏—Ä–µ–Ω—ñ –∑–∞–≤–¥–∞–Ω–Ω—è –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è –∑ –º–æ–∂–ª–∏–≤–æ—Å—Ç—è–º–∏ –ª–æ–≥—ñ—á–Ω–æ–≥–æ –º–∏—Å–ª–µ–Ω–Ω—è

### –î–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∏—Ö –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω—å
- **Qwen2.5-VL**: –†–æ–∑—É–º—ñ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å, –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è —â–æ–¥–æ –∑–æ–±—Ä–∞–∂–µ–Ω—å
- **Qwen-Audio**: –û–±—Ä–æ–±–∫–∞ –∞—É–¥—ñ–æ —Ç–∞ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –º–æ–≤–∏

### –î–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è
- **Qwen2.5-32B/72B**: –í–∏—Å–æ–∫–æ–ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–µ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –º–æ–≤–∏
- **Qwen3-235B-A22B**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –¥–ª—è –≤–∏–º–æ–≥–ª–∏–≤–∏—Ö –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω—å

## –ü–ª–∞—Ç—Ñ–æ—Ä–º–∏ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è —Ç–∞ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å
### –•–º–∞—Ä–Ω—ñ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏
- **Hugging Face Hub**: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∏–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ–π –º–æ–¥–µ–ª–µ–π —ñ–∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é —Å–ø—ñ–ª—å–Ω–æ—Ç–∏
- **ModelScope**: –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –º–æ–¥–µ–ª–µ–π Alibaba –∑ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
- **–†—ñ–∑–Ω—ñ —Ö–º–∞—Ä–Ω—ñ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏**: –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è

### –õ–æ–∫–∞–ª—å–Ω—ñ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ —Ä–æ–∑—Ä–æ–±–∫–∏
- **Transformers**: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è Hugging Face –¥–ª—è –ª–µ–≥–∫–æ–≥–æ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è
- **vLLM**: –í–∏—Å–æ–∫–æ–ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–µ –æ–±—Å–ª—É–≥–æ–≤—É–≤–∞–Ω–Ω—è –¥–ª—è –≤–∏—Ä–æ–±–Ω–∏—á–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â
- **Ollama**: –°–ø—Ä–æ—â–µ–Ω–µ –ª–æ–∫–∞–ª—å–Ω–µ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è —Ç–∞ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è
- **ONNX Runtime**: –ö—Ä–æ—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–ª—è —Ä—ñ–∑–Ω–æ–≥–æ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è
- **llama.cpp**: –ï—Ñ–µ–∫—Ç–∏–≤–Ω–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è –Ω–∞ C++ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º

### –ù–∞–≤—á–∞–ª—å–Ω—ñ —Ä–µ—Å—É—Ä—Å–∏
- **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è Qwen**: –û—Ñ—ñ—Ü—ñ–π–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è —Ç–∞ –∫–∞—Ä—Ç–∫–∏ –º–æ–¥–µ–ª–µ–π
- **Hugging Face Model Hub**: –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ñ –¥–µ–º–æ —Ç–∞ –ø—Ä–∏–∫–ª–∞–¥–∏ –≤—ñ–¥ —Å–ø—ñ–ª—å–Ω–æ—Ç–∏
- **–ù–∞—É–∫–æ–≤—ñ —Å—Ç–∞—Ç—Ç—ñ**: –¢–µ—Ö–Ω—ñ—á–Ω—ñ —Å—Ç–∞—Ç—Ç—ñ –Ω–∞ arxiv –¥–ª—è –≥–ª–∏–±–æ–∫–æ–≥–æ —Ä–æ–∑—É–º—ñ–Ω–Ω—è
- **–§–æ—Ä—É–º–∏ —Å–ø—ñ–ª—å–Ω–æ—Ç–∏**: –ê–∫—Ç–∏–≤–Ω–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ —Å–ø—ñ–ª—å–Ω–æ—Ç–∏ —Ç–∞ –æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è

### –Ø–∫ –ø–æ—á–∞—Ç–∏ —Ä–æ–±–æ—Ç—É –∑ –º–æ–¥–µ–ª—è–º–∏ Qwen

#### –ü–ª–∞—Ç—Ñ–æ—Ä–º–∏ —Ä–æ–∑—Ä–æ–±–∫–∏
1. **Hugging Face Transformers**: –ü–æ—á–Ω—ñ—Ç—å –∑—ñ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ—ó —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó Python
2. **ModelScope**: –î–æ—Å–ª—ñ–¥–∂—É–π—Ç–µ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è –≤—ñ–¥ Alibaba
3. **–õ–æ–∫–∞–ª—å–Ω–µ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è**: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ Ollama –∞–±–æ –ø—Ä—è–º—ñ Transformers –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è

#### –®–ª—è—Ö –Ω–∞–≤—á–∞–Ω–Ω—è
1. **–ó—Ä–æ–∑—É–º—ñ–π—Ç–µ –æ—Å–Ω–æ–≤–Ω—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó**: –í–∏–≤—á—ñ—Ç—å –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É —Ç–∞ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ —Å—ñ–º–µ–π—Å—Ç–≤–∞ Qwen
2. **–ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–π—Ç–µ –∑ –≤–∞—Ä—ñ–∞–Ω—Ç–∞–º–∏**: –°–ø—Ä–æ–±—É–π—Ç–µ —Ä—ñ–∑–Ω—ñ —Ä–æ–∑–º—ñ—Ä–∏ –º–æ–¥–µ–ª–µ–π, —â–æ–± –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –∫–æ–º–ø—Ä–æ–º—ñ—Å–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
3. **–ü—Ä–∞–∫—Ç–∏–∫—É–π—Ç–µ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è**: –†–æ–∑–≥–æ—Ä–Ω—ñ—Ç—å –º–æ–¥–µ–ª—ñ –≤ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞—Ö —Ä–æ–∑—Ä–æ–±–∫–∏
4. **–û–ø—Ç–∏–º—ñ–∑—É–π—Ç–µ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è**: –ù–∞–ª–∞—à—Ç—É–π—Ç–µ –¥–ª—è –≤–∏—Ä–æ–±–Ω–∏—á–∏—Ö –≤–∏–ø–∞–¥–∫—ñ–≤ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

#### –ù–∞–π–∫—Ä–∞—â—ñ –ø—Ä–∞–∫—Ç–∏–∫–∏
- **–ü–æ—á–∏–Ω–∞–π—Ç–µ –∑ –º–∞–ª–æ–≥–æ**: –ü–æ—á–Ω—ñ—Ç—å —ñ–∑ –º–µ–Ω—à–∏—Ö –º–æ–¥–µ–ª–µ–π (1.5B-7B) –¥–ª—è –ø–æ—á–∞—Ç–∫–æ–≤–æ—ó —Ä–æ–∑—Ä–æ–±–∫–∏
- **–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ —à–∞–±–ª–æ–Ω–∏ —á–∞—Ç—ñ–≤**: –ó–∞—Å—Ç–æ—Å–æ–≤—É–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–µ —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
- **–ö–æ–Ω—Ç—Ä–æ–ª—é–π—Ç–µ —Ä–µ—Å—É—Ä—Å–∏**: –í—ñ–¥—Å—Ç–µ–∂—É–π—Ç–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–∞–º'—è—Ç—ñ —Ç–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å —ñ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—ó
- **–†–æ–∑–≥–ª—è–¥–∞–π—Ç–µ —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—é**: –û–±–∏—Ä–∞–π—Ç–µ –≤–∞—Ä—ñ–∞–Ω—Ç–∏, —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –¥–ª—è –¥–æ–º–µ–Ω—É, –∫–æ–ª–∏ —Ü–µ –¥–æ—Ä–µ—á–Ω–æ

## –†–æ–∑—à–∏—Ä–µ–Ω—ñ —à–∞–±–ª–æ–Ω–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

### –ü—Ä–∏–∫–ª–∞–¥–∏ —Ç–æ–Ω–∫–æ–≥–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### –°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ —ñ–Ω–∂–µ–Ω–µ—Ä—ñ—è –ø—ñ–¥–∫–∞–∑–æ–∫

**–î–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –∑–∞–≤–¥–∞–Ω—å –ª–æ–≥—ñ—á–Ω–æ–≥–æ –º–∏—Å–ª–µ–Ω–Ω—è:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**–î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–¥—É –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### –ë–∞–≥–∞—Ç–æ–º–æ–≤–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (‰∏≠Êñá)",
        "es": "Spanish (Espa√±ol)",
        "fr": "French (Fran√ßais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (Êó•Êú¨Ë™û)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### üîß –®–∞–±–ª–æ–Ω–∏ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è —É –≤–∏—Ä–æ–±–Ω–∏—Ü—Ç–≤—ñ

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## –°—Ç—Ä–∞—Ç–µ–≥—ñ—ó –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ

### –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ø–∞–º'—è—Ç—ñ

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è —ñ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—ó

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## –ù–∞–π–∫—Ä–∞—â—ñ –ø—Ä–∞–∫—Ç–∏–∫–∏ —Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó

### –ë–µ–∑–ø–µ–∫–∞ —Ç–∞ –∫–æ–Ω—Ñ—ñ–¥–µ–Ω—Ü—ñ–π–Ω—ñ—Å—Ç—å

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ —Ç–∞ –æ—Ü—ñ–Ω–∫–∞

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## –í–∏—Å–Ω–æ–≤–æ–∫

–°—ñ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Qwen –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î –∫–æ–º–ø–ª–µ–∫—Å–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥ –¥–æ –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü—ñ—ó —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å —É —Ä—ñ–∑–Ω–∏—Ö –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è—Ö. –ó–∞–≤–¥—è–∫–∏ –≤—ñ–¥–∫—Ä–∏—Ç–æ–º—É –¥–æ—Å—Ç—É–ø—É, –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω–∏–º –º–æ–∂–ª–∏–≤–æ—Å—Ç—è–º —ñ –≥–Ω—É—á–∫–∏–º –≤–∞—Ä—ñ–∞–Ω—Ç–∞–º —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è Qwen –¥–æ–∑–≤–æ–ª—è—î –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—è–º —ñ —Ä–æ–∑—Ä–æ–±–Ω–∏–∫–∞–º –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ø–æ—Ç—É–∂–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —ó—Ö–Ω—ñ—Ö —Ä–µ—Å—É—Ä—Å—ñ–≤ —á–∏ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏—Ö –≤–∏–º–æ–≥.

### –û—Å–Ω–æ–≤–Ω—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

**–í—ñ–¥–∫—Ä–∏—Ç–∏–π –¥–æ—Å—Ç—É–ø**: Qwen –¥–µ–º–æ–Ω—Å—Ç—Ä—É—î, —â–æ –º–æ–¥–µ–ª—ñ –∑ –≤—ñ–¥–∫—Ä–∏—Ç–∏–º –∫–æ–¥–æ–º –º–æ–∂—É—Ç—å –¥–æ—Å—è–≥–∞—Ç–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—ó –∑ –ø—Ä–æ–ø—Ä—ñ—î—Ç–∞—Ä–Ω–∏–º–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞–º–∏, –∑–∞–±–µ–∑–ø–µ—á—É—é—á–∏ –ø—Ä–æ–∑–æ—Ä—ñ—Å—Ç—å, –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å.

**–ú–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞**: –î—ñ–∞–ø–∞–∑–æ–Ω –≤—ñ–¥ 0.5B –¥–æ 235B –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –¥–æ–∑–≤–æ–ª—è—î —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è –Ω–∞ –≤—Å—å–æ–º—É —Å–ø–µ–∫—Ç—Ä—ñ –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â ‚Äî –≤—ñ–¥ –º–æ–±—ñ–ª—å–Ω–∏—Ö –ø—Ä–∏—Å—Ç—Ä–æ—ó–≤ –¥–æ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤.

**–°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ**: –í–∞—Ä—ñ–∞–Ω—Ç–∏, —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –¥–ª—è –¥–æ–º–µ–Ω—É, —Ç–∞–∫—ñ —è–∫ Qwen-Coder, Qwen-Math —ñ Qwen-VL, –∑–∞–±–µ–∑–ø–µ—á—É—é—Ç—å —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—É –µ–∫—Å–ø–µ—Ä—Ç–∏–∑—É, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ –∑–∞–≥–∞–ª—å–Ω–µ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –º–æ–≤–∏.

**–ì–ª–æ–±–∞–ª—å–Ω–∞ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å**: –°–∏–ª—å–Ω–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ –±–∞–≥–∞—Ç—å–æ—Ö –º–æ–≤ (119+) —Ä–æ–±–∏—Ç—å Qwen –ø—Ä–∏–¥–∞—Ç–Ω–∏–º –¥–ª—è –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏—Ö –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω—å —ñ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤.

**–ü–æ—Å—Ç—ñ–π–Ω—ñ —ñ–Ω–Ω–æ–≤–∞—Ü—ñ—ó**: –ï–≤–æ–ª—é—Ü—ñ—è –≤—ñ–¥ Qwen 1.0 –¥–æ Qwen3 –¥–µ–º–æ–Ω—Å—Ç—Ä—É—î –ø–æ—Å—Ç—ñ–π–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π, –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ —Ç–∞ –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è.

### –ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∏ –º–∞–π–±—É—Ç–Ω—å–æ–≥–æ

–£ –º—ñ—Ä—É —Ä–æ–∑–≤–∏—Ç–∫—É —Å—ñ–º–µ–π—Å—Ç–≤–∞ Qwen –º–æ–∂–Ω–∞ –æ—á—ñ–∫—É–≤–∞—Ç–∏:

- **–ü–æ–∫—Ä–∞—â–µ–Ω–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å**: –ü–æ–¥–∞–ª—å—à–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–ª—è –∫—Ä–∞—â–∏—Ö —Å–ø—ñ–≤–≤—ñ–¥–Ω–æ—à–µ–Ω—å –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –¥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- **–†–æ–∑—à–∏—Ä–µ–Ω—ñ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ**: –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –±—ñ–ª—å—à —Å–∫–ª–∞–¥–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏ –∑–æ–±—Ä–∞–∂–µ–Ω—å, –∞—É–¥—ñ–æ —Ç–∞ —Ç–µ–∫—Å—Ç—É
- **–ü–æ–∫—Ä–∞—â–µ–Ω–µ –º–∏—Å–ª–µ–Ω–Ω—è**: –†–æ–∑—à–∏—Ä–µ–Ω—ñ –º–µ—Ö–∞–Ω—ñ–∑–º–∏ –º–∏—Å–ª–µ–Ω–Ω—è —Ç–∞ –±–∞–≥–∞—Ç–æ–∫—Ä–æ–∫–æ–≤–µ —Ä–æ–∑–≤'—è–∑–∞–Ω–Ω—è –∑–∞–¥–∞—á
- **–ö—Ä–∞—â—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è**: –ü–æ–∫—Ä–∞—â–µ–Ω—ñ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ —Ç–∞ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è
- **–†–æ–∑–≤–∏—Ç–æ–∫ —Å–ø—ñ–ª—å–Ω–æ—Ç–∏**: –†–æ–∑—à–∏—Ä–µ–Ω–∞ –µ–∫–æ—Å–∏—Å—Ç–µ–º–∞ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤, –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω—å —ñ –≤–Ω–µ—Å–∫—ñ–≤ —Å–ø—ñ–ª—å–Ω–æ—Ç–∏

### –ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏

–ù–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Ç–æ–≥–æ, —á–∏ —Å—Ç–≤–æ—Ä—é—î—Ç–µ –≤–∏ —á–∞—Ç-–±–æ—Ç–∞, —Ä–æ–∑—Ä–æ–±–ª—è—î—Ç–µ –æ—Å–≤—ñ—Ç–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏, —Å—Ç–≤–æ—Ä—é—î—Ç–µ –ø–æ–º—ñ—á–Ω–∏–∫—ñ–≤ –¥–ª—è –∫–æ–¥—É–≤–∞–Ω–Ω—è –∞–±–æ –ø—Ä–∞—Ü—é—î—Ç–µ –Ω–∞–¥ –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω–∏–º–∏ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è–º–∏, —Å—ñ–º–µ–π—Å—Ç–≤–æ Qwen –ø—Ä–æ–ø–æ–Ω—É—î –º–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω—ñ —Ä—ñ—à–µ–Ω–Ω—è –∑ —Å–∏–ª—å–Ω–æ—é –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é —Å–ø—ñ–ª—å–Ω–æ—Ç–∏ —Ç–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—î—é.

–î–ª—è –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –æ–Ω–æ–≤–ª–µ–Ω—å, –≤–∏–ø—É—Å–∫—ñ–≤ –º–æ–¥–µ–ª–µ–π —ñ –¥–µ—Ç–∞–ª—å–Ω–æ—ó —Ç–µ—Ö–Ω—ñ—á–Ω–æ—ó –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—ó –≤—ñ–¥–≤—ñ–¥–∞–π—Ç–µ –æ—Ñ—ñ—Ü—ñ–π–Ω—ñ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—ó Qwen –Ω–∞ Hugging Face —Ç–∞ –¥–æ—Å–ª—ñ–¥–∂—É–π—Ç–µ –∞–∫—Ç–∏–≤–Ω—ñ –æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è —Å–ø—ñ–ª—å–Ω–æ—Ç–∏ —Ç–∞ –ø—Ä–∏–∫–ª–∞–¥–∏.

–ú–∞–π–±—É—Ç–Ω—î —Ä–æ–∑—Ä–æ–±–∫–∏ —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É –ª–µ–∂–∏—Ç—å —É –¥–æ—Å—Ç—É–ø–Ω–∏—Ö, –ø—Ä–æ–∑–æ—Ä–∏—Ö —ñ –ø–æ—Ç—É–∂–Ω–∏—Ö —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ö, —è–∫—ñ —Å–ø—Ä–∏—è—é—Ç—å —ñ–Ω–Ω–æ–≤–∞—Ü—ñ—è–º —É –≤—Å—ñ—Ö —Å–µ–∫—Ç–æ—Ä–∞—Ö —ñ –º–∞—Å—à—Ç–∞–±–∞—Ö. –°—ñ–º–µ–π—Å—Ç–≤–æ Qwen –≤—Ç—ñ–ª—é—î —Ü–µ –±–∞—á–µ–Ω–Ω—è, –∑–∞–±–µ–∑–ø–µ—á—É—é—á–∏ –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—ó —Ç–∞ —Ä–æ–∑—Ä–æ–±–Ω–∏–∫—ñ–≤ –æ—Å–Ω–æ–≤–æ—é –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ –ø–æ–∫–æ–ª—ñ–Ω–Ω—è –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É.

## –î–æ–¥–∞—Ç–∫–æ–≤—ñ —Ä–µ—Å—É—Ä—Å–∏

- **–û—Ñ—ñ—Ü—ñ–π–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è**: [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è Qwen](https://qwen.readthedocs.io/)
- **Model Hub**: [–ö–æ–ª–µ–∫—Ü—ñ—ó Qwen –Ω–∞ Hugging Face](https://huggingface.co/collections/Qwen/)
- **–ù–∞—É–∫–æ–≤—ñ —Å—Ç–∞—Ç—Ç—ñ**: [–ù–∞—É–∫–æ–≤—ñ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó Qwen](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **–°–ø—ñ–ª—å–Ω–æ—Ç–∞**: [–û–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ –ø–∏—Ç–∞–Ω–Ω—è –Ω–∞ GitHub](https://github.com/QwenLM/)
- **–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ ModelScope**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è

–ü—ñ—Å–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ü—å–æ–≥–æ –º–æ–¥—É–ª—è –≤–∏ –∑–º–æ–∂–µ—Ç–µ:

1. –ü–æ—è—Å–Ω–∏—Ç–∏ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ –ø–µ—Ä–µ–≤–∞–≥–∏ —Å—ñ–º–µ–π—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π Qwen —Ç–∞ –π–æ–≥–æ –ø—ñ–¥—Ö—ñ–¥ –∑ –≤—ñ–¥–∫—Ä–∏—Ç–∏–º –∫–æ–¥–æ–º
2. –í–∏–±—Ä–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç Qwen –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –≤–∏–º–æ–≥ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ç–∞ –æ–±–º–µ–∂–µ–Ω—å —Ä–µ—Å—É—Ä—Å—ñ–≤
3. –í–ø—Ä–æ–≤–∞–¥–∂—É–≤–∞—Ç–∏ –º–æ–¥–µ–ª—ñ Qwen —É —Ä—ñ–∑–Ω–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—è—Ö —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è –∑ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–º–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è–º–∏
4. –ó–∞—Å—Ç–æ—Å–æ–≤—É–≤–∞—Ç–∏ —Ç–µ—Ö–Ω—ñ–∫–∏ –∫–≤–∞–Ω—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª–µ–π Qwen
5. –û—Ü—ñ–Ω—é–≤–∞—Ç–∏ –∫–æ–º–ø—Ä–æ–º—ñ—Å–∏ –º—ñ–∂ —Ä–æ–∑–º—ñ—Ä–æ–º –º–æ–¥–µ–ª—ñ, –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—é —Ç–∞ –º–æ–∂–ª–∏–≤–æ—Å—Ç—è–º–∏ –≤ –º–µ–∂–∞—Ö —Å—ñ–º–µ–π—Å—Ç–≤–∞ Qwen

## –©–æ –¥–∞–ª—ñ

- [03: –û—Å–Ω–æ–≤–∏ —Å—ñ–º–µ–π—Å—Ç–≤–∞ Gemma](03.GemmaFamily.md)

---

**–í—ñ–¥–º–æ–≤–∞ –≤—ñ–¥ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω–æ—Å—Ç—ñ**:  
–¶–µ–π –¥–æ–∫—É–º–µ–Ω—Ç –±—É–≤ –ø–µ—Ä–µ–∫–ª–∞–¥–µ–Ω–∏–π –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Å–µ—Ä–≤—ñ—Å—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É [Co-op Translator](https://github.com/Azure/co-op-translator). –•–æ—á–∞ –º–∏ –ø—Ä–∞–≥–Ω–µ–º–æ –¥–æ —Ç–æ—á–Ω–æ—Å—Ç—ñ, –±—É–¥—å –ª–∞—Å–∫–∞, –º–∞–π—Ç–µ –Ω–∞ —É–≤–∞–∑—ñ, —â–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω—ñ –ø–µ—Ä–µ–∫–ª–∞–¥–∏ –º–æ–∂—É—Ç—å –º—ñ—Å—Ç–∏—Ç–∏ –ø–æ–º–∏–ª–∫–∏ –∞–±–æ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç—ñ. –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –π–æ–≥–æ —Ä—ñ–¥–Ω—ñ–π –º–æ–≤—ñ —Å–ª—ñ–¥ –≤–≤–∞–∂–∞—Ç–∏ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω–∏–º –¥–∂–µ—Ä–µ–ª–æ–º. –î–ª—è –∫—Ä–∏—Ç–∏—á–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó —Ä–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–∏–π –ª—é–¥—Å—å–∫–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥. –ú–∏ –Ω–µ –Ω–µ—Å–µ–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω–æ—Å—Ç—ñ –∑–∞ –±—É–¥—å-—è–∫—ñ –Ω–µ–ø–æ—Ä–æ–∑—É–º—ñ–Ω–Ω—è –∞–±–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ñ —Ç–ª—É–º–∞—á–µ–Ω–Ω—è, —â–æ –≤–∏–Ω–∏–∫–∞—é—Ç—å –≤–Ω–∞—Å–ª—ñ–¥–æ–∫ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ü—å–æ–≥–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É.