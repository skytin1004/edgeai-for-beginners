<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "562ac0eae12d808c9f45fbb77eb5c84f",
  "translation_date": "2025-09-24T22:49:53+00:00",
  "source_file": "Module08/samples/04/README.md",
  "language_code": "sv"
}
-->
# Exempel 04: Produktionsklara chattapplikationer med Chainlit

Ett omfattande exempel som visar flera metoder fÃ¶r att bygga produktionsklara chattapplikationer med Microsoft Foundry Local, inklusive moderna webbgrÃ¤nssnitt, strÃ¶mmande svar och avancerade webblÃ¤sarteknologier.

## Vad som ingÃ¥r

- **ðŸš€ Chainlit Chattapp** (`app.py`): Produktionsklar chattapplikation med strÃ¶mmande svar
- **ðŸŒ WebGPU Demo** (`webgpu-demo/`): AI-inferens i webblÃ¤saren med hÃ¥rdvaruacceleration
- **ðŸŽ¨ Open WebUI Integration** (`open-webui-guide.md`): Professionellt grÃ¤nssnitt likt ChatGPT
- **ðŸ“š Utbildningsanteckningsbok** (`chainlit_app.ipynb`): Interaktiva lÃ¤rmaterial

## Snabbstart

### 1. Chainlit Chattapplikation

```cmd
# Navigate to Module08 directory
cd Module08

# Start your model
foundry model run phi-4-mini

# Run Chainlit app (using port 8080 to avoid conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Ã–ppnas pÃ¥: `http://localhost:8080`

### 2. WebGPU WebblÃ¤sardemo

```cmd
# Navigate to WebGPU demo
cd Module08\samples\04\webgpu-demo

# Serve the demo
python -m http.server 5173
```

Ã–ppnas pÃ¥: `http://localhost:5173`

### 3. Open WebUI Installation

```cmd
# Run Open WebUI with Docker
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

Ã–ppnas pÃ¥: `http://localhost:3000`

## ArkitekturmÃ¶nster

### Lokal vs Moln Beslutsmatris

| Scenario | Rekommendation | Orsak |
|----------|----------------|-------|
| **IntegritetskÃ¤nsliga data** | ðŸ  Lokal (Foundry) | Data lÃ¤mnar aldrig enheten |
| **Komplexa resonemang** | â˜ï¸ Moln (Azure OpenAI) | TillgÃ¥ng till stÃ¶rre modeller |
| **Realtidschatt** | ðŸ  Lokal (Foundry) | LÃ¤gre latens, snabbare svar |
| **Dokumentanalys** | ðŸ”„ Hybrid | Lokal fÃ¶r extraktion, moln fÃ¶r analys |
| **Kodgenerering** | ðŸ  Lokal (Foundry) | Integritet + specialiserade modeller |
| **Forskningsuppgifter** | â˜ï¸ Moln (Azure OpenAI) | KrÃ¤ver bred kunskapsbas |

### TeknologijÃ¤mfÃ¶relse

| Teknologi | AnvÃ¤ndningsomrÃ¥de | FÃ¶rdelar | Nackdelar |
|-----------|-------------------|----------|-----------|
| **Chainlit** | Python-utvecklare, snabb prototypframtagning | Enkel installation, stÃ¶d fÃ¶r strÃ¶mning | Endast Python |
| **WebGPU** | Maximal integritet, offline-scenarier | WebblÃ¤sarbaserad, ingen server behÃ¶vs | BegrÃ¤nsad modellstorlek |
| **Open WebUI** | Produktionsdistribution, team | Professionellt grÃ¤nssnitt, anvÃ¤ndarhantering | KrÃ¤ver Docker |

## FÃ¶rutsÃ¤ttningar

- **Foundry Local**: Installerad och igÃ¥ng ([Ladda ner](https://aka.ms/foundry-local-installer))
- **Python**: 3.10+ med virtuellt miljÃ¶
- **Modell**: Minst en laddad (`foundry model run phi-4-mini`)
- **WebblÃ¤sare**: Chrome/Edge med WebGPU-stÃ¶d fÃ¶r demo
- **Docker**: FÃ¶r Open WebUI (valfritt)

## Installation & Konfiguration

### 1. Python-miljÃ¶instÃ¤llning

```cmd
# Navigate to Module08 directory
cd Module08

# Create and activate virtual environment
py -m venv .venv
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Foundry Local Konfiguration

```cmd
# Verify Foundry Local installation
foundry --version

# Start the service
foundry service start

# Load a model
foundry model run phi-4-mini

# Verify model is running
foundry service ps
```

## Exempelapplikationer

### Chainlit Chattapplikation

**Funktioner:**
- ðŸš€ **RealtidsstrÃ¶mning**: Tokens visas medan de genereras
- ðŸ›¡ï¸ **Robust felhantering**: Smidig nedgradering och Ã¥terhÃ¤mtning
- ðŸŽ¨ **Modernt grÃ¤nssnitt**: Professionellt chattgrÃ¤nssnitt direkt
- ðŸ”§ **Flexibel konfiguration**: MiljÃ¶variabler och automatisk upptÃ¤ckt
- ðŸ“± **Responsiv design**: Fungerar pÃ¥ bÃ¥de dator och mobila enheter

**Snabbstart:**
```cmd
# Run with default settings (recommended)
chainlit run samples\04\app.py -w --port 8080

# Use specific model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Manual endpoint configuration
set BASE_URL=http://localhost:51211
set API_KEY=your-api-key
chainlit run samples\04\app.py -w --port 8080
```

### WebGPU WebblÃ¤sardemo

**Funktioner:**
- ðŸŒ **WebblÃ¤sarbaserad AI**: Ingen server krÃ¤vs, kÃ¶rs helt i webblÃ¤saren
- âš¡ **WebGPU-acceleration**: HÃ¥rdvaruacceleration nÃ¤r tillgÃ¤ngligt
- ðŸ”’ **Maximal integritet**: Ingen data lÃ¤mnar din enhet
- ðŸŽ¯ **Ingen installation**: Fungerar i alla kompatibla webblÃ¤sare
- ðŸ”„ **Smidig fallback**: Faller tillbaka till CPU om WebGPU inte Ã¤r tillgÃ¤ngligt

**KÃ¶rning:**
```cmd
cd samples\04\webgpu-demo
python -m http.server 5173
# Open http://localhost:5173
```

### Open WebUI Integration

**Funktioner:**
- ðŸŽ¨ **GrÃ¤nssnitt likt ChatGPT**: Professionellt och vÃ¤lbekant UI
- ðŸ‘¥ **StÃ¶d fÃ¶r flera anvÃ¤ndare**: AnvÃ¤ndarkonton och konversationshistorik
- ðŸ“ **Filbearbetning**: Ladda upp och analysera dokument
- ðŸ”„ **ModellvÃ¤xling**: Enkel vÃ¤xling mellan olika modeller
- ðŸ³ **Docker-distribution**: Produktionsklar containerbaserad installation

**Snabbinstallation:**
```cmd
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

## Konfigurationsreferens

### MiljÃ¶variabler

| Variabel | Beskrivning | Standard | Exempel |
|----------|-------------|----------|---------|
| `MODEL` | Modellalias att anvÃ¤nda | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | Foundry Local-endpunkt | Automatisk upptÃ¤ckt | `http://localhost:51211` |
| `API_KEY` | API-nyckel (valfritt fÃ¶r lokal) | `""` | `your-api-key` |

## FelsÃ¶kning

### Vanliga problem

**Chainlit-applikation:**

1. **TjÃ¤nsten Ã¤r inte tillgÃ¤nglig:**
   ```cmd
   # Check Foundry Local status
   foundry service status
   foundry service ps
   
   # Validate API endpoint (note: port 51211)
   curl http://localhost:51211/v1/models
   ```

2. **Portkonflikter:**
   ```cmd
   # Check what's using port 8080
   netstat -ano | findstr :8080
   
   # Use different port if needed
   chainlit run samples\04\app.py -w --port 3000
   ```

3. **Problem med Python-miljÃ¶:**
   ```cmd
   # Verify correct interpreter in VS Code
   # Ctrl+Shift+P â†’ Python: Select Interpreter
   # Choose: Module08/.venv/Scripts/python.exe
   
   # Reinstall dependencies
   pip install -r requirements.txt
   ```

**WebGPU Demo:**

1. **WebGPU stÃ¶ds inte:**
   - Uppdatera till Chrome/Edge 113+
   - Aktivera WebGPU: `chrome://flags/#enable-unsafe-webgpu`
   - Kontrollera GPU-status: `chrome://gpu`
   - Demo faller automatiskt tillbaka till CPU

2. **Fel vid modellinlÃ¤sning:**
   - Kontrollera internetanslutning fÃ¶r modellnedladdning
   - Kontrollera webblÃ¤sarkonsolen fÃ¶r CORS-fel
   - Verifiera att du serverar via HTTP (inte file://)

**Open WebUI:**

1. **Anslutning nekad:**
   ```cmd
   # Check Docker is running
   docker --version
   
   # Check container status
   docker ps | findstr open-webui
   
   # View container logs
   docker logs open-webui
   ```

2. **Modeller visas inte:**
   ```cmd
   # Verify Foundry Local endpoint
   curl http://localhost:51211/v1/models
   
   # Restart Open WebUI
   docker restart open-webui
   ```

### Valideringschecklista

```cmd
# âœ… 1. Foundry Local Setup
foundry --version                    # Should show version
foundry service status               # Should show "running"
foundry model list                   # Should show loaded models
curl http://localhost:51211/v1/models  # Should return JSON

# âœ… 2. Python Environment  
python --version                     # Should be 3.10+
pip list | findstr chainlit         # Should show chainlit package
pip list | findstr openai           # Should show openai package

# âœ… 3. Application Testing
chainlit run samples\04\app.py -w --port 8080  # Should open browser
# Test WebGPU demo at localhost:5173
# Test Open WebUI at localhost:3000
```

## Avancerad anvÃ¤ndning

### Prestandaoptimering

**Chainlit:**
- AnvÃ¤nd strÃ¶mning fÃ¶r bÃ¤ttre upplevd prestanda
- Implementera anslutningspooler fÃ¶r hÃ¶g samtidighet
- Cacha modellens svar fÃ¶r upprepade frÃ¥gor
- Ã–vervaka minnesanvÃ¤ndning med stora konversationshistoriker

**WebGPU:**
- AnvÃ¤nd WebGPU fÃ¶r maximal integritet och hastighet
- Implementera modellkvantisering fÃ¶r mindre modeller
- AnvÃ¤nd Web Workers fÃ¶r bakgrundsprocesser
- Cacha kompilerade modeller i webblÃ¤sarens lagring

**Open WebUI:**
- AnvÃ¤nd persistenta volymer fÃ¶r konversationshistorik
- Konfigurera resursbegrÃ¤nsningar fÃ¶r Docker-container
- Implementera backupstrategier fÃ¶r anvÃ¤ndardata
- StÃ¤ll in omvÃ¤nd proxy fÃ¶r SSL-terminering

### IntegrationsmÃ¶nster

**Hybrid Lokal/Moln:**
```python
# Route based on complexity and privacy requirements
async def intelligent_routing(prompt: str, metadata: dict):
    if metadata.get("contains_pii"):
        return await foundry_local_completion(prompt)  # Privacy-sensitive
    elif len(prompt.split()) > 200:
        return await azure_openai_completion(prompt)   # Complex reasoning
    else:
        return await foundry_local_completion(prompt)  # Default local
```

**Multimodal Pipeline:**
```python
# Combine different AI capabilities
async def analyze_document(file_path: str):
    # 1. OCR with WebGPU (browser-based)
    text = await webgpu_ocr(file_path)
    
    # 2. Analysis with Foundry Local (private)
    summary = await foundry_local_analyze(text)
    
    # 3. Enhancement with cloud (if needed)
    if summary.confidence < 0.8:
        summary = await azure_openai_enhance(summary)
    
    return summary
```

## Produktionsdistribution

### SÃ¤kerhetsÃ¶vervÃ¤ganden

- **API-nycklar**: AnvÃ¤nd miljÃ¶variabler, hÃ¥rdkoda aldrig
- **NÃ¤tverk**: AnvÃ¤nd HTTPS i produktion, Ã¶vervÃ¤g VPN fÃ¶r teamÃ¥tkomst
- **Ã…tkomstkontroll**: Implementera autentisering fÃ¶r Open WebUI
- **Dataintegritet**: Granska vilken data som stannar lokalt vs. skickas till molnet
- **Uppdateringar**: HÃ¥ll Foundry Local och containrar uppdaterade

### Ã–vervakning och underhÃ¥ll

- **HÃ¤lsokontroller**: Implementera Ã¶vervakning av slutpunkter
- **Loggning**: Centralisera loggar frÃ¥n alla komponenter
- **MÃ¤tvÃ¤rden**: SpÃ¥ra svarstider, felprocent, resursanvÃ¤ndning
- **Backup**: Regelbunden backup av konversationsdata och konfigurationer

## Referenser och resurser

### Dokumentation
- [Chainlit Dokumentation](https://docs.chainlit.io/) - Komplett ramverksguide
- [Foundry Local Dokumentation](https://learn.microsoft.com/azure/ai-foundry/foundry-local/) - Officiell Microsoft-dokumentation
- [ONNX Runtime Web](https://onnxruntime.ai/docs/get-started/with-javascript/web.html) - WebGPU-integration
- [Open WebUI Dokumentation](https://docs.openwebui.com/) - Avancerad konfiguration

### Exempelfiler
- [`app.py`](../../../../../Module08/samples/04/app.py) - Produktionsklar Chainlit-applikation
- [`chainlit_app.ipynb`](./chainlit_app.ipynb) - Utbildningsanteckningsbok
- [`webgpu-demo/`](../../../../../Module08/samples/04/webgpu-demo) - AI-inferens i webblÃ¤saren
- [`open-webui-guide.md`](./open-webui-guide.md) - Komplett Open WebUI-installation

### Relaterade exempel
- [Session 4 Dokumentation](../../04.CuttingEdgeModels.md) - Komplett sessionsguide
- [Foundry Local Exempel](https://github.com/microsoft/foundry-local/tree/main/samples) - Officiella exempel

---

