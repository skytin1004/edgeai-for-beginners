<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c6ab7eb1b8009ae984eaf6b85568f77a",
  "translation_date": "2025-09-18T07:07:13+00:00",
  "source_file": "Module02/03.GemmaFamily.md",
  "language_code": "sv"
}
-->
# Avsnitt 3: Grundl√§ggande om Gemma-familjen

Gemma-modellfamiljen representerar Googles omfattande strategi f√∂r √∂ppna stora spr√•kmodeller och multimodal AI, och visar att tillg√§ngliga modeller kan uppn√• imponerande prestanda samtidigt som de kan implementeras i olika scenarier, fr√•n mobila enheter till arbetsstationer f√∂r f√∂retag. Det √§r viktigt att f√∂rst√• hur Gemma-familjen m√∂jligg√∂r kraftfulla AI-funktioner med flexibla implementeringsalternativ samtidigt som den bibeh√•ller konkurrenskraftig prestanda och ansvarsfulla AI-principer.

## Introduktion

I denna handledning kommer vi att utforska Googles Gemma-modellfamilj och dess grundl√§ggande koncept. Vi kommer att t√§cka utvecklingen av Gemma-familjen, de innovativa tr√§ningsmetoder som g√∂r Gemma-modeller effektiva, viktiga varianter i familjen och praktiska till√§mpningar i olika implementeringsscenarier.

## L√§randem√•l

I slutet av denna handledning kommer du att kunna:

- F√∂rst√• designfilosofin och utvecklingen av Googles Gemma-modellfamilj
- Identifiera de viktigaste innovationerna som g√∂r att Gemma-modeller kan uppn√• h√∂g prestanda √∂ver olika parametrar
- K√§nna igen f√∂rdelarna och begr√§nsningarna med olika Gemma-modellvarianter
- Anv√§nda kunskap om Gemma-modeller f√∂r att v√§lja l√§mpliga varianter f√∂r verkliga scenarier

## F√∂rst√• det moderna AI-modellandskapet

AI-landskapet har utvecklats avsev√§rt, med olika organisationer som f√∂ljer olika tillv√§gag√•ngss√§tt f√∂r utveckling av spr√•kmodeller. Medan vissa fokuserar p√• propriet√§ra, slutna modeller som endast √§r tillg√§ngliga via API:er, betonar andra √∂ppenhet och tillg√§nglighet genom √∂ppen k√§llkod. Det traditionella tillv√§gag√•ngss√§ttet inneb√§r antingen massiva propriet√§ra modeller med l√∂pande kostnader eller √∂ppna modeller som kan kr√§va betydande teknisk expertis f√∂r implementering.

Denna paradigm skapar utmaningar f√∂r organisationer som s√∂ker kraftfulla AI-funktioner samtidigt som de beh√•ller kontroll √∂ver sina data, kostnader och implementeringsflexibilitet. Det konventionella tillv√§gag√•ngss√§ttet kr√§ver ofta att man v√§ljer mellan banbrytande prestanda och praktiska implementerings√∂verv√§ganden.

## Utmaningen med tillg√§nglig AI-excellens

Behovet av h√∂gkvalitativ, tillg√§nglig AI har blivit allt viktigare i olika scenarier. T√§nk p√• till√§mpningar som kr√§ver flexibla implementeringsalternativ f√∂r olika organisatoriska behov, kostnadseffektiva l√∂sningar d√§r API-kostnader kan bli betydande, multimodala funktioner f√∂r omfattande f√∂rst√•else eller specialiserad implementering p√• mobila och edge-enheter.

### Viktiga implementeringskrav

Moderna AI-implementeringar st√•r inf√∂r flera grundl√§ggande krav som begr√§nsar praktisk anv√§ndbarhet:

- **Tillg√§nglighet**: √ñppen k√§llkod f√∂r transparens och anpassning
- **Kostnadseffektivitet**: Rimliga ber√§kningskrav f√∂r olika budgetar
- **Flexibilitet**: Flera modellstorlekar f√∂r olika implementeringsscenarier
- **Multimodal f√∂rst√•else**: F√∂rm√•ga att bearbeta vision, text och ljud
- **Edge-implementering**: Optimerad prestanda p√• mobila och resursbegr√§nsade enheter

## Gemma-modellens filosofi

Gemma-modellfamiljen representerar Googles omfattande strategi f√∂r AI-modellutveckling, med fokus p√• √∂ppen k√§llkod, multimodala funktioner och praktisk implementering samtidigt som den bibeh√•ller konkurrenskraftiga prestandaegenskaper. Gemma-modeller uppn√•r detta genom olika modellstorlekar, h√∂gkvalitativa tr√§ningsmetoder baserade p√• Gemini-forskning och specialiserade varianter f√∂r olika dom√§ner och implementeringsscenarier.

Gemma-familjen omfattar olika tillv√§gag√•ngss√§tt som √§r utformade f√∂r att ge alternativ √∂ver prestanda-effektivitets-spektrumet, vilket m√∂jligg√∂r implementering fr√•n mobila enheter till f√∂retagsservrar samtidigt som meningsfulla AI-funktioner tillhandah√•lls. M√•let √§r att demokratisera tillg√•ngen till h√∂gkvalitativ AI-teknik samtidigt som flexibilitet i implementeringsval erbjuds.

### Grundl√§ggande designprinciper f√∂r Gemma

Gemma-modeller √§r byggda p√• flera grundl√§ggande principer som skiljer dem fr√•n andra spr√•kmodellsfamiljer:

- **√ñppen k√§llkod f√∂rst**: Fullst√§ndig transparens och tillg√§nglighet f√∂r forskning och kommersiell anv√§ndning
- **Forskningsdriven utveckling**: Byggd med samma forskning och teknik som driver Gemini-modeller
- **Skalbar arkitektur**: Flera modellstorlekar f√∂r att matcha olika ber√§kningskrav
- **Ansvarsfull AI**: Integrerade s√§kerhets√•tg√§rder och ansvarsfulla utvecklingsmetoder

## Viktiga teknologier som m√∂jligg√∂r Gemma-familjen

### Avancerade tr√§ningsmetoder

En av de definierande aspekterna av Gemma-familjen √§r den sofistikerade tr√§ningsmetoden som h√§rr√∂r fr√•n Googles Gemini-forskning. Gemma-modeller anv√§nder distillation fr√•n st√∂rre modeller, f√∂rst√§rkningsinl√§rning fr√•n m√§nsklig feedback (RLHF) och modellkombinationstekniker f√∂r att uppn√• f√∂rb√§ttrad prestanda inom matematik, kodning och instruktioner.

Tr√§ningsprocessen involverar distillation fr√•n st√∂rre instruktionsmodeller, f√∂rst√§rkningsinl√§rning fr√•n m√§nsklig feedback (RLHF) f√∂r att anpassa sig till m√§nskliga preferenser, f√∂rst√§rkningsinl√§rning fr√•n maskinfeedback (RLMF) f√∂r matematisk resonemang och f√∂rst√§rkningsinl√§rning fr√•n exekveringsfeedback (RLEF) f√∂r kodningsf√∂rm√•ga.

### Multimodal integration och f√∂rst√•else

De senaste Gemma-modellerna inkluderar sofistikerade multimodala funktioner som m√∂jligg√∂r omfattande f√∂rst√•else √∂ver olika inputtyper:

**Vision-spr√•k-integration (Gemma 3)**: Gemma 3 kan bearbeta b√•de text och bilder samtidigt, vilket g√∂r att den kan analysera bilder, svara p√• fr√•gor om visuellt inneh√•ll, extrahera text fr√•n bilder och f√∂rst√• komplexa visuella data.

**Ljudbearbetning (Gemma 3n)**: Gemma 3n har avancerade ljudfunktioner inklusive automatisk taligenk√§nning (ASR) och automatisk tal√∂vers√§ttning (AST), med s√§rskilt stark prestanda f√∂r √∂vers√§ttning mellan engelska och spanska, franska, italienska och portugisiska.

**Bearbetning av blandade input**: Gemma-modeller st√∂djer blandade input √∂ver modaliteter, vilket m√∂jligg√∂r f√∂rst√•else av komplexa multimodala interaktioner d√§r text, bilder och ljud kan bearbetas tillsammans.

### Arkitektoniska innovationer

Gemma-familjen inneh√•ller flera arkitektoniska optimeringar som √§r utformade f√∂r b√•de prestanda och effektivitet:

**Ut√∂kning av kontextf√∂nster**: Gemma 3-modeller har ett 128K-token kontextf√∂nster, 16 g√•nger st√∂rre √§n tidigare Gemma-modeller, vilket m√∂jligg√∂r bearbetning av stora m√§ngder information inklusive flera dokument eller hundratals bilder.

**Mobil-f√∂rst-arkitektur (Gemma 3n)**: Gemma 3n anv√§nder Per-Layer Embeddings (PLE)-teknologi och MatFormer-arkitektur, vilket g√∂r att st√∂rre modeller kan k√∂ras med minneskrav j√§mf√∂rbara med mindre traditionella modeller.

**Funktionanrop**: Gemma 3 st√∂djer funktionanrop, vilket g√∂r det m√∂jligt f√∂r utvecklare att bygga naturliga spr√•kgr√§nssnitt f√∂r programmeringsgr√§nssnitt och skapa intelligenta automationssystem.

## Modellstorlek och implementeringsalternativ

Moderna implementeringsmilj√∂er drar nytta av Gemma-modellernas flexibilitet √∂ver olika ber√§kningskrav:

### Sm√• modeller (0.6B-4B)

Gemma erbjuder effektiva sm√• modeller som √§r l√§mpliga f√∂r edge-implementering, mobila applikationer och resursbegr√§nsade milj√∂er samtidigt som imponerande funktioner bibeh√•lls. 1B-modellen √§r idealisk f√∂r sm√• applikationer, medan 4B-modellen erbjuder balanserad prestanda och flexibilitet med multimodal support.

### Medelstora modeller (8B-14B)

Mellanmodeller erbjuder f√∂rb√§ttrade funktioner f√∂r professionella applikationer, med utm√§rkt balans mellan prestanda och ber√§kningskrav f√∂r arbetsstationer och serverimplementering.

### Stora modeller (27B+)

Fullskaliga modeller levererar toppmodern prestanda f√∂r kr√§vande applikationer, forskning och f√∂retagsimplementeringar som kr√§ver maximal kapacitet. 27B-modellen representerar det mest kapabla alternativet som fortfarande kan k√∂ras p√• en enda GPU.

### Mobiloptimerade modeller (Gemma 3n)

Gemma 3n E2B och E4B-modeller √§r specifikt utformade f√∂r mobil och edge-implementering, med effektiva parameterantal p√• 2B respektive 4B, samtidigt som innovativ arkitektur anv√§nds f√∂r att minimera minneskraven till s√• lite som 2GB f√∂r E2B och 3GB f√∂r E4B.

## F√∂rdelar med Gemma-modellfamiljen

### Tillg√§nglighet med √∂ppen k√§llkod

Gemma-modeller erbjuder fullst√§ndig transparens och anpassningsm√∂jligheter med √∂ppna vikter som till√•ter ansvarsfull kommersiell anv√§ndning, vilket g√∂r det m√∂jligt f√∂r organisationer att anpassa och implementera dem i sina egna projekt och applikationer.

### Implementeringsflexibilitet

Utbudet av modellstorlekar m√∂jligg√∂r implementering √∂ver olika h√•rdvarukonfigurationer, fr√•n mobila enheter till avancerade servrar, med optimering f√∂r olika plattformar inklusive Google Cloud TPUs, NVIDIA GPUs, AMD GPUs via ROCm och CPU-exekvering via Gemma.cpp.

### Flerspr√•kig excellens

Gemma-modeller utm√§rker sig i flerspr√•kig f√∂rst√•else och generering, med st√∂d f√∂r √∂ver 140 spr√•k och o√∂vertr√§ffade flerspr√•kiga funktioner, vilket g√∂r dem l√§mpliga f√∂r globala applikationer.

### Konkurrenskraftig prestanda

Gemma-modeller uppn√•r konsekvent konkurrenskraftiga resultat p√• benchmarks, med Gemma 3 som rankas h√∂gt bland b√•de popul√§ra propriet√§ra och √∂ppna modeller i anv√§ndarpreferensutv√§rderingar.

### Specialiserade funktioner

Dom√§nspecifika applikationer drar nytta av Gemmas multimodala f√∂rst√•else, funktionanropsm√∂jligheter och optimerad prestanda √∂ver olika h√•rdvaruplattformar.

## Praktiska exempel och anv√§ndningsomr√•den

Innan vi dyker in i de tekniska detaljerna, l√•t oss utforska n√•gra konkreta exempel p√• vad Gemma-modeller kan √•stadkomma:

### Multimodal f√∂rst√•else exempel

Gemma 3 kan bearbeta b√•de text och bilder samtidigt f√∂r omfattande analys:

```
User: [Image of a complex chart showing financial data] 
What trends can you identify in this quarterly revenue chart, and what might be causing the fluctuation in Q3?

Gemma 3: Looking at this quarterly revenue chart, I can identify several key patterns:

**Revenue Trends:**
1. **Steady Growth Pattern**: Q1 and Q2 show consistent upward trajectory with approximately 15% quarter-over-quarter growth
2. **Q3 Anomaly**: Notable dip of roughly 8% compared to Q2, breaking the growth pattern
3. **Q4 Recovery**: Strong rebound with 22% growth, reaching new highs

**Potential Q3 Factors:**
- **Seasonal Business Cycles**: Many industries experience summer slowdowns
- **Market Disruption**: Possible competitive pressure or economic uncertainty
- **Operational Changes**: Internal restructuring or supply chain adjustments
- **Product Lifecycle**: End-of-cycle for key products before Q4 launches

**Strategic Implications:**
The Q4 recovery suggests the Q3 dip was temporary rather than systemic, indicating resilient underlying business fundamentals with effective adaptation strategies.
```

### Flerspr√•kigt st√∂d exempel

Gemma-modeller visar starka flerspr√•kiga funktioner √∂ver 140+ spr√•k:

```
User: Explain the concept of sustainable development in English, Spanish, and Chinese.

Gemma: **English:**
Sustainable development refers to meeting the needs of the present without compromising the ability of future generations to meet their own needs. It balances economic growth, environmental protection, and social equity to create long-term prosperity while preserving natural resources and ecosystems for future use.

**Espa√±ol:**
El desarrollo sostenible se refiere a satisfacer las necesidades del presente sin comprometer la capacidad de las generaciones futuras para satisfacer sus propias necesidades. Equilibra el crecimiento econ√≥mico, la protecci√≥n ambiental y la equidad social para crear prosperidad a largo plazo mientras preserva los recursos naturales y ecosistemas para uso futuro.

**‰∏≠Êñá:**
ÂèØÊåÅÁª≠ÂèëÂ±ïÊòØÊåáÂú®Êª°Ë∂≥ÂΩì‰ª£‰∫∫ÈúÄÊ±ÇÁöÑÂêåÊó∂Ôºå‰∏çÊçüÂÆ≥Âêé‰ª£Êª°Ë∂≥ÂÖ∂ÈúÄÊ±ÇÁöÑËÉΩÂäõ„ÄÇÂÆÉÂπ≥Ë°°ÁªèÊµéÂ¢ûÈïø„ÄÅÁéØÂ¢É‰øùÊä§ÂíåÁ§æ‰ºöÂÖ¨Âπ≥ÔºåÂàõÈÄ†ÈïøÊúüÁπÅËç£ÔºåÂêåÊó∂‰∏∫Êú™Êù•‰ΩøÁî®‰øùÊä§Ëá™ÁÑ∂ËµÑÊ∫êÂíåÁîüÊÄÅÁ≥ªÁªü„ÄÇ
```

### Funktionanrop exempel

Gemma 3 st√∂djer funktionanrop f√∂r att bygga naturliga spr√•kgr√§nssnitt:

```python
# Example function definitions
available_functions = {
    "get_weather": {
        "description": "Get current weather for a location",
        "parameters": {
            "location": "string",
            "units": "string (optional, default: celsius)"
        }
    },
    "schedule_meeting": {
        "description": "Schedule a meeting with participants",
        "parameters": {
            "title": "string",
            "participants": "array of strings",
            "datetime": "ISO datetime string",
            "duration": "integer (minutes)"
        }
    }
}

User: "Can you check the weather in Tokyo and schedule a team meeting for tomorrow at 2 PM about project updates?"

Gemma 3: I'll help you with both tasks. Let me check the weather and schedule your meeting.

Function Calls:
1. get_weather(location="Tokyo", units="celsius")
2. schedule_meeting(
   title="Project Updates Team Meeting",
   participants=["user", "team"],
   datetime="2025-07-08T14:00:00",
   duration=60
)

Based on the results: Tokyo currently has partly cloudy weather at 24¬∞C. Your project updates meeting has been scheduled for tomorrow (July 8th) at 2:00 PM for one hour. Meeting invitations will be sent to team participants.
```

### Mobil implementering exempel (Gemma 3n)

Gemma 3n √§r optimerad f√∂r mobil och edge-implementering med effektiv minnesanv√§ndning:

```python
# Mobile-optimized inference with Gemma 3n
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Gemma 3n for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3n-E2B-it",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # Further optimize for mobile
)

tokenizer = AutoTokenizer.from_pretrained("google/gemma-3n-E2B-it")

def mobile_inference(prompt, max_tokens=100):
    """Optimized inference for mobile devices"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.7,
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile usage
user_query = "Quick summary of renewable energy benefits"
response = mobile_inference(user_query)
print(f"Mobile Response: {response}")
```

### Ljudbearbetning exempel (Gemma 3n)

Gemma 3n inkluderar avancerade ljudfunktioner f√∂r taligenk√§nning och √∂vers√§ttning:

```python
# Audio processing with Gemma 3n
def process_audio_input(audio_file_path, task="transcribe", target_language="en"):
    """
    Process audio input for transcription or translation
    
    Args:
        audio_file_path: Path to audio file
        task: "transcribe" or "translate"
        target_language: Target language for translation
    """
    
    # Gemma 3n audio processing pipeline
    if task == "transcribe":
        prompt = f"<audio>{audio_file_path}</audio>\nTranscribe this audio:"
    elif task == "translate":
        prompt = f"<audio>{audio_file_path}</audio>\nTranslate this audio to {target_language}:"
    
    # Process with Gemma 3n
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=256)
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result.replace(prompt, "").strip()

# Example usage
# Transcribe Spanish audio to text
transcription = process_audio_input("spanish_audio.wav", task="transcribe")
print(f"Transcription: {transcription}")

# Translate Spanish speech to English text
translation = process_audio_input("spanish_audio.wav", task="translate", target_language="English")
print(f"Translation: {translation}")
```

## Gemma-familjens utveckling

### Gemma 1.0 och 2.0: Grundmodeller

De tidiga Gemma-modellerna etablerade de grundl√§ggande principerna f√∂r √∂ppen k√§llkod och praktisk implementering:

- **Gemma-2B och 7B**: Initialt fokus p√• effektiv spr√•kf√∂rst√•else
- **Gemma 1.5-serien**: Ut√∂kad kontexthantering och f√∂rb√§ttrad prestanda
- **Gemma 2-familjen**: Introduktion av multimodala funktioner och ut√∂kade modellstorlekar

### Gemma 3: Multimodal excellens

Gemma 3-serien markerade betydande framsteg inom multimodala funktioner och prestanda. Byggd p√• samma forskning och teknik som driver Gemini 2.0-modeller, introducerade Gemma 3 vision-spr√•kf√∂rst√•else, 128K-token kontextf√∂nster, funktionanrop och st√∂d f√∂r √∂ver 140 spr√•k.

### Gemma 3n: Mobil-f√∂rst-innovation

Gemma 3n representerar ett genombrott inom mobil-f√∂rst AI-arkitektur, med banbrytande Per-Layer Embeddings (PLE)-teknologi, MatFormer-arkitektur f√∂r ber√§kningsflexibilitet och omfattande multimodala funktioner inklusive ljudbearbetning.

## Till√§mpningar av Gemma-modeller

### F√∂retagsapplikationer

Organisationer anv√§nder Gemma-modeller f√∂r dokumentanalys med visuellt inneh√•ll, kundserviceautomation med multimodalt st√∂d, intelligent kodningsassistans och aff√§rsintelligensapplikationer.

### Mobil och edge computing

Mobilapplikationer anv√§nder Gemma 3n f√∂r realtids-AI direkt p√• enheter, vilket m√∂jligg√∂r personliga och privata upplevelser med blixtsnabba multimodala AI-funktioner.

### Utbildningsteknologi

Utbildningsplattformar anv√§nder Gemma-modeller f√∂r multimodala handledningsupplevelser, automatiserad inneh√•llsgenerering med visuella element, spr√•kinl√§rningsassistans med ljudbearbetning och interaktiva utbildningsupplevelser.

### Globala applikationer

Internationella applikationer drar nytta av Gemma-modellernas starka flerspr√•kiga och tv√§rkulturella funktioner, vilket m√∂jligg√∂r konsekventa AI-upplevelser √∂ver olika spr√•k och kulturella sammanhang.

## Utmaningar och begr√§nsningar

### Ber√§kningskrav

√Ñven om Gemma erbjuder modeller i olika storlekar, kr√§ver st√∂rre varianter fortfarande betydande ber√§kningsresurser f√∂r optimal prestanda.

### Specialiserad dom√§nprestanda

√Ñven om Gemma-modeller presterar v√§l √∂ver generella dom√§ner och multimodala uppgifter, kan mycket specialiserade applikationer dra nytta av dom√§nspecifik finjustering.

### Modellvalskomplexitet

Det breda utbudet av tillg√§ngliga modeller, varianter och implementeringsalternativ kan g√∂ra valet utmanande f√∂r anv√§ndare som √§r nya i ekosystemet.

### H√•rdvaruoptimering

√Ñven om Gemma-modeller √§r optimerade f√∂r olika plattformar, kan prestanda variera beroende p√• h√•rdvarukonfiguration.

## Framtiden f√∂r Gemma-modellfamiljen

Gemma-modellfamiljen representerar den p√•g√•ende utvecklingen mot demokratiserad, h√∂gkvalitativ AI med fortsatt utveckling av f√∂rb√§ttrade effektivitetsoptimeringar, ut√∂kade multimodala funktioner och b√§ttre integration √∂ver olika implementeringsscenarier.

## Utvecklings- och integrations exempel

### Snabbstart med Transformers

S√• h√§r kommer du ig√•ng med Gemma-modeller med Hugging Face Transformers-biblioteket:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Gemma 3-8B model
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation
messages = [
    {"role": "user", "content": "Explain quantum computing and its potential applications."}
]

# Generate response
input_text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.9
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Multimodal anv√§ndning med Gemma 3

```python
from transformers import AutoProcessor, AutoModelForVision2Seq
from PIL import Image

# Load Gemma 3 vision model
model_name = "google/gemma-3-4b-it"
model = AutoModelForVision2Seq.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
processor = AutoProcessor.from_pretrained(model_name)

# Process image and text input
image = Image.open("chart.jpg")
messages = [
    {
        "role": "user", 
        "content": [
            {"type": "image"},
            {"type": "text", "text": "Analyze this chart and explain the key trends you observe."}
        ]
    }
]

# Prepare inputs
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = processor(text=text, images=image, return_tensors="pt").to(model.device)

# Generate multimodal response
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

response = processor.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### Funktionanropsimplementering

```python
import json

# Define available functions
functions = [
    {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "City and state, e.g. San Francisco, CA"},
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
            },
            "required": ["location"]
        }
    },
    {
        "name": "calculate_math",
        "description": "Perform mathematical calculations",
        "parameters": {
            "type": "object",
            "properties": {
                "expression": {"type": "string", "description": "Mathematical expression to evaluate"}
            },
            "required": ["expression"]
        }
    }
]

def gemma_function_calling(user_query, available_functions):
    """Implement function calling with Gemma 3"""
    
    # Create system prompt with function definitions
    system_prompt = f"""You are a helpful assistant with access to the following functions:

{json.dumps(available_functions, indent=2)}

When the user asks for something that requires a function call, respond with a JSON object containing:
- "function_name": the name of the function to call
- "parameters": the parameters to pass to the function

If no function call is needed, respond normally."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    # Process with Gemma
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)
    
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=256,
        temperature=0.3  # Lower temperature for more structured output
    )
    
    response = tokenizer.decode(generated_ids[0][len(model_inputs.input_ids[0]):], skip_special_tokens=True)
    
    # Parse function call if present
    try:
        function_call = json.loads(response.strip())
        if "function_name" in function_call:
            return function_call
    except json.JSONDecodeError:
        pass
    
    return {"response": response}

# Example usage
user_request = "What's the weather like in Tokyo and calculate 15% of 850"
result = gemma_function_calling(user_request, functions)
print(result)
```

### üì± Mobil implementering med Gemma 3n

```python
# Optimized mobile deployment
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class MobileGemmaService:
    """Optimized Gemma 3n service for mobile deployment"""
    
    def __init__(self, model_name="google/gemma-3n-E2B-it"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self._load_optimized_model()
    
    def _load_optimized_model(self):
        """Load model with mobile optimizations"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        # Load with optimizations for mobile
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_8bit=True,  # Quantization for efficiency
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if torch.cuda.is_available():
            self.model = torch.jit.trace(self.model, example_inputs=...)  # JIT optimization
    
    def mobile_chat(self, user_input, max_tokens=150):
        """Optimized chat for mobile devices"""
        messages = [{"role": "user", "content": user_input}]
        
        input_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            max_length=512,
            truncation=True
        )
        
        inputs = self.tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                early_stopping=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response.replace(input_text, "").strip()
    
    def process_audio(self, audio_path, task="transcribe"):
        """Process audio input with Gemma 3n"""
        if task == "transcribe":
            prompt = f"<audio>{audio_path}</audio>\nTranscribe:"
        elif task == "translate":
            prompt = f"<audio>{audio_path}</audio>\nTranslate to English:"
        
        inputs = self.tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.3
            )
        
        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return result.replace(prompt, "").strip()

# Initialize mobile service
mobile_gemma = MobileGemmaService()

# Example mobile usage
quick_response = mobile_gemma.mobile_chat("Summarize renewable energy benefits in 2 sentences")
print(f"Mobile Response: {quick_response}")

# Audio processing example
# audio_transcript = mobile_gemma.process_audio("voice_note.wav", task="transcribe")
# print(f"Audio Transcript: {audio_transcript}")
```

### API-implementering med vLLM

```python
from vllm import LLM, SamplingParams
import asyncio
from typing import List, Dict

class GemmaAPIService:
    """High-performance Gemma API service using vLLM"""
    
    def __init__(self, model_name="google/gemma-3-8b-it"):
        self.llm = LLM(
            model=model_name,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.8,
            trust_remote_code=True
        )
        
        self.sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=512,
            stop_token_ids=[self.llm.get_tokenizer().eos_token_id]
        )
    
    def format_messages(self, messages: List[Dict[str, str]]) -> str:
        """Format messages for API processing"""
        tokenizer = self.llm.get_tokenizer()
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_batch(self, batch_messages: List[List[Dict[str, str]]]) -> List[str]:
        """Generate responses for batch of conversations"""
        formatted_prompts = [self.format_messages(messages) for messages in batch_messages]
        
        # Generate responses
        outputs = self.llm.generate(formatted_prompts, self.sampling_params)
        
        # Extract responses
        responses = []
        for output in outputs:
            response = output.outputs[0].text.strip()
            responses.append(response)
        
        return responses
    
    async def stream_generate(self, messages: List[Dict[str, str]]):
        """Stream generation for real-time applications"""
        formatted_prompt = self.format_messages(messages)
        
        # Note: vLLM streaming implementation would go here
        # This is a simplified example
        outputs = self.llm.generate([formatted_prompt], self.sampling_params)
        response = outputs[0].outputs[0].text
        
        # Simulate streaming by yielding chunks
        words = response.split()
        for i in range(0, len(words), 3):  # Yield 3 words at a time
            chunk = " ".join(words[i:i+3])
            yield chunk
            await asyncio.sleep(0.1)  # Simulate streaming delay

# Example API usage
async def api_example():
    api_service = GemmaAPIService()
    
    # Batch processing
    batch_conversations = [
        [{"role": "user", "content": "Explain machine learning briefly"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}],
        [{"role": "user", "content": "How does photosynthesis work?"}]
    ]
    
    responses = await api_service.generate_batch(batch_conversations)
    for i, response in enumerate(responses):
        print(f"Response {i+1}: {response}")
    
    # Streaming example
    print("\nStreaming response:")
    stream_messages = [{"role": "user", "content": "Write a short story about AI and humans working together"}]
    async for chunk in api_service.stream_generate(stream_messages):
        print(chunk, end=" ", flush=True)

# Run API example
# asyncio.run(api_example())
```

## Prestandabenchmark och prestationer

Gemma-modellfamiljen har uppn√•tt anm√§rkningsv√§rd prestanda √∂ver olika benchmarks samtidigt som den bibeh√•ller √∂ppen k√§llkod och effektiva implementeringsegenskaper:

### Viktiga prestandah√∂jdpunkter

**Multimodal excellens:**
- Gemma 3 levererar kraftfulla funktioner f√∂r utvecklare med avancerade text- och visuella resonemangsf√∂rm√•gor, och st√∂djer bild- och textinmatning f√∂r multimodal f√∂rst√•else.  
- Gemma 3n rankas h√∂gt bland b√•de popul√§ra propriet√§ra och √∂ppna modeller i Chatbot Arena Elo-po√§ng, vilket indikerar stark anv√§ndarpreferens.  

**Effektivitetsframsteg:**  
- Gemma 3-modeller kan hantera promptinmatningar upp till 128K tokens, vilket √§r ett 16 g√•nger st√∂rre kontextf√∂nster √§n tidigare Gemma-modeller.  
- Gemma 3n anv√§nder Per-Layer Embeddings (PLE) som ger en betydande minskning av RAM-anv√§ndning samtidigt som den bibeh√•ller kapaciteten hos st√∂rre modeller.  

**Mobiloptimering:**  
- Gemma 3n E2B fungerar med s√• lite som 2GB minne medan E4B endast kr√§ver 3GB, trots att de har r√•parameterr√§knare p√• 5B respektive 8B.  
- Realtids-AI-funktioner direkt p√• mobila enheter med integritet i fokus och offline-redo drift.  

**Tr√§ningsskala:**  
- Gemma 3 tr√§nades p√• 2T tokens f√∂r 1B, 4T f√∂r 4B, 12T f√∂r 12B och 14T tokens f√∂r 27B-modeller med hj√§lp av Google TPUs och JAX Framework.  

### Modellj√§mf√∂relsematris  

| Modellserie | Parameterintervall | Kontextl√§ngd | Nyckelstyrkor | B√§sta anv√§ndningsomr√•den |  
|-------------|--------------------|---------------|---------------|--------------------------|  
| **Gemma 3** | 1B-27B | 128K | Multimodal f√∂rst√•else, funktionsanrop | Generella applikationer, vision-spr√•k-uppgifter |  
| **Gemma 3n** | E2B (5B), E4B (8B) | Variabel | Mobiloptimering, ljudbearbetning | Mobilappar, edge computing, realtids-AI |  
| **Gemma 2.5** | 0.5B-72B | 32K-128K | Balanserad prestanda, flerspr√•kig | Produktionsutveckling, befintliga arbetsfl√∂den |  
| **Gemma-VL** | Varierande | Variabel | Vision-spr√•k-specialisering | Bildanalys, visuella fr√•gesvar |  

## Guide f√∂r modellval  

### F√∂r grundl√§ggande applikationer  
- **Gemma 3-1B**: L√§tta textuppgifter, enkla mobilapplikationer  
- **Gemma 3-4B**: Balanserad prestanda med multimodal support f√∂r allm√§n anv√§ndning  

### F√∂r multimodala applikationer  
- **Gemma 3-4B/12B**: Bildf√∂rst√•else, visuella fr√•gesvar  
- **Gemma 3n**: Mobila multimodala appar med ljudbearbetningsfunktioner  

### F√∂r mobil- och edge-distribution  
- **Gemma 3n E2B**: Resursbegr√§nsade enheter, realtidsmobil-AI  
- **Gemma 3n E4B**: F√∂rb√§ttrad mobilprestanda med ljudfunktioner  

### F√∂r f√∂retagsdistribution  
- **Gemma 3-12B/27B**: H√∂gpresterande spr√•k- och bildf√∂rst√•else  
- **Funktionsanropsfunktioner**: Bygga intelligenta automationssystem  

### F√∂r globala applikationer  
- **Alla Gemma 3-varianter**: St√∂d f√∂r 140+ spr√•k med kulturell f√∂rst√•else  
- **Gemma 3n**: Mobil-f√∂rst globala applikationer med ljud√∂vers√§ttning  

## Distributionsplattformar och tillg√§nglighet  

### Molnplattformar  
- **Vertex AI**: Helhetsl√∂sning f√∂r MLOps med serverl√∂s upplevelse  
- **Google Kubernetes Engine (GKE)**: Skalbar containerdistribution f√∂r komplexa arbetsbelastningar  
- **Google GenAI API**: Direkt API-√•tkomst f√∂r snabb prototypframtagning  
- **NVIDIA API Catalog**: Optimerad prestanda p√• NVIDIA GPU:er  

### Lokala utvecklingsramverk  
- **Hugging Face Transformers**: Standardintegration f√∂r utveckling  
- **Ollama**: F√∂renklad lokal distribution och hantering  
- **vLLM**: H√∂gpresterande servering f√∂r produktion  
- **Gemma.cpp**: CPU-optimerad exekvering  
- **Google AI Edge**: Optimering f√∂r mobil- och edge-distribution  

### L√§rresurser  
- **Google AI Studio**: Testa Gemma-modeller med bara n√•gra klick  
- **Kaggle och Hugging Face**: Ladda ner modellvikter och exempel fr√•n communityn  
- **Tekniska rapporter**: Omfattande dokumentation och forskningsartiklar  
- **Community-forum**: Aktivt community-st√∂d och diskussioner  

### Komma ig√•ng med Gemma-modeller  

#### Utvecklingsplattformar  
1. **Google AI Studio**: B√∂rja med webbaserade experiment  
2. **Hugging Face Hub**: Utforska modeller och community-implementationer  
3. **Lokal distribution**: Anv√§nd Ollama eller Transformers f√∂r utveckling  

#### Inl√§rningsv√§g  
1. **F√∂rst√• k√§rnkoncept**: Studera multimodala funktioner och distributionsalternativ  
2. **Experimentera med varianter**: Testa olika modellstorlekar och specialiserade versioner  
3. **Praktisera implementation**: Distribuera modeller i utvecklingsmilj√∂er  
4. **Optimera f√∂r produktion**: Finjustera f√∂r specifika anv√§ndningsfall och plattformar  

#### B√§sta praxis  
- **B√∂rja sm√•tt**: B√∂rja med Gemma 3-4B f√∂r initial utveckling och testning  
- **Anv√§nd officiella mallar**: Anv√§nd r√§tt chattmallar f√∂r optimala resultat  
- **√ñvervaka resurser**: Sp√•ra minnesanv√§ndning och inferensprestanda  
- **√ñverv√§g specialisering**: V√§lj l√§mpliga varianter f√∂r multimodala eller mobila behov  

## Avancerade anv√§ndningsm√∂nster  

### Exempel p√• finjustering  

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset
import torch

# Load Gemma model for fine-tuning
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration optimized for Gemma
training_args = TrainingArguments(
    output_dir="./gemma-finetuned",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False,
    dataloader_pin_memory=False
)

def format_gemma_instruction(example):
    """Format instruction for Gemma chat template"""
    messages = [
        {"role": "user", "content": example['instruction']},
        {"role": "assistant", "content": example['output']}
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

# Load and prepare dataset
dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(format_gemma_instruction, remove_columns=dataset["train"].column_names)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./gemma-custom-model")
tokenizer.save_pretrained("./gemma-custom-model")
```  

### Specialiserad promptdesign  

**F√∂r multimodala uppgifter:**  
```python
def create_multimodal_prompt(text_query, image_description="", task_type="analysis"):
    """Create structured prompt for multimodal tasks"""
    
    if task_type == "analysis":
        system_msg = """You are Gemma, an AI assistant with advanced vision capabilities. 
        When analyzing images, provide detailed, accurate descriptions and insights.
        Structure your analysis with clear sections for visual elements, context, and implications."""
    
    elif task_type == "comparison":
        system_msg = """You are Gemma, an AI assistant specialized in visual comparison tasks.
        Compare images systematically, highlighting similarities, differences, and key insights.
        Organize your comparison with clear categories and specific observations."""
    
    messages = [
        {"role": "system", "content": system_msg},
        {
            "role": "user", 
            "content": [
                {"type": "image"},
                {"type": "text", "text": f"{text_query}\n\nAdditional context: {image_description}"}
            ]
        }
    ]
    
    return messages

# Example usage for image analysis
analysis_prompt = create_multimodal_prompt(
    "Analyze this business chart and identify key trends, patterns, and actionable insights.",
    "This appears to be a quarterly revenue chart spanning 2 years",
    task_type="analysis"
)
```  

**F√∂r funktionsanrop med kontext:**  
```python
def create_function_calling_prompt(user_query, available_functions, context=""):
    """Create structured prompt for function calling with Gemma 3"""
    
    function_descriptions = []
    for func in available_functions:
        func_desc = f"- {func['name']}: {func['description']}"
        if 'parameters' in func:
            params = ", ".join([f"{k} ({v.get('type', 'string')})" 
                              for k, v in func['parameters'].get('properties', {}).items()])
            func_desc += f"\n  Parameters: {params}"
        function_descriptions.append(func_desc)
    
    system_prompt = f"""You are Gemma, an AI assistant with access to specific functions.

Available Functions:
{chr(10).join(function_descriptions)}

When a user request requires function calls:
1. Identify which function(s) to use
2. Extract appropriate parameters from the user's request
3. Respond with function calls in this format:
   FUNCTION_CALL: function_name(param1="value1", param2="value2")

If multiple functions are needed, list them separately.
If no function is needed, respond normally.

{f"Context: {context}" if context else ""}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    return messages

# Example function definitions
weather_functions = [
    {
        "name": "get_weather_forecast",
        "description": "Get weather forecast for a specific location and time period",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "days": {"type": "integer", "description": "Number of days (1-7)"},
                "units": {"type": "string", "description": "Temperature units (celsius/fahrenheit)"}
            }
        }
    },
    {
        "name": "get_weather_alerts",
        "description": "Get current weather alerts and warnings",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "severity": {"type": "string", "description": "Minimum alert severity"}
            }
        }
    }
]

# Create function calling prompt
user_request = "I'm traveling to Tokyo next week. Can you check the 5-day weather forecast and any severe weather warnings?"
prompt = create_function_calling_prompt(user_request, weather_functions)
```  

### Flerspr√•kiga applikationer med kulturell kontext  

```python
def create_culturally_aware_prompt(query, target_cultures, response_style="formal"):
    """Create prompts that consider cultural context"""
    
    cultural_guidelines = {
        "japanese": "Use respectful honorifics, avoid direct confrontation, emphasize group harmony",
        "american": "Be direct and practical, focus on individual benefits, use casual tone",
        "german": "Be precise and thorough, provide detailed explanations, maintain professionalism",
        "brazilian": "Be warm and personable, use inclusive language, consider social aspects",
        "indian": "Show respect for hierarchy, consider diverse regional perspectives, be comprehensive"
    }
    
    style_instructions = {
        "formal": "Use professional language, proper grammar, and respectful tone",
        "casual": "Use conversational language while remaining informative",
        "educational": "Explain concepts clearly with examples and context"
    }
    
    cultural_notes = []
    for culture in target_cultures:
        if culture.lower() in cultural_guidelines:
            cultural_notes.append(f"For {culture} audience: {cultural_guidelines[culture.lower()]}")
    
    system_prompt = f"""You are Gemma, a culturally-aware AI assistant. Provide responses that are 
    appropriate for different cultural contexts while maintaining accuracy and helpfulness.

    Response Style: {style_instructions.get(response_style, style_instructions['formal'])}
    
    Cultural Considerations:
    {chr(10).join(cultural_notes)}
    
    Provide your response in the requested languages with cultural appropriateness."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": query}
    ]
    
    return messages

# Example culturally-aware usage
multicultural_query = """Explain the concept of work-life balance and provide practical advice 
for maintaining it. Please provide responses suitable for Japanese and American workplace cultures."""

culturally_aware_prompt = create_culturally_aware_prompt(
    multicultural_query, 
    target_cultures=["Japanese", "American"],
    response_style="educational"
)
```  

### Distributionsm√∂nster f√∂r produktion  

```python
import asyncio
import logging
from typing import List, Dict, Optional, Union
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import time

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

@dataclass
class MultimodalInput:
    text: str
    images: Optional[List[Union[str, Image.Image]]] = None
    audio_path: Optional[str] = None

class ProductionGemmaService:
    """Production-ready Gemma service with multimodal support"""
    
    def __init__(self, model_name: str, device: str = "auto", enable_multimodal: bool = True):
        self.model_name = model_name
        self.device = device
        self.enable_multimodal = enable_multimodal
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.logger = self._setup_logging()
        self._load_model()
    
    def _setup_logging(self):
        """Setup production logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(f"GemmaService-{self.model_name}")
    
    def _load_model(self):
        """Load model and tokenizer with error handling"""
        try:
            self.logger.info(f"Loading model: {self.model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if self.enable_multimodal and "gemma-3" in self.model_name.lower():
                # Load multimodal variant
                from transformers import AutoProcessor
                self.processor = AutoProcessor.from_pretrained(self.model_name)
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            else:
                # Load text-only model
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # Optimize for inference
            self.model.eval()
            self.logger.info("Model loaded successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}")
            raise
    
    def _prepare_multimodal_input(self, multimodal_input: MultimodalInput) -> Dict:
        """Prepare multimodal input for processing"""
        if not self.enable_multimodal or not self.processor:
            # Text-only processing
            return {"text": multimodal_input.text}
        
        inputs = {"text": multimodal_input.text}
        
        if multimodal_input.images:
            # Process images
            processed_images = []
            for img in multimodal_input.images:
                if isinstance(img, str):
                    img = Image.open(img)
                processed_images.append(img)
            inputs["images"] = processed_images
        
        if multimodal_input.audio_path:
            # Process audio (Gemma 3n specific)
            inputs["audio"] = multimodal_input.audio_path
        
        return inputs
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig(),
        multimodal_input: Optional[MultimodalInput] = None
    ) -> str:
        """Async generation with multimodal support"""
        
        start_time = time.time()
        
        try:
            if multimodal_input and self.enable_multimodal:
                # Multimodal processing
                processed_input = self._prepare_multimodal_input(multimodal_input)
                
                if self.processor:
                    # Use processor for multimodal input
                    text = self.processor.apply_chat_template(
                        messages, 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                    
                    model_inputs = self.processor(
                        text=text,
                        images=processed_input.get("images"),
                        return_tensors="pt"
                    ).to(self.model.device)
                else:
                    # Fallback to text-only
                    formatted_text = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    model_inputs = self.tokenizer(
                        formatted_text,
                        return_tensors="pt"
                    ).to(self.model.device)
            else:
                # Text-only processing
                formatted_text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                model_inputs = self.tokenizer(
                    formatted_text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=4096
                ).to(self.model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.model.generate(
                        **model_inputs,
                        max_new_tokens=config.max_tokens,
                        temperature=config.temperature,
                        top_p=config.top_p,
                        repetition_penalty=config.repetition_penalty,
                        do_sample=config.do_sample,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                )
            
            # Extract generated text
            if self.processor and multimodal_input:
                generated_text = self.processor.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            else:
                generated_text = self.tokenizer.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            
            generation_time = time.time() - start_time
            self.logger.info(f"Generation completed in {generation_time:.2f}s")
            
            return generated_text.strip()
            
        except Exception as e:
            self.logger.error(f"Generation failed: {str(e)}")
            raise
    
    def health_check(self) -> Dict[str, Union[str, bool, float]]:
        """Comprehensive health check"""
        health_status = {
            "status": "healthy",
            "model_loaded": False,
            "multimodal_enabled": self.enable_multimodal,
            "response_time": None,
            "memory_usage": None,
            "errors": []
        }
        
        try:
            # Check if model is loaded
            if self.model is not None and self.tokenizer is not None:
                health_status["model_loaded"] = True
            else:
                health_status["errors"].append("Model not properly loaded")
                health_status["status"] = "unhealthy"
                return health_status
            
            # Test basic functionality
            start_time = time.time()
            test_messages = [{"role": "user", "content": "Hello"}]
            
            # Synchronous test for health check
            formatted_text = self.tokenizer.apply_chat_template(
                test_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_text, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=10,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response_time = time.time() - start_time
            health_status["response_time"] = response_time
            
            # Check memory usage
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                health_status["memory_usage"] = memory_used
            
            # Evaluate response time
            if response_time > 5.0:  # seconds
                health_status["errors"].append("Response time exceeds threshold")
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["errors"].append(f"Health check failed: {str(e)}")
        
        return health_status

# Example production usage
async def production_example():
    # Initialize production service
    gemma_service = ProductionGemmaService(
        model_name="google/gemma-3-8b-it",
        enable_multimodal=True
    )
    
    # Health check
    health = gemma_service.health_check()
    print(f"Service Health: {health}")
    
    if health["status"] != "healthy":
        print("Service not healthy, aborting")
        return
    
    # Text-only generation
    text_messages = [
        {"role": "user", "content": "Explain the importance of sustainable development"}
    ]
    
    response = await gemma_service.generate_async(text_messages)
    print(f"Text Response: {response}")
    
    # Multimodal generation (if supported)
    if gemma_service.enable_multimodal:
        multimodal_messages = [
            {
                "role": "user", 
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": "Describe what you see in this image and explain its significance"}
                ]
            }
        ]
        
        # Example with image
        multimodal_input = MultimodalInput(
            text="Analyze this business chart",
            images=["path/to/chart.jpg"]
        )
        
        multimodal_response = await gemma_service.generate_async(
            multimodal_messages,
            multimodal_input=multimodal_input
        )
        print(f"Multimodal Response: {multimodal_response}")

# Run production example
# asyncio.run(production_example())
```  

## Strategier f√∂r prestandaoptimering  

### Minnesoptimering  

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# Memory-efficient loading strategies for Gemma models
def load_optimized_gemma(model_name, optimization_level="balanced"):
    """Load Gemma with various optimization levels"""
    
    if optimization_level == "maximum_efficiency":
        # 4-bit quantization for maximum memory efficiency
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
    elif optimization_level == "balanced":
        # 8-bit quantization for balanced performance
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        
    elif optimization_level == "performance":
        # Full precision for maximum performance
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    
    return model

# Example usage
efficient_model = load_optimized_gemma("google/gemma-3-8b-it", "maximum_efficiency")
```  

### Inferensoptimering  

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel
from transformers import AutoTokenizer
import time

class OptimizedGemmaInference:
    """Optimized inference class for Gemma models"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self._setup_optimizations()
    
    def _setup_optimizations(self):
        """Configure various inference optimizations"""
        
        # Enable optimized attention mechanisms
        if torch.cuda.is_available():
            torch.backends.cuda.enable_flash_sdp(True)
            torch.backends.cuda.enable_math_sdp(True)
            torch.backends.cuda.enable_mem_efficient_sdp(True)
        
        # Set optimal threading for CPU operations
        torch.set_num_threads(min(8, torch.get_num_threads()))
        
        # Enable JIT compilation for repeated patterns
        if hasattr(torch.jit, 'set_fusion_strategy'):
            torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])
        
        # Optimize model for inference
        self.model.eval()
        
        # Enable torch.compile if available (PyTorch 2.0+)
        if hasattr(torch, 'compile'):
            try:
                self.model = torch.compile(self.model, mode="reduce-overhead")
            except Exception as e:
                print(f"Torch compile failed: {e}")
    
    def fast_generate(self, messages, max_tokens=256, use_cache=True):
        """Optimized generation with various performance enhancements"""
        
        start_time = time.time()
        
        # Format input
        formatted_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(
            formatted_text,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        # Use optimized attention backend
        with torch.no_grad():
            if torch.cuda.is_available():
                with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        use_cache=use_cache,
                        pad_token_id=self.tokenizer.eos_token_id,
                        early_stopping=True
                    )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    use_cache=use_cache,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
        
        # Extract response
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        generation_time = time.time() - start_time
        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            "response": response.strip(),
            "generation_time": generation_time,
            "tokens_per_second": tokens_per_second,
            "tokens_generated": tokens_generated
        }
    
    def batch_generate(self, batch_messages, max_tokens=256):
        """Optimized batch generation"""
        
        # Format all inputs
        formatted_texts = []
        for messages in batch_messages:
            formatted_text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            formatted_texts.append(formatted_text)
        
        # Tokenize batch with padding
        inputs = self.tokenizer(
            formatted_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                use_cache=True,
                pad_token_id=self.tokenizer.eos_token_id,
                early_stopping=True
            )
        
        # Extract all responses
        responses = []
        for i, output in enumerate(outputs):
            response = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(response.strip())
        
        generation_time = time.time() - start_time
        
        return {
            "responses": responses,
            "batch_generation_time": generation_time,
            "average_time_per_item": generation_time / len(batch_messages)
        }

# Example usage
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
model = load_optimized_gemma("google/gemma-3-8b-it", "balanced")

optimized_inference = OptimizedGemmaInference(model, tokenizer)

# Single optimized generation
test_messages = [{"role": "user", "content": "Explain machine learning in simple terms"}]
result = optimized_inference.fast_generate(test_messages)
print(f"Response: {result['response']}")
print(f"Speed: {result['tokens_per_second']:.1f} tokens/second")
```  

## B√§sta praxis och riktlinjer  

### S√§kerhet och integritet  

```python
import hashlib
import time
import re
from typing import List, Dict, Optional
import logging

class SecureGemmaService:
    """Security-focused Gemma service implementation"""
    
    def __init__(self, model_name: str, max_requests_per_hour: int = 100):
        self.model_name = model_name
        self.max_requests_per_hour = max_requests_per_hour
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self.logger = logging.getLogger("SecureGemmaService")
        self._load_model()
    
    def _load_model(self):
        """Load model with security considerations"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True  # Only enable for trusted models
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
    
    def _sanitize_input(self, text: str) -> str:
        """Comprehensive input sanitization"""
        if not isinstance(text, str):
            raise ValueError("Input must be a string")
        
        # Remove potentially harmful patterns
        dangerous_patterns = [
            r"<script[^>]*>.*?</script>",
            r"javascript:",
            r"data:text/html",
            r"<iframe[^>]*>.*?</iframe>",
            r"<object[^>]*>.*?</object>",
            r"<embed[^>]*>.*?</embed>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = re.sub(pattern, "", sanitized, flags=re.IGNORECASE | re.DOTALL)
        
        # Limit length to prevent resource exhaustion
        max_length = 8192
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]
            self.logger.warning(f"Input truncated to {max_length} characters")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str) -> bool:
        """Advanced rate limiting with sliding window"""
        current_time = time.time()
        window_size = 3600  # 1 hour in seconds
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests outside the time window
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        # Check if limit exceeded
        if len(self.request_logs[user_id]) >= self.max_requests_per_hour:
            self.logger.warning(f"Rate limit exceeded for user {user_id[:8]}...")
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _validate_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Validate and sanitize message structure"""
        if not isinstance(messages, list):
            raise ValueError("Messages must be a list")
        
        if len(messages) == 0:
            raise ValueError("Messages list cannot be empty")
        
        if len(messages) > 50:  # Reasonable conversation limit
            raise ValueError("Too many messages in conversation")
        
        validated_messages = []
        for message in messages:
            if not isinstance(message, dict):
                raise ValueError("Each message must be a dictionary")
            
            if "role" not in message or "content" not in message:
                raise ValueError("Each message must have 'role' and 'content' fields")
            
            # Validate role
            valid_roles = ["user", "assistant", "system"]
            if message["role"] not in valid_roles:
                raise ValueError(f"Invalid role: {message['role']}")
            
            # Sanitize content
            sanitized_content = self._sanitize_input(message["content"])
            
            validated_messages.append({
                "role": message["role"],
                "content": sanitized_content
            })
        
        return validated_messages
    
    def _content_filter(self, text: str) -> tuple[bool, str]:
        """Basic content filtering for inappropriate content"""
        # This is a simplified example - in production, use more sophisticated filtering
        prohibited_keywords = [
            "violence", "hate", "illegal", "harmful",
            # Add more as needed for your use case
        ]
        
        text_lower = text.lower()
        for keyword in prohibited_keywords:
            if keyword in text_lower:
                return False, f"Content contains prohibited keyword: {keyword}"
        
        return True, ""
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Dict[str, any]:
        """Generate response with comprehensive security measures"""
        
        try:
            # Rate limiting
            if not self._rate_limit_check(user_id):
                return {
                    "success": False,
                    "error": "Rate limit exceeded. Please try again later.",
                    "error_code": "RATE_LIMIT_EXCEEDED"
                }
            
            # Input validation
            validated_messages = self._validate_messages(messages)
            
            # Content filtering
            for message in validated_messages:
                is_safe, filter_message = self._content_filter(message["content"])
                if not is_safe:
                    self.logger.warning(f"Content filtered for user {user_id[:8]}...: {filter_message}")
                    return {
                        "success": False,
                        "error": "Content violates safety guidelines",
                        "error_code": "CONTENT_FILTERED"
                    }
            
            # Log request (with hashed content for privacy)
            content_hash = self._hash_sensitive_data(str(validated_messages))
            self.logger.info(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
            
            # Validate token limit
            max_allowed_tokens = min(max_tokens, 1024)
            
            # Generate response
            start_time = time.time()
            
            formatted_prompt = self.tokenizer.apply_chat_template(
                validated_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_allowed_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            generation_time = time.time() - start_time
            
            # Filter response content
            is_safe_response, filter_message = self._content_filter(response)
            if not is_safe_response:
                self.logger.warning(f"Response filtered for user {user_id[:8]}...: {filter_message}")
                return {
                    "success": False,
                    "error": "Generated response violates safety guidelines",
                    "error_code": "RESPONSE_FILTERED"
                }
            
            # Log successful generation
            self.logger.info(f"Successful generation for user {user_id[:8]}... in {generation_time:.2f}s")
            
            return {
                "success": True,
                "response": response,
                "generation_time": generation_time,
                "tokens_used": max_allowed_tokens
            }
            
        except ValueError as e:
            self.logger.error(f"Validation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": f"Input validation failed: {str(e)}",
                "error_code": "VALIDATION_ERROR"
            }
        
        except Exception as e:
            self.logger.error(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": "An error occurred while processing your request",
                "error_code": "GENERATION_ERROR"
            }
    
    def get_usage_statistics(self, user_id: str) -> Dict[str, any]:
        """Get usage statistics for a user"""
        current_time = time.time()
        window_size = 3600  # 1 hour
        
        if user_id not in self.request_logs:
            return {
                "requests_last_hour": 0,
                "remaining_requests": self.max_requests_per_hour,
                "reset_time": current_time + window_size
            }
        
        # Count recent requests
        recent_requests = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        requests_last_hour = len(recent_requests)
        remaining_requests = max(0, self.max_requests_per_hour - requests_last_hour)
        
        # Calculate reset time (when oldest request expires)
        reset_time = min(recent_requests) + window_size if recent_requests else current_time
        
        return {
            "requests_last_hour": requests_last_hour,
            "remaining_requests": remaining_requests,
            "reset_time": reset_time
        }

# Example secure usage
secure_service = SecureGemmaService("google/gemma-3-8b-it", max_requests_per_hour=50)

# Safe generation
user_messages = [
    {"role": "user", "content": "Explain the benefits of renewable energy"}
]

result = secure_service.secure_generate(user_messages, user_id="user123")

if result["success"]:
    print(f"Secure Response: {result['response']}")
else:
    print(f"Error: {result['error']} (Code: {result['error_code']})")

# Check usage statistics
usage_stats = secure_service.get_usage_statistics("user123")
print(f"Usage Statistics: {usage_stats}")
```  

### √ñvervakning och utv√§rdering  

```python
import time
import psutil
import torch
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import statistics

@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for Gemma models"""
    timestamp: float
    response_time: float
    memory_usage_mb: float
    gpu_memory_mb: float
    token_count: int
    tokens_per_second: float
    model_name: str
    input_length: int
    success: bool
    error_message: Optional[str] = None

@dataclass
class QualityMetrics:
    """Quality assessment metrics"""
    relevance_score: float  # 0-1
    coherence_score: float  # 0-1
    safety_score: float     # 0-1
    factual_accuracy: float # 0-1
    user_satisfaction: Optional[float] = None  # 0-1

class GemmaMonitoringService:
    """Comprehensive monitoring service for Gemma models"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.performance_history: List[PerformanceMetrics] = []
        self.quality_history: List[QualityMetrics] = []
        self.alert_thresholds = {
            "max_response_time": 10.0,  # seconds
            "max_memory_usage": 8192,   # MB
            "min_tokens_per_second": 5.0,
            "min_success_rate": 0.95    # 95%
        }
    
    def measure_performance(
        self, 
        model, 
        tokenizer, 
        messages: List[Dict[str, str]],
        expected_tokens: int = 256
    ) -> PerformanceMetrics:
        """Comprehensive performance measurement"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics
        gpu_memory = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        success = True
        error_message = None
        token_count = 0
        
        try:
            # Format input
            formatted_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            input_length = len(tokenizer.encode(formatted_text))
            
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=expected_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            token_count = outputs.shape[1] - inputs.input_ids.shape[1]
            
        except Exception as e:
            success = False
            error_message = str(e)
            input_length = 0
        
        # Calculate final metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        tokens_per_second = token_count / response_time if response_time > 0 and success else 0
        
        metrics = PerformanceMetrics(
            timestamp=start_time,
            response_time=response_time,
            memory_usage_mb=memory_usage,
            gpu_memory_mb=gpu_memory,
            token_count=token_count,
            tokens_per_second=tokens_per_second,
            model_name=self.model_name,
            input_length=input_length,
            success=success,
            error_message=error_message
        )
        
        self.performance_history.append(metrics)
        return metrics
    
    def evaluate_quality(
        self, 
        input_text: str, 
        generated_text: str,
        reference_text: Optional[str] = None
    ) -> QualityMetrics:
        """Evaluate response quality using various metrics"""
        
        # Simple quality assessment (in production, use more sophisticated methods)
        relevance_score = self._assess_relevance(input_text, generated_text)
        coherence_score = self._assess_coherence(generated_text)
        safety_score = self._assess_safety(generated_text)
        factual_accuracy = self._assess_factual_accuracy(generated_text, reference_text)
        
        quality_metrics = QualityMetrics(
            relevance_score=relevance_score,
            coherence_score=coherence_score,
            safety_score=safety_score,
            factual_accuracy=factual_accuracy
        )
        
        self.quality_history.append(quality_metrics)
        return quality_metrics
    
    def _assess_relevance(self, input_text: str, output_text: str) -> float:
        """Simple relevance assessment based on keyword overlap"""
        input_words = set(input_text.lower().split())
        output_words = set(output_text.lower().split())
        
        if len(input_words) == 0:
            return 0.0
        
        overlap = len(input_words.intersection(output_words))
        return min(1.0, overlap / len(input_words) * 2)  # Scale appropriately
    
    def _assess_coherence(self, text: str) -> float:
        """Simple coherence assessment"""
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.8  # Short text, assume reasonably coherent
        
        # Simple heuristic: check for reasonable sentence length
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        if 5 <= avg_sentence_length <= 25:
            return 0.9
        elif 3 <= avg_sentence_length <= 30:
            return 0.7
        else:
            return 0.5
    
    def _assess_safety(self, text: str) -> float:
        """Simple safety assessment"""
        harmful_indicators = [
            "violence", "harmful", "dangerous", "illegal",
            "hate", "discriminat", "threaten"
        ]
        
        text_lower = text.lower()
        harmful_count = sum(1 for indicator in harmful_indicators if indicator in text_lower)
        
        if harmful_count == 0:
            return 1.0
        elif harmful_count <= 2:
            return 0.7
        else:
            return 0.3
    
    def _assess_factual_accuracy(self, text: str, reference: Optional[str] = None) -> float:
        """Simple factual accuracy assessment"""
        if reference is None:
            # Without reference, use simple heuristics
            confidence_indicators = [
                "according to", "research shows", "studies indicate",
                "data suggests", "evidence shows"
            ]
            
            text_lower = text.lower()
            confidence_count = sum(1 for indicator in confidence_indicators if indicator in text_lower)
            
            return min(0.8, 0.5 + confidence_count * 0.1)
        
        # With reference, calculate similarity (simplified)
        ref_words = set(reference.lower().split())
        text_words = set(text.lower().split())
        
        if len(ref_words) == 0:
            return 0.5
        
        overlap = len(ref_words.intersection(text_words))
        return min(1.0, overlap / len(ref_words))
    
    def get_performance_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get comprehensive performance summary"""
        if not self.performance_history:
            return {"error": "No performance data available"}
        
        recent_metrics = self.performance_history[-last_n:]
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            return {"error": "No successful operations in recent history"}
        
        summary = {
            "total_requests": len(recent_metrics),
            "successful_requests": len(successful_metrics),
            "success_rate": len(successful_metrics) / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in successful_metrics),
            "avg_memory_usage": statistics.mean(m.memory_usage_mb for m in successful_metrics),
            "avg_tokens_per_second": statistics.mean(m.tokens_per_second for m in successful_metrics),
            "p95_response_time": statistics.quantiles([m.response_time for m in successful_metrics], n=20)[18],
            "total_tokens_generated": sum(m.token_count for m in successful_metrics),
        }
        
        if torch.cuda.is_available():
            summary["avg_gpu_memory"] = statistics.mean(m.gpu_memory_mb for m in successful_metrics)
        
        return summary
    
    def get_quality_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get quality metrics summary"""
        if not self.quality_history:
            return {"error": "No quality data available"}
        
        recent_quality = self.quality_history[-last_n:]
        
        return {
            "total_evaluations": len(recent_quality),
            "avg_relevance": statistics.mean(q.relevance_score for q in recent_quality),
            "avg_coherence": statistics.mean(q.coherence_score for q in recent_quality),
            "avg_safety": statistics.mean(q.safety_score for q in recent_quality),
            "avg_factual_accuracy": statistics.mean(q.factual_accuracy for q in recent_quality),
            "overall_quality": statistics.mean([
                (q.relevance_score + q.coherence_score + q.safety_score + q.factual_accuracy) / 4
                for q in recent_quality
            ])
        }
    
    def check_alerts(self) -> List[Dict[str, Any]]:
        """Check for performance alerts"""
        alerts = []
        
        if not self.performance_history:
            return alerts
        
        recent_metrics = self.performance_history[-10:]  # Last 10 requests
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            alerts.append({
                "type": "error",
                "message": "No successful requests in recent history",
                "severity": "high",
                "timestamp": time.time()
            })
            return alerts
        
        # Check response time
        avg_response_time = statistics.mean(m.response_time for m in successful_metrics)
        if avg_response_time > self.alert_thresholds["max_response_time"]:
            alerts.append({
                "type": "performance",
                "message": f"High response time: {avg_response_time:.2f}s (threshold: {self.alert_thresholds['max_response_time']}s)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check memory usage
        avg_memory = statistics.mean(m.memory_usage_mb for m in successful_metrics)
        if avg_memory > self.alert_thresholds["max_memory_usage"]:
            alerts.append({
                "type": "memory",
                "message": f"High memory usage: {avg_memory:.1f}MB (threshold: {self.alert_thresholds['max_memory_usage']}MB)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check tokens per second
        avg_tps = statistics.mean(m.tokens_per_second for m in successful_metrics)
        if avg_tps < self.alert_thresholds["min_tokens_per_second"]:
            alerts.append({
                "type": "performance",
                "message": f"Low generation speed: {avg_tps:.1f} tokens/s (threshold: {self.alert_thresholds['min_tokens_per_second']} tokens/s)",
                "severity": "low",
                "timestamp": time.time()
            })
        
        # Check success rate
        success_rate = len(successful_metrics) / len(recent_metrics)
        if success_rate < self.alert_thresholds["min_success_rate"]:
            alerts.append({
                "type": "reliability",
                "message": f"Low success rate: {success_rate:.2%} (threshold: {self.alert_thresholds['min_success_rate']:.2%})",
                "severity": "high",
                "timestamp": time.time()
            })
        
        return alerts
    
    def export_metrics(self, filepath: str):
        """Export metrics to JSON file"""
        export_data = {
            "model_name": self.model_name,
            "export_timestamp": datetime.now().isoformat(),
            "performance_metrics": [asdict(m) for m in self.performance_history],
            "quality_metrics": [asdict(q) for q in self.quality_history],
            "performance_summary": self.get_performance_summary(),
            "quality_summary": self.get_quality_summary(),
            "current_alerts": self.check_alerts()
        }
        
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)

# Example monitoring usage
def monitoring_example():
    """Example of comprehensive Gemma monitoring"""
    
    # Initialize monitoring
    monitor = GemmaMonitoringService("google/gemma-3-8b-it")
    
    # Load model for testing
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model = AutoModelForCausalLM.from_pretrained(
        "google/gemma-3-8b-it",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
    
    # Test various scenarios
    test_scenarios = [
        [{"role": "user", "content": "What is machine learning?"}],
        [{"role": "user", "content": "Explain quantum computing in simple terms"}],
        [{"role": "user", "content": "Write a short story about AI"}],
        [{"role": "user", "content": "How does photosynthesis work?"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}]
    ]
    
    print("Running performance tests...")
    for i, messages in enumerate(test_scenarios):
        print(f"Test {i+1}/5: {messages[0]['content'][:50]}...")
        
        # Measure performance
        perf_metrics = monitor.measure_performance(model, tokenizer, messages)
        print(f"  Response time: {perf_metrics.response_time:.2f}s")
        print(f"  Tokens/sec: {perf_metrics.tokens_per_second:.1f}")
        print(f"  Success: {perf_metrics.success}")
        
        if perf_metrics.success:
            # Generate actual response for quality evaluation
            formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)
            
            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            # Evaluate quality
            quality_metrics = monitor.evaluate_quality(messages[0]['content'], response)
            print(f"  Quality score: {(quality_metrics.relevance_score + quality_metrics.coherence_score + quality_metrics.safety_score + quality_metrics.factual_accuracy) / 4:.2f}")
        
        print()
    
    # Get comprehensive summary
    perf_summary = monitor.get_performance_summary()
    quality_summary = monitor.get_quality_summary()
    alerts = monitor.check_alerts()
    
    print("=== Performance Summary ===")
    print(f"Success Rate: {perf_summary.get('success_rate', 0):.2%}")
    print(f"Average Response Time: {perf_summary.get('avg_response_time', 0):.2f}s")
    print(f"Average Tokens/Second: {perf_summary.get('avg_tokens_per_second', 0):.1f}")
    print(f"Total Tokens Generated: {perf_summary.get('total_tokens_generated', 0)}")
    
    print("\n=== Quality Summary ===")
    print(f"Overall Quality Score: {quality_summary.get('overall_quality', 0):.2f}")
    print(f"Average Relevance: {quality_summary.get('avg_relevance', 0):.2f}")
    print(f"Average Safety: {quality_summary.get('avg_safety', 0):.2f}")
    
    print(f"\n=== Alerts ({len(alerts)}) ===")
    for alert in alerts:
        print(f"[{alert['severity'].upper()}] {alert['type']}: {alert['message']}")
    
    # Export metrics
    monitor.export_metrics("gemma_monitoring_report.json")
    print("\nMetrics exported to gemma_monitoring_report.json")

# Run monitoring example
# monitoring_example()
```  

## Slutsats  

Gemma-modellfamiljen representerar Googles omfattande strategi f√∂r att demokratisera AI-teknologi samtidigt som den bibeh√•ller konkurrenskraftig prestanda √∂ver olika applikationer och distributionsscenarier. Genom sitt engagemang f√∂r √∂ppen k√§llkod, multimodala funktioner och innovativa arkitektoniska designer m√∂jligg√∂r Gemma f√∂r organisationer och utvecklare att utnyttja kraftfulla AI-funktioner oavsett resurser eller specifika krav.  

### Viktiga insikter  

**√ñppen k√§llkod i toppklass**: Gemma visar att √∂ppna modeller kan uppn√• prestanda som konkurrerar med propriet√§ra alternativ samtidigt som de erbjuder transparens, anpassning och kontroll √∂ver AI-distribution.  

**Multimodal innovation**: Integrationen av text-, bild- och ljudfunktioner i Gemma 3 och Gemma 3n representerar ett betydande framsteg inom tillg√§nglig multimodal AI, vilket m√∂jligg√∂r omfattande f√∂rst√•else √∂ver olika inmatningstyper.  

**Mobil-f√∂rst arkitektur**: Gemma 3n:s banbrytande Per-Layer Embeddings (PLE)-teknologi och mobiloptimering visar att kraftfull AI kan fungera effektivt p√• resursbegr√§nsade enheter utan att kompromissa med kapaciteten.  

**Skalbar distribution**: Intervallet fr√•n 1B till 27B parametrar, med specialiserade mobila varianter, m√∂jligg√∂r distribution √∂ver hela spektrumet av ber√§kningsmilj√∂er samtidigt som kvalitet och prestanda bibeh√•lls.  

**Ansvarsfull AI-integration**: Inbyggda s√§kerhets√•tg√§rder genom ShieldGemma 2 och ansvarsfulla utvecklingsmetoder s√§kerst√§ller att kraftfulla AI-funktioner kan distribueras s√§kert och etiskt.  

### Framtidsutsikter  

N√§r Gemma-familjen forts√§tter att utvecklas kan vi f√∂rv√§nta oss:  

**F√∂rb√§ttrade mobila funktioner**: Ytterligare optimering f√∂r mobil- och edge-distribution med integration av Gemma 3n-arkitektur i st√∂rre plattformar som Android och Chrome.  

**Ut√∂kad multimodal f√∂rst√•else**: Fortsatta framsteg inom integration av vision-spr√•k-ljud f√∂r mer omfattande AI-upplevelser.  

**F√∂rb√§ttrad effektivitet**: Fortsatta arkitektoniska innovationer f√∂r att leverera b√§ttre prestanda per parameter och minskade ber√§kningskrav.  

**Bredare ekosystemintegration**: F√∂rb√§ttrat st√∂d √∂ver utvecklingsramverk, molnplattformar och distributionsverktyg f√∂r s√∂ml√∂s integration i befintliga arbetsfl√∂den.  

**Community-tillv√§xt**: Fortsatt expansion av Gemmaverse med community-skapade modeller, verktyg och applikationer som ut√∂kar k√§rnkapabiliteterna.  

### N√§sta steg  

Oavsett om du bygger mobilapplikationer med realtids-AI-funktioner, utvecklar multimodala utbildningsverktyg, skapar intelligenta automationssystem eller arbetar med globala applikationer som kr√§ver flerspr√•kigt st√∂d, erbjuder Gemma-familjen skalbara l√∂sningar med starkt community-st√∂d och omfattande dokumentation.  

**Rekommendationer f√∂r att komma ig√•ng:**  
1. **Experimentera med Google AI Studio** f√∂r omedelbar praktisk erfarenhet  
2. **Ladda ner modeller fr√•n Hugging Face** f√∂r lokal utveckling och anpassning  
3. **Utforska specialiserade varianter** som Gemma 3n f√∂r mobilapplikationer  
4. **Implementera multimodala funktioner** f√∂r omfattande AI-upplevelser  
5. **F√∂lj s√§kerhetsb√§sta praxis** f√∂r produktionsdistribution  

**F√∂r mobilutveckling**: B√∂rja med Gemma 3n E2B f√∂r resurseffektiv distribution med ljud- och bildfunktioner.  

**F√∂r f√∂retagsapplikationer**: √ñverv√§g Gemma 3-12B eller 27B-modeller f√∂r maximal kapacitet med funktionsanrop och avancerat resonemang.  

**F√∂r globala applikationer**: Utnyttja Gemmas st√∂d f√∂r 140+ spr√•k med kulturellt medveten promptdesign.  

**F√∂r specialiserade anv√§ndningsfall**: Utforska finjusteringsmetoder och dom√§nspecifika optimeringstekniker.  

### üîÆ Demokratiseringen av AI  

Gemma-familjen exemplifierar framtiden f√∂r AI-utveckling d√§r kraftfulla, kapabla modeller √§r tillg√§ngliga f√∂r alla, fr√•n enskilda utvecklare till stora f√∂retag. Genom att kombinera banbrytande forskning med √∂ppen k√§llkod har Google skapat en grund som m√∂jligg√∂r innovation √∂ver alla sektorer och skalor.  

Gemmas framg√•ng med √∂ver 100 miljoner nedladdningar och 60 000+ community-varianter visar kraften i √∂ppen samverkan f√∂r att driva AI-teknologin fram√•t. N√§r vi g√•r fram√•t kommer Gemma-familjen att forts√§tta fungera som en katalysator f√∂r AI-innovation, vilket m√∂jligg√∂r utveckling av applikationer som tidigare endast var m√∂jliga med propriet√§ra, dyra modeller.  

Framtiden f√∂r AI √§r √∂ppen, tillg√§nglig och kraftfull ‚Äì och Gemma-familjen leder v√§gen f√∂r att g√∂ra denna vision till verklighet.  

## Ytterligare resurser  

**Officiell dokumentation och modeller:**  
- **Google AI Studio**: [Testa Gemma-modeller direkt](https://aistudio.google.com)  
- **Hugging Face Collections**:  
  - [Gemma 3 Release](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)  
  - [Gemma 3n Preview](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)  
- **Google AI Developer Documentation**: [Omfattande Gemma-guider](https://ai.google.dev/gemma)  
- **Vertex AI Documentation**: [F√∂retagsdistributionsguider](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma)  

**Tekniska resurser:**  
- **Forskningsartiklar och tekniska rapporter**: [Google DeepMind-publikationer](https://deepmind.google/models/gemma/)  
- **Utvecklarblogginl√§gg**: [Senaste nyheterna och handledningarna](https://developers.googleblog.com)  
- **Modellkort**: Detaljerade tekniska specifikationer och prestandabed√∂mningar  

**Community och support:**  
- **Hugging Face Community**: Aktiva diskussioner och exempel fr√•n communityn  
- **GitHub-repositorier**: √ñppen k√§llkod-implementationer och verktyg  
- **Utvecklarforum**: Google AI Developer-communityns support  
- **Stack Overflow**: Taggade fr√•gor och community-l√∂sningar  

**Utvecklingsverktyg:**  
- **Ollama**: [Enkel lokal distribution](https://ollama.ai)  
- **vLLM**: [H√∂gpresterande servering](https://github.com/vllm-project/vllm)  
- **Transformers Library**: [Hugging Face-integration](https://huggingface.co/docs/transformers)  
- **Google AI Edge**: Optimering f√∂r mobil- och edge-distribution  

**Inl√§rningsv√§gar:**  
- **Nyb√∂rjare**: B√∂rja med Google AI Studio ‚Üí Hugging Face-exempel ‚Üí Lokal distribution  
- **Utvecklare**: Transformers-integration ‚Üí Anpassade applikationer ‚Üí Produktionsdistribution  
- **Forskare**: Tekniska artiklar ‚Üí Finjustering ‚Üí Nya applikationer  
- **F√∂retag**: Vertex AI-distribution ‚Üí S√§kerhetsimplementering ‚Üí Skala optimering  

Gemma-modellfamiljen representerar inte bara en samling AI-modeller, utan ett komplett ekosystem f√∂r att bygga framtiden f√∂r tillg√§ngliga, kraftfulla och ansvarsfulla AI-applikationer. B√∂rja utforska idag och g√• med i det v√§xande communityt av utvecklare och forskare som driver gr√§nserna f√∂r vad som √§r m√∂jligt med √∂ppen k√§llkod-AI.  

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r det noteras att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som kan uppst√• vid anv√§ndning av denna √∂vers√§ttning.