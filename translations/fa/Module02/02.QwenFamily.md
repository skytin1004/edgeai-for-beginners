<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-01T20:24:49+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "fa"
}
-->
# ุจุฎุด ฒ: ุงุตูู ุฎุงููุงุฏู ูุฏูโูุง Qwen

ุฎุงููุงุฏู ูุฏูโูุง Qwen ููุงุงูฺฏุฑ ุฑูฺฉุฑุฏ ุฌุงูุน Alibaba Cloud ุจู ูุฏูโูุง ุฒุจุงู ุจุฒุฑฺฏ ู ููุด ูุตููุน ฺูุฏุฑุณุงููโุง ุงุณุช. ุงู ูุฏูโูุง ูุดุงู ูโุฏููุฏ ฺฉู ูุฏูโูุง ูุชูโุจุงุฒ ูโุชูุงููุฏ ุนููฺฉุฑุฏ ฺุดูฺฏุฑ ุฏุงุดุชู ุจุงุดูุฏ ู ุฏุฑ ุนู ุญุงู ุฏุฑ ุณูุงุฑููุง ูุฎุชูู ูุงุจู ุงุฌุฑุง ุจุงุดูุฏ. ุฏุฑฺฉ ูุญููโุง ฺฉู ุฎุงููุงุฏู Qwen ูุงุจูุชโูุง ูุฏุฑุชููุฏ ููุด ูุตููุน ุฑุง ุจุง ฺฏุฒููโูุง ุงูุนุทุงูโูพุฐุฑ ุงุฌุฑุง ูุฑุงูู ูโฺฉูุฏุ ุฏุฑ ุญุงู ฺฉู ุนููฺฉุฑุฏ ุฑูุงุจุช ุฏุฑ ูุธุงู ูุชููุน ุญูุธ ูโุดูุฏุ ุจุณุงุฑ ููู ุงุณุช.

## ููุงุจุน ุจุฑุง ุชูุณุนูโุฏููุฏฺฏุงู

### ูุฎุฒู ูุฏู ุฏุฑ Hugging Face
ูุฏูโูุง ููุชุฎุจ ุฎุงููุงุฏู Qwen ุงุฒ ุทุฑู [Hugging Face](https://huggingface.co/models?search=qwen) ุฏุฑ ุฏุณุชุฑุณ ูุณุชูุฏ ู ุงูฺฉุงู ุฏุณุชุฑุณ ุจู ุจุฑุฎ ุงุฒ ุงููุงุน ุงู ูุฏูโูุง ุฑุง ูุฑุงูู ูโฺฉููุฏ. ุดูุง ูโุชูุงูุฏ ุงููุงุน ููุฌูุฏ ุฑุง ุจุฑุฑุณ ฺฉูุฏุ ุขูโูุง ุฑุง ุจุฑุง ููุงุฑุฏ ุงุณุชูุงุฏู ุฎุงุต ุฎูุฏ ุชูุธู ฺฉูุฏ ู ุงุฒ ุทุฑู ฺุงุฑฺูุจโูุง ูุฎุชูู ุงุฌุฑุง ฺฉูุฏ.

### ุงุจุฒุงุฑูุง ุชูุณุนู ูุญู
ุจุฑุง ุชูุณุนู ู ุขุฒูุงุด ูุญูุ ูโุชูุงูุฏ ุงุฒ [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) ุงุณุชูุงุฏู ฺฉูุฏ ุชุง ูุฏูโูุง Qwen ููุฌูุฏ ุฑุง ุจุง ุนููฺฉุฑุฏ ุจููู ุฑู ุฏุณุชฺฏุงู ุชูุณุนู ุฎูุฏ ุงุฌุฑุง ฺฉูุฏ.

### ููุงุจุน ูุณุชูุฏุงุช
- [ูุณุชูุฏุงุช ูุฏู Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [ุจูููโุณุงุฒ ูุฏูโูุง Qwen ุจุฑุง ุงุฌุฑุง ุฏุฑ Edge](https://github.com/microsoft/olive)

## ููุฏูู

ุฏุฑ ุงู ุขููุฒุดุ ุฎุงููุงุฏู ูุฏูโูุง Qwen ุงุฒ Alibaba ู ููุงูู ุงุณุงุณ ุขู ุฑุง ุจุฑุฑุณ ุฎูุงูู ฺฉุฑุฏ. ูุง ุจู ุชฺฉุงูู ุฎุงููุงุฏู Qwenุ ุฑูุดโูุง ููุขูุฑุงูู ุขููุฒุด ฺฉู ูุฏูโูุง Qwen ุฑุง ูุคุซุฑ ูโุณุงุฒูุฏุ ุงููุงุน ฺฉูุฏ ุฏุฑ ุงู ุฎุงููุงุฏู ู ฺฉุงุฑุจุฑุฏูุง ุนูู ุฏุฑ ุณูุงุฑููุง ูุฎุชูู ุฎูุงูู ูพุฑุฏุงุฎุช.

## ุงูุฏุงู ุงุฏฺฏุฑ

ุฏุฑ ูพุงุงู ุงู ุขููุฒุดุ ุดูุง ูุงุฏุฑ ุฎูุงูุฏ ุจูุฏ:

- ููุณูู ุทุฑุงุญ ู ุชฺฉุงูู ุฎุงููุงุฏู ูุฏูโูุง Qwen ุงุฒ Alibaba ุฑุง ุฏุฑฺฉ ฺฉูุฏ
- ููุขูุฑโูุง ฺฉูุฏ ฺฉู ุจู ูุฏูโูุง Qwen ุงูฺฉุงู ุฏุณุชุงุจ ุจู ุนููฺฉุฑุฏ ุจุงูุง ุฏุฑ ุงูุฏุงุฒูโูุง ูุฎุชูู ูพุงุฑุงูุชุฑ ุฑุง ูโุฏููุฏุ ุดูุงุณุง ฺฉูุฏ
- ูุฒุงุง ู ูุญุฏูุฏุชโูุง ุงููุงุน ูุฎุชูู ูุฏูโูุง Qwen ุฑุง ุจุดูุงุณุฏ
- ุฏุงูุด ุฎูุฏ ุงุฒ ูุฏูโูุง Qwen ุฑุง ุจุฑุง ุงูุชุฎุงุจ ุงููุงุน ููุงุณุจ ุฏุฑ ุณูุงุฑููุง ูุงูุน ุจู ฺฉุงุฑ ุจุจุฑุฏ

## ุฏุฑฺฉ ฺุดูโุงูุฏุงุฒ ูุฏุฑู ูุฏูโูุง ููุด ูุตููุน

ฺุดูโุงูุฏุงุฒ ููุด ูุตููุน ุจู ุทูุฑ ูุงุจู ุชูุฌู ุชฺฉุงูู ุงูุชู ุงุณุชุ ู ุณุงุฒูุงูโูุง ูุฎุชูู ุฑูฺฉุฑุฏูุง ูุชูุงูุช ุฑุง ุจุฑุง ุชูุณุนู ูุฏูโูุง ุฒุจุงู ุฏูุจุงู ูโฺฉููุฏ. ุฏุฑ ุญุงู ฺฉู ุจุฑุฎ ุจุฑ ูุฏูโูุง ุงุฎุชุตุงุต ู ุจุณุชู ุชูุฑฺฉุฒ ุฏุงุฑูุฏุ ุฏฺฏุฑุงู ุจุฑ ุฏุณุชุฑุณ ูุชูโุจุงุฒ ู ุดูุงูุช ุชุฃฺฉุฏ ูโฺฉููุฏ. ุฑูฺฉุฑุฏ ุณูุช ุดุงูู ูุฏูโูุง ุงุฎุชุตุงุต ุนุธู ุงุณุช ฺฉู ููุท ุงุฒ ุทุฑู APIโูุง ูุงุจู ุฏุณุชุฑุณ ูุณุชูุฏ ุง ูุฏูโูุง ูุชูโุจุงุฒ ฺฉู ููฺฉู ุงุณุช ุฏุฑ ูุงุจูุชโูุง ุนูุจโุชุฑ ุจุงุดูุฏ.

ุงู ูพุงุฑุงุฏุงู ฺุงูุดโูุง ุฑุง ุจุฑุง ุณุงุฒูุงูโูุง ุงุฌุงุฏ ูโฺฉูุฏ ฺฉู ุจู ุฏูุจุงู ูุงุจูุชโูุง ูุฏุฑุชููุฏ ููุด ูุตููุน ูุณุชูุฏ ู ุฏุฑ ุนู ุญุงู ฺฉูุชุฑู ุจุฑ ุฏุงุฏูโูุงุ ูุฒููโูุง ู ุงูุนุทุงูโูพุฐุฑ ุงุฌุฑุง ุฑุง ุญูุธ ูโฺฉููุฏ. ุฑูฺฉุฑุฏ ูุนููู ุงุบูุจ ูุงุฒููุฏ ุงูุชุฎุงุจ ุจู ุนููฺฉุฑุฏ ูพุดุฑูุชู ู ููุงุญุธุงุช ุนูู ุงุฌุฑุง ุงุณุช.

## ฺุงูุด ุจุฑุชุฑ ููุด ูุตููุน ูุงุจู ุฏุณุชุฑุณ

ูุงุฒ ุจู ููุด ูุตููุน ุจุง ฺฉูุช ุจุงูุง ู ูุงุจู ุฏุณุชุฑุณ ุฏุฑ ุณูุงุฑููุง ูุฎุชูู ุจู ุทูุฑ ูุฒุงูุฏูโุง ููู ุดุฏู ุงุณุช. ุจู ฺฉุงุฑุจุฑุฏูุง ูฺฉุฑ ฺฉูุฏ ฺฉู ฺฏุฒููโูุง ุงุฌุฑุง ุงูุนุทุงูโูพุฐุฑ ุจุฑุง ูุงุฒูุง ูุฎุชูู ุณุงุฒูุงูุ ูพุงุฏูโุณุงุฒโูุง ููุฑููโุจูโุตุฑูู ฺฉู ูุฒููโูุง API ูโุชูุงูุฏ ูุงุจู ุชูุฌู ุจุงุดุฏุ ูุงุจูุชโูุง ฺูุฏุฒุจุงูู ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ุฌูุงูุ ุง ุชุฎุตุต ุฏุงูููโุง ุฏุฑ ุฒูููโูุง ูุงููุฏ ฺฉุฏููุณ ู ุฑุงุถุงุช ุฑุง ูุงุฒ ุฏุงุฑูุฏ.

### ุงูุฒุงูุงุช ฺฉูุฏ ุงุฌุฑุง

ุงุฌุฑุง ูุฏุฑู ููุด ูุตููุน ุจุง ฺูุฏู ุงูุฒุงู ุงุณุงุณ ููุงุฌู ุงุณุช ฺฉู ฺฉุงุฑุจุฑุฏ ุนูู ุฑุง ูุญุฏูุฏ ูโฺฉูุฏ:

- **ุฏุณุชุฑุณโูพุฐุฑ**: ุฏุณุชุฑุณ ูุชูโุจุงุฒ ุจุฑุง ุดูุงูุช ู ุณูุงุฑุดโุณุงุฒ
- **ููุฑููโุจูโุตุฑูู ุจูุฏู**: ูุงุฒูุง ูุญุงุณุจุงุช ูุนููู ุจุฑุง ุจูุฏุฌูโูุง ูุฎุชูู
- **ุงูุนุทุงูโูพุฐุฑ**: ุงูุฏุงุฒูโูุง ูุฎุชูู ูุฏู ุจุฑุง ุณูุงุฑููุง ุงุฌุฑุง ูุชูุงูุช
- **ุฏุณุชุฑุณ ุฌูุงู**: ูุงุจูุชโูุง ฺูุฏุฒุจุงูู ู ุจูโูุฑููฺฏ ูู
- **ุชุฎุตุต**: ุงููุงุน ุฎุงุต ุฏุงููู ุจุฑุง ููุงุฑุฏ ุงุณุชูุงุฏู ุฎุงุต

## ููุณูู ูุฏู Qwen

ุฎุงููุงุฏู ูุฏูโูุง Qwen ููุงุงูฺฏุฑ ุฑูฺฉุฑุฏ ุฌุงูุน ุจู ุชูุณุนู ูุฏูโูุง ููุด ูุตููุน ุงุณุช ฺฉู ุฏุณุชุฑุณ ูุชูโุจุงุฒุ ูุงุจูุชโูุง ฺูุฏุฒุจุงูู ู ุงุฌุฑุง ุนูู ุฑุง ุฏุฑ ุงูููุช ูุฑุงุฑ ูโุฏูุฏ ู ุฏุฑ ุนู ุญุงู ูฺฺฏโูุง ุนููฺฉุฑุฏ ุฑูุงุจุช ุฑุง ุญูุธ ูโฺฉูุฏ. ูุฏูโูุง Qwen ุงู ุงูุฏุงู ุฑุง ุงุฒ ุทุฑู ุงูุฏุงุฒูโูุง ูุชููุน ูุฏูุ ุฑูุดโูุง ุขููุฒุด ุจุง ฺฉูุช ุจุงูุง ู ุงููุงุน ุฎุงุต ุฏุงููู ุจุฑุง ุญูุฒูโูุง ูุฎุชูู ุจู ุฏุณุช ูโุขูุฑูุฏ.

ุฎุงููุงุฏู Qwen ุดุงูู ุฑูฺฉุฑุฏูุง ูุฎุชูู ุงุณุช ฺฉู ฺฏุฒููโูุง ุฑุง ุฏุฑ ุทู ุนููฺฉุฑุฏ-ฺฉุงุฑุง ูุฑุงูู ูโฺฉูุฏุ ู ุงูฺฉุงู ุงุฌุฑุง ุงุฒ ุฏุณุชฺฏุงูโูุง ููุจุงู ุชุง ุณุฑูุฑูุง ุณุงุฒูุงู ุฑุง ูุฑุงูู ูโฺฉูุฏ ู ุฏุฑ ุนู ุญุงู ูุงุจูุชโูุง ูุนูุงุฏุงุฑ ููุด ูุตููุน ุฑุง ุงุฑุงุฆู ูโุฏูุฏ. ูุฏู ุงู ุงุณุช ฺฉู ุฏุณุชุฑุณ ุจู ููุด ูุตููุน ุจุง ฺฉูุช ุจุงูุง ุฑุง ุฏููฺฉุฑุงุชฺฉ ฺฉูู ู ุฏุฑ ุนู ุญุงู ุงูุนุทุงูโูพุฐุฑ ุฏุฑ ุงูุชุฎุงุจโูุง ุงุฌุฑุง ุฑุง ูุฑุงูู ฺฉูู.

### ุงุตูู ุทุฑุงุญ ุงุตู Qwen

ูุฏูโูุง Qwen ุจุฑ ุงุณุงุณ ฺูุฏู ุงุตู ุจูุงุฏ ุณุงุฎุชู ุดุฏูโุงูุฏ ฺฉู ุขูโูุง ุฑุง ุงุฒ ุฎุงููุงุฏูโูุง ุฏฺฏุฑ ูุฏูโูุง ุฒุจุงู ูุชูุงุฒ ูโฺฉูุฏ:

- **ุงูููุช ูุชูโุจุงุฒ**: ุดูุงูุช ฺฉุงูู ู ุฏุณุชุฑุณ ุจุฑุง ุงุณุชูุงุฏู ุชุญููุงุช ู ุชุฌุงุฑ
- **ุขููุฒุด ุฌุงูุน**: ุขููุฒุด ุจุฑ ุฑู ูุฌููุนู ุฏุงุฏูโูุง ุนุธู ู ูุชููุน ฺฉู ุดุงูู ฺูุฏู ุฒุจุงู ู ุฏุงููู ุงุณุช
- **ูุนูุงุฑ ููุงุณโูพุฐุฑ**: ุงูุฏุงุฒูโูุง ูุฎุชูู ูุฏู ุจุฑุง ุชุทุงุจู ุจุง ูุงุฒูุง ูุญุงุณุจุงุช ูุชูุงูุช
- **ุชุฎุตุต ุจูููโุดุฏู**: ุงููุงุน ุฎุงุต ุฏุงููู ฺฉู ุจุฑุง ูุธุงู ุฎุงุต ุจููู ุดุฏูโุงูุฏ

## ููุงูุฑโูุง ฺฉูุฏ ฺฉู ุฎุงููุงุฏู Qwen ุฑุง ููฺฉู ูโุณุงุฒูุฏ

### ุขููุฒุด ุฏุฑ ููุงุณ ุนุธู

ฺฉ ุงุฒ ุฌูุจูโูุง ุชุนุฑูโฺฉููุฏู ุฎุงููุงุฏู Qwen ููุงุณ ุนุธู ุฏุงุฏูโูุง ุขููุฒุด ู ููุงุจุน ูุญุงุณุจุงุช ุงุณุช ฺฉู ุฏุฑ ุชูุณุนู ูุฏู ุณุฑูุงูโฺฏุฐุงุฑ ุดุฏูโุงูุฏ. ูุฏูโูุง Qwen ุงุฒ ูุฌููุนู ุฏุงุฏูโูุง ฺูุฏุฒุจุงูู ุจุง ุฏูุช ุงูุชุฎุงุจโุดุฏู ฺฉู ุดุงูู ุชุฑูููโูุง ุชูฺฉู ุงุณุชุ ุงุณุชูุงุฏู ูโฺฉููุฏ ู ุทุฑุงุญ ุดุฏูโุงูุฏ ุชุง ุฏุงูุด ุฌุงูุน ุฌูุงู ู ูุงุจูุชโูุง ุงุณุชุฏูุงู ุฑุง ูุฑุงูู ฺฉููุฏ.

ุงู ุฑูฺฉุฑุฏ ุจุง ุชุฑฺฉุจ ูุญุชูุง ูุจ ุจุง ฺฉูุช ุจุงูุงุ ุงุฏุจุงุช ุนููุ ูุฎุงุฒู ฺฉุฏ ู ููุงุจุน ฺูุฏุฒุจุงูู ฺฉุงุฑ ูโฺฉูุฏ. ุฑูุด ุขููุฒุด ุจุฑ ุนูู ุฏุงูุด ู ุนูู ุฏุฑฺฉ ุฏุฑ ุฏุงูููโูุง ู ุฒุจุงูโูุง ูุฎุชูู ุชุฃฺฉุฏ ุฏุงุฑุฏ.

### ุงุณุชุฏูุงู ู ุชูฺฉุฑ ูพุดุฑูุชู

ูุฏูโูุง ุงุฎุฑ Qwen ูุงุจูุชโูุง ุงุณุชุฏูุงู ูพฺุฏูโุง ุฑุง ุดุงูู ูโุดููุฏ ฺฉู ุงูฺฉุงู ุญู ูุณุงุฆู ฺูุฏูุฑุญููโุง ูพฺุฏู ุฑุง ูุฑุงูู ูโฺฉูุฏ:

**ุญุงูุช ุชูฺฉุฑ (Qwen3)**: ูุฏูโูุง ูโุชูุงููุฏ ูุจู ุงุฒ ุงุฑุงุฆู ูพุงุณุฎ ููุงุ ุฏุฑฺฏุฑ ุงุณุชุฏูุงู ูุฑุญููโุจูโูุฑุญูู ุดููุฏุ ูุดุงุจู ุฑูฺฉุฑุฏูุง ุญู ูุณุฆูู ุงูุณุงู.

**ุนููฺฉุฑุฏ ุฏู ุญุงูุชู**: ุชูุงูุง ุชุบุฑ ุจู ุญุงูุช ูพุงุณุฎ ุณุฑุน ุจุฑุง ูพุฑุณุดโูุง ุณุงุฏู ู ุญุงูุช ุชูฺฉุฑ ุนูู ุจุฑุง ูุณุงุฆู ูพฺุฏู.

**ุงุฏุบุงู ุฒูุฌุฑูโุง ุงุฒ ุชูฺฉุฑ**: ุชุฑฺฉุจ ุทุจุน ูุฑุงุญู ุงุณุชุฏูุงู ฺฉู ุดูุงูุช ู ุฏูุช ุฑุง ุฏุฑ ูุธุงู ูพฺุฏู ุจูุจูุฏ ูโุจุฎุดุฏ.

### ููุขูุฑโูุง ูุนูุงุฑ

ุฎุงููุงุฏู Qwen ุดุงูู ฺูุฏู ุจูููโุณุงุฒ ูุนูุงุฑ ุงุณุช ฺฉู ุจุฑุง ุนููฺฉุฑุฏ ู ฺฉุงุฑุง ุทุฑุงุญ ุดุฏูโุงูุฏ:

**ุทุฑุงุญ ููุงุณโูพุฐุฑ**: ูุนูุงุฑ ุณุงุฒฺฏุงุฑ ุฏุฑ ุงูุฏุงุฒูโูุง ูุฎุชูู ูุฏู ฺฉู ุงูฺฉุงู ููุงุณโูพุฐุฑ ู ููุงุณู ุขุณุงู ุฑุง ูุฑุงูู ูโฺฉูุฏ.

**ุงุฏุบุงู ฺูุฏุฑุณุงููโุง**: ุงุฏุบุงู ุจโููุต ูุงุจูุชโูุง ูพุฑุฏุงุฒุด ูุชูุ ุชุตูุฑ ู ุตูุช ุฏุฑ ูุนูุงุฑโูุง ฺฉูพุงุฑฺู.

**ุจูููโุณุงุฒ ุงุฌุฑุง**: ฺฏุฒููโูุง ูุฎุชูู ฺฉูุงูุชุฒุงุณูู ู ูุฑูุชโูุง ุงุฌุฑุง ุจุฑุง ูพฺฉุฑุจูุฏโูุง ุณุฎุชโุงูุฒุงุฑ ูุฎุชูู.

## ุงูุฏุงุฒู ูุฏู ู ฺฏุฒููโูุง ุงุฌุฑุง

ูุญุทโูุง ุงุฌุฑุง ูุฏุฑู ุงุฒ ุงูุนุทุงูโูพุฐุฑ ูุฏูโูุง Qwen ุฏุฑ ูุงุฒูุง ูุญุงุณุจุงุช ูุฎุชูู ุจูุฑูโููุฏ ูโุดููุฏ:

### ูุฏูโูุง ฺฉูฺฺฉ (ฐ.ตB-ณB)

Qwen ูุฏูโูุง ฺฉูฺฺฉ ฺฉุงุฑุขูุฏ ุฑุง ุงุฑุงุฆู ูโุฏูุฏ ฺฉู ุจุฑุง ุงุฌุฑุง ุฏุฑ Edgeุ ุจุฑูุงููโูุง ููุจุงู ู ูุญุทโูุง ูุญุฏูุฏ ููุงุจุน ููุงุณุจ ูุณุชูุฏ ู ุฏุฑ ุนู ุญุงู ูุงุจูุชโูุง ฺุดูฺฏุฑ ุฑุง ุญูุธ ูโฺฉููุฏ.

### ูุฏูโูุง ูุชูุณุท (ทB-ณฒB)

ูุฏูโูุง ูุงูโุฑุฏู ูุงุจูุชโูุง ูพุดุฑูุชูโุง ุฑุง ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ุญุฑููโุง ุงุฑุงุฆู ูโุฏููุฏ ู ุชุนุงุฏู ุนุงู ุจู ุนููฺฉุฑุฏ ู ูุงุฒูุง ูุญุงุณุจุงุช ูุฑุงูู ูโฺฉููุฏ.

### ูุฏูโูุง ุจุฒุฑฺฏ (ทฒB+)

ูุฏูโูุง ุชูุงูโููุงุณ ุนููฺฉุฑุฏ ูพุดุฑูุชูโุง ุฑุง ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ูพุฑุชูุงุถุงุ ุชุญููุงุช ู ุงุฌุฑุงูุง ุณุงุฒูุงู ฺฉู ุจู ุญุฏุงฺฉุซุฑ ูุงุจูุช ูุงุฒ ุฏุงุฑูุฏุ ุงุฑุงุฆู ูโุฏููุฏ.

## ูุฒุงุง ุฎุงููุงุฏู ูุฏูโูุง Qwen

### ุฏุณุชุฑุณ ูุชูโุจุงุฒ

ูุฏูโูุง Qwen ุดูุงูุช ฺฉุงูู ู ูุงุจูุชโูุง ุณูุงุฑุดโุณุงุฒ ุฑุง ูุฑุงูู ูโฺฉููุฏ ู ุจู ุณุงุฒูุงูโูุง ุงูฺฉุงู ูโุฏููุฏ ูุฏูโูุง ุฑุง ุจุฑุง ูุงุฒูุง ุฎุงุต ุฎูุฏ ุฏุฑฺฉุ ุชุบุฑ ู ุชุทุจู ุฏููุฏ ุจุฏูู ููู ุดุฏู ุจู ฺฉ ูุฑูุดูุฏู.

### ุงูุนุทุงูโูพุฐุฑ ุงุฌุฑุง

ูุญุฏูุฏู ุงูุฏุงุฒูโูุง ูุฏู ุงูฺฉุงู ุงุฌุฑุง ุฏุฑ ูพฺฉุฑุจูุฏโูุง ุณุฎุชโุงูุฒุงุฑ ูุชููุนุ ุงุฒ ุฏุณุชฺฏุงูโูุง ููุจุงู ุชุง ุณุฑูุฑูุง ูพุดุฑูุชูุ ุฑุง ูุฑุงูู ูโฺฉูุฏ ู ุจู ุณุงุฒูุงูโูุง ุงูุนุทุงูโูพุฐุฑ ุฏุฑ ุงูุชุฎุงุจ ุฒุฑุณุงุฎุช ููุด ูุตููุน ุฎูุฏ ูโุฏูุฏ.

### ุจุฑุชุฑ ฺูุฏุฒุจุงูู

ูุฏูโูุง Qwen ุฏุฑ ุฏุฑฺฉ ู ุชููุฏ ฺูุฏุฒุจุงูู ุนุงู ูุณุชูุฏ ู ุงุฒ ุฏูโูุง ุฒุจุงู ุจุง ูุฏุฑุช ุฎุงุต ุฏุฑ ุงูฺฏูุณ ู ฺู ูพุดุชุจุงู ูโฺฉููุฏุ ฺฉู ุขูโูุง ุฑุง ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ุฌูุงู ููุงุณุจ ูโุณุงุฒุฏ.

### ุนููฺฉุฑุฏ ุฑูุงุจุช

ูุฏูโูุง Qwen ุจู ุทูุฑ ูุฏุงูู ูุชุงุฌ ุฑูุงุจุช ุฏุฑ ูุนุงุฑูุง ุจู ุฏุณุช ูโุขูุฑูุฏ ู ุฏุฑ ุนู ุญุงู ุฏุณุชุฑุณ ูุชูโุจุงุฒ ุฑุง ูุฑุงูู ูโฺฉููุฏุ ูุดุงู ูโุฏููุฏ ฺฉู ูุฏูโูุง ูุชูโุจุงุฒ ูโุชูุงููุฏ ุจุง ุฌุงฺฏุฒูโูุง ุงุฎุชุตุงุต ุฑูุงุจุช ฺฉููุฏ.

### ูุงุจูุชโูุง ุชุฎุตุต

ุงููุงุน ุฎุงุต ุฏุงููู ูุงููุฏ Qwen-Coder ู Qwen-Math ุชุฎุตุต ูฺูโุง ุฑุง ุงุฑุงุฆู ูโุฏููุฏ ู ุฏุฑ ุนู ุญุงู ูุงุจูุชโูุง ุนููู ุฏุฑฺฉ ุฒุจุงู ุฑุง ุญูุธ ูโฺฉููุฏ.

## ูุซุงูโูุง ู ููุงุฑุฏ ุงุณุชูุงุฏู ุนูู

ูุจู ุงุฒ ูุฑูุฏ ุจู ุฌุฒุฆุงุช ููุ ุจุงุฏ ุจุฑุฎ ุงุฒ ูุซุงูโูุง ููููุณ ุงุฒ ุขูฺู ูุฏูโูุง Qwen ูโุชูุงููุฏ ุงูุฌุงู ุฏููุฏ ุฑุง ุจุฑุฑุณ ฺฉูู:

### ูุซุงู ุงุณุชุฏูุงู ุฑุงุถ

Qwen-Math ุฏุฑ ุญู ูุณุงุฆู ุฑุงุถ ูุฑุญููโุจูโูุฑุญูู ุนุงู ุนูู ูโฺฉูุฏ. ุจุฑุง ูุซุงูุ ููุช ุงุฒ ุขู ุฎูุงุณุชู ูโุดูุฏ ฺฉ ูุณุฆูู ูพฺุฏู ุญุณุงุจ ุฏูุฑุงูุณู ู ุงูุชฺฏุฑุงู ุฑุง ุญู ฺฉูุฏ:

```
User: Find the derivative of f(x) = xยณ + 2xยฒ - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = xยณ + 2xยฒ - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xโฟ) = nยทxโฟโปยน

Breaking down each term:
- d/dx(xยณ) = 3xยฒ
- d/dx(2xยฒ) = 2 ยท 2xยน = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3xยฒ + 4x - 5
```

### ูุซุงู ูพุดุชุจุงู ฺูุฏุฒุจุงูู

ูุฏูโูุง Qwen ูุงุจูุชโูุง ฺูุฏุฒุจุงูู ูู ุฑุง ุฏุฑ ุฒุจุงูโูุง ูุฎุชูู ูุดุงู ูโุฏููุฏ:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**ไธญๆ:**
ไบบๅทฅๆบ่ฝ๏ผAI๏ผๆฏๆๆบๅจ๏ผ็นๅซๆฏ่ฎก็ฎๆบ็ณป็ป๏ผๅฏนไบบ็ฑปๆบ่ฝ่ฟ็จ็ๆจกๆใ่ฟไบ่ฟ็จๅๆฌๅญฆไนใๆจ็ใ่งฃๅณ้ฎ้ขใๆ็ฅๅ่ฏญ่จ็่งฃใไบบๅทฅๆบ่ฝไฝฟๆบๅจ่ฝๅคๆง่ก้ๅธธ้่ฆไบบ็ฑป่ฎค็ฅ่ฝๅ็ไปปๅกใ
```

### ูุซุงู ูุงุจูุชโูุง ฺูุฏุฑุณุงููโุง

Qwen-VL ูโุชูุงูุฏ ูุชู ู ุชุตุงูุฑ ุฑุง ุจู ุทูุฑ ููุฒูุงู ูพุฑุฏุงุฒุด ฺฉูุฏ:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### ูุซุงู ุชููุฏ ฺฉุฏ

Qwen-Coder ุฏุฑ ุชููุฏ ู ุชูุถุญ ฺฉุฏ ุฏุฑ ุฒุจุงูโูุง ุจุฑูุงููโููุณ ูุฎุชูู ุนุงู ุนูู ูโฺฉูุฏ:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

ุงู ูพุงุฏูโุณุงุฒ ุงุฒ ุจูุชุฑู ุดููโูุง ุจุง ูุงูโูุง ูุชุบุฑ ูุงุถุญุ ูุณุชูุฏุงุช ุฌุงูุน ู ููุทู ฺฉุงุฑุขูุฏ ูพุฑู ูโฺฉูุฏ.
```

### Edge Deployment Example

Qwen models can be deployed on various edge devices with optimized configurations:

```
# ูุซุงู ุงุฌุฑุง ุฏุฑ ุฏุณุชฺฏุงู ููุจุงู ุจุง ฺฉูุงูุชุฒุงุณูู
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ุจุงุฑฺฏุฐุงุฑ ูุฏู ฺฉูุงูุชุฒูโุดุฏู ุจุฑุง ุงุฌุฑุง ุฏุฑ ููุจุงู

```
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## ุชฺฉุงูู ุฎุงููุงุฏู Qwen

### Qwen 1.0 ู 1.5: ูุฏูโูุง ูพุงู

ูุฏูโูุง ุงููู Qwen ุงุตูู ุจูุงุฏ ุขููุฒุด ุฌุงูุน ู ุฏุณุชุฑุณ ูุชูโุจุงุฒ ุฑุง ุงุฌุงุฏ ฺฉุฑุฏูุฏ:

- **Qwen-7B (ทB ูพุงุฑุงูุชุฑ)**: ุงูุชุดุงุฑ ุงููู ุจุง ุชูุฑฺฉุฒ ุจุฑ ุฏุฑฺฉ ุฒุจุงู ฺู ู ุงูฺฏูุณ
- **Qwen-14B (ฑดB ูพุงุฑุงูุชุฑ)**: ูุงุจูุชโูุง ุจูุจูุฏโุงูุชู ุจุง ุงุณุชุฏูุงู ู ุฏุงูุด ุจูุชุฑ
- **Qwen-72B (ทฒB ูพุงุฑุงูุชุฑ)**: ูุฏู ุจุฒุฑฺฏโููุงุณ ฺฉู ุนููฺฉุฑุฏ ูพุดุฑูุชูโุง ุงุฑุงุฆู ูโุฏูุฏ
- **ุณุฑ Qwen1.5**: ฺฏุณุชุฑุด ุจู ุงูุฏุงุฒูโูุง ูุฎุชูู (ฐ.ตB ุชุง ฑฑฐB) ุจุง ุจูุจูุฏ ุฏุฑ ูุฏุฑุช ุฒููู ุทููุงู

### ุฎุงููุงุฏู Qwen2: ฺฏุณุชุฑุด ฺูุฏุฑุณุงููโุง

ุณุฑ Qwen2 ูพุดุฑูุช ูุงุจู ุชูุฌู ุฏุฑ ูุงุจูุชโูุง ุฒุจุงู ู ฺูุฏุฑุณุงููโุง ุงุฌุงุฏ ฺฉุฑุฏ:

- **Qwen2-0.5B ุชุง ทฒB**: ูุญุฏูุฏู ุฌุงูุน ูุฏูโูุง ุฒุจุงู ุจุฑุง ูุงุฒูุง ูุฎุชูู ุงุฌุฑุง
- **Qwen2-57B-A14B (MoE)**: ูุนูุงุฑ ุชุฑฺฉุจ ูุชุฎุตุตุงู ุจุฑุง ุงุณุชูุงุฏู ฺฉุงุฑุขูุฏ ุงุฒ ูพุงุฑุงูุชุฑูุง
- **Qwen2-VL**: ูุงุจูุชโูุง ูพุดุฑูุชู ุฒุจุงู-ุชุตูุฑ ุจุฑุง ุฏุฑฺฉ ุชุตูุฑ
- **Qwen2-Audio**: ูพุฑุฏุงุฒุด ู ุฏุฑฺฉ ุตูุช
- **Qwen2-Math**: ุงุณุชุฏูุงู ุฑุงุถ ุชุฎุตุต ู ุญู ูุณุฆูู

### ุฎุงููุงุฏู Qwen2.5: ุนููฺฉุฑุฏ ุจูุจูุฏโุงูุชู

ุณุฑ Qwen2.5 ุจูุจูุฏโูุง ูุงุจู ุชูุฌู ุฏุฑ ุชูุงู ุงุจุนุงุฏ ุงุฌุงุฏ ฺฉุฑุฏ:

- **ุขููุฒุด ฺฏุณุชุฑุดโุงูุชู**: ฑธ ุชุฑููู ุชูฺฉู ุฏุงุฏู ุขููุฒุด ุจุฑุง ูุงุจูุชโูุง ุจูุจูุฏโุงูุชู
- **ุฒููู ฺฏุณุชุฑุดโุงูุชู**: ุทูู ุฒููู ุชุง ฑฒธK ุชูฺฉูุ ุจุง ูุณุฎู Turbo ฺฉู ุงุฒ ฑM ุชูฺฉู ูพุดุชุจุงู ูโฺฉูุฏ
- **ุชุฎุตุต ุจูุจูุฏโุงูุชู**: ูุฏูโูุง ุจูุจูุฏโุงูุชู Qwen2.5-Coder ู Qwen2.5-Math
- **ูพุดุชุจุงู ฺูุฏุฒุจุงูู ุจูุชุฑ**: ุนููฺฉุฑุฏ ุจูุจูุฏโุงูุชู ุฏุฑ ุจุด ุงุฒ ฒท ุฒุจุงู

### ุฎุงููุงุฏู Qwen3: ุงุณุชุฏูุงู ูพุดุฑูุชู

ูุณู ุฌุฏุฏ ูุฑุฒูุง ุงุณุชุฏูุงู ู ูุงุจูุชโูุง ุชูฺฉุฑ ุฑุง ฺฏุณุชุฑุด ูโุฏูุฏ:

- **Qwen3-235B-A22B**: ูุฏู ุชุฑฺฉุจ ูุชุฎุตุตุงู ูพุฑฺูโุฏุงุฑ ุจุง ฒณตB ูพุงุฑุงูุชุฑ ฺฉู
- **Qwen3-30B-A3B**: ูุฏู MoE ฺฉุงุฑุขูุฏ ุจุง ุนููฺฉุฑุฏ ูู ุฏุฑ ูุฑ ูพุงุฑุงูุชุฑ ูุนุงู
- **ูุฏูโูุง ูุชุฑุงฺฉู**: Qwen3-32Bุ ฑดBุ ธBุ ดBุ ฑ.ทBุ ฐ.ถB ุจุฑุง ุณูุงุฑููุง ูุฎุชูู ุงุฌุฑุง
- **ุญุงูุช ุชูฺฉุฑ**: ุฑูฺฉุฑุฏ ุงุณุชุฏูุงู ุชุฑฺฉุจ ฺฉู ุงุฒ ูพุงุณุฎโูุง ุณุฑุน ู ุชูฺฉุฑ ุนูู ูพุดุชุจุงู ูโฺฉูุฏ
- **ุจุฑุชุฑ ฺูุฏุฒุจุงูู**: ูพุดุชุจุงู ุงุฒ ฑฑน ุฒุจุงู ู ฺฏูุด
- **ุขููุฒุด ุจูุจูุฏโุงูุชู**: ณถ ุชุฑููู ุชูฺฉู ุฏุงุฏู ุขููุฒุด ูุชููุน ู ุจุง ฺฉูุช ุจุงูุง

## ฺฉุงุฑุจุฑุฏูุง ูุฏูโูุง Qwen

### ฺฉุงุฑุจุฑุฏูุง ุณุงุฒูุงู

ุณุงุฒูุงูโูุง ุงุฒ ูุฏูโูุง Qwen ุจุฑุง ุชุญูู ุงุณูุงุฏุ ุฎูุฏฺฉุงุฑุณุงุฒ ุฎุฏูุงุช ูุดุชุฑุ ฺฉูฺฉ ุจู ุชููุฏ ฺฉุฏ ู ฺฉุงุฑุจุฑุฏูุง ููุด ุชุฌุงุฑ ุงุณุชูุงุฏู ูโฺฉููุฏ. ูุงูุช ูุชูโุจุงุฒ ุงูฺฉุงู ุณูุงุฑุดโุณุงุฒ ุจุฑุง ูุงุฒูุง ุฎุงุต ฺฉุณุจโูฺฉุงุฑ ุฑุง ูุฑุงูู ูโฺฉูุฏ ู ุฏุฑ ุนู ุญุงู ุญุฑู ุฎุตูุต ุฏุงุฏูโูุง ู ฺฉูุชุฑู ุฑุง ุญูุธ ูโฺฉูุฏ.

### ูุญุงุณุจุงุช ููุจุงู ู Edge

ุจุฑูุงููโูุง ููุจุงู ุงุฒ ูุฏูโูุง Qwen ุจุฑุง ุชุฑุฌูู ุจูุงุฏุฑูฺฏุ ุฏุณุชุงุฑูุง ููุดููุฏุ ุชููุฏ ูุญุชูุง ู ุชูุตูโูุง ุดุฎุตโุณุงุฒโุดุฏู ุงุณุชูุงุฏู ูโฺฉููุฏ. ูุญุฏูุฏู ุงูุฏุงุฒูโูุง ูุฏู ุงูฺฉุงู ุงุฌุฑุง ุงุฒ ุฏุณุชฺฏุงูโูุง ููุจุงู ุชุง ุณุฑูุฑูุง Edge ุฑุง ูุฑุงูู ูโฺฉูุฏ.

### ููุงูุฑ ุขููุฒุด

ูพูุชูุฑูโูุง ุขููุฒุด ุงุฒ ูุฏูโูุง Qwen ุจุฑุง ุชุฏุฑุณ ุดุฎุตโุณุงุฒโุดุฏูุ ุชููุฏ ุฎูุฏฺฉุงุฑ ูุญุชูุงุ ฺฉูฺฉ ุจู ุงุฏฺฏุฑ ุฒุจุงู ู ุชุฌุฑุจุงุช ุขููุฒุด ุชุนุงูู ุงุณุชูุงุฏู ูโฺฉููุฏ. ูุฏูโูุง ุชุฎุตุต ูุงููุฏ Qwen-Math ุชุฎุตุต ุฏุงูููโุง ุงุฑุงุฆู ูโุฏููุฏ.

### ฺฉุงุฑุจุฑุฏูุง ุฌูุงู

ฺฉุงุฑุจุฑุฏูุง ุจูโุงูููู ุงุฒ ูุงุจูุชโูุง ฺูุฏุฒุจุงูู ูู ูุฏูโูุง Qwen ุจูุฑูโููุฏ ูโุดููุฏ ู ุชุฌุฑุจูโูุง ููุด ูุตููุน ุณุงุฒฺฏุงุฑ ุฏุฑ ุฒุจุงูโูุง ู ุฒูููโูุง ูุฑููฺฏ ูุฎุชูู ุฑุง ุงูฺฉุงูโูพุฐุฑ ูโฺฉููุฏ.

## ฺุงูุดโูุง ู ูุญุฏูุฏุชโูุง

### ูุงุฒูุง ูุญุงุณุจุงุช

ุฏุฑ ุญุงู ฺฉู Qwen ูุฏูโูุง ุฏุฑ ุงูุฏุงุฒูโูุง ูุฎุชูู ุงุฑุงุฆู ูโุฏูุฏุ ุงููุงุน ุจุฒุฑฺฏโุชุฑ ูููุฒ ุจู ููุงุจุน ูุญุงุณุจุงุช ูุงุจู ุชูุฌู ุจุฑุง ุนููฺฉุฑุฏ ุจููู ูุงุฒ ุฏุงุฑูุฏุ ฺฉู ููฺฉู ุงุณุช ฺฏุฒููโูุง ุงุฌุฑุง ุฑุง ุจุฑุง ุจุฑุฎ ุณุงุฒูุงูโูุง ูุญุฏูุฏ ฺฉูุฏ.

### ุนููฺฉุฑุฏ ุฏุงููู ุชุฎุตุต

ุฏุฑ ุญุงู ฺฉู ูุฏูโูุง Qwen ุฏุฑ ุฏุงูููโูุง ุนููู ุนููฺฉุฑุฏ ุฎูุจ ุฏุงุฑูุฏุ ฺฉุงุฑุจุฑุฏูุง ุจุณุงุฑ ุชุฎุตุต ููฺฉู ุงุณุช ุงุฒ ุชูุธู ุฏูู ุฏุงููู ุง ูุฏูโูุง ุชุฎุตุต ุจูุฑูโููุฏ ุดููุฏ.

### ูพฺุฏฺฏ ุงูุชุฎุงุจ ูุฏู

ูุญุฏูุฏู ฺฏุณุชุฑุฏู ูุฏูโูุง ู ุงููุงุน ููุฌูุฏ ูโุชูุงูุฏ ุงูุชุฎุงุจ ุฑุง ุจุฑุง ฺฉุงุฑุจุฑุงู ุฌุฏุฏ ุฏุฑ ุงู ุงฺฉูุณุณุชู ฺุงูุดโุจุฑุงูฺฏุฒ ฺฉูุฏ.

### ุนุฏู ุชุนุงุฏู ุฒุจุงู

ุฏุฑ ุญุงู ฺฉู ุงุฒ ุฒุจุงูโูุง ุฒุงุฏ ูพุดุชุจุงู ูโุดูุฏุ ุนููฺฉุฑุฏ ููฺฉู ุงุณุช ุฏุฑ ุฒุจุงูโูุง ูุฎุชูู ูุชูุงูุช ุจุงุดุฏุ ุจุง ูุงุจูุชโูุง ููโุชุฑ ุฏุฑ ุงูฺฏูุณ ู ฺู.

## ุขูุฏู ุฎุงููุงุฏู ูุฏูโูุง Qwen

ุฎุงููุงุฏู ูุฏูโูุง Qwen ููุงุงูฺฏุฑ ุชฺฉุงูู ูุฏุงูู ุจู ุณูุช ููุด ูุตููุน ุฏููฺฉุฑุงุชฺฉ ู ุจุง ฺฉูุช ุจุงูุง ุงุณุช. ุชูุณุนูโูุง ุขูุฏู ุดุงูู ุจูููโุณุงุฒโูุง ฺฉุงุฑุง ูพุดุฑูุชูุ ูุงุจูุชโูุง ฺูุฏุฑุณุงููโุง ฺฏุณุชุฑุดโุงูุชูุ ูฺฉุงูุฒูโูุง ุงุณุชุฏูุงู ุจูุจูุฏโุงูุชู ู ุงุฏุบุงู ุจูุชุฑ ุฏุฑ ุณูุงุฑููุง ูุฎุชูู ุงุฌุฑุง ุฎูุงูุฏ ุจูุฏ.

ุจุง ุงุฏุงูู ุชฺฉุงูู ููุงูุฑุ ุงูุชุธุงุฑ ูโุฑูุฏ ูุฏูโูุง Qwen ุจู ุทูุฑ ูุฒุงูุฏูโุง ุชูุงูููุฏ ุดููุฏ ู ุฏุฑ ุนู ุญุงู ุฏุณุชุฑุณ ูุชูโุจุงุฒ ุฎูุฏ ุฑุง ุญูุธ ฺฉููุฏุ ู ุงูฺฉุงู ุงุฌุฑุง ููุด ูุตููุน ุฏุฑ ุณูุงุฑููุง ู ููุงุฑุฏ ุงุณุชูุงุฏู ูุชููุน ุฑุง ูุฑุงูู ฺฉููุฏ.

ุฎุงููุงุฏู Qwen ูุดุงู ูโุฏูุฏ ฺฉู ุขูุฏู ุชูุณุนู ููุด ูุตููุน ูโุชูุงูุฏ ูู ุนููฺฉุฑุฏ ูพุดุฑูุชู ู ูู ุฏุณุชุฑุณ ูุชูโุจุงุฒ ุฑุง ุฏุฑ ุจุฑ ุจฺฏุฑุฏุ ู ุงุจุฒุงุฑูุง ูุฏุฑุชููุฏ ุฑุง ุจุฑุง ุณุงุฒูุงูโูุง ูุฑุงูู ฺฉูุฏ ู ุฏุฑ ุนู ุญุงู ุดูุงูุช ู ฺฉูุชุฑู ุฑุง ุญูุธ ฺฉูุฏ.

## ูุซุงูโูุง ุชูุณุนู ู ุงุฏุบุงู

### ุดุฑูุน ุณุฑุน ุจุง Transformers

ุฏุฑ ุงูุฌุง ูุญูู ุดุฑูุน ุจุง ูุฏูโูุง Qwen ุจุง ุงุณุชูุงุฏู ุงุฒ ฺฉุชุงุจุฎุงูู Transformers ุงุฒ Hugging Face ุขูุฏู ุงุณุช:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

###
- Qwen3-235B-A22B ุฏุฑ ุงุฑุฒุงุจโูุง ูุนุงุฑูุง ฺฉุฏููุณุ ุฑุงุถ ู ูุงุจูุชโูุง ุนูููุ ูุชุงุฌ ุฑูุงุจุช ุฏุฑ ููุงุณู ุจุง ูุฏูโูุง ุจุฑุชุฑ ุฏฺฏุฑ ูุงููุฏ DeepSeek-R1ุ o1ุ o3-miniุ Grok-3 ู Gemini-2.5-Pro ุจู ุฏุณุช ุขูุฑุฏู ุงุณุช.
- Qwen3-30B-A3B ุนููฺฉุฑุฏ ุจูุชุฑ ูุณุจุช ุจู QwQ-32B ุจุง 10 ุจุฑุงุจุฑ ูพุงุฑุงูุชุฑูุง ูุนุงู ุฏุงุฑุฏ.
- Qwen3-4B ูโุชูุงูุฏ ุจุง ุนููฺฉุฑุฏ Qwen2.5-72B-Instruct ุฑูุงุจุช ฺฉูุฏ.

**ุฏุณุชุงูุฑุฏูุง ุจูุฑูโูุฑ:**
- ูุฏูโูุง ูพุงู Qwen3-MoE ุนููฺฉุฑุฏ ูุดุงุจู ุจุง ูุฏูโูุง ูพุงู ูุชุฑุงฺฉู Qwen2.5 ุฏุงุฑูุฏุ ุฏุฑ ุญุงู ฺฉู ุชููุง ุงุฒ 10ูช ูพุงุฑุงูุชุฑูุง ูุนุงู ุงุณุชูุงุฏู ูโฺฉููุฏ.
- ุตุฑููโุฌู ูุงุจู ุชูุฌู ุฏุฑ ูุฒููโูุง ุขููุฒุด ู ุงุณุชูุชุงุฌ ุฏุฑ ููุงุณู ุจุง ูุฏูโูุง ูุชุฑุงฺฉู.

**ูุงุจูุชโูุง ฺูุฏุฒุจุงูู:**
- ูุฏูโูุง Qwen3 ุงุฒ 119 ุฒุจุงู ู ฺฏูุด ูพุดุชุจุงู ูโฺฉููุฏ.
- ุนููฺฉุฑุฏ ูู ุฏุฑ ุฒูููโูุง ุฒุจุงู ู ูุฑููฺฏ ูุชููุน.

**ููุงุณ ุขููุฒุด:**
- Qwen3 ุชูุฑุจุงู ุฏู ุจุฑุงุจุฑ Qwen2.5 ุฏุงุฏู ุงุณุชูุงุฏู ูโฺฉูุฏุ ุจุง ุญุฏูุฏ 36 ุชุฑููู ุชูฺฉู ฺฉู 119 ุฒุจุงู ู ฺฏูุด ุฑุง ูพูุดุด ูโุฏูุฏุ ุฏุฑ ููุงุณู ุจุง 18 ุชุฑููู ุชูฺฉู Qwen2.5.

### ูุงุชุฑุณ ููุงุณู ูุฏูโูุง

| ุณุฑ ูุฏูโูุง | ูุญุฏูุฏู ูพุงุฑุงูุชุฑูุง | ุทูู ฺฉุงูุชฺฉุณุช | ููุงุท ููุช ฺฉูุฏ | ุจูุชุฑู ููุงุฑุฏ ุงุณุชูุงุฏู |
|------------|------------------|--------------|-----------------|-----------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | ุนููฺฉุฑุฏ ูุชุนุงุฏูุ ฺูุฏุฒุจุงูู | ฺฉุงุฑุจุฑุฏูุง ุนูููุ ุงุณุชูุฑุงุฑ ุฏุฑ ุชููุฏ |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | ุชููุฏ ฺฉุฏุ ุจุฑูุงููโููุณ | ุชูุณุนู ูุฑูโุงูุฒุงุฑุ ฺฉูฺฉ ุจู ฺฉุฏููุณ |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | ุงุณุชุฏูุงู ุฑุงุถ | ูพูุชูุฑูโูุง ุขููุฒุดุ ฺฉุงุฑุจุฑุฏูุง STEM |
| **Qwen2.5-VL** | ูุชููุน | ูุชุบุฑ | ุฏุฑฺฉ ุฒุจุงู-ุชุตูุฑ | ฺฉุงุฑุจุฑุฏูุง ฺูุฏูุฌูุ ุชุญูู ุชุตุงูุฑ |
| **Qwen3** | 0.6B-235B | ูุชุบุฑ | ุงุณุชุฏูุงู ูพุดุฑูุชูุ ุญุงูุช ุชูฺฉุฑ | ุงุณุชุฏูุงู ูพฺุฏูุ ฺฉุงุฑุจุฑุฏูุง ุชุญููุงุช |
| **Qwen3 MoE** | ูุฌููุนุงู 30B-235B | ูุชุบุฑ | ุนููฺฉุฑุฏ ฺฉุงุฑุขูุฏ ุฏุฑ ููุงุณ ุจุฒุฑฺฏ | ฺฉุงุฑุจุฑุฏูุง ุณุงุฒูุงูุ ูุงุฒูุง ุจุง ุนููฺฉุฑุฏ ุจุงูุง |

## ุฑุงูููุง ุงูุชุฎุงุจ ูุฏู

### ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ูพุงู
- **Qwen2.5-0.5B/1.5B**: ุงูพูฺฉุดูโูุง ููุจุงูุ ุฏุณุชฺฏุงูโูุง ูุจูโุงุ ฺฉุงุฑุจุฑุฏูุง ุจูุงุฏุฑูฺฏ
- **Qwen2.5-3B/7B**: ฺุชโุจุงุชโูุง ุนูููุ ุชููุฏ ูุญุชูุงุ ุณุณุชูโูุง ูพุฑุณุด ู ูพุงุณุฎ

### ุจุฑุง ูุธุงู ุฑุงุถ ู ุงุณุชุฏูุงู
- **Qwen2.5-Math**: ุญู ูุณุงุฆู ุฑุงุถ ู ุขููุฒุด STEM
- **Qwen3 ุจุง ุญุงูุช ุชูฺฉุฑ**: ุงุณุชุฏูุงู ูพฺุฏู ฺฉู ูุงุฒ ุจู ุชุญูู ูุฑุญูู ุจู ูุฑุญูู ุฏุงุฑุฏ

### ุจุฑุง ุจุฑูุงููโููุณ ู ุชูุณุนู
- **Qwen2.5-Coder**: ุชููุฏ ฺฉุฏุ ุงุดฺฉุงูโุฒุฏุงุ ฺฉูฺฉ ุจู ุจุฑูุงููโููุณ
- **Qwen3**: ูุธุงู ูพุดุฑูุชู ุจุฑูุงููโููุณ ุจุง ูุงุจูุชโูุง ุงุณุชุฏูุงู

### ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ฺูุฏูุฌู
- **Qwen2.5-VL**: ุฏุฑฺฉ ุชุตูุฑุ ูพุงุณุฎ ุจู ุณูุงูุงุช ุจุตุฑ
- **Qwen-Audio**: ูพุฑุฏุงุฒุด ุตูุช ู ุฏุฑฺฉ ฺฏูุชุงุฑ

### ุจุฑุง ุงุณุชูุฑุงุฑ ุณุงุฒูุงู
- **Qwen2.5-32B/72B**: ุฏุฑฺฉ ุฒุจุงู ุจุง ุนููฺฉุฑุฏ ุจุงูุง
- **Qwen3-235B-A22B**: ุญุฏุงฺฉุซุฑ ูุงุจูุช ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ูพุฑุชูุงุถุง

## ูพูุชูุฑูโูุง ุงุณุชูุฑุงุฑ ู ุฏุณุชุฑุณ
### ูพูุชูุฑูโูุง ุงุจุฑ
- **Hugging Face Hub**: ูุฎุฒู ุฌุงูุน ูุฏู ุจุง ูพุดุชุจุงู ุฌุงูุนู
- **ModelScope**: ูพูุชูุฑู ูุฏู ุนูโุจุงุจุง ุจุง ุงุจุฒุงุฑูุง ุจูููโุณุงุฒ
- **ุงุฑุงุฆูโุฏููุฏฺฏุงู ุงุจุฑ ูุฎุชูู**: ูพุดุชุจุงู ุงุฒ ุทุฑู ูพูุชูุฑูโูุง ุงุณุชุงูุฏุงุฑุฏ ML

### ฺุงุฑฺูุจโูุง ุชูุณุนู ูุญู
- **Transformers**: ฺฉูพุงุฑฺฺฏ ุงุณุชุงูุฏุงุฑุฏ Hugging Face ุจุฑุง ุงุณุชูุฑุงุฑ ุขุณุงู
- **vLLM**: ุณุฑูุณโุฏู ุจุง ุนููฺฉุฑุฏ ุจุงูุง ุจุฑุง ูุญุทโูุง ุชููุฏ
- **Ollama**: ุงุณุชูุฑุงุฑ ู ูุฏุฑุช ูุญู ุณุงุฏูโุดุฏู
- **ONNX Runtime**: ุจูููโุณุงุฒ ฺูุฏูพูุชูุฑู ุจุฑุง ุณุฎุชโุงูุฒุงุฑูุง ูุฎุชูู
- **llama.cpp**: ูพุงุฏูโุณุงุฒ ฺฉุงุฑุขูุฏ C++ ุจุฑุง ูพูุชูุฑูโูุง ูุชููุน

### ููุงุจุน ุขููุฒุด
- **ูุณุชูุฏุงุช Qwen**: ูุณุชูุฏุงุช ุฑุณู ู ฺฉุงุฑุชโูุง ูุฏู
- **Hugging Face Model Hub**: ุฏูููุง ุชุนุงูู ู ูุซุงูโูุง ุฌุงูุนู
- **ููุงูุงุช ุชุญููุงุช**: ููุงูุงุช ูู ุฏุฑ arxiv ุจุฑุง ุฏุฑฺฉ ุนูู
- **ูุฑููโูุง ุฌุงูุนู**: ูพุดุชุจุงู ูุนุงู ุฌุงูุนู ู ุจุญุซโูุง

### ุดุฑูุน ฺฉุงุฑ ุจุง ูุฏูโูุง Qwen

#### ูพูุชูุฑูโูุง ุชูุณุนู
1. **Hugging Face Transformers**: ุดุฑูุน ุจุง ฺฉูพุงุฑฺฺฏ ุงุณุชุงูุฏุงุฑุฏ ูพุงุชูู
2. **ModelScope**: ุงุจุฒุงุฑูุง ุงุณุชูุฑุงุฑ ุจููู ุนูโุจุงุจุง ุฑุง ุจุฑุฑุณ ฺฉูุฏ
3. **ุงุณุชูุฑุงุฑ ูุญู**: ุงุฒ Ollama ุง Transformers ูุณุชูู ุจุฑุง ุขุฒูุงุด ูุญู ุงุณุชูุงุฏู ฺฉูุฏ

#### ูุณุฑ ุงุฏฺฏุฑ
1. **ุฏุฑฺฉ ููุงูู ุงุตู**: ูุนูุงุฑ ุฎุงููุงุฏู Qwen ู ูุงุจูุชโูุง ุขู ุฑุง ูุทุงูุนู ฺฉูุฏ
2. **ุขุฒูุงุด ุจุง ุงููุงุน ูุฎุชูู**: ูุฏูโูุง ูุฎุชูู ุฑุง ุงูุชุญุงู ฺฉูุฏ ุชุง ุชูุงุฒู ุนููฺฉุฑุฏ ุฑุง ุฏุฑฺฉ ฺฉูุฏ
3. **ุชูุฑู ูพุงุฏูโุณุงุฒ**: ูุฏูโูุง ุฑุง ุฏุฑ ูุญุทโูุง ุชูุณุนู ูุณุชูุฑ ฺฉูุฏ
4. **ุจูููโุณุงุฒ ุงุณุชูุฑุงุฑ**: ุจุฑุง ููุงุฑุฏ ุงุณุชูุงุฏู ุชููุฏ ุชูุธู ฺฉูุฏ

#### ุจูุชุฑู ุฑูุดโูุง
- **ฺฉูฺฺฉ ุดุฑูุน ฺฉูุฏ**: ุจุง ูุฏูโูุง ฺฉูฺฺฉโุชุฑ (1.5B-7B) ุจุฑุง ุชูุณุนู ุงููู ุดุฑูุน ฺฉูุฏ
- **ุงุฒ ูุงูุจโูุง ฺุช ุงุณุชูุงุฏู ฺฉูุฏ**: ูุงูุจโุจูุฏ ููุงุณุจ ุฑุง ุจุฑุง ูุชุงุฌ ุจููู ุงุนูุงู ฺฉูุฏ
- **ููุงุจุน ุฑุง ูุธุงุฑุช ฺฉูุฏ**: ุงุณุชูุงุฏู ุงุฒ ุญุงูุธู ู ุณุฑุนุช ุงุณุชูุชุงุฌ ุฑุง ูพฺฏุฑ ฺฉูุฏ
- **ุชุฎุตุต ุฑุง ุฏุฑ ูุธุฑ ุจฺฏุฑุฏ**: ููฺฏุงู ูุงุฒุ ุงููุงุน ุฎุงุต ุฏุงููู ุฑุง ุงูุชุฎุงุจ ฺฉูุฏ

## ุงูฺฏููุง ุงุณุชูุงุฏู ูพุดุฑูุชู

### ูุซุงูโูุง ุชูุธู ุฏูู

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### ูููุฏุณ ูพุฑุงููพุช ุชุฎุตุต

**ุจุฑุง ูุธุงู ุงุณุชุฏูุงู ูพฺุฏู:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**ุจุฑุง ุชููุฏ ฺฉุฏ ุจุง ฺฉุงูุชฺฉุณุช:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### ฺฉุงุฑุจุฑุฏูุง ฺูุฏุฒุจุงูู

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ไธญๆ)",
        "es": "Spanish (Espaรฑol)",
        "fr": "French (Franรงais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (ๆฅๆฌ่ช)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ๐ง ุงูฺฏููุง ุงุณุชูุฑุงุฑ ุชููุฏ

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## ุงุณุชุฑุงุชฺโูุง ุจูููโุณุงุฒ ุนููฺฉุฑุฏ

### ุจูููโุณุงุฒ ุญุงูุธู

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### ุจูููโุณุงุฒ ุงุณุชูุชุงุฌ

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## ุจูุชุฑู ุฑูุดโูุง ู ุฏุณุชูุฑุงูุนููโูุง

### ุงููุช ู ุญุฑู ุฎุตูุต

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### ูุธุงุฑุช ู ุงุฑุฒุงุจ

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## ูุชุฌูโฺฏุฑ

ุฎุงููุงุฏู ูุฏูโูุง Qwen ููุงุงูฺฏุฑ ุฑูฺฉุฑุฏ ุฌุงูุน ุจุฑุง ุฏููฺฉุฑุงุชุฒู ฺฉุฑุฏู ููุงูุฑ ููุด ูุตููุน ุงุณุชุ ุฏุฑ ุญุงู ฺฉู ุนููฺฉุฑุฏ ุฑูุงุจุช ุฏุฑ ฺฉุงุฑุจุฑุฏูุง ูุชููุน ุฑุง ุญูุธ ูโฺฉูุฏ. ุจุง ุชุนูุฏ ุจู ุฏุณุชุฑุณ ููุจุน ุจุงุฒุ ูุงุจูุชโูุง ฺูุฏุฒุจุงูู ู ฺฏุฒููโูุง ุงุณุชูุฑุงุฑ ุงูุนุทุงูโูพุฐุฑุ Qwen ุจู ุณุงุฒูุงูโูุง ู ุชูุณุนูโุฏููุฏฺฏุงู ุงูฺฉุงู ูโุฏูุฏ ุชุง ุงุฒ ูุงุจูุชโูุง ูุฏุฑุชููุฏ ููุด ูุตููุน ุจูุฑูโููุฏ ุดููุฏุ ุตุฑูโูุธุฑ ุงุฒ ููุงุจุน ุง ูุงุฒูุง ุฎุงุต ุขูโูุง.

### ูฺฉุงุช ฺฉูุฏ

**ุจุฑุชุฑ ููุจุน ุจุงุฒ**: Qwen ูุดุงู ูโุฏูุฏ ฺฉู ูุฏูโูุง ููุจุน ุจุงุฒ ูโุชูุงููุฏ ุนููฺฉุฑุฏ ุฑูุงุจุช ุจุง ุฌุงฺฏุฒูโูุง ุงุฎุชุตุงุต ุฏุงุดุชู ุจุงุดูุฏุ ุฏุฑ ุญุงู ฺฉู ุดูุงูุชุ ุณูุงุฑุดโุณุงุฒ ู ฺฉูุชุฑู ุฑุง ูุฑุงูู ูโฺฉููุฏ.

**ูุนูุงุฑ ููุงุณโูพุฐุฑ**: ูุญุฏูุฏู 0.5B ุชุง 235B ูพุงุฑุงูุชุฑ ุงูฺฉุงู ุงุณุชูุฑุงุฑ ุฏุฑ ุทู ฺฉุงูู ูุญุทโูุง ูุญุงุณุจุงุชุ ุงุฒ ุฏุณุชฺฏุงูโูุง ููุจุงู ุชุง ุฎูุดูโูุง ุณุงุฒูุงู ุฑุง ูุฑุงูู ูโฺฉูุฏ.

**ูุงุจูุชโูุง ุชุฎุตุต**: ุงููุงุน ุฎุงุต ุฏุงููู ูุงููุฏ Qwen-Coderุ Qwen-Math ู Qwen-VL ุชุฎุตุต ูฺูโุง ุฑุง ุงุฑุงุฆู ูโุฏููุฏุ ุฏุฑ ุญุงู ฺฉู ุฏุฑฺฉ ุนููู ุฒุจุงู ุฑุง ุญูุธ ูโฺฉููุฏ.

**ุฏุณุชุฑุณ ุฌูุงู**: ูพุดุชุจุงู ฺูุฏุฒุจุงูู ูู ุฏุฑ ุจุด ุงุฒ 119 ุฒุจุงูุ Qwen ุฑุง ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ุจูโุงูููู ู ูพุงฺฏุงูโูุง ฺฉุงุฑุจุฑ ูุชููุน ููุงุณุจ ูโุณุงุฒุฏ.

**ููุขูุฑ ูุฏุงูู**: ุชฺฉุงูู ุงุฒ Qwen 1.0 ุจู Qwen3 ุจูุจูุฏ ูุฏุงูู ุฏุฑ ูุงุจูุชโูุงุ ุจูุฑูโูุฑ ู ฺฏุฒููโูุง ุงุณุชูุฑุงุฑ ุฑุง ูุดุงู ูโุฏูุฏ.

### ฺุดูโุงูุฏุงุฒ ุขูุฏู

ุจุง ุงุฏุงูู ุชฺฉุงูู ุฎุงููุงุฏู Qwenุ ูโุชูุงู ุงูุชุธุงุฑ ุฏุงุดุช:

- **ุจูุฑูโูุฑ ุจูุจูุฏ ุงูุชู**: ุจูููโุณุงุฒ ูุฏุงูู ุจุฑุง ูุณุจุชโูุง ุจูุชุฑ ุนููฺฉุฑุฏ ุจู ูพุงุฑุงูุชุฑ
- **ูุงุจูุชโูุง ฺูุฏูุฌู ฺฏุณุชุฑุดโุงูุชู**: ุงุฏุบุงู ูพุฑุฏุงุฒุด ูพฺุฏูโุชุฑ ุชุตูุฑุ ุตูุช ู ูุชู
- **ุงุณุชุฏูุงู ุจูุจูุฏ ุงูุชู**: ูฺฉุงูุฒูโูุง ุชูฺฉุฑ ูพุดุฑูุชู ู ูุงุจูุชโูุง ุญู ูุณุฆูู ฺูุฏูุฑุญููโุง
- **ุงุจุฒุงุฑูุง ุงุณุชูุฑุงุฑ ุจูุชุฑ**: ฺุงุฑฺูุจโูุง ู ุงุจุฒุงุฑูุง ุจูููโุณุงุฒ ูพุดุฑูุชู ุจุฑุง ุณูุงุฑููุง ุงุณุชูุฑุงุฑ ูุชููุน
- **ุฑุดุฏ ุฌุงูุนู**: ฺฏุณุชุฑุด ุงฺฉูุณุณุชู ุงุจุฒุงุฑูุงุ ฺฉุงุฑุจุฑุฏูุง ู ูุดุงุฑฺฉุชโูุง ุฌุงูุนู

### ูุฑุงุญู ุจุนุฏ

ฺู ุฏุฑ ุญุงู ุณุงุฎุช ฺุชโุจุงุช ุจุงุดุฏุ ฺู ุงุจุฒุงุฑูุง ุขููุฒุด ุชูุณุนู ุฏูุฏุ ฺู ุฏุณุชุงุฑูุง ฺฉุฏููุณ ุงุฌุงุฏ ฺฉูุฏ ุง ุฑู ฺฉุงุฑุจุฑุฏูุง ฺูุฏุฒุจุงูู ฺฉุงุฑ ฺฉูุฏุ ุฎุงููุงุฏู Qwen ุฑุงูโุญูโูุง ููุงุณโูพุฐุฑ ุจุง ูพุดุชุจุงู ูู ุฌุงูุนู ู ูุณุชูุฏุงุช ุฌุงูุน ุงุฑุงุฆู ูโุฏูุฏ.

ุจุฑุง ุขุฎุฑู ุจูโุฑูุฒุฑุณุงูโูุงุ ุงูุชุดุงุฑ ูุฏูโูุง ู ูุณุชูุฏุงุช ูู ุฏููุ ุจู ูุฎุงุฒู ุฑุณู Qwen ุฏุฑ Hugging Face ูุฑุงุฌุนู ฺฉูุฏ ู ุจุญุซโูุง ู ูุซุงูโูุง ูุนุงู ุฌุงูุนู ุฑุง ุจุฑุฑุณ ฺฉูุฏ.

ุขูุฏู ุชูุณุนู ููุด ูุตููุน ุฏุฑ ุงุจุฒุงุฑูุง ูุงุจู ุฏุณุชุฑุณุ ุดูุงู ู ูุฏุฑุชููุฏ ูููุชู ุงุณุช ฺฉู ููุขูุฑ ุฑุง ุฏุฑ ุชูุงู ุจุฎุดโูุง ู ููุงุณโูุง ุงูฺฉุงูโูพุฐุฑ ูโุณุงุฒุฏ. ุฎุงููุงุฏู Qwen ุงู ฺุดูโุงูุฏุงุฒ ุฑุง ุจู ููุงุด ูโฺฏุฐุงุฑุฏ ู ุจู ุณุงุฒูุงูโูุง ู ุชูุณุนูโุฏููุฏฺฏุงู ูพุงูโุง ุจุฑุง ุณุงุฎุช ูุณู ุจุนุฏ ฺฉุงุฑุจุฑุฏูุง ูุจุชู ุจุฑ ููุด ูุตููุน ุงุฑุงุฆู ูโุฏูุฏ.

## ููุงุจุน ุงุถุงู

- **ูุณุชูุฏุงุช ุฑุณู**: [ูุณุชูุฏุงุช Qwen](https://qwen.readthedocs.io/)
- **ูุฑฺฉุฒ ูุฏู**: [ูุฌููุนูโูุง Qwen ุฏุฑ Hugging Face](https://huggingface.co/collections/Qwen/)
- **ููุงูุงุช ูู**: [ุงูุชุดุงุฑุงุช ุชุญููุงุช Qwen](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **ุฌุงูุนู**: [ุจุญุซโูุง ู ูุดฺฉูุงุช GitHub](https://github.com/QwenLM/)
- **ูพูุชูุฑู ModelScope**: [ModelScope ุนูโุจุงุจุง](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## ูุชุงุฌ ุงุฏฺฏุฑ

ูพุณ ุงุฒ ุชฺฉูู ุงู ูุงฺููุ ุดูุง ูุงุฏุฑ ุฎูุงูุฏ ุจูุฏ:

1. ูุฒุงุง ูุนูุงุฑ ุฎุงููุงุฏู ูุฏูโูุง Qwen ู ุฑูฺฉุฑุฏ ููุจุน ุจุงุฒ ุขู ุฑุง ุชูุถุญ ุฏูุฏ.
2. ููุน ููุงุณุจ Qwen ุฑุง ุจุฑ ุงุณุงุณ ูุงุฒูุง ฺฉุงุฑุจุฑุฏ ุฎุงุต ู ูุญุฏูุฏุชโูุง ููุงุจุน ุงูุชุฎุงุจ ฺฉูุฏ.
3. ูุฏูโูุง Qwen ุฑุง ุฏุฑ ุณูุงุฑููุง ูุฎุชูู ุงุณุชูุฑุงุฑ ุจุง ูพฺฉุฑุจูุฏโูุง ุจููู ูพุงุฏูโุณุงุฒ ฺฉูุฏ.
4. ุชฺฉูฺฉโูุง ฺฉูุงูุชุฒุงุณูู ู ุจูููโุณุงุฒ ุฑุง ุจุฑุง ุจูุจูุฏ ุนููฺฉุฑุฏ ูุฏูโูุง Qwen ุงุนูุงู ฺฉูุฏ.
5. ุชูุงุฒู ุจู ุงูุฏุงุฒู ูุฏูุ ุนููฺฉุฑุฏ ู ูุงุจูุชโูุง ุฑุง ุฏุฑ ุณุฑุงุณุฑ ุฎุงููุงุฏู Qwen ุงุฑุฒุงุจ ฺฉูุฏ.

## ฺู ฺุฒ ุฏุฑ ุงุฏุงูู ุงุณุช

- [03: ุงุตูู ุฎุงููุงุฏู Gemma](03.GemmaFamily.md)

---

**ุณูุจ ูุณุฆููุช**:  
ุงู ุณูุฏ ุจุง ุงุณุชูุงุฏู ุงุฒ ุณุฑูุณ ุชุฑุฌูู ููุด ูุตููุน [Co-op Translator](https://github.com/Azure/co-op-translator) ุชุฑุฌูู ุดุฏู ุงุณุช. ุฏุฑ ุญุงู ฺฉู ูุง ุชูุงุด ูโฺฉูู ุฏูุช ุฑุง ุญูุธ ฺฉููุ ูุทูุงู ุชูุฌู ุฏุงุดุชู ุจุงุดุฏ ฺฉู ุชุฑุฌููโูุง ุฎูุฏฺฉุงุฑ ููฺฉู ุงุณุช ุดุงูู ุฎุทุงูุง ุง ูุงุฏุฑุณุชโูุง ุจุงุดูุฏ. ุณูุฏ ุงุตู ุจู ุฒุจุงู ุงุตู ุขู ุจุงุฏ ุจู ุนููุงู ููุจุน ูุนุชุจุฑ ุฏุฑ ูุธุฑ ฺฏุฑูุชู ุดูุฏ. ุจุฑุง ุงุทูุงุนุงุช ุญุณุงุณุ ุชุฑุฌูู ุญุฑููโุง ุงูุณุงู ุชูุตู ูโุดูุฏ. ูุง ูุณุฆููุช ุฏุฑ ูุจุงู ุณูุก ุชูุงููโูุง ุง ุชูุณุฑูุง ูุงุฏุฑุณุช ูุงุด ุงุฒ ุงุณุชูุงุฏู ุงุฒ ุงู ุชุฑุฌูู ูุฏุงุฑู.