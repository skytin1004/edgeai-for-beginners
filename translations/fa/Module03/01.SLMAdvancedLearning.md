<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "27be883865b4bad1e3c7e02c696da642",
  "translation_date": "2025-09-17T16:01:24+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "fa"
}
-->
# ุจุฎุด ฑ: ุงุฏฺฏุฑ ูพุดุฑูุชู SLM - ูุจุงู ู ุจูููโุณุงุฒ

ูุฏูโูุง ุฒุจุงู ฺฉูฺฺฉ (SLM) ฺฉ ูพุดุฑูุช ููู ุฏุฑ EdgeAI ูุณุชูุฏ ฺฉู ุงูฺฉุงู ูพุฑุฏุงุฒุด ูพุดุฑูุชู ุฒุจุงู ุทุจุน ุฑุง ุฏุฑ ุฏุณุชฺฏุงูโูุง ุจุง ููุงุจุน ูุญุฏูุฏ ูุฑุงูู ูโฺฉููุฏ. ุฏุฑฺฉ ูุญูู ุงุณุชูุฑุงุฑุ ุจูููโุณุงุฒ ู ุงุณุชูุงุฏู ูุคุซุฑ ุงุฒ SLMโูุง ุจุฑุง ุณุงุฎุช ุฑุงูโุญูโูุง ุนูู ูุจุชู ุจุฑ ููุด ูุตููุน ุฏุฑ ูุจู ุถุฑูุฑ ุงุณุช.

## ููุฏูู

ุฏุฑ ุงู ุฏุฑุณุ ูุฏูโูุง ุฒุจุงู ฺฉูฺฺฉ (SLM) ู ุงุณุชุฑุงุชฺโูุง ูพุดุฑูุชู ูพุงุฏูโุณุงุฒ ุขูโูุง ุฑุง ุจุฑุฑุณ ุฎูุงูู ฺฉุฑุฏ. ููุงูู ุจูุงุฏ SLMโูุงุ ูุฑุฒูุง ูพุงุฑุงูุชุฑ ู ุทุจููโุจูุฏ ุขูโูุงุ ุชฺฉูฺฉโูุง ุจูููโุณุงุฒ ู ุงุณุชุฑุงุชฺโูุง ุนูู ุงุณุชูุฑุงุฑ ุฏุฑ ูุญุทโูุง ูุญุงุณุจุงุช ูุจู ุฑุง ูพูุดุด ุฎูุงูู ุฏุงุฏ.

## ุงูุฏุงู ุงุฏฺฏุฑ

ุฏุฑ ูพุงุงู ุงู ุฏุฑุณุ ุดูุง ูุงุฏุฑ ุฎูุงูุฏ ุจูุฏ:

- ๐ข ุฏุฑฺฉ ูุฑุฒูุง ูพุงุฑุงูุชุฑ ู ุทุจููโุจูุฏ ูุฏูโูุง ุฒุจุงู ฺฉูฺฺฉ.
- ๐๏ธ ุดูุงุณุง ุชฺฉูฺฉโูุง ฺฉูุฏ ุจูููโุณุงุฒ ุจุฑุง ุงุณุชูุฑุงุฑ SLMโูุง ุฏุฑ ุฏุณุชฺฏุงูโูุง ูุจู.
- ๐ ุงุฏฺฏุฑ ูพุงุฏูโุณุงุฒ ุงุณุชุฑุงุชฺโูุง ูพุดุฑูุชู ฺฉูุงูุชุฒุงุณูู ู ูุดุฑุฏูโุณุงุฒ ุจุฑุง SLMโูุง.

## ุฏุฑฺฉ ูุฑุฒูุง ูพุงุฑุงูุชุฑ ู ุทุจููโุจูุฏ SLMโูุง

ูุฏูโูุง ุฒุจุงู ฺฉูฺฺฉ (SLM) ูุฏูโูุง ููุด ูุตููุน ูุณุชูุฏ ฺฉู ุจุฑุง ูพุฑุฏุงุฒุดุ ุฏุฑฺฉ ู ุชููุฏ ูุญุชูุง ุฒุจุงู ุทุจุน ุจุง ุชุนุฏุงุฏ ูพุงุฑุงูุชุฑูุง ุจูโูุฑุงุชุจ ฺฉูุชุฑ ูุณุจุช ุจู ูุฏูโูุง ุจุฒุฑฺฏ ุทุฑุงุญ ุดุฏูโุงูุฏ. ุฏุฑ ุญุงู ฺฉู ูุฏูโูุง ุฒุจุงู ุจุฒุฑฺฏ (LLM) ุดุงูู ุตุฏูุง ููุงุฑุฏ ุชุง ุชุฑููู ูพุงุฑุงูุชุฑ ูุณุชูุฏุ SLMโูุง ุจู ุทูุฑ ุฎุงุต ุจุฑุง ฺฉุงุฑุง ู ุงุณุชูุฑุงุฑ ุฏุฑ ูุจู ุทุฑุงุญ ุดุฏูโุงูุฏ.

ฺุงุฑฺูุจ ุทุจููโุจูุฏ ูพุงุฑุงูุชุฑ ุจู ูุง ฺฉูฺฉ ูโฺฉูุฏ ุชุง ุฏุณุชูโูุง ูุฎุชูู SLMโูุง ู ููุงุฑุฏ ุงุณุชูุงุฏู ููุงุณุจ ุขูโูุง ุฑุง ุฏุฑฺฉ ฺฉูู. ุงู ุทุจููโุจูุฏ ุจุฑุง ุงูุชุฎุงุจ ูุฏู ููุงุณุจ ุจุฑุง ุณูุงุฑููุง ุฎุงุต ูุญุงุณุจุงุช ูุจู ุจุณุงุฑ ููู ุงุณุช.

### ฺุงุฑฺูุจ ุทุจููโุจูุฏ ูพุงุฑุงูุชุฑ

ุฏุฑฺฉ ูุฑุฒูุง ูพุงุฑุงูุชุฑ ุจู ุงูุชุฎุงุจ ูุฏูโูุง ููุงุณุจ ุจุฑุง ุณูุงุฑููุง ูุฎุชูู ูุญุงุณุจุงุช ูุจู ฺฉูฺฉ ูโฺฉูุฏ:

- **๐ฌ ูฺฉุฑู SLMโูุง**: ฑฐฐ ูููู - ฑ.ด ููุงุฑุฏ ูพุงุฑุงูุชุฑ (ููู ุณุจฺฉ ุจุฑุง ุฏุณุชฺฏุงูโูุง ููุจุงู)
- **๐ฑ SLMโูุง ฺฉูฺฺฉ**: ฑ.ต ููุงุฑุฏ - ฑณ.น ููุงุฑุฏ ูพุงุฑุงูุชุฑ (ุชุนุงุฏู ุจู ุนููฺฉุฑุฏ ู ฺฉุงุฑุง)
- **โ๏ธ SLMโูุง ูุชูุณุท**: ฑด ููุงุฑุฏ - ณฐ ููุงุฑุฏ ูพุงุฑุงูุชุฑ (ูุฒุฏฺฉ ุจู ูุงุจูุชโูุง LLM ุฏุฑ ุญุงู ฺฉู ฺฉุงุฑุง ุญูุธ ูโุดูุฏ)

ูุฑุฒ ุฏูู ุฏุฑ ุฌุงูุนู ุชุญููุงุช ุณุงู ุจุงู ูโูุงูุฏุ ุงูุง ุงฺฉุซุฑ ูุชุฎุตุตุงู ูุฏูโูุง ุจุง ฺฉูุชุฑ ุงุฒ ณฐ ููุงุฑุฏ ูพุงุฑุงูุชุฑ ุฑุง "ฺฉูฺฺฉ" ุฏุฑ ูุธุฑ ูโฺฏุฑูุฏุ ุจุง ุจุฑุฎ ููุงุจุน ฺฉู ุขุณุชุงูู ุฑุง ุญุช ูพุงูโุชุฑุ ุฏุฑ ฑฐ ููุงุฑุฏ ูพุงุฑุงูุชุฑ ุชุนู ูโฺฉููุฏ.

### ูุฒุงุง ฺฉูุฏ SLMโูุง

SLMโูุง ฺูุฏู ูุฒุช ุจูุงุฏ ุงุฑุงุฆู ูโุฏููุฏ ฺฉู ุขูโูุง ุฑุง ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ูุญุงุณุจุงุช ูุจู ุงุฏูโุขู ูโฺฉูุฏ:

**ฺฉุงุฑุง ุนููุงุช**: SLMโูุง ุฒูุงูโูุง ุงุณุชูุชุงุฌ ุณุฑุนโุชุฑ ุฑุง ุจู ุฏูู ูพุฑุฏุงุฒุด ูพุงุฑุงูุชุฑูุง ฺฉูุชุฑ ุงุฑุงุฆู ูโุฏููุฏุ ฺฉู ุขูโูุง ุฑุง ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ุจูุงุฏุฑูฺฏ ุงุฏูโุขู ูโฺฉูุฏ. ุขูโูุง ุจู ููุงุจุน ูุญุงุณุจุงุช ฺฉูุชุฑ ูุงุฒ ุฏุงุฑูุฏุ ฺฉู ุงูฺฉุงู ุงุณุชูุฑุงุฑ ุฏุฑ ุฏุณุชฺฏุงูโูุง ุจุง ููุงุจุน ูุญุฏูุฏ ุฑุง ูุฑุงูู ูโฺฉูุฏุ ุฏุฑ ุญุงู ฺฉู ูุตุฑู ุงูุฑฺ ฺฉูุชุฑ ู ุฑุฏูพุง ฺฉุฑุจู ฺฉุงูุดโุงูุชู ุฑุง ุญูุธ ูโฺฉููุฏ.

**ุงูุนุทุงูโูพุฐุฑ ุงุณุชูุฑุงุฑ**: ุงู ูุฏูโูุง ูุงุจูุชโูุง ููุด ูุตููุน ุฑู ุฏุณุชฺฏุงู ุฑุง ุจุฏูู ูุงุฒ ุจู ุงุชุตุงู ุงูุชุฑูุช ูุฑุงูู ูโฺฉููุฏุ ุงุฒ ุทุฑู ูพุฑุฏุงุฒุด ูุญู ุญุฑู ุฎุตูุต ู ุงููุช ุฑุง ุงูุฒุงุด ูโุฏููุฏุ ูโุชูุงููุฏ ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ุฎุงุต ุญูุฒู ุณูุงุฑุด ุดููุฏ ู ุจุฑุง ูุญุทโูุง ูุฎุชูู ูุญุงุณุจุงุช ูุจู ููุงุณุจ ูุณุชูุฏ.

**ููุฑููโุจูโุตุฑูู ุจูุฏู**: SLMโูุง ุขููุฒุด ู ุงุณุชูุฑุงุฑ ููุฑููโุจูโุตุฑููโุชุฑ ูุณุจุช ุจู LLMโูุง ุงุฑุงุฆู ูโุฏููุฏุ ุจุง ฺฉุงูุด ูุฒููโูุง ุนููุงุช ู ูุงุฒูุง ูพููุง ุจุงูุฏ ฺฉูุชุฑ ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ูุจู.

## ุงุณุชุฑุงุชฺโูุง ูพุดุฑูุชู ุฏุณุชุงุจ ุจู ูุฏู

### ุงฺฉูุณุณุชู Hugging Face

Hugging Face ุจู ุนููุงู ูุฑฺฉุฒ ุงุตู ุจุฑุง ฺฉุดู ู ุฏุณุชุฑุณ ุจู SLMโูุง ูพุดุฑูุชู ุนูู ูโฺฉูุฏ. ุงู ูพูุชูุฑู ููุงุจุน ุฌุงูุน ุจุฑุง ฺฉุดู ู ุงุณุชูุฑุงุฑ ูุฏู ุงุฑุงุฆู ูโุฏูุฏ:

**ูฺฺฏโูุง ฺฉุดู ูุฏู**: ูพูุชูุฑู ููุชุฑูุง ูพุดุฑูุชู ุจุฑ ุงุณุงุณ ุชุนุฏุงุฏ ูพุงุฑุงูุชุฑุ ููุน ูุฌูุฒ ู ูุนุงุฑูุง ุนููฺฉุฑุฏ ุงุฑุงุฆู ูโุฏูุฏ. ฺฉุงุฑุจุฑุงู ูโุชูุงููุฏ ุจู ุงุจุฒุงุฑูุง ููุงุณู ูุฏู ฺฉูุงุฑ ููุ ูุนุงุฑูุง ุนููฺฉุฑุฏ ุจูุงุฏุฑูฺฏ ู ูุชุงุฌ ุงุฑุฒุงุจุ ู ุฏูููุง WebGPU ุจุฑุง ุขุฒูุงุด ููุฑ ุฏุณุชุฑุณ ุฏุงุดุชู ุจุงุดูุฏ.

**ูุฌููุนูโูุง SLM ููุชุฎุจ**: ูุฏูโูุง ูุญุจูุจ ุดุงูู Phi-4-mini-3.8B ุจุฑุง ูุธุงู ุงุณุชุฏูุงู ูพุดุฑูุชูุ ุณุฑ Qwen3 (0.6B/1.7B/4B) ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ฺูุฏุฒุจุงููุ Google Gemma3 ุจุฑุง ูุธุงู ุนููู ฺฉุงุฑุขูุฏุ ู ูุฏูโูุง ุขุฒูุงุด ูุงููุฏ BitNET ุจุฑุง ุงุณุชูุฑุงุฑ ูููโุงูุนุงุฏู ฺฉูโุฏูุช ูุณุชูุฏ. ูพูุชูุฑู ููฺูู ูุฌููุนูโูุง ูุจุชู ุจุฑ ุฌุงูุนู ุจุง ูุฏูโูุง ุชุฎุตุต ุจุฑุง ุญูุฒูโูุง ุฎุงุต ู ุงููุงุน ูพุดโุขููุฒุด ู ุชูุธู ุฏุณุชูุฑุงูุนูู ุจูููโุดุฏู ุจุฑุง ููุงุฑุฏ ุงุณุชูุงุฏู ูุฎุชูู ุฑุง ุงุฑุงุฆู ูโุฏูุฏ.

### ฺฉุงุชุงููฺฏ ูุฏู Azure AI Foundry

ฺฉุงุชุงููฺฏ ูุฏู Azure AI Foundry ุฏุณุชุฑุณ ุฏุฑ ุณุทุญ ุณุงุฒูุงู ุจู SLMโูุง ุจุง ูุงุจูุชโูุง ฺฉูพุงุฑฺูโุณุงุฒ ูพุดุฑูุชู ุฑุง ูุฑุงูู ูโฺฉูุฏ:

**ฺฉูพุงุฑฺูโุณุงุฒ ุณุงุฒูุงู**: ฺฉุงุชุงููฺฏ ุดุงูู ูุฏูโูุง ุงุณุช ฺฉู ูุณุชููุงู ุชูุณุท Azure ุจุง ูพุดุชุจุงู ุฏุฑ ุณุทุญ ุณุงุฒูุงู ู SLAโูุง ูุฑูุฎุชู ูโุดููุฏุ ุงุฒ ุฌููู Phi-4-mini-3.8B ุจุฑุง ูุงุจูุชโูุง ุงุณุชุฏูุงู ูพุดุฑูุชู ู Llama 3-8B ุจุฑุง ุงุณุชูุฑุงุฑ ุชููุฏ. ููฺูู ูุฏูโูุง ูุงููุฏ Qwen3 8B ุงุฒ ูุฏูโูุง ููุจุน ุจุงุฒ ูุนุชุจุฑ ุดุฎุต ุซุงูุซ ุฑุง ุดุงูู ูโุดูุฏ.

**ูุฒุงุง ุณุงุฒูุงู**: ุงุจุฒุงุฑูุง ุฏุงุฎู ุจุฑุง ุชูุธู ุฏููุ ูุดุงูุฏูโูพุฐุฑ ู ููุด ูุตููุน ูุณุฆููุงูู ุจุง ุชูุงู ุนููุงุช ูุงุจู ุชุฎุตุต ุฏุฑ ูุงู ุฎุงููุงุฏูโูุง ูุฏู ฺฉูพุงุฑฺู ุดุฏูโุงูุฏ. ูพุดุชุจุงู ูุณุชูู ูุงฺฉุฑูุณุงูุช ุจุง SLAโูุง ุณุงุฒูุงูุ ูฺฺฏโูุง ุงููุช ู ุงูุทุจุงู ฺฉูพุงุฑฺูุ ู ฺฏุฑุฏุด ฺฉุงุฑูุง ุฌุงูุน ุงุณุชูุฑุงุฑ ุชุฌุฑุจู ุณุงุฒูุงู ุฑุง ุจูุจูุฏ ูโุจุฎุดูุฏ.

## ุชฺฉูฺฉโูุง ูพุดุฑูุชู ฺฉูุงูุชุฒุงุณูู ู ุจูููโุณุงุฒ

### ฺุงุฑฺูุจ ุจูููโุณุงุฒ Llama.cpp

Llama.cpp ุชฺฉูฺฉโูุง ฺฉูุงูุชุฒุงุณูู ูพุดุฑูุชู ุจุฑุง ุญุฏุงฺฉุซุฑ ฺฉุงุฑุง ุฏุฑ ุงุณุชูุฑุงุฑ ูุจู ุงุฑุงุฆู ูโุฏูุฏ:

**ุฑูุดโูุง ฺฉูุงูุชุฒุงุณูู**: ฺุงุฑฺูุจ ุงุฒ ุณุทูุญ ูุฎุชูู ฺฉูุงูุชุฒุงุณูู ูพุดุชุจุงู ูโฺฉูุฏุ ุงุฒ ุฌููู Q4_0 (ฺฉูุงูุชุฒุงุณูู ด ุจุช ุจุง ฺฉุงูุด ุงูุฏุงุฒู ุนุงู - ุงุฏูโุขู ุจุฑุง ุงุณุชูุฑุงุฑ ููุจุงู Qwen3-0.6B)ุ Q5_1 (ฺฉูุงูุชุฒุงุณูู ต ุจุช ุจุง ุชุนุงุฏู ฺฉูุช ู ูุดุฑุฏูโุณุงุฒ - ููุงุณุจ ุจุฑุง ุงุณุชูุชุงุฌ ูุจู Phi-4-mini-3.8B)ุ ู Q8_0 (ฺฉูุงูุชุฒุงุณูู ธ ุจุช ุจุฑุง ฺฉูุช ูุฒุฏฺฉ ุจู ุงุตู - ุชูุตูโุดุฏู ุจุฑุง ุงุณุชูุงุฏู ุชููุฏ Google Gemma3). BitNET ููุงุงูฺฏุฑ ูพุดุฑูุชูโุชุฑู ุณุทุญ ุจุง ฺฉูุงูุชุฒุงุณูู ฑ ุจุช ุจุฑุง ุณูุงุฑููุง ูุดุฑุฏูโุณุงุฒ ุดุฏุฏ ุงุณุช.

**ูุฒุงุง ูพุงุฏูโุณุงุฒ**: ุงุณุชูุชุงุฌ ุจูููโุดุฏู ุจุฑุง CPU ุจุง ุดุชุงุจ SIMD ุจุงุฑฺฏุฐุงุฑ ู ุงุฌุฑุง ูุฏู ุฑุง ุจููู ูโฺฉูุฏ. ุณุงุฒฺฏุงุฑ ฺูุฏูพูุชูุฑู ุฏุฑ ูุนูุงุฑโูุง x86ุ ARM ู Apple Silicon ูุงุจูุชโูุง ุงุณุชูุฑุงุฑ ูุณุชูู ุงุฒ ุณุฎุชโุงูุฒุงุฑ ุฑุง ูุฑุงูู ูโฺฉูุฏ.

**ูุซุงู ูพุงุฏูโุณุงุฒ ุนูู**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**ููุงุณู ุฑุฏูพุง ุญุงูุธู**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### ูุฌููุนู ุจูููโุณุงุฒ Microsoft Olive

Microsoft Olive ฺฏุฑุฏุด ฺฉุงุฑูุง ุฌุงูุน ุจูููโุณุงุฒ ูุฏู ุทุฑุงุญโุดุฏู ุจุฑุง ูุญุทโูุง ุชููุฏ ุงุฑุงุฆู ูโุฏูุฏ:

**ุชฺฉูฺฉโูุง ุจูููโุณุงุฒ**: ุงู ูุฌููุนู ุดุงูู ฺฉูุงูุชุฒุงุณูู ูพูุง ุจุฑุง ุงูุชุฎุงุจ ุฎูุฏฺฉุงุฑ ุฏูุช (ุจูโูฺู ูุคุซุฑ ุจุง ูุฏูโูุง ุณุฑ Qwen3)ุ ุจูููโุณุงุฒ ฺฏุฑุงู ู ุชุฑฺฉุจ ุงูพุฑุงุชูุฑ (ุจูููโุดุฏู ุจุฑุง ูุนูุงุฑ Google Gemma3)ุ ุจูููโุณุงุฒโูุง ุฎุงุต ุณุฎุชโุงูุฒุงุฑ ุจุฑุง CPUุ GPU ู NPU (ุจุง ูพุดุชุจุงู ูฺู ุงุฒ Phi-4-mini-3.8B ุฏุฑ ุฏุณุชฺฏุงูโูุง ARM)ุ ู ุฎุทูุท ูููู ุจูููโุณุงุฒ ฺูุฏูุฑุญููโุง ุงุณุช. ูุฏูโูุง BitNET ูุงุฒ ุจู ฺฏุฑุฏุด ฺฉุงุฑูุง ฺฉูุงูุชุฒุงุณูู ฑ ุจุช ุชุฎุตุต ุฏุฑ ฺุงุฑฺูุจ Olive ุฏุงุฑูุฏ.

**ุงุชููุงุณูู ฺฏุฑุฏุด ฺฉุงุฑ**: ูุนุงุฑฺฏุฐุงุฑ ุฎูุฏฺฉุงุฑ ุฏุฑ ูุงู ุงููุงุน ุจูููโุณุงุฒ ฺฉูุช ูุนุงุฑูุง ุฑุง ุฏุฑ ุทูู ุจูููโุณุงุฒ ุญูุธ ูโฺฉูุฏ. ฺฉูพุงุฑฺูโุณุงุฒ ุจุง ฺุงุฑฺูุจโูุง ูุญุจูุจ ML ูุงููุฏ PyTorch ู ONNX ูุงุจูุชโูุง ุจูููโุณุงุฒ ุงุณุชูุฑุงุฑ ุฏุฑ ุงุจุฑ ู ูุจู ุฑุง ูุฑุงูู ูโฺฉูุฏ.

**ูุซุงู ูพุงุฏูโุณุงุฒ ุนูู**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### ฺุงุฑฺูุจ Apple MLX

Apple MLX ุจูููโุณุงุฒ ุจูู ุทุฑุงุญโุดุฏู ุจูโุทูุฑ ุฎุงุต ุจุฑุง ุฏุณุชฺฏุงูโูุง Apple Silicon ุงุฑุงุฆู ูโุฏูุฏ:

**ุจูููโุณุงุฒ Apple Silicon**: ฺุงุฑฺูุจ ุงุฒ ูุนูุงุฑ ุญุงูุธู ฺฉูพุงุฑฺู ุจุง ฺฉูพุงุฑฺูโุณุงุฒ Metal Performance Shadersุ ุงุณุชูุชุงุฌ ุฏูุช ูุฎุชูุท ุฎูุฏฺฉุงุฑ (ุจูโูฺู ูุคุซุฑ ุจุง Google Gemma3)ุ ู ุงุณุชูุงุฏู ุจููู ุงุฒ ูพููุง ุจุงูุฏ ุญุงูุธู ุงุณุชูุงุฏู ูโฺฉูุฏ. Phi-4-mini-3.8B ุนููฺฉุฑุฏ ุงุณุชุซูุง ุฏุฑ ุชุฑุงุดูโูุง ุณุฑ M ูุดุงู ูโุฏูุฏุ ุฏุฑ ุญุงู ฺฉู Qwen3-1.7B ุชุนุงุฏู ูุทููุจ ุจุฑุง ุงุณุชูุฑุงุฑ ุฏุฑ MacBook Air ูุฑุงูู ูโฺฉูุฏ.

**ูฺฺฏโูุง ุชูุณุนู**: ูพุดุชุจุงู ุงุฒ APIโูุง Python ู Swift ุจุง ุนููุงุช ุขุฑุงู ุณุงุฒฺฏุงุฑ ุจุง NumPyุ ูุงุจูุชโูุง ุชูฺฉฺฉ ุฎูุฏฺฉุงุฑุ ู ฺฉูพุงุฑฺูโุณุงุฒ ุจโุฏุฑุฏุณุฑ ุจุง ุงุจุฒุงุฑูุง ุชูุณุนู Apple ูุญุท ุชูุณุนู ุฌุงูุน ุฑุง ูุฑุงูู ูโฺฉูุฏ.

**ูุซุงู ูพุงุฏูโุณุงุฒ ุนูู**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## ุงุณุชุฑุงุชฺโูุง ุงุณุชูุฑุงุฑ ุชููุฏ ู ุงุณุชูุชุงุฌ

### Ollama: ุงุณุชูุฑุงุฑ ูุญู ุณุงุฏูโุดุฏู

Ollama ุงุณุชูุฑุงุฑ SLM ุฑุง ุจุง ูฺฺฏโูุง ุขูุงุฏู ุณุงุฒูุงู ุจุฑุง ูุญุทโูุง ูุญู ู ูุจู ุณุงุฏู ูโฺฉูุฏ:

**ูุงุจูุชโูุง ุงุณุชูุฑุงุฑ**: ูุตุจ ู ุงุฌุฑุง ูุฏู ุจุง ฺฉ ูุฑูุงู ุจุง ฺฉุดุฏู ู ุฐุฎุฑู ุฎูุฏฺฉุงุฑ ูุฏู. ูพุดุชุจุงู ุงุฒ Phi-4-mini-3.8Bุ ฺฉู ุณุฑ Qwen3 (0.6B/1.7B/4B)ุ ู Google Gemma3 ุจุง REST API ุจุฑุง ฺฉูพุงุฑฺูโุณุงุฒ ุจุฑูุงูู ู ูุงุจูุชโูุง ูุฏุฑุช ู ุชุบุฑ ูุฏู ฺูุฏฺฏุงูู. ูุฏูโูุง BitNET ูุงุฒ ุจู ูพฺฉุฑุจูุฏโูุง ุณุงุฎุช ุขุฒูุงุด ุจุฑุง ูพุดุชุจุงู ุงุฒ ฺฉูุงูุชุฒุงุณูู ฑ ุจุช ุฏุงุฑูุฏ.

**ูฺฺฏโูุง ูพุดุฑูุชู**: ูพุดุชุจุงู ุงุฒ ุชูุธู ุฏูู ูุฏู ุณูุงุฑุดุ ุชููุฏ Dockerfile ุจุฑุง ุงุณุชูุฑุงุฑ ฺฉุงูุชูุฑุ ุดุชุงุจ GPU ุจุง ุชุดุฎุต ุฎูุฏฺฉุงุฑุ ู ฺฏุฒููโูุง ฺฉูุงูุชุฒุงุณูู ู ุจูููโุณุงุฒ ูุฏู ุงูุนุทุงูโูพุฐุฑ ุงุณุชูุฑุงุฑ ุฌุงูุน ุฑุง ูุฑุงูู ูโฺฉูุฏ.

### VLLM: ุงุณุชูุชุงุฌ ุจุง ุนููฺฉุฑุฏ ุจุงูุง

VLLM ุจูููโุณุงุฒ ุงุณุชูุชุงุฌ ุฏุฑ ุณุทุญ ุชููุฏ ุจุฑุง ุณูุงุฑููุง ุจุง ุชูุงู ุจุงูุง ุงุฑุงุฆู ูโุฏูุฏ:

**ุจูููโุณุงุฒโูุง ุนููฺฉุฑุฏ**: PagedAttention ุจุฑุง ูุญุงุณุจู ุชูุฌู ุญุงูุธูโฺฉุงุฑุขูุฏ (ุจูโูฺู ููุฏ ุจุฑุง ูุนูุงุฑ ุชุฑุงูุณููุฑูุฑ Phi-4-mini-3.8B)ุ ุฏุณุชูโุจูุฏ ูพูุง ุจุฑุง ุจูููโุณุงุฒ ุชูุงู (ุจูููโุดุฏู ุจุฑุง ูพุฑุฏุงุฒุด ููุงุฒ ุณุฑ Qwen3)ุ ููุงุฒโุณุงุฒ ุชูุณูุฑ ุจุฑุง ููุงุณโฺฏุฐุงุฑ ฺูุฏ GPU (ูพุดุชุจุงู ุงุฒ Google Gemma3)ุ ู ุฑูุฒฺฏุดุง ุญุฏุณ ุจุฑุง ฺฉุงูุด ุชุฃุฎุฑ. ูุฏูโูุง BitNET ูุงุฒ ุจู ูุณุชูโูุง ุงุณุชูุชุงุฌ ุชุฎุตุต ุจุฑุง ุนููุงุช ฑ ุจุช ุฏุงุฑูุฏ.

**ฺฉูพุงุฑฺูโุณุงุฒ ุณุงุฒูุงู**: ููุงุท ูพุงุงู API ุณุงุฒฺฏุงุฑ ุจุง OpenAIุ ูพุดุชุจุงู ุงุฒ ุงุณุชูุฑุงุฑ Kubernetesุ ฺฉูพุงุฑฺูโุณุงุฒ ูุธุงุฑุช ู ูุดุงูุฏูโูพุฐุฑุ ู ูุงุจูุชโูุง ููุงุณโฺฏุฐุงุฑ ุฎูุฏฺฉุงุฑ ุฑุงูโุญูโูุง ุงุณุชูุฑุงุฑ ุฏุฑ ุณุทุญ ุณุงุฒูุงู ุฑุง ูุฑุงูู ูโฺฉููุฏ.

### Foundry Local: ุฑุงูโุญู ูุจู ูุงฺฉุฑูุณุงูุช

Foundry Local ูุงุจูุชโูุง ุฌุงูุน ุงุณุชูุฑุงุฑ ูุจู ุจุฑุง ูุญุทโูุง ุณุงุฒูุงู ุงุฑุงุฆู ูโุฏูุฏ:

**ูฺฺฏโูุง ูุญุงุณุจุงุช ูุจู**: ุทุฑุงุญ ูุนูุงุฑ ุขููุงู ุงูู ุจุง ุจูููโุณุงุฒ ูุญุฏูุฏุช ููุงุจุนุ ูุฏุฑุช ุฑุฌุณุชุฑ ูุฏู ูุญูุ ู ูุงุจูุชโูุง ููฺฏุงูโุณุงุฒ ูุจู ุจู ุงุจุฑ ุงุณุชูุฑุงุฑ ูุจู ูุงุจูโุงุนุชูุงุฏ ุฑุง ุชุถูู ูโฺฉูุฏ.

**ุงููุช ู ุงูุทุจุงู**: ูพุฑุฏุงุฒุด ุฏุงุฏูโูุง ูุญู ุจุฑุง ุญูุธ ุญุฑู ุฎุตูุตุ ฺฉูุชุฑูโูุง ุงููุช ุณุงุฒูุงูุ ฺฏุฒุงุฑุดโฺฏุฑ ุงูุทุจุงู ู ุซุจุช ุญุณุงุจุฑุณุ ู ูุฏุฑุช ุฏุณุชุฑุณ ูุจุชู ุจุฑ ููุด ุงููุช ุฌุงูุน ุจุฑุง ุงุณุชูุฑุงุฑูุง ูุจู ูุฑุงูู ูโฺฉูุฏ.

## ุจูุชุฑู ุดููโูุง ุจุฑุง ูพุงุฏูโุณุงุฒ SLM

### ุฏุณุชูุฑุงูุนููโูุง ุงูุชุฎุงุจ ูุฏู

ููฺฏุงู ุงูุชุฎุงุจ SLMโูุง ุจุฑุง ุงุณุชูุฑุงุฑ ูุจูุ ุนูุงูู ุฒุฑ ุฑุง ุฏุฑ ูุธุฑ ุจฺฏุฑุฏ:

**ููุงุญุธุงุช ุชุนุฏุงุฏ ูพุงุฑุงูุชุฑ**: ูุฏูโูุง ูฺฉุฑู SLM ูุงููุฏ Qwen3-0.6B ุฑุง ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ููุจุงู ููู ุณุจฺฉุ SLMโูุง ฺฉูฺฺฉ ูุงููุฏ Qwen3-1.7B ุง Google Gemma3 ุฑุง ุจุฑุง ุณูุงุฑููุง ุนููฺฉุฑุฏ ูุชุนุงุฏูุ ู SLMโูุง ูุชูุณุท ูุงููุฏ Phi-4-mini-3.8B ุง Qwen3-4B ุฑุง ููฺฏุงู ูุฒุฏฺฉ ุดุฏู ุจู ูุงุจูุชโูุง LLM ุฏุฑ ุญุงู ฺฉู ฺฉุงุฑุง ุญูุธ ูโุดูุฏ ุงูุชุฎุงุจ ฺฉูุฏ. ูุฏูโูุง BitNET ูุดุฑุฏูโุณุงุฒ ูููโุงูุนุงุฏู ุขุฒูุงุด ุฑุง ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ุชุญููุงุช ุฎุงุต ุงุฑุงุฆู ูโุฏููุฏ.

**ููโุฑุงุณุชุง ุจุง ููุงุฑุฏ ุงุณุชูุงุฏู**: ูุงุจูุชโูุง ูุฏู ุฑุง ุจุง ุงูุฒุงูุงุช ฺฉุงุฑุจุฑุฏ ุฎุงุต ูุทุงุจูุช ุฏูุฏุ ุนูุงูู ูุงููุฏ ฺฉูุช ูพุงุณุฎุ ุณุฑุนุช ุงุณุชูุชุงุฌุ ูุญุฏูุฏุชโูุง ุญุงูุธู ู ุงูุฒุงูุงุช ุนููุงุช ุขููุงู ุฑุง ุฏุฑ ูุธุฑ ุจฺฏุฑุฏ.

### ุงูุชุฎุงุจ ุงุณุชุฑุงุชฺ ุจูููโุณุงุฒ

**ุฑูฺฉุฑุฏ ฺฉูุงูุชุฒุงุณูู**: ุณุทูุญ ฺฉูุงูุชุฒุงุณูู ููุงุณุจ ุฑุง ุจุฑ ุงุณุงุณ ุงูุฒุงูุงุช ฺฉูุช ู ูุญุฏูุฏุชโูุง ุณุฎุชโุงูุฒุงุฑ ุงูุชุฎุงุจ ฺฉูุฏ. Q4_0 ุฑุง ุจุฑุง ุญุฏุงฺฉุซุฑ ูุดุฑุฏูโุณุงุฒ (ุงุฏูโุขู ุจุฑุง ุงุณุชูุฑุงุฑ ููุจุงู Qwen3-0.6B)ุ Q5_1 ุฑุง ุจุฑุง ุชุนุงุฏู ฺฉูุช-ูุดุฑุฏูโุณุงุฒ (ููุงุณุจ ุจุฑุง Phi-4-mini-3.8B ู Google Gemma3)ุ ู Q8_0 ุฑุง ุจุฑุง ุญูุธ ฺฉูุช ูุฒุฏฺฉ ุจู ุงุตู (ุชูุตูโุดุฏู ุจุฑุง ูุญุทโูุง ุชููุฏ Qwen3-4B) ุฏุฑ ูุธุฑ ุจฺฏุฑุฏ. ฺฉูุงูุชุฒุงุณูู ฑ ุจุช BitNET ููุงุงูฺฏุฑ ูุฑุฒ ูุดุฑุฏูโุณุงุฒ ุดุฏุฏ ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ุชุฎุตุต ุงุณุช.

**ุงูุชุฎุงุจ ฺุงุฑฺูุจ**: ฺุงุฑฺูุจโูุง ุจูููโุณุงุฒ ุฑุง ุจุฑ ุงุณุงุณ ุณุฎุชโุงูุฒุงุฑ ูุฏู ู ุงูุฒุงูุงุช ุงุณุชูุฑุงุฑ ุงูุชุฎุงุจ ฺฉูุฏ. ุงุฒ Llama.cpp ุจุฑุง ุงุณุชูุฑุงุฑ ุจูููโุดุฏู ุจุฑุง CPUุ Microsoft Olive ุจุฑุง ฺฏุฑุฏุด ฺฉุงุฑูุง ุฌุงูุน ุจูููโุณุงุฒุ ู Apple MLX ุจุฑุง ุฏุณุชฺฏุงูโูุง Apple Silicon ุงุณุชูุงุฏู ฺฉูุฏ.

## ูุซุงูโูุง ุนูู ูุฏู ู ููุงุฑุฏ ุงุณุชูุงุฏู

### ุณูุงุฑููุง ุงุณุชูุฑุงุฑ ูุงูุน

**ฺฉุงุฑุจุฑุฏูุง ููุจุงู**: Qwen3-0.6B ุฏุฑ ฺฉุงุฑุจุฑุฏูุง ฺุชโุจุงุช ฺฏูุดโูุง ููุดููุฏ ุจุง ุฑุฏูพุง ุญุงูุธู ุญุฏุงูู ุนุงู ุนูู ูโฺฉูุฏุ ุฏุฑ ุญุงู ฺฉู Google Gemma3 ุนููฺฉุฑุฏ ูุชุนุงุฏู ุจุฑุง ุงุจุฒุงุฑูุง ุขููุฒุด ูุจุชู ุจุฑ ุชุจูุช ุงุฑุงุฆู ูโุฏูุฏ. Phi-4-mini-3.8B ูุงุจูุชโูุง ุงุณุชุฏูุงู ุจุฑุชุฑ ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ุจูุฑูโูุฑ ููุจุงู ุงุฑุงุฆู ูโุฏูุฏ.

**ูุญุงุณุจุงุช ุฏุณฺฉุชุงูพ ู ูุจู**: Qwen3-1.7B ุนููฺฉุฑุฏ ุจูููโุง ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ุฏุณุชุงุฑ ุฏุณฺฉุชุงูพ ุงุฑุงุฆู ูโุฏูุฏุ Phi-4-mini-3.8B ูุงุจูุชโูุง ูพุดุฑูุชู ุชููุฏ ฺฉุฏ ุจุฑุง ุงุจุฒุงุฑูุง ุชูุณุนูโุฏููุฏู ูุฑุงูู ูโฺฉูุฏุ ู Qwen3-4B ุชุญูู ุงุณูุงุฏ ูพฺุฏู ุฑุง ุฏุฑ ูุญุทโูุง ุงุณุชฺฏุงู ฺฉุงุฑ ููฺฉู ูโุณุงุฒุฏ.

**ุชุญูู ู ุขุฒูุงุด**: ูุฏูโูุง BitNET ุงูฺฉุงู ุจุฑุฑุณ ุงุณุชูุชุงุฌ ูููโุงูุนุงุฏู ฺฉูโุฏูุช ุจุฑุง ุชุญููุงุช ุฏุงูุดฺฏุงู ู ฺฉุงุฑุจุฑุฏูุง ุงุซุจุงุช ููููู ฺฉู ูุงุฒ ุจู ูุญุฏูุฏุชโูุง ุดุฏุฏ ููุงุจุน ุฏุงุฑูุฏ ุฑุง ูุฑุงูู ูโฺฉููุฏ.

### ูุนุงุฑูุง ุนููฺฉุฑุฏ ู ููุงุณูโูุง

**ุณุฑุนุช ุงุณุชูุชุงุฌ**: Qwen3-0.6B ุณุฑุนโุชุฑู ุฒูุงูโูุง ุงุณุชูุชุงุฌ ุฑุง ุฏุฑ CPUูุง ููุจุงู ุจู ุฏุณุช ูโุขูุฑุฏุ Google Gemma3 ูุณุจุช ุณุฑุนุช-ฺฉูุช ูุชุนุงุฏู ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ุนููู ุงุฑุงุฆู ูโุฏูุฏุ Phi-4-mini-3.8B ุณุฑุนุช ุงุณุชุฏูุงู ุจุฑุชุฑ ุจุฑุง ูุธุงู ูพฺุฏู ุงุฑุงุฆู ูโุฏูุฏุ ู BitNET ุญุฏุงฺฉุซุฑ ุชูุงู ูุธุฑ ุฑุง ุจุง ุณุฎุชโุงูุฒุงุฑ ุชุฎุตุต ุงุฑุงุฆู ูโุฏูุฏ.

**ูุงุฒูุง ุญุงูุธู**: ุฑุฏูพุง ุญุงูุธู ูุฏูโูุง ุงุฒ Qwen3-0.6B (ฺฉูุชุฑ ุงุฒ ฑ ฺฏฺฏุงุจุงุช ฺฉูุงูุชุฒูโุดุฏู) ุชุง Phi-4-mini-3.8B (ุชูุฑุจุงู ณ-ด ฺฏฺฏุงุจุงุช ฺฉูุงูุชุฒูโุดุฏู) ูุชุบุฑ ุงุณุชุ ุจุง BitNET ฺฉู ุฑุฏูพุง ุฒุฑ ตฐฐ ูฺฏุงุจุงุช ุฑุง ุฏุฑ ูพฺฉุฑุจูุฏโูุง ุขุฒูุงุด ุจู ุฏุณุช ูโุขูุฑุฏ.

## ฺุงูุดโูุง ู ููุงุญุธุงุช

### ูุตุงูุญูโูุง ุนููฺฉุฑุฏ

ุงุณุชูุฑุงุฑ SLM ุดุงูู ุจุฑุฑุณ ุฏูู ูุตุงูุญูโูุง ุจู ุงูุฏุงุฒู ูุฏูุ ุณุฑุนุช ุงุณุชูุชุงุฌ ู ฺฉูุช ุฎุฑูุฌ ุงุณุช. ุจู ุนููุงู ูุซุงูุ ุฏุฑ ุญุงู ฺฉู Qwen3-0.6B ุณุฑุนุช ู ฺฉุงุฑุง ุงุณุชุซูุง ุงุฑุงุฆู ูโุฏูุฏุ Phi-4-mini-3.8B ูุงุจูุชโูุง ุงุณุชุฏูุงู ุจุฑุชุฑ ุฑุง ุจุง ูุฒูู ุงูุฒุงุด ูุงุฒูุง ููุงุจุน ูุฑุงูู ูโฺฉูุฏ. Google Gemma3 ุชุนุงุฏู ููุงุณุจ ุจุฑุง ุงฺฉุซุฑ ฺฉุงุฑุจุฑุฏูุง ุนููู ุงุฌุงุฏ ูโฺฉูุฏ.

### ุณุงุฒฺฏุงุฑ ุณุฎุชโุงูุฒุงุฑ

ุฏุณุชฺฏุงูโูุง ูุจู ูุฎุชูู ูุงุจูุชโูุง ู ูุญุฏูุฏุชโูุง ูุชูุงูุช ุฏุงุฑูุฏ. Qwen3-0.6B ุจูโุทูุฑ ฺฉุงุฑุขูุฏ ุฏุฑ ูพุฑุฏุงุฒูุฏูโูุง ARM ูพุงู ุงุฌุฑุง ูโุดูุฏุ Google Gemma3 ุจู ููุงุจุน ูุญุงุณุจุงุช ูุชูุณุท ูุงุฒ ุฏุงุฑุฏุ ู Phi-4-mini-3.8B ุงุฒ ุณุฎุชโุงูุฒุงุฑ ูุจู ูพุดุฑูุชูโุชุฑ ุจูุฑู ูโุจุฑุฏ. ูุฏูโูุง BitNET ูุงุฒ ุจู ุณุฎุชโุงูุฒุงุฑ ุง ูพุงุฏูโุณุงุฒโูุง ูุฑูโุงูุฒุงุฑ ุชุฎุตุต ุจุฑุง ุนููุงุช ฑ ุจุช ุจููู ุฏุงุฑูุฏ.

### ุงููุช ู ุญุฑู ุฎุตูุต

ุฏุฑ ุญุงู ฺฉู S

---

**ุณูุจ ูุณุฆููุช**:  
ุงู ุณูุฏ ุจุง ุงุณุชูุงุฏู ุงุฒ ุณุฑูุณ ุชุฑุฌูู ููุด ูุตููุน [Co-op Translator](https://github.com/Azure/co-op-translator) ุชุฑุฌูู ุดุฏู ุงุณุช. ุฏุฑ ุญุงู ฺฉู ูุง ุชูุงุด ูโฺฉูู ุฏูุช ุฑุง ุญูุธ ฺฉููุ ูุทูุงู ุชูุฌู ุฏุงุดุชู ุจุงุดุฏ ฺฉู ุชุฑุฌููโูุง ุฎูุฏฺฉุงุฑ ููฺฉู ุงุณุช ุดุงูู ุฎุทุงูุง ุง ูุงุฏุฑุณุชโูุง ุจุงุดูุฏ. ุณูุฏ ุงุตู ุจู ุฒุจุงู ุงุตู ุขู ุจุงุฏ ุจู ุนููุงู ููุจุน ูุนุชุจุฑ ุฏุฑ ูุธุฑ ฺฏุฑูุชู ุดูุฏ. ุจุฑุง ุงุทูุงุนุงุช ุญุณุงุณุ ุชูุตู ูโุดูุฏ ุงุฒ ุชุฑุฌูู ุญุฑููโุง ุงูุณุงู ุงุณุชูุงุฏู ฺฉูุฏ. ูุง ูุณุฆููุช ุฏุฑ ูุจุงู ุณูุกุชูุงููโูุง ุง ุชูุณุฑูุง ูุงุฏุฑุณุช ูุงุด ุงุฒ ุงุณุชูุงุฏู ุงุฒ ุงู ุชุฑุฌูู ูุฏุงุฑู.