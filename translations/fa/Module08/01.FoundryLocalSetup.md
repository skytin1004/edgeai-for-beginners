<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6a574846c3919c56f1d02bf1de2003ca",
  "translation_date": "2025-09-30T23:11:02+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "fa"
}
-->
# Ø¬Ù„Ø³Ù‡ Û±: Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ú©Ø§Ø± Ø¨Ø§ Foundry Local

## Ù…Ø±ÙˆØ± Ú©Ù„ÛŒ

Microsoft Foundry Local Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Azure AI Foundry Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¨Ù‡ Ù…Ø­ÛŒØ· ØªÙˆØ³Ø¹Ù‡ ÙˆÛŒÙ†Ø¯ÙˆØ² Û±Û± Ø´Ù…Ø§ Ù…ÛŒâ€ŒØ¢ÙˆØ±Ø¯ Ùˆ Ø§Ù…Ú©Ø§Ù† ØªÙˆØ³Ø¹Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø§ Ø­ÙØ¸ Ø­Ø±ÛŒÙ… Ø®ØµÙˆØµÛŒØŒ ØªØ£Ø®ÛŒØ± Ú©Ù… Ùˆ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø³Ø·Ø­ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ Ø±Ø§ ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§ÛŒÙ† Ø¬Ù„Ø³Ù‡ Ø´Ø§Ù…Ù„ Ù†ØµØ¨ Ú©Ø§Ù…Ù„ØŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø¨ÙˆØ¨ Ø§Ø² Ø¬Ù…Ù„Ù‡ phiØŒ qwenØŒ deepseek Ùˆ GPT-OSS-20B Ø§Ø³Øª.

## Ø§Ù‡Ø¯Ø§Ù Ø¢Ù…ÙˆØ²Ø´ÛŒ

ØªØ§ Ù¾Ø§ÛŒØ§Ù† Ø§ÛŒÙ† Ø¬Ù„Ø³Ù‡ØŒ Ø´Ù…Ø§:
- Foundry Local Ø±Ø§ Ø±ÙˆÛŒ ÙˆÛŒÙ†Ø¯ÙˆØ² Û±Û± Ù†ØµØ¨ Ùˆ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø®ÙˆØ§Ù‡ÛŒØ¯ Ú©Ø±Ø¯
- Ø¯Ø³ØªÙˆØ±Ø§Øª CLI Ùˆ Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø±Ø§ Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ ÛŒØ§Ø¯ Ø®ÙˆØ§Ù‡ÛŒØ¯ Ú¯Ø±ÙØª
- Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø´ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ÛŒÙ†Ù‡ Ø±Ø§ Ø¯Ø±Ú© Ø®ÙˆØ§Ù‡ÛŒØ¯ Ú©Ø±Ø¯
- Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ phiØŒ qwenØŒ deepseek Ùˆ GPT-OSS-20B Ø±Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¬Ø±Ø§ Ø®ÙˆØ§Ù‡ÛŒØ¯ Ú©Ø±Ø¯
- Ø§ÙˆÙ„ÛŒÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Foundry Local Ø§ÛŒØ¬Ø§Ø¯ Ø®ÙˆØ§Ù‡ÛŒØ¯ Ú©Ø±Ø¯

## Ù¾ÛŒØ´â€ŒÙ†ÛŒØ§Ø²Ù‡Ø§

### Ø§Ù„Ø²Ø§Ù…Ø§Øª Ø³ÛŒØ³ØªÙ…
- **ÙˆÛŒÙ†Ø¯ÙˆØ² Û±Û±**: Ù†Ø³Ø®Ù‡ 22H2 ÛŒØ§ Ø¨Ø§Ù„Ø§ØªØ±
- **Ø±Ù…**: Ø­Ø¯Ø§Ù‚Ù„ Û±Û¶ Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØªØŒ ØªÙˆØµÛŒÙ‡ Ø´Ø¯Ù‡ Û³Û² Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª
- **ÙØ¶Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ**: ÛµÛ° Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª ÙØ¶Ø§ÛŒ Ø¢Ø²Ø§Ø¯ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ú©Ø´
- **Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±**: Ø¯Ø³ØªÚ¯Ø§Ù‡ Ù…Ø¬Ù‡Ø² Ø¨Ù‡ NPU ÛŒØ§ GPU ØªØ±Ø¬ÛŒØ­Ø§Ù‹ (Copilot+ PC ÛŒØ§ GPU Ø§Ù†ÙˆÛŒØ¯ÛŒØ§)
- **Ø´Ø¨Ú©Ù‡**: Ø§ÛŒÙ†ØªØ±Ù†Øª Ù¾Ø±Ø³Ø±Ø¹Øª Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§

### Ù…Ø­ÛŒØ· ØªÙˆØ³Ø¹Ù‡
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion

# Set up Python environment (recommended)
py -m venv .venv
.venv\Scripts\activate

# Install required dependencies
pip install openai foundry-local-sdk
```

## Ø¨Ø®Ø´ Û±: Ù†ØµØ¨ Ùˆ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ

### Ù…Ø±Ø­Ù„Ù‡ Û±: Ù†ØµØ¨ Foundry Local

Foundry Local Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Winget Ù†ØµØ¨ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ù†ØµØ¨â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø±Ø§ Ø§Ø² GitHub Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### Ù…Ø±Ø­Ù„Ù‡ Û²: ØªØ£ÛŒÛŒØ¯ Ù†ØµØ¨

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## Ø¨Ø®Ø´ Û²: Ø¯Ø±Ú© CLI

### Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø³ØªÙˆØ±Ø§Øª Ø§ØµÙ„ÛŒ

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## Ø¨Ø®Ø´ Û³: Ù…Ø¯ÛŒØ±ÛŒØª Ùˆ Ú©Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§

Foundry Local Ø§Ø² Ú©Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## Ø¨Ø®Ø´ Û´: Ø§Ø¬Ø±Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§

### Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Microsoft Phi

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### Ú©Ø§Ø± Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Qwen

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b
foundry model run qwen2.5-14b
```

### Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ DeepSeek

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-7b
```

### Ø§Ø¬Ø±Ø§ÛŒ GPT-OSS-20B

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## Ø¨Ø®Ø´ Ûµ: Ø§ÛŒØ¬Ø§Ø¯ Ø§ÙˆÙ„ÛŒÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø´Ù…Ø§

### Ø¨Ø±Ù†Ø§Ù…Ù‡ Ú†Øª Ù…Ø¯Ø±Ù† (OpenAI SDK + Foundry Local)

ÛŒÚ© Ø¨Ø±Ù†Ø§Ù…Ù‡ Ú†Øª Ø¢Ù…Ø§Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² OpenAI SDK Ùˆ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ Foundry Local Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ÛŒØ¯ØŒ Ø¨Ø§ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Û°Û±.

```python
# chat_quickstart.py (Sample 01 pattern)
import os
import sys
from openai import OpenAI

try:
    from foundry_local import FoundryLocalManager
    FOUNDRY_SDK_AVAILABLE = True
except ImportError:
    FOUNDRY_SDK_AVAILABLE = False
    print("âš ï¸ Install foundry-local-sdk: pip install foundry-local-sdk")

def create_client():
    """Create OpenAI client with Foundry Local or Azure OpenAI."""
    # Check for Azure OpenAI configuration
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    
    if azure_endpoint and azure_api_key:
        # Azure OpenAI path
        model = os.environ.get("MODEL", "your-deployment-name")
        client = OpenAI(
            base_url=f"{azure_endpoint}/openai",
            api_key=azure_api_key,
            default_query={"api-version": "2024-08-01-preview"},
        )
        print(f"ğŸŒ Using Azure OpenAI with model: {model}")
        return client, model
    
    # Foundry Local path with SDK management
    alias = os.environ.get("MODEL", "phi-4-mini")
    if FOUNDRY_SDK_AVAILABLE:
        try:
            # Use FoundryLocalManager for proper service management
            manager = FoundryLocalManager(alias)
            model_info = manager.get_model_info(alias)
            
            client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            model = model_info.id
            print(f"ğŸ  Using Foundry Local SDK with model: {model}")
            return client, model
        except Exception as e:
            print(f"âš ï¸ Foundry SDK failed ({e}), using manual configuration")
    
    # Fallback to manual configuration
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    api_key = os.environ.get("API_KEY", "")
    model = alias
    
    client = OpenAI(
        base_url=f"{base_url}/v1",
        api_key=api_key
    )
    print(f"ğŸ”§ Manual configuration with model: {model}")
    return client, model

def main():
    """Main chat function."""
    client, model = create_client()
    
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    conversation_history = []
    
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        
        try:
            # Add user message to history
            conversation_history.append({"role": "user", "content": user_input})
            
            # Create chat completion
            response = client.chat.completions.create(
                model=model,
                messages=conversation_history,
                max_tokens=500,
                temperature=0.7
            )
            
            assistant_message = response.choices[0].message.content
            conversation_history.append({"role": "assistant", "content": assistant_message})
            
            print(f"Assistant: {assistant_message}\n")
            
        except Exception as e:
            print(f"Error: {e}\n")

if __name__ == "__main__":
    main()
```

### Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ú†Øª

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Option 1: Using FoundryLocalManager (recommended)
python chat_quickstart.py "Explain what Foundry Local is"

# Option 2: Manual configuration with environment variables
set BASE_URL=http://localhost:8000
set MODEL=phi-4-mini
set API_KEY=
python chat_quickstart.py "Write a welcome message"

# Option 3: Azure OpenAI configuration
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name
python chat_quickstart.py "Hello from Azure OpenAI"
```

## Ø¨Ø®Ø´ Û¶: Ø±ÙØ¹ Ø§Ø´Ú©Ø§Ù„ Ùˆ Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´â€ŒÙ‡Ø§

### Ù…Ø´Ú©Ù„Ø§Øª Ø±Ø§ÛŒØ¬ Ùˆ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§

```powershell
# Issue: "Could not use Foundry SDK" warning
pip install foundry-local-sdk
# Or set environment variables for manual configuration

# Issue: Connection refused
foundry service status
foundry service ps  # Check loaded models

# Issue: Model not found
foundry model list
foundry model run phi-4-mini

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal

# Test API endpoint
curl http://localhost:8000/v1/models
```

### Ù†Ø¸Ø§Ø±Øª Ø¨Ø± Ù…Ù†Ø§Ø¨Ø¹ Ø³ÛŒØ³ØªÙ… (ÙˆÛŒÙ†Ø¯ÙˆØ²)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ

| Ù…ØªØºÛŒØ± | ØªÙˆØ¶ÛŒØ­Ø§Øª | Ù¾ÛŒØ´â€ŒÙØ±Ø¶ | Ø¶Ø±ÙˆØ±ÛŒ |
|-------|---------|---------|--------|
| `MODEL` | Ù†Ø§Ù… ÛŒØ§ Ù†Ø§Ù… Ù…Ø³ØªØ¹Ø§Ø± Ù…Ø¯Ù„ | `phi-4-mini` | Ø®ÛŒØ± |
| `BASE_URL` | Ø¢Ø¯Ø±Ø³ Ù¾Ø§ÛŒÙ‡ Foundry Local | `http://localhost:8000` | Ø®ÛŒØ± |
| `API_KEY` | Ú©Ù„ÛŒØ¯ API (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ù„ÛŒ Ù†ÛŒØ§Ø² Ù†ÛŒØ³Øª) | `""` | Ø®ÛŒØ± |
| `AZURE_OPENAI_ENDPOINT` | Ù†Ù‚Ø·Ù‡ Ù¾Ø§ÛŒØ§Ù†ÛŒ Azure OpenAI | - | Ø¨Ø±Ø§ÛŒ Azure |
| `AZURE_OPENAI_API_KEY` | Ú©Ù„ÛŒØ¯ API Azure OpenAI | - | Ø¨Ø±Ø§ÛŒ Azure |
| `AZURE_OPENAI_API_VERSION` | Ù†Ø³Ø®Ù‡ API Azure | `2024-08-01-preview` | Ø®ÛŒØ± |

### Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´â€ŒÙ‡Ø§

- **Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² OpenAI SDK**: Ø¨Ù‡ Ø¬Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… HTTPØŒ Ø§Ø² OpenAI SDK Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ø¨Ù‡ØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
- **FoundryLocalManager**: Ø§Ø² SDK Ø±Ø³Ù…ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø³Ø±ÙˆÛŒØ³ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ØŒ Ø¯Ø± ØµÙˆØ±Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¨ÙˆØ¯Ù†
- **Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§**: Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯
- **Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ù†Ø¸Ù…**: Foundry Local Ø±Ø§ Ø¨Ù‡â€ŒØ±ÙˆØ² Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒØ¯ ØªØ§ Ø¨Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ø§ØµÙ„Ø§Ø­Ø§Øª Ø¬Ø¯ÛŒØ¯ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯
- **Ø´Ø±ÙˆØ¹ Ú©ÙˆÚ†Ú©**: Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©â€ŒØªØ± (Phi miniØŒ Qwen 7B) Ø´Ø±ÙˆØ¹ Ú©Ù†ÛŒØ¯ Ùˆ Ø¨Ù‡ ØªØ¯Ø±ÛŒØ¬ Ù…Ù‚ÛŒØ§Ø³ Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯
- **Ù†Ø¸Ø§Ø±Øª Ø¨Ø± Ù…Ù†Ø§Ø¨Ø¹**: Ù‡Ù†Ú¯Ø§Ù… ØªÙ†Ø¸ÛŒÙ… Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§ØªØŒ CPU/GPU/Ø±Ù… Ø±Ø§ Ù¾ÛŒÚ¯ÛŒØ±ÛŒ Ú©Ù†ÛŒØ¯

## Ø¨Ø®Ø´ Û·: ØªÙ…Ø±ÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ

### ØªÙ…Ø±ÛŒÙ† Û±: ØªØ³Øª Ø³Ø±ÛŒØ¹ Ú†Ù†Ø¯ Ù…Ø¯Ù„

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### ØªÙ…Ø±ÛŒÙ† Û²: ØªØ³Øª ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ OpenAI SDK

```python
# sdk_integration_test.py (matching Sample 01 pattern)
import os
from openai import OpenAI
from foundry_local import FoundryLocalManager

def test_model_integration(model_alias):
    """Test OpenAI SDK integration with different models."""
    try:
        # Use FoundryLocalManager for proper setup
        manager = FoundryLocalManager(model_alias)
        model_info = manager.get_model_info(model_alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Test basic completion
        response = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Say hello and state your model name."}],
            max_tokens=50
        )
        
        print(f"âœ… {model_alias}: {response.choices[0].message.content}")
        return True
    except Exception as e:
        print(f"âŒ {model_alias}: {e}")
        return False

# Test multiple models
models_to_test = ["phi-4-mini", "qwen2.5-7b"]
for model in models_to_test:
    test_model_integration(model)
```

### ØªÙ…Ø±ÛŒÙ† Û³: Ø¨Ø±Ø±Ø³ÛŒ Ø¬Ø§Ù…Ø¹ Ø³Ù„Ø§Ù…Øª Ø³Ø±ÙˆÛŒØ³

```python
# health_check.py
from openai import OpenAI
from foundry_local import FoundryLocalManager

def comprehensive_health_check():
    """Perform comprehensive health check of Foundry Local service."""
    try:
        # Initialize with a common model
        manager = FoundryLocalManager("phi-4-mini")
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # 1. Check service connectivity
        models_response = client.models.list()
        available_models = [model.id for model in models_response.data]
        print(f"âœ… Service healthy - {len(available_models)} models available")
        
        # 2. Test each available model
        for model_id in available_models:
            try:
                response = client.chat.completions.create(
                    model=model_id,
                    messages=[{"role": "user", "content": "Test"}],
                    max_tokens=10
                )
                print(f"âœ… {model_id}: Working")
            except Exception as e:
                print(f"âŒ {model_id}: {e}")
        
        return True
    except Exception as e:
        print(f"âŒ Service check failed: {e}")
        return False

comprehensive_health_check()
```

## Ù…Ù†Ø§Ø¨Ø¹

- **Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ú©Ø§Ø± Ø¨Ø§ Foundry Local**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- **Ù…Ø±Ø¬Ø¹ CLI Ùˆ Ù…Ø±ÙˆØ± Ø¯Ø³ØªÙˆØ±Ø§Øª**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- **ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ OpenAI SDK**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- **Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Hugging Face**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- **GitHub Microsoft Foundry Local**: https://github.com/microsoft/Foundry-Local
- **OpenAI Python SDK**: https://github.com/openai/openai-python
- **Ù†Ù…ÙˆÙ†Ù‡ Û°Û±: Ú†Øª Ø³Ø±ÛŒØ¹ Ø¨Ø§ OpenAI SDK**: samples/01/README.md
- **Ù†Ù…ÙˆÙ†Ù‡ Û°Û²: ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ SDK**: samples/02/README.md

---

**Ø³Ù„Ø¨ Ù…Ø³Ø¦ÙˆÙ„ÛŒØª**:  
Ø§ÛŒÙ† Ø³Ù†Ø¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³Ø±ÙˆÛŒØ³ ØªØ±Ø¬Ù…Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ [Co-op Translator](https://github.com/Azure/co-op-translator) ØªØ±Ø¬Ù…Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ù…Ø§ ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ±Ø¬Ù…Ù‡â€ŒÙ‡Ø§ Ø¯Ù‚ÛŒÙ‚ Ø¨Ø§Ø´Ù†Ø¯ØŒ Ù„Ø·ÙØ§Ù‹ ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ Ú©Ù‡ ØªØ±Ø¬Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø´Ø§Ù…Ù„ Ø®Ø·Ø§Ù‡Ø§ ÛŒØ§ Ù†Ø§Ø¯Ø±Ø³ØªÛŒâ€ŒÙ‡Ø§ Ø¨Ø§Ø´Ù†Ø¯. Ø³Ù†Ø¯ Ø§ØµÙ„ÛŒ Ø¨Ù‡ Ø²Ø¨Ø§Ù† Ø§ØµÙ„ÛŒ Ø¢Ù† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ù†Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø± Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø´ÙˆØ¯. Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø­Ø³Ø§Ø³ØŒ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø§Ø² ØªØ±Ø¬Ù…Ù‡ Ø§Ù†Ø³Ø§Ù†ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯. Ù…Ø§ Ù…Ø³Ø¦ÙˆÙ„ÛŒØªÛŒ Ø¯Ø± Ù‚Ø¨Ø§Ù„ Ø³ÙˆØ¡ ØªÙØ§Ù‡Ù…â€ŒÙ‡Ø§ ÛŒØ§ ØªÙØ³ÛŒØ±Ù‡Ø§ÛŒ Ù†Ø§Ø¯Ø±Ø³Øª Ù†Ø§Ø´ÛŒ Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ† ØªØ±Ø¬Ù…Ù‡ Ù†Ø¯Ø§Ø±ÛŒÙ….