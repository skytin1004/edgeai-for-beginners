<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "949045c54206448d55d987e8b23a82cf",
  "translation_date": "2025-09-24T12:19:52+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "fa"
}
-->
# Ø¬Ù„Ø³Ù‡ Û´: Ø³Ø§Ø®Øª Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ú†Øª ØªÙˆÙ„ÛŒØ¯ÛŒ Ø¨Ø§ Chainlit

## Ù…Ø±ÙˆØ± Ú©Ù„ÛŒ

Ø§ÛŒÙ† Ø¬Ù„Ø³Ù‡ Ø¨Ø± Ø³Ø§Ø®Øª Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ú†Øª Ø¢Ù…Ø§Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Chainlit Ùˆ Microsoft Foundry Local ØªÙ…Ø±Ú©Ø² Ø¯Ø§Ø±Ø¯. Ø´Ù…Ø§ ÛŒØ§Ø¯ Ø®ÙˆØ§Ù‡ÛŒØ¯ Ú¯Ø±ÙØª Ú©Ù‡ Ø±Ø§Ø¨Ø·â€ŒÙ‡Ø§ÛŒ ÙˆØ¨ Ù…Ø¯Ø±Ù† Ø¨Ø±Ø§ÛŒ Ù…Ú©Ø§Ù„Ù…Ø§Øª Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ÛŒØ¯ØŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ±ÛŒÙ… Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ú†Øª Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ Ùˆ Ø·Ø±Ø§Ø­ÛŒ ØªØ¬Ø±Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø±Ø§ Ù…Ø³ØªÙ‚Ø± Ú©Ù†ÛŒØ¯.

**Ø¢Ù†Ú†Ù‡ Ø®ÙˆØ§Ù‡ÛŒØ¯ Ø³Ø§Ø®Øª:**
- **Ø¨Ø±Ù†Ø§Ù…Ù‡ Ú†Øª Chainlit**: Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ ÙˆØ¨ Ù…Ø¯Ø±Ù† Ø¨Ø§ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ±ÛŒÙ…
- **Ø¯Ù…ÙˆÛŒ WebGPU**: Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù…Ø±ÙˆØ±Ú¯Ø± Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¯Ø§Ø± Ø­Ø±ÛŒÙ… Ø®ØµÙˆØµÛŒ  
- **Ø§Ø¯ØºØ§Ù… Open WebUI**: Ø±Ø§Ø¨Ø· Ú†Øª Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨Ø§ Foundry Local
- **Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ÛŒ**: Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ØŒ Ù†Ø¸Ø§Ø±Øª Ùˆ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªÙ‚Ø±Ø§Ø±

## Ø§Ù‡Ø¯Ø§Ù ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ

- Ø³Ø§Ø®Øª Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ú†Øª Ø¢Ù…Ø§Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ Chainlit
- Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ±ÛŒÙ… Ø¨Ø±Ø§ÛŒ ØªØ¬Ø±Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ù‡ØªØ±
- ØªØ³Ù„Ø· Ø¨Ø± Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø§Ø¯ØºØ§Ù… SDK Foundry Local
- Ø§Ø¹Ù…Ø§Ù„ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ Ùˆ Ú©Ø§Ù‡Ø´ ØªØ¯Ø±ÛŒØ¬ÛŒ Ù…Ù†Ø§Ø³Ø¨
- Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ùˆ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ú†Øª Ø¨Ø±Ø§ÛŒ Ù…Ø­ÛŒØ·â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
- Ø¯Ø±Ú© Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ ÙˆØ¨ Ù…Ø¯Ø±Ù† Ø¨Ø±Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡â€ŒØ§ÛŒ

## Ù¾ÛŒØ´â€ŒÙ†ÛŒØ§Ø²Ù‡Ø§

- **Foundry Local**: Ù†ØµØ¨ Ùˆ Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡ ([Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ù†ØµØ¨](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Ù¾Ø§ÛŒØªÙˆÙ†**: Ù†Ø³Ø®Ù‡ Û³.Û±Û° ÛŒØ§ Ø¨Ø§Ù„Ø§ØªØ± Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ù…Ø­ÛŒØ· Ù…Ø¬Ø§Ø²ÛŒ
- **Ù…Ø¯Ù„**: Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ (`foundry model run phi-4-mini`)
- **Ù…Ø±ÙˆØ±Ú¯Ø±**: Ù…Ø±ÙˆØ±Ú¯Ø± ÙˆØ¨ Ù…Ø¯Ø±Ù† Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ WebGPU (Chrome/Edge)
- **Docker**: Ø¨Ø±Ø§ÛŒ Ø§Ø¯ØºØ§Ù… Open WebUI (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)

## Ø¨Ø®Ø´ Û±: Ø¯Ø±Ú© Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ú†Øª Ù…Ø¯Ø±Ù†

### Ù…Ø±ÙˆØ± Ù…Ø¹Ù…Ø§Ø±ÛŒ

```
User Browser â†â†’ Chainlit UI â†â†’ Python Backend â†â†’ Foundry Local â†â†’ AI Model
      â†“              â†“              â†“              â†“            â†“
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### ÙÙ†Ø§ÙˆØ±ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ

**Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ SDK Foundry Local:**
- `FoundryLocalManager(alias)`: Ù…Ø¯ÛŒØ±ÛŒØª Ø®ÙˆØ¯Ú©Ø§Ø± Ø³Ø±ÙˆÛŒØ³
- `manager.endpoint` Ùˆ `manager.api_key`: Ø¬Ø²Ø¦ÛŒØ§Øª Ø§ØªØµØ§Ù„
- `manager.get_model_info(alias).id`: Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…Ø¯Ù„

**Ú†Ø§Ø±Ú†ÙˆØ¨ Chainlit:**
- `@cl.on_chat_start`: Ø´Ø±ÙˆØ¹ Ø¬Ù„Ø³Ø§Øª Ú†Øª
- `@cl.on_message`: Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†  
- `cl.Message().stream_token()`: Ø§Ø³ØªØ±ÛŒÙ… Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ
- ØªÙˆÙ„ÛŒØ¯ Ø®ÙˆØ¯Ú©Ø§Ø± Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª WebSocket

## Ø¨Ø®Ø´ Û²: Ù…Ø§ØªØ±ÛŒØ³ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù…Ø­Ù„ÛŒ Ø¯Ø± Ù…Ù‚Ø§Ø¨Ù„ Ø§Ø¨Ø±ÛŒ

### ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ÛŒ

| Ø¬Ù†Ø¨Ù‡ | Ù…Ø­Ù„ÛŒ (Foundry) | Ø§Ø¨Ø±ÛŒ (Azure OpenAI) |
|------|----------------|---------------------|
| **ØªØ§Ø®ÛŒØ±** | ğŸš€ ÛµÛ°-Û²Û°Û° Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡ (Ø¨Ø¯ÙˆÙ† Ø´Ø¨Ú©Ù‡) | â±ï¸ Û²Û°Û°-Û²Û°Û°Û° Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡ (ÙˆØ§Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ø´Ø¨Ú©Ù‡) |
| **Ø­Ø±ÛŒÙ… Ø®ØµÙˆØµÛŒ** | ğŸ”’ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù‡Ø±Ú¯Ø² Ø¯Ø³ØªÚ¯Ø§Ù‡ Ø±Ø§ ØªØ±Ú© Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ | âš ï¸ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø§Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ |
| **Ù‡Ø²ÛŒÙ†Ù‡** | ğŸ’° Ø±Ø§ÛŒÚ¯Ø§Ù† Ù¾Ø³ Ø§Ø² Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± | ğŸ’¸ Ù¾Ø±Ø¯Ø§Ø®Øª Ø¨Ù‡ Ø§Ø²Ø§ÛŒ Ù‡Ø± ØªÙˆÚ©Ù† |
| **Ø¢ÙÙ„Ø§ÛŒÙ†** | âœ… Ú©Ø§Ø± Ø¨Ø¯ÙˆÙ† Ø§ÛŒÙ†ØªØ±Ù†Øª | âŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§ÛŒÙ†ØªØ±Ù†Øª |
| **Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…Ø¯Ù„** | âš ï¸ Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± | âœ… Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§ |
| **Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ** | âš ï¸ ÙˆØ§Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± | âœ… Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ Ù†Ø§Ù…Ø­Ø¯ÙˆØ¯ |

### Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ

**Ø§ÙˆÙ„ÙˆÛŒØª Ù…Ø­Ù„ÛŒ Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Ù…Ø³ÛŒØ±â€ŒÛŒØ§Ø¨ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± ÙˆØ¸ÛŒÙÙ‡:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Ø¨Ø®Ø´ Û³: Ù†Ù…ÙˆÙ†Ù‡ Û°Û´ - Ø¨Ø±Ù†Ø§Ù…Ù‡ Ú†Øª Chainlit

### Ø´Ø±ÙˆØ¹ Ø³Ø±ÛŒØ¹

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ù‡ Ø·ÙˆØ± Ø®ÙˆØ¯Ú©Ø§Ø± Ø¯Ø± `http://localhost:8080` Ø¨Ø§ ÛŒÚ© Ø±Ø§Ø¨Ø· Ú†Øª Ù…Ø¯Ø±Ù† Ø¨Ø§Ø² Ù…ÛŒâ€ŒØ´ÙˆØ¯.

### Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§ØµÙ„ÛŒ

Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Û°Û´ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯:

**Ú©Ø´Ù Ø®ÙˆØ¯Ú©Ø§Ø± Ø³Ø±ÙˆÛŒØ³:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Ù…Ø¯ÛŒØ±ÛŒØª Ú†Øª Ø§Ø³ØªØ±ÛŒÙ…:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ

**Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ:**

| Ù…ØªØºÛŒØ± | ØªÙˆØ¶ÛŒØ­Ø§Øª | Ù¾ÛŒØ´â€ŒÙØ±Ø¶ | Ù…Ø«Ø§Ù„ |
|-------|---------|---------|-------|
| `MODEL` | Ù†Ø§Ù… Ù…Ø³ØªØ¹Ø§Ø± Ù…Ø¯Ù„ Ù…ÙˆØ±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | Ù†Ù‚Ø·Ù‡ Ù¾Ø§ÛŒØ§Ù†ÛŒ Foundry Local | Ø®ÙˆØ¯Ú©Ø§Ø± Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ | `http://localhost:51211` |
| `API_KEY` | Ú©Ù„ÛŒØ¯ API (Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ù„ÛŒ) | `""` | `your-api-key` |

**Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡:**
```cmd
# Use different model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Ø¨Ø®Ø´ Û´: Ø§ÛŒØ¬Ø§Ø¯ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Jupyter

### Ù…Ø±ÙˆØ± Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©

Ù†Ù…ÙˆÙ†Ù‡ Û°Û´ Ø´Ø§Ù…Ù„ ÛŒÚ© Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø¬Ø§Ù…Ø¹ Jupyter (`chainlit_app.ipynb`) Ø§Ø³Øª Ú©Ù‡ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯:

- **ğŸ“š Ù…Ø­ØªÙˆØ§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ**: Ù…ÙˆØ§Ø¯ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ú¯Ø§Ù… Ø¨Ù‡ Ú¯Ø§Ù…
- **ğŸ”¬ Ú©Ø§ÙˆØ´ ØªØ¹Ø§Ù…Ù„ÛŒ**: Ø§Ø¬Ø±Ø§ÛŒ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´ Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø¯
- **ğŸ“Š Ù†Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§ÛŒ Ø¨ØµØ±ÛŒ**: Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ØŒ Ø¯ÛŒØ§Ú¯Ø±Ø§Ù…â€ŒÙ‡Ø§ Ùˆ ØªØ¬Ø³Ù… Ø®Ø±ÙˆØ¬ÛŒ
- **ğŸ› ï¸ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ ØªÙˆØ³Ø¹Ù‡**: Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ ØªØ³Øª Ùˆ Ø§Ø´Ú©Ø§Ù„â€ŒØ²Ø¯Ø§ÛŒÛŒ

### Ø§ÛŒØ¬Ø§Ø¯ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯ØªØ§Ù†

#### Ù…Ø±Ø­Ù„Ù‡ Û±: ØªÙ†Ø¸ÛŒÙ… Ù…Ø­ÛŒØ· Jupyter

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### Ù…Ø±Ø­Ù„Ù‡ Û²: Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø¬Ø¯ÛŒØ¯

**Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² VS Code:**
1. VS Code Ø±Ø§ Ø¯Ø± Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Module08 Ø¨Ø§Ø² Ú©Ù†ÛŒØ¯
2. ÛŒÚ© ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ Ù¾Ø³ÙˆÙ†Ø¯ `.ipynb` Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ÛŒØ¯
3. Ù‡Ù†Ú¯Ø§Ù… Ø¯Ø±Ø®ÙˆØ§Ø³ØªØŒ Ù‡Ø³ØªÙ‡ "Foundry Local" Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯
4. Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ø§ÙØ²ÙˆØ¯Ù† Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…Ø­ØªÙˆØ§ÛŒ Ø®ÙˆØ¯ Ú©Ù†ÛŒØ¯

**Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Jupyter Lab:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Ø¨Ù‡ØªØ±ÛŒÙ† Ø´ÛŒÙˆÙ‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©

#### Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("âœ… Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ Ùˆ ØªÙ…Ø±ÛŒÙ†â€ŒÙ‡Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ

#### ØªÙ…Ø±ÛŒÙ† Û±: ØªØ³Øª Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ú©Ù„Ø§ÛŒÙ†Øª

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b-instruct"},
]

for config in configurations:
    print(f"\nğŸ§ª Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'âœ… Success' if result['status'] == 'ok' else 'âŒ Failed'}")
```

#### ØªÙ…Ø±ÛŒÙ† Û²: Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® Ø§Ø³ØªØ±ÛŒÙ…

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("ğŸŒŠ Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\nâœ… Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Ø¨Ø®Ø´ Ûµ: Ø¯Ù…ÙˆÛŒ Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù…Ø±ÙˆØ±Ú¯Ø± WebGPU

### Ù…Ø±ÙˆØ± Ú©Ù„ÛŒ

WebGPU Ø§Ù…Ú©Ø§Ù† Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¯Ø± Ù…Ø±ÙˆØ±Ú¯Ø± Ø¨Ø±Ø§ÛŒ Ø­Ø¯Ø§Ú©Ø«Ø± Ø­Ø±ÛŒÙ… Ø®ØµÙˆØµÛŒ Ùˆ ØªØ¬Ø±Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ù†ØµØ¨ ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§ÛŒÙ† Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø¬Ø±Ø§ÛŒ ONNX Runtime Web Ø¨Ø§ WebGPU Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.

### Ù…Ø±Ø­Ù„Ù‡ Û±: Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ WebGPU

**Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±:**
- Chrome/Edge Ù†Ø³Ø®Ù‡ Û±Û±Û³+ Ø¨Ø§ WebGPU ÙØ¹Ø§Ù„
- Ø¨Ø±Ø±Ø³ÛŒ: `chrome://gpu` â†’ ØªØ£ÛŒÛŒØ¯ ÙˆØ¶Ø¹ÛŒØª "WebGPU"
- Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### Ù…Ø±Ø­Ù„Ù‡ Û²: Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ù…ÙˆÛŒ WebGPU

Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>ğŸš€ WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = 'âŒ WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'ğŸ” WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('âœ… ONNX Runtime session created with WebGPU');
        log(`ğŸ“Š Input names: ${session.inputNames.join(', ')}`);
        log(`ğŸ“Š Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = 'âœ… WebGPU inference complete!';
        log(`ğŸ¯ Predicted class: ${maxIdx}`);
        log(`ğŸ“ˆ Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `âŒ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### Ù…Ø±Ø­Ù„Ù‡ Û³: Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ù…Ùˆ

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Ø¨Ø®Ø´ Û¶: Ø§Ø¯ØºØ§Ù… Open WebUI

### Ù…Ø±ÙˆØ± Ú©Ù„ÛŒ

Open WebUI ÛŒÚ© Ø±Ø§Ø¨Ø· Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ ChatGPT Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ø¨Ù‡ API Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ OpenAI Foundry Local Ù…ØªØµÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

### Ù…Ø±Ø­Ù„Ù‡ Û±: Ù¾ÛŒØ´â€ŒÙ†ÛŒØ§Ø²Ù‡Ø§

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### Ù…Ø±Ø­Ù„Ù‡ Û²: ØªÙ†Ø¸ÛŒÙ… Docker (ØªÙˆØµÛŒÙ‡â€ŒØ´Ø¯Ù‡)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**ØªÙˆØ¬Ù‡:** `host.docker.internal` Ø¨Ù‡ Ú©Ø§Ù†ØªÛŒÙ†Ø±Ù‡Ø§ÛŒ Docker Ø§Ø¬Ø§Ø²Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ø¨Ù‡ Ù…Ø§Ø´ÛŒÙ† Ù…ÛŒØ²Ø¨Ø§Ù† Ø¯Ø± ÙˆÛŒÙ†Ø¯ÙˆØ² Ø¯Ø³ØªØ±Ø³ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ù†Ø¯.

### Ù…Ø±Ø­Ù„Ù‡ Û³: Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ

1. **Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† Ù…Ø±ÙˆØ±Ú¯Ø±:** Ø¨Ù‡ `http://localhost:3000` Ø¨Ø±ÙˆÛŒØ¯
2. **ØªÙ†Ø¸ÛŒÙ… Ø§ÙˆÙ„ÛŒÙ‡:** Ø§ÛŒØ¬Ø§Ø¯ Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ù…Ø¯ÛŒØ±
3. **Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„:**
   - ØªÙ†Ø¸ÛŒÙ…Ø§Øª â†’ Ù…Ø¯Ù„â€ŒÙ‡Ø§ â†’ API OpenAI  
   - Ø¢Ø¯Ø±Ø³ Ù¾Ø§ÛŒÙ‡: `http://host.docker.internal:51211/v1`
   - Ú©Ù„ÛŒØ¯ API: `foundry-local-key` (Ù‡Ø± Ù…Ù‚Ø¯Ø§Ø±ÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯)
4. **ØªØ³Øª Ø§ØªØµØ§Ù„:** Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ù…Ù†ÙˆÛŒ Ú©Ø´ÙˆÛŒÛŒ Ø¸Ø§Ù‡Ø± Ø´ÙˆÙ†Ø¯

### Ø±ÙØ¹ Ø§Ø´Ú©Ø§Ù„

**Ù…Ø´Ú©Ù„Ø§Øª Ø±Ø§ÛŒØ¬:**

1. **Ø§ØªØµØ§Ù„ Ø±Ø¯ Ø´Ø¯:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¸Ø§Ù‡Ø± Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯:**
   - ØªØ£ÛŒÛŒØ¯ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: `foundry model list`
   - Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø§Ø³Ø® API: `curl http://localhost:51211/v1/models`
   - Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¬Ø¯Ø¯ Ú©Ø§Ù†ØªÛŒÙ†Ø± Open WebUI

## Ø¨Ø®Ø´ Û·: Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ø³ØªÙ‚Ø±Ø§Ø± ØªÙˆÙ„ÛŒØ¯ÛŒ

### Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ø­ÛŒØ·

**ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØªÙˆØ³Ø¹Ù‡:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Ø§Ø³ØªÙ‚Ø±Ø§Ø± ØªÙˆÙ„ÛŒØ¯ÛŒ:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Ù…Ø´Ú©Ù„Ø§Øª Ø±Ø§ÛŒØ¬ Ù¾ÙˆØ±Øª Ùˆ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§

**Ù¾ÛŒØ´Ú¯ÛŒØ±ÛŒ Ø§Ø² ØªØ¹Ø§Ø±Ø¶ Ù¾ÙˆØ±Øª ÛµÛ±Û²Û±Û±:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Ù†Ø¸Ø§Ø±Øª Ø¨Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯

**Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## Ø®Ù„Ø§ØµÙ‡

Ø¬Ù„Ø³Ù‡ Û´ Ø¨Ù‡ Ø³Ø§Ø®Øª Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Chainlit Ø¢Ù…Ø§Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡â€ŒØ§ÛŒ Ù¾Ø±Ø¯Ø§Ø®Øª. Ø´Ù…Ø§ ÛŒØ§Ø¯ Ú¯Ø±ÙØªÛŒØ¯:

- âœ… **Ú†Ø§Ø±Ú†ÙˆØ¨ Chainlit**: Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ù…Ø¯Ø±Ù† Ùˆ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø³ØªØ±ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ú†Øª
- âœ… **Ø§Ø¯ØºØ§Ù… Foundry Local**: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² SDK Ùˆ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ  
- âœ… **Ø§Ø³ØªÙ†ØªØ§Ø¬ WebGPU**: Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù…Ø±ÙˆØ±Ú¯Ø± Ø¨Ø±Ø§ÛŒ Ø­Ø¯Ø§Ú©Ø«Ø± Ø­Ø±ÛŒÙ… Ø®ØµÙˆØµÛŒ
- âœ… **ØªÙ†Ø¸ÛŒÙ… Open WebUI**: Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø±Ø§Ø¨Ø· Ú†Øª Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ
- âœ… **Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ÛŒ**: Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ØŒ Ù†Ø¸Ø§Ø±Øª Ùˆ Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ

Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Û°Û´ Ø¨Ù‡ØªØ±ÛŒÙ† Ø´ÛŒÙˆÙ‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ø±Ø§Ø¨Ø·â€ŒÙ‡Ø§ÛŒ Ú†Øª Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ú©Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…Ø­Ù„ÛŒ Ø§Ø² Ø·Ø±ÛŒÙ‚ Microsoft Foundry Local Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ Ùˆ ØªØ¬Ø±Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¹Ø§Ù„ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯ØŒ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.

## Ù…Ù†Ø§Ø¨Ø¹

- **[Ù†Ù…ÙˆÙ†Ù‡ Û°Û´: Ø¨Ø±Ù†Ø§Ù…Ù‡ Chainlit](samples/04/README.md)**: Ø¨Ø±Ù†Ø§Ù…Ù‡ Ú©Ø§Ù…Ù„ Ø¨Ø§ Ù…Ø³ØªÙ†Ø¯Ø§Øª
- **[Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø¢Ù…ÙˆØ²Ø´ÛŒ Chainlit](samples/04/chainlit_app.ipynb)**: Ù…ÙˆØ§Ø¯ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ
- **[Ù…Ø³ØªÙ†Ø¯Ø§Øª Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Ù…Ø³ØªÙ†Ø¯Ø§Øª Ú©Ø§Ù…Ù„ Ù¾Ù„ØªÙØ±Ù…
- **[Ù…Ø³ØªÙ†Ø¯Ø§Øª Chainlit](https://docs.chainlit.io/)**: Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø±Ø³Ù…ÛŒ Ú†Ø§Ø±Ú†ÙˆØ¨
- **[Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø¯ØºØ§Ù… Open WebUI](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: Ø¢Ù…ÙˆØ²Ø´ Ø±Ø³Ù…ÛŒ

---

