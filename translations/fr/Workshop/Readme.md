<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "48d0fb38be925084a6ebd957d4b045e5",
  "translation_date": "2025-10-08T19:17:31+00:00",
  "source_file": "Workshop/Readme.md",
  "language_code": "fr"
}
-->
# EdgeAI pour D√©butants - Atelier

> **Parcours pratique pour cr√©er des applications Edge AI pr√™tes pour la production**
>
> Ma√Ætrisez le d√©ploiement d'IA locale avec Microsoft Foundry Local, depuis votre premi√®re compl√©tion de chat jusqu'√† l'orchestration multi-agents en 6 sessions progressives.

---

## üéØ Introduction

Bienvenue √† l'**Atelier EdgeAI pour D√©butants** - votre guide pratique pour construire des applications intelligentes fonctionnant enti√®rement sur du mat√©riel local. Cet atelier transforme les concepts th√©oriques de l'Edge AI en comp√©tences concr√®tes gr√¢ce √† des exercices progressifs utilisant Microsoft Foundry Local et des Small Language Models (SLMs).

### Pourquoi cet atelier ?

**La r√©volution Edge AI est l√†**

Les organisations du monde entier passent de l'IA d√©pendante du cloud √† l'informatique en p√©riph√©rie pour trois raisons essentielles :

1. **Confidentialit√© & Conformit√©** - Traitez les donn√©es sensibles localement sans transmission au cloud (HIPAA, RGPD, r√©glementations financi√®res)
2. **Performance** - √âliminez la latence r√©seau (50-500ms en local contre 500-2000ms pour un aller-retour cloud)
3. **Ma√Ætrise des co√ªts** - Supprimez les co√ªts par token des API et √©voluez sans frais cloud

**Mais l'Edge AI est diff√©rente**

Faire fonctionner l'IA sur site n√©cessite de nouvelles comp√©tences :
- S√©lection et optimisation des mod√®les pour des contraintes de ressources
- Gestion des services locaux et acc√©l√©ration mat√©rielle
- Ing√©nierie des prompts pour des mod√®les plus petits
- Mod√®les de d√©ploiement en production pour les appareils en p√©riph√©rie

**Cet atelier vous apporte ces comp√©tences**

En 6 sessions cibl√©es (~3 heures au total), vous passerez de "Hello World" au d√©ploiement de syst√®mes multi-agents pr√™ts pour la production - tout cela fonctionnant localement sur votre machine.

---

## üìö Objectifs d'apprentissage

En compl√©tant cet atelier, vous serez capable de :

### Comp√©tences principales
1. **D√©ployer et g√©rer des services IA locaux**
   - Installer et configurer Microsoft Foundry Local
   - S√©lectionner les mod√®les appropri√©s pour un d√©ploiement en p√©riph√©rie
   - G√©rer le cycle de vie des mod√®les (t√©l√©chargement, chargement, mise en cache)
   - Surveiller l'utilisation des ressources et optimiser les performances

2. **Cr√©er des applications aliment√©es par l'IA**
   - Impl√©menter des compl√©tions de chat compatibles OpenAI localement
   - Concevoir des prompts efficaces pour les Small Language Models
   - G√©rer les r√©ponses en streaming pour une meilleure exp√©rience utilisateur
   - Int√©grer des mod√®les locaux dans des applications existantes

3. **Cr√©er des syst√®mes RAG (Retrieval Augmented Generation)**
   - Construire une recherche s√©mantique avec des embeddings
   - Ancrer les r√©ponses des LLM dans des connaissances sp√©cifiques au domaine
   - √âvaluer la qualit√© des RAG avec des m√©triques standard de l'industrie
   - Passer du prototype √† la production

4. **Optimiser les performances des mod√®les**
   - √âvaluer plusieurs mod√®les pour votre cas d'utilisation
   - Mesurer la latence, le d√©bit et le temps du premier token
   - S√©lectionner les mod√®les optimaux en fonction des compromis vitesse/qualit√©
   - Comparer les compromis entre SLM et LLM dans des sc√©narios r√©els

5. **Orchestrer des syst√®mes multi-agents**
   - Concevoir des agents sp√©cialis√©s pour diff√©rentes t√¢ches
   - Impl√©menter la m√©moire et la gestion du contexte des agents
   - Coordonner les agents dans des flux de travail complexes
   - Acheminer intelligemment les requ√™tes entre plusieurs mod√®les

6. **D√©ployer des solutions pr√™tes pour la production**
   - Impl√©menter la gestion des erreurs et la logique de reprise
   - Surveiller l'utilisation des tokens et les ressources syst√®me
   - Construire des architectures √©volutives avec des mod√®les comme outils
   - Planifier des migrations de l'edge vers des solutions hybrides (edge + cloud)

---

## üéì R√©sultats d'apprentissage

### Ce que vous construirez

√Ä la fin de cet atelier, vous aurez cr√©√© :

| Session | Livrable | Comp√©tences d√©montr√©es |
|---------|----------|-------------------------|
| **1** | Application de chat avec streaming | Configuration du service, compl√©tions de base, UX en streaming |
| **2** | Syst√®me RAG avec √©valuation | Embeddings, recherche s√©mantique, m√©triques de qualit√© |
| **3** | Suite de benchmarks multi-mod√®les | Mesure des performances, comparaison des mod√®les |
| **4** | Comparateur SLM vs LLM | Analyse des compromis, strat√©gies d'optimisation |
| **5** | Orchestrateur multi-agents | Conception d'agents, gestion de la m√©moire, coordination |
| **6** | Syst√®me de routage intelligent | D√©tection d'intention, s√©lection de mod√®les, √©volutivit√© |

### Matrice de comp√©tences

| Niveau de comp√©tence | Session 1-2 | Session 3-4 | Session 5-6 |
|-----------------------|-------------|-------------|-------------|
| **D√©butant** | ‚úÖ Configuration & bases | ‚ö†Ô∏è D√©fiant | ‚ùå Trop avanc√© |
| **Interm√©diaire** | ‚úÖ R√©vision rapide | ‚úÖ Apprentissage cl√© | ‚ö†Ô∏è Objectifs ambitieux |
| **Avanc√©** | ‚úÖ Facile | ‚úÖ Affinement | ‚úÖ Mod√®les de production |

### Comp√©tences pr√™tes pour le march√©

**Apr√®s cet atelier, vous serez pr√™t(e) √† :**

‚úÖ **Cr√©er des applications ax√©es sur la confidentialit√©**
- Applications de sant√© traitant des donn√©es PHI/PII localement
- Services financiers conformes aux r√©glementations
- Syst√®mes gouvernementaux respectant la souverainet√© des donn√©es

‚úÖ **Optimiser pour les environnements Edge**
- Appareils IoT avec des ressources limit√©es
- Applications mobiles fonctionnant hors ligne
- Syst√®mes en temps r√©el √† faible latence

‚úÖ **Concevoir des architectures intelligentes**
- Syst√®mes multi-agents pour des flux de travail complexes
- D√©ploiements hybrides edge-cloud
- Infrastructures IA optimis√©es pour les co√ªts

‚úÖ **Diriger des initiatives Edge AI**
- √âvaluer la faisabilit√© de l'Edge AI pour des projets
- S√©lectionner les mod√®les et frameworks appropri√©s
- Architecturer des solutions IA locales √©volutives

---

## üó∫Ô∏è Structure de l'atelier

### Aper√ßu des sessions (6 sessions √ó 30 minutes = 3 heures)

| Session | Sujet | Focus | Dur√©e |
|---------|-------|-------|-------|
| **1** | D√©marrage avec Foundry Local | Installation, validation, premi√®res compl√©tions | 30 min |
| **2** | Construire des solutions IA avec RAG | Ing√©nierie des prompts, embeddings, √©valuation | 30 min |
| **3** | Mod√®les open source | D√©couverte, benchmarking, s√©lection | 30 min |
| **4** | Mod√®les de pointe | SLM vs LLM, optimisation, frameworks | 30 min |
| **5** | Agents aliment√©s par l'IA | Conception d'agents, orchestration, m√©moire | 30 min |
| **6** | Mod√®les comme outils | Routage, encha√Ænement, strat√©gies d'√©volutivit√© | 30 min |

---

## üöÄ D√©marrage rapide

### Pr√©requis

**Configuration syst√®me :**
- **OS** : Windows 10/11, macOS 11+ ou Linux (Ubuntu 20.04+)
- **RAM** : 8 Go minimum, 16 Go+ recommand√©
- **Stockage** : 10 Go+ d'espace libre pour les mod√®les
- **CPU** : Processeur moderne avec support AVX2
- **GPU** (optionnel) : Compatible CUDA ou NPU Qualcomm pour l'acc√©l√©ration

**Logiciels requis :**
- **Python 3.8+** ([T√©l√©charger](https://www.python.org/downloads/))
- **Microsoft Foundry Local** ([Guide d'installation](../../../Workshop))
- **Git** ([T√©l√©charger](https://git-scm.com/downloads))
- **Visual Studio Code** (recommand√©) ([T√©l√©charger](https://code.visualstudio.com/))

### Configuration en 3 √©tapes

#### 1. Installer Foundry Local

**Windows :**
```powershell
winget install Microsoft.FoundryLocal
```

**macOS :**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**V√©rifier l'installation :**
```bash
foundry --version
foundry service status
```

#### 2. Cloner le d√©p√¥t & installer les d√©pendances

```bash
# Clone repository
git clone https://github.com/microsoft/edgeai-for-beginners.git
cd edgeai-for-beginners/Workshop

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.\.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

#### 3. Ex√©cuter votre premier exemple

```bash
# Start Foundry Local and load a model
foundry model run phi-4-mini

# Run the chat bootstrap sample
cd samples/session01
python chat_bootstrap.py "What is edge AI?"
```

**‚úÖ Succ√®s !** Vous devriez voir une r√©ponse en streaming sur l'Edge AI.

---

## üì¶ Ressources de l'atelier

### Exemples Python

Exemples pratiques progressifs d√©montrant chaque concept :

| Session | Exemple | Description | Temps d'ex√©cution |
|---------|---------|-------------|-------------------|
| 1 | [`chat_bootstrap.py`](../../../Workshop/samples/session01/chat_bootstrap.py) | Chat de base & streaming | ~30s |
| 2 | [`rag_pipeline.py`](../../../Workshop/samples/session02/rag_pipeline.py) | RAG avec embeddings | ~45s |
| 2 | [`rag_eval_ragas.py`](../../../Workshop/samples/session02/rag_eval_ragas.py) | √âvaluation de la qualit√© RAG | ~60s |
| 3 | [`benchmark_oss_models.py`](../../../Workshop/samples/session03/benchmark_oss_models.py) | Benchmarking multi-mod√®les | ~2-3m |
| 4 | [`model_compare.py`](../../../Workshop/samples/session04/model_compare.py) | Comparaison SLM vs LLM | ~45s |
| 5 | [`agents_orchestrator.py`](../../../Workshop/samples/session05/agents_orchestrator.py) | Syst√®me multi-agents | ~60s |
| 6 | [`models_router.py`](../../../Workshop/samples/session06/models_router.py) | Routage bas√© sur l'intention | ~45s |
| 6 | [`models_pipeline.py`](../../../Workshop/samples/session06/models_pipeline.py) | Pipeline multi-√©tapes | ~60s |

### Notebooks Jupyter

Exploration interactive avec explications et visualisations :

| Session | Notebook | Description | Difficult√© |
|---------|----------|-------------|------------|
| 1 | [`session01_chat_bootstrap.ipynb`](./notebooks/session01_chat_bootstrap.ipynb) | Bases du chat & streaming | ‚≠ê D√©butant |
| 2 | [`session02_rag_pipeline.ipynb`](./notebooks/session02_rag_pipeline.ipynb) | Construire un syst√®me RAG | ‚≠ê‚≠ê Interm√©diaire |
| 2 | [`session02_rag_eval_ragas.ipynb`](./notebooks/session02_rag_eval_ragas.ipynb) | √âvaluer la qualit√© RAG | ‚≠ê‚≠ê Interm√©diaire |
| 3 | [`session03_benchmark_oss_models.ipynb`](./notebooks/session03_benchmark_oss_models.ipynb) | Benchmarking des mod√®les | ‚≠ê‚≠ê Interm√©diaire |
| 4 | [`session04_model_compare.ipynb`](./notebooks/session04_model_compare.ipynb) | Comparaison des mod√®les | ‚≠ê‚≠ê Interm√©diaire |
| 5 | [`session05_agents_orchestrator.ipynb`](./notebooks/session05_agents_orchestrator.ipynb) | Orchestration d'agents | ‚≠ê‚≠ê‚≠ê Avanc√© |
| 6 | [`session06_models_router.ipynb`](./notebooks/session06_models_router.ipynb) | Routage bas√© sur l'intention | ‚≠ê‚≠ê‚≠ê Avanc√© |
| 6 | [`session06_models_pipeline.ipynb`](./notebooks/session06_models_pipeline.ipynb) | Orchestration de pipeline | ‚≠ê‚≠ê‚≠ê Avanc√© |

### Documentation

Guides et r√©f√©rences complets :

| Document | Description | Utilisation |
|----------|-------------|-------------|
| [QUICK_START.md](./QUICK_START.md) | Guide de configuration rapide | D√©marrage de z√©ro |
| [QUICK_REFERENCE.md](./QUICK_REFERENCE.md) | Fiche de r√©f√©rence commandes & API | R√©ponses rapides |
| [FOUNDRY_SDK_QUICKREF.md](./FOUNDRY_SDK_QUICKREF.md) | Mod√®les & exemples SDK | √âcriture de code |
| [ENV_CONFIGURATION.md](./ENV_CONFIGURATION.md) | Guide des variables d'environnement | Configuration des exemples |
| [SAMPLES_UPDATE_SUMMARY.md](./SAMPLES_UPDATE_SUMMARY.md) | Am√©liorations des exemples | Comprendre les changements |
| [SDK_MIGRATION_NOTES.md](./SDK_MIGRATION_NOTES.md) | Guide de migration | Mise √† jour du code |
| [notebooks/TROUBLESHOOTING.md](./notebooks/TROUBLESHOOTING.md) | Probl√®mes courants & solutions | D√©bogage |

---

## üéì Recommandations de parcours d'apprentissage

### Pour les d√©butants (3-4 heures)
1. ‚úÖ Session 1 : D√©marrage (focus sur la configuration et le chat de base)
2. ‚úÖ Session 2 : Bases du RAG (ignorez l'√©valuation au d√©but)
3. ‚úÖ Session 3 : Benchmarking simple (2 mod√®les seulement)
4. ‚è≠Ô∏è Ignorez les sessions 4-6 pour l'instant
5. üîÑ Revenez aux sessions 4-6 apr√®s avoir construit votre premi√®re application

### Pour les d√©veloppeurs interm√©diaires (3 heures)
1. ‚ö° Session 1 : Validation rapide de la configuration
2. ‚úÖ Session 2 : Pipeline RAG complet avec √©valuation
3. ‚úÖ Session 3 : Suite compl√®te de benchmarking
4. ‚úÖ Session 4 : Optimisation des mod√®les
5. ‚úÖ Sessions 5-6 : Focus sur les mod√®les d'architecture

### Pour les praticiens avanc√©s (2-3 heures)
1. ‚ö° Sessions 1-3 : R√©vision rapide et validation
2. ‚úÖ Session 4 : Approfondissement de l'optimisation
3. ‚úÖ Session 5 : Architecture multi-agents
4. ‚úÖ Session 6 : Mod√®les de production et √©volutivit√©
5. üöÄ Extension : Construire une logique de routage personnalis√©e et des d√©ploiements hybrides

---

## Pack de sessions d'atelier (Laboratoires cibl√©s de 30 minutes)

Si vous suivez le format condens√© en 6 sessions, utilisez ces guides d√©di√©s (chacun correspond et compl√®te les modules plus larges ci-dessus) :

| Session d'atelier | Guide | Focus principal |
|-------------------|-------|-----------------|
| 1 | [Session01-GettingStartedFoundryLocal](./Session01-GettingStartedFoundryLocal.md) | Installation, validation, ex√©cution de phi & GPT-OSS-20B, acc√©l√©ration |
| 2 | [Session02-BuildAISolutionsRAG](./Session02-BuildAISolutionsRAG.md) | Ing√©nierie des prompts, mod√®les RAG, ancrage dans des documents, migration |
| 3 | [Session03-OpenSourceModels](./Session03-OpenSourceModels.md) | Int√©gration Hugging Face, benchmarking, s√©lection de mod√®les |
| 4 | [Session04-CuttingEdgeModels](./Session04-CuttingEdgeModels.md) | SLM vs LLM, WebGPU, Chainlit RAG, acc√©l√©ration ONNX |
| 5 | [Session05-AIPoweredAgents](./Session05-AIPoweredAgents.md) | R√¥les des agents, m√©moire, outils, orchestration |
| 6 | [Session06-ModelsAsTools](./Session06-ModelsAsTools.md) | Routage, encha√Ænement, chemin vers Azure |
Chaque fichier de session inclut : r√©sum√©, objectifs d'apprentissage, d√©roulement de la d√©monstration de 30 minutes, projet de d√©part, liste de validation, d√©pannage et r√©f√©rences au SDK Python Local Foundry officiel.

### Scripts d'exemple

Installer les d√©pendances de l'atelier (Windows) :

```powershell
cd Workshop
py -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt
```

macOS / Linux :

```bash
cd Workshop
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Si le service Foundry Local est ex√©cut√© sur une autre machine (Windows) ou VM depuis macOS, exportez le point de terminaison :

```bash
export FOUNDRY_LOCAL_ENDPOINT=http://<windows-host>:5273/v1
```

| Session | Script(s) | Description |
|---------|-----------|-------------|
| 1 | `samples/session01/chat_bootstrap.py` | Service de d√©marrage & chat en streaming |
| 2 | `samples/session02/rag_pipeline.py` | RAG minimal (embeddings en m√©moire) |
|   | `samples/session02/rag_eval_ragas.py` | √âvaluation RAG avec m√©triques ragas |
| 3 | `samples/session03/benchmark_oss_models.py` | Benchmarking de latence & d√©bit multi-mod√®les |
| 4 | `samples/session04/model_compare.py` | Comparaison SLM vs LLM (latence & sortie d'√©chantillon) |
| 5 | `samples/session05/agents_orchestrator.py` | Pipeline de recherche √† deux agents ‚Üí √©ditorial |
| 6 | `samples/session06/models_router.py` | D√©mo de routage bas√© sur l'intention |
|   | `samples/session06/models_pipeline.py` | Cha√Æne de planification/ex√©cution/raffinement en plusieurs √©tapes |

### Variables d'environnement (communes √† tous les exemples)

| Variable | Objectif | Exemple |
|----------|----------|---------|
| `FOUNDRY_LOCAL_ALIAS` | Alias par d√©faut pour un mod√®le unique dans les exemples de base | `phi-4-mini` |
| `SLM_ALIAS` / `LLM_ALIAS` | Mod√®le SLM explicite vs mod√®le plus grand pour comparaison | `phi-4-mini` / `gpt-oss-20b` |
| `BENCH_MODELS` | Liste d'alias de mod√®les √† tester | `qwen2.5-0.5b,gemma-2-2b,mistral-7b` |
| `BENCH_ROUNDS` | R√©p√©titions de benchmark par mod√®le | `3` |
| `BENCH_PROMPT` | Prompt utilis√© pour le benchmarking | `Explain retrieval augmented generation briefly.` |
| `EMBED_MODEL` | Mod√®le d'embedding de type sentence-transformers | `sentence-transformers/all-MiniLM-L6-v2` |
| `RAG_QUESTION` | Remplacement de la requ√™te de test pour le pipeline RAG | `Why use RAG with local inference?` |
| `AGENT_QUESTION` | Remplacement de la requ√™te pour le pipeline d'agents | `Explain why edge AI matters for compliance.` |
| `AGENT_MODEL_PRIMARY` | Alias du mod√®le pour l'agent de recherche | `phi-4-mini` |
| `AGENT_MODEL_EDITOR` | Alias du mod√®le pour l'agent √©diteur (peut √™tre diff√©rent) | `gpt-oss-20b` |
| `SHOW_USAGE` | Lorsque `1`, affiche l'utilisation des tokens par compl√©tion | `1` |
| `RETRY_ON_FAIL` | Lorsque `1`, r√©essaye une fois en cas d'erreurs de chat transitoires | `1` |
| `RETRY_BACKOFF` | Temps d'attente avant de r√©essayer (en secondes) | `1.0` |

Si une variable n'est pas d√©finie, les scripts utilisent des valeurs par d√©faut. Pour les d√©monstrations avec un seul mod√®le, vous avez g√©n√©ralement seulement besoin de `FOUNDRY_LOCAL_ALIAS`.

### Module utilitaire

Tous les exemples partagent d√©sormais un helper `samples/workshop_utils.py` qui fournit :

* Cr√©ation mise en cache de `FoundryLocalManager` + client OpenAI
* Helper `chat_once()` avec option de r√©essai + affichage de l'utilisation
* Rapport simple sur l'utilisation des tokens (activ√© via `SHOW_USAGE=1`)

Cela r√©duit la duplication et met en avant les bonnes pratiques pour une orchestration efficace des mod√®les locaux.

## Am√©liorations optionnelles (inter-sessions)

| Th√®me | Am√©lioration | Sessions | Env / Toggle |
|-------|-------------|----------|--------------|
| D√©terminisme | Temp√©rature fixe + ensembles de prompts stables | 1‚Äì6 | D√©finir `temperature=0`, `top_p=1` |
| Visibilit√© de l'utilisation des tokens | Enseignement coh√©rent sur les co√ªts/efficacit√© | 1‚Äì6 | `SHOW_USAGE=1` |
| Premier token en streaming | M√©trique de latence per√ßue | 1,3,4,6 | `BENCH_STREAM=1` (benchmark) |
| R√©silience au r√©essai | G√®re les d√©marrages √† froid transitoires | Toutes | `RETRY_ON_FAIL=1` + `RETRY_BACKOFF` |
| Agents multi-mod√®les | Sp√©cialisation des r√¥les h√©t√©rog√®nes | 5 | `AGENT_MODEL_PRIMARY`, `AGENT_MODEL_EDITOR` |
| Routage adaptatif | Heuristiques d'intention + co√ªt | 6 | √âtendre le routeur avec une logique d'escalade |
| M√©moire vectorielle | Rappel s√©mantique √† long terme | 2,5,6 | Int√©grer un index d'embedding FAISS/Chroma |
| Export de trace | Audit & √©valuation | 2,5,6 | Ajouter des lignes JSON par √©tape |
| Rubriques de qualit√© | Suivi qualitatif | 3‚Äì6 | Prompts de scoring secondaires |
| Tests de fum√©e | Validation rapide avant atelier | Toutes | `python Workshop/tests/smoke.py` |

### D√©marrage rapide d√©terministe

```powershell
set FOUNDRY_LOCAL_ALIAS=phi-4-mini
set SHOW_USAGE=1
python Workshop\tests\smoke.py
```

Attendez-vous √† des comptes de tokens stables pour des entr√©es identiques r√©p√©t√©es.

### √âvaluation RAG (Session 2)

Utilisez `rag_eval_ragas.py` pour calculer la pertinence des r√©ponses, leur fid√©lit√© et la pr√©cision contextuelle sur un petit ensemble de donn√©es synth√©tiques :

```powershell
python samples/session02/rag_eval_ragas.py
```

√âtendez en fournissant un JSONL plus grand contenant des questions, des contextes et des v√©rit√©s terrain, puis convertissez-le en un `Dataset` Hugging Face.

## Annexe sur la pr√©cision des commandes CLI

L'atelier utilise d√©lib√©r√©ment uniquement les commandes CLI Foundry Local document√©es / stables.

### Commandes stables r√©f√©renc√©es

| Cat√©gorie | Commande | Objectif |
|-----------|----------|----------|
| Core | `foundry --version` | Afficher la version install√©e |
| Core | `foundry init` | Initialiser la configuration |
| Service | `foundry service start` | D√©marrer le service local (si non automatique) |
| Service | `foundry status` | Afficher le statut du service |
| Mod√®les | `foundry model list` | Lister le catalogue / mod√®les disponibles |
| Mod√®les | `foundry model download <alias>` | T√©l√©charger les poids du mod√®le dans le cache |
| Mod√®les | `foundry model run <alias>` | Lancer (charger) un mod√®le localement ; combiner avec `--prompt` pour une ex√©cution unique |
| Mod√®les | `foundry model unload <alias>` / `foundry model stop <alias>` | D√©charger un mod√®le de la m√©moire (si pris en charge) |
| Cache | `foundry cache list` | Lister les mod√®les mis en cache (t√©l√©charg√©s) |
| Syst√®me | `foundry system info` | Instantan√© des capacit√©s mat√©rielles & d'acc√©l√©ration |
| Syst√®me | `foundry system gpu-info` | Infos de diagnostic GPU |
| Config | `foundry config list` | Afficher les valeurs de configuration actuelles |
| Config | `foundry config set <key> <value>` | Mettre √† jour la configuration |

### Mod√®le de prompt unique

Au lieu de la sous-commande obsol√®te `model chat`, utilisez :

```powershell
foundry model run <alias> --prompt "Your question here"
```

Cela ex√©cute un cycle unique de prompt/r√©ponse puis quitte.

### Mod√®les supprim√©s / √©vit√©s

| Obsol√®te / Non document√© | Remplacement / Recommandation |
|--------------------------|------------------------------|
| `foundry model chat <model> "..."` | `foundry model run <model> --prompt "..."` |
| `foundry model list --running` | Utilisez simplement `foundry model list` + activit√© r√©cente / journaux |
| `foundry model list --cached` | `foundry cache list` |
| `foundry model stats <model>` | Utilisez le script de benchmark Python + outils OS (Gestionnaire de t√¢ches / `nvidia-smi`) |
| `foundry model benchmark ...` | `samples/session03/benchmark_oss_models.py` |

### Benchmarking & T√©l√©m√©trie

- Latence, p95, tokens/sec : `samples/session03/benchmark_oss_models.py`
- Latence du premier token (streaming) : d√©finir `BENCH_STREAM=1`
- Utilisation des ressources : moniteurs OS (Gestionnaire de t√¢ches, Moniteur d'activit√©, `nvidia-smi`) + `foundry system info`.

√Ä mesure que de nouvelles commandes de t√©l√©m√©trie CLI se stabilisent en amont, elles peuvent √™tre int√©gr√©es avec des modifications minimales aux fichiers markdown de session.

### Garde de lint automatis√©e

Un linter automatis√© emp√™che la r√©introduction de mod√®les CLI obsol√®tes dans les blocs de code des fichiers markdown :

Script : `Workshop/scripts/lint_markdown_cli.py`

Les mod√®les obsol√®tes sont bloqu√©s dans les blocs de code.

Remplacements recommand√©s :
| Obsol√®te | Remplacement |
|----------|-------------|
| `foundry model chat <a> "..."` | `foundry model run <a> --prompt "..."` |
| `model list --running` | `model list` |
| `model list --cached` | `cache list` |
| `model stats` | Script de benchmark + outils syst√®me |
| `model benchmark` | `samples/session03/benchmark_oss_models.py` |
| `model list --available` | `model list` |

Ex√©cuter localement :
```powershell
python Workshop\scripts\lint_markdown_cli.py --verbose
```

Action GitHub : `.github/workflows/markdown-cli-lint.yml` s'ex√©cute √† chaque push & PR.

Hook pr√©-commit optionnel :
```bash
echo "python Workshop/scripts/lint_markdown_cli.py" > .git/hooks/pre-commit
chmod +x .git/hooks/pre-commit
```

## Tableau de migration rapide CLI ‚Üí SDK

| T√¢che | Ligne de commande CLI | √âquivalent SDK (Python) | Remarques |
|-------|-----------------------|-------------------------|-----------|
| Ex√©cuter un mod√®le une fois (prompt) | `foundry model run phi-4-mini --prompt "Hello"` | `manager=FoundryLocalManager("phi-4-mini"); client=OpenAI(base_url=manager.endpoint, api_key=manager.api_key or "not-needed"); client.chat.completions.create(model=manager.get_model_info("phi-4-mini").id, messages=[{"role":"user","content":"Hello"}])` | Le SDK initialise automatiquement le service & le cache |
| T√©l√©charger (mettre en cache) un mod√®le | `foundry model download qwen2.5-0.5b` | `FoundryLocalManager("qwen2.5-0.5b")  # d√©clenche le t√©l√©chargement/chargement` | Le gestionnaire choisit la meilleure variante si l'alias correspond √† plusieurs versions |
| Lister le catalogue | `foundry model list` | `# utiliser le gestionnaire pour chaque alias ou maintenir une liste connue` | Le CLI agr√®ge ; le SDK actuellement par instanciation d'alias |
| Lister les mod√®les mis en cache | `foundry cache list` | `manager.list_cached_models()` | Apr√®s l'initialisation du gestionnaire (n'importe quel alias) |
| Activer l'acc√©l√©ration GPU | `foundry config set compute.onnx.enable_gpu true` | `# Action CLI ; le SDK suppose que la configuration est d√©j√† appliqu√©e` | La configuration est un effet secondaire externe |
| Obtenir l'URL du point de terminaison | (implicite) | `manager.endpoint` | Utilis√© pour cr√©er un client compatible OpenAI |
| Pr√©parer un mod√®le | `foundry model run <alias>` puis premier prompt | `chat_once(alias, messages=[...])` (utilitaire) | Les utilitaires g√®rent le pr√©chauffage initial de la latence √† froid |
| Mesurer la latence | `python benchmark_oss_models.py` | `import benchmark_oss_models` (ou nouveau script d'exportation) | Pr√©f√©rez le script pour des m√©triques coh√©rentes |
| Arr√™ter / d√©charger un mod√®le | `foundry model unload <alias>` | (Non expos√© ‚Äì red√©marrer le service / processus) | G√©n√©ralement non requis pour le flux de l'atelier |
| R√©cup√©rer l'utilisation des tokens | (voir la sortie) | `resp.usage.total_tokens` | Fourni si l'objet d'utilisation est retourn√© par le backend |

## Export Markdown de Benchmark

Utilisez le script `Workshop/scripts/export_benchmark_markdown.py` pour ex√©cuter un benchmark r√©cent (m√™me logique que `samples/session03/benchmark_oss_models.py`) et g√©n√©rer un tableau Markdown adapt√© √† GitHub ainsi qu'un fichier JSON brut.

### Exemple

```powershell
python Workshop\scripts\export_benchmark_markdown.py --models "qwen2.5-0.5b,gemma-2-2b,mistral-7b" --prompt "Explain retrieval augmented generation briefly." --rounds 3 --output benchmark_report.md
```

Fichiers g√©n√©r√©s :
| Fichier | Contenu |
|---------|---------|
| `benchmark_report.md` | Tableau Markdown + conseils d'interpr√©tation |
| `benchmark_report.json` | Tableau de m√©triques brutes (pour comparaison / suivi des tendances) |

D√©finir `BENCH_STREAM=1` dans l'environnement pour inclure la latence du premier token si pris en charge.

---

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.