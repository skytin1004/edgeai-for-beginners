<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2f1754a482b6a84e07287a5b775e65b6",
  "translation_date": "2025-09-30T22:55:54+00:00",
  "source_file": "Module08/samples/04/README.md",
  "language_code": "fr"
}
-->
# Exemple 04 : Applications de chat en production avec Chainlit

Un exemple complet dÃ©montrant plusieurs approches pour crÃ©er des applications de chat prÃªtes pour la production en utilisant Microsoft Foundry Local, avec des interfaces web modernes, des rÃ©ponses en streaming et des technologies de navigateur de pointe.

## Contenu inclus

- **ðŸš€ Application de chat Chainlit** (`app.py`) : Application de chat prÃªte pour la production avec streaming
- **ðŸŒ DÃ©mo WebGPU** (`webgpu-demo/`) : InfÃ©rence IA basÃ©e sur le navigateur avec accÃ©lÃ©ration matÃ©rielle
- **ðŸŽ¨ IntÃ©gration Open WebUI** (`open-webui-guide.md`) : Interface professionnelle similaire Ã  ChatGPT
- **ðŸ“š Notebook Ã©ducatif** (`chainlit_app.ipynb`) : MatÃ©riel d'apprentissage interactif

## DÃ©marrage rapide

### 1. Application de chat Chainlit

```cmd
# Navigate to Module08 directory
cd Module08

# Start your model
foundry model run phi-4-mini

# Run Chainlit app (using port 8080 to avoid conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Accessible Ã  : `http://localhost:8080`

### 2. DÃ©mo WebGPU dans le navigateur

```cmd
# Navigate to WebGPU demo
cd Module08\samples\04\webgpu-demo

# Serve the demo
python -m http.server 5173
```

Accessible Ã  : `http://localhost:5173`

### 3. Configuration Open WebUI

```cmd
# Run Open WebUI with Docker
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

Accessible Ã  : `http://localhost:3000`

## ModÃ¨les d'architecture

### Matrice de dÃ©cision Local vs Cloud

| ScÃ©nario | Recommandation | Raison |
|----------|----------------|--------|
| **DonnÃ©es sensibles** | ðŸ  Local (Foundry) | Les donnÃ©es ne quittent jamais l'appareil |
| **Raisonnement complexe** | â˜ï¸ Cloud (Azure OpenAI) | AccÃ¨s Ã  des modÃ¨les plus grands |
| **Chat en temps rÃ©el** | ðŸ  Local (Foundry) | Latence rÃ©duite, rÃ©ponses plus rapides |
| **Analyse de documents** | ðŸ”„ Hybride | Extraction locale, analyse dans le cloud |
| **GÃ©nÃ©ration de code** | ðŸ  Local (Foundry) | ConfidentialitÃ© + modÃ¨les spÃ©cialisÃ©s |
| **TÃ¢ches de recherche** | â˜ï¸ Cloud (Azure OpenAI) | NÃ©cessite une base de connaissances Ã©tendue |

### Comparaison des technologies

| Technologie | Cas d'utilisation | Avantages | InconvÃ©nients |
|-------------|--------------------|-----------|---------------|
| **Chainlit** | DÃ©veloppeurs Python, prototypage rapide | Configuration facile, support du streaming | LimitÃ© Ã  Python |
| **WebGPU** | ConfidentialitÃ© maximale, scÃ©narios hors ligne | Natif au navigateur, aucun serveur requis | Taille de modÃ¨le limitÃ©e |
| **Open WebUI** | DÃ©ploiement en production, Ã©quipes | Interface professionnelle, gestion des utilisateurs | NÃ©cessite Docker |

## PrÃ©requis

- **Foundry Local** : InstallÃ© et en cours d'exÃ©cution ([TÃ©lÃ©charger](https://aka.ms/foundry-local-installer))
- **Python** : Version 3.10+ avec environnement virtuel
- **ModÃ¨le** : Au moins un modÃ¨le chargÃ© (`foundry model run phi-4-mini`)
- **Navigateur** : Chrome/Edge avec support WebGPU pour les dÃ©mos
- **Docker** : Pour Open WebUI (optionnel)

## Installation et configuration

### 1. Configuration de l'environnement Python

```cmd
# Navigate to Module08 directory
cd Module08

# Create and activate virtual environment
py -m venv .venv
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configuration de Foundry Local

```cmd
# Verify Foundry Local installation
foundry --version

# Start the service
foundry service start

# Load a model
foundry model run phi-4-mini

# Verify model is running
foundry service ps
```

## Applications d'exemple

### Application de chat Chainlit

**CaractÃ©ristiques :**
- ðŸš€ **Streaming en temps rÃ©el** : Les tokens apparaissent au fur et Ã  mesure de leur gÃ©nÃ©ration
- ðŸ›¡ï¸ **Gestion robuste des erreurs** : DÃ©gradation et rÃ©cupÃ©ration en douceur
- ðŸŽ¨ **Interface moderne** : Interface de chat professionnelle prÃªte Ã  l'emploi
- ðŸ”§ **Configuration flexible** : Variables d'environnement et dÃ©tection automatique
- ðŸ“± **Design rÃ©actif** : Fonctionne sur ordinateurs et appareils mobiles

**DÃ©marrage rapide :**
```cmd
# Run with default settings (recommended)
chainlit run samples\04\app.py -w --port 8080

# Use specific model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Manual endpoint configuration
set BASE_URL=http://localhost:51211
set API_KEY=your-api-key
chainlit run samples\04\app.py -w --port 8080
```

### DÃ©mo WebGPU dans le navigateur

**CaractÃ©ristiques :**
- ðŸŒ **IA native au navigateur** : Aucun serveur requis, fonctionne entiÃ¨rement dans le navigateur
- âš¡ **AccÃ©lÃ©ration WebGPU** : AccÃ©lÃ©ration matÃ©rielle lorsque disponible
- ðŸ”’ **ConfidentialitÃ© maximale** : Les donnÃ©es ne quittent jamais votre appareil
- ðŸŽ¯ **Installation zÃ©ro** : Fonctionne dans tout navigateur compatible
- ðŸ”„ **Fallback en douceur** : Bascule sur le CPU si WebGPU n'est pas disponible

**ExÃ©cution :**
```cmd
cd samples\04\webgpu-demo
python -m http.server 5173
# Open http://localhost:5173
```

### IntÃ©gration Open WebUI

**CaractÃ©ristiques :**
- ðŸŽ¨ **Interface similaire Ã  ChatGPT** : UI professionnelle et familiÃ¨re
- ðŸ‘¥ **Support multi-utilisateurs** : Comptes utilisateurs et historique des conversations
- ðŸ“ **Traitement de fichiers** : TÃ©lÃ©chargement et analyse de documents
- ðŸ”„ **Changement de modÃ¨le** : Commutation facile entre diffÃ©rents modÃ¨les
- ðŸ³ **DÃ©ploiement Docker** : Configuration prÃªte pour la production en conteneur

**Configuration rapide :**
```cmd
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

## RÃ©fÃ©rences de configuration

### Variables d'environnement

| Variable | Description | Valeur par dÃ©faut | Exemple |
|----------|-------------|-------------------|---------|
| `MODEL` | Alias du modÃ¨le Ã  utiliser | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Point de terminaison Foundry Local | DÃ©tectÃ© automatiquement | `http://localhost:51211` |
| `API_KEY` | ClÃ© API (optionnelle pour local) | `""` | `your-api-key` |

## RÃ©solution des problÃ¨mes

### ProblÃ¨mes courants

**Application Chainlit :**

1. **Service non disponible :**
   ```cmd
   # Check Foundry Local status
   foundry service status
   foundry service ps
   
   # Validate API endpoint (note: port 51211)
   curl http://localhost:51211/v1/models
   ```

2. **Conflits de port :**
   ```cmd
   # Check what's using port 8080
   netstat -ano | findstr :8080
   
   # Use different port if needed
   chainlit run samples\04\app.py -w --port 3000
   ```

3. **ProblÃ¨mes d'environnement Python :**
   ```cmd
   # Verify correct interpreter in VS Code
   # Ctrl+Shift+P â†’ Python: Select Interpreter
   # Choose: Module08/.venv/Scripts/python.exe
   
   # Reinstall dependencies
   pip install -r requirements.txt
   ```

**DÃ©mo WebGPU :**

1. **WebGPU non pris en charge :**
   - Mettez Ã  jour vers Chrome/Edge 113+
   - Activez WebGPU : `chrome://flags/#enable-unsafe-webgpu`
   - VÃ©rifiez le statut GPU : `chrome://gpu`
   - La dÃ©mo basculera automatiquement sur le CPU

2. **Erreurs de chargement de modÃ¨le :**
   - Assurez-vous d'avoir une connexion internet pour tÃ©lÃ©charger le modÃ¨le
   - VÃ©rifiez la console du navigateur pour les erreurs CORS
   - VÃ©rifiez que vous servez via HTTP (pas file://)

**Open WebUI :**

1. **Connexion refusÃ©e :**
   ```cmd
   # Check Docker is running
   docker --version
   
   # Check container status
   docker ps | findstr open-webui
   
   # View container logs
   docker logs open-webui
   ```

2. **ModÃ¨les non affichÃ©s :**
   ```cmd
   # Verify Foundry Local endpoint
   curl http://localhost:51211/v1/models
   
   # Restart Open WebUI
   docker restart open-webui
   ```

### Liste de validation

```cmd
# âœ… 1. Foundry Local Setup
foundry --version                    # Should show version
foundry service status               # Should show "running"
foundry model list                   # Should show loaded models
curl http://localhost:51211/v1/models  # Should return JSON

# âœ… 2. Python Environment  
python --version                     # Should be 3.10+
pip list | findstr chainlit         # Should show chainlit package
pip list | findstr openai           # Should show openai package

# âœ… 3. Application Testing
chainlit run samples\04\app.py -w --port 8080  # Should open browser
# Test WebGPU demo at localhost:5173
# Test Open WebUI at localhost:3000
```

## Utilisation avancÃ©e

### Optimisation des performances

**Chainlit :**
- Utilisez le streaming pour une meilleure perception des performances
- ImplÃ©mentez le pooling de connexions pour une haute concurrence
- Mettez en cache les rÃ©ponses des modÃ¨les pour les requÃªtes rÃ©pÃ©tÃ©es
- Surveillez l'utilisation de la mÃ©moire avec des historiques de conversation volumineux

**WebGPU :**
- Utilisez WebGPU pour une confidentialitÃ© et une vitesse maximales
- ImplÃ©mentez la quantification des modÃ¨les pour des modÃ¨les plus petits
- Utilisez les Web Workers pour le traitement en arriÃ¨re-plan
- Mettez en cache les modÃ¨les compilÃ©s dans le stockage du navigateur

**Open WebUI :**
- Utilisez des volumes persistants pour l'historique des conversations
- Configurez des limites de ressources pour le conteneur Docker
- ImplÃ©mentez des stratÃ©gies de sauvegarde pour les donnÃ©es utilisateur
- Configurez un proxy inverse pour la terminaison SSL

### ModÃ¨les d'intÃ©gration

**Hybride Local/Cloud :**
```python
# Route based on complexity and privacy requirements
async def intelligent_routing(prompt: str, metadata: dict):
    if metadata.get("contains_pii"):
        return await foundry_local_completion(prompt)  # Privacy-sensitive
    elif len(prompt.split()) > 200:
        return await azure_openai_completion(prompt)   # Complex reasoning
    else:
        return await foundry_local_completion(prompt)  # Default local
```

**Pipeline multi-modal :**
```python
# Combine different AI capabilities
async def analyze_document(file_path: str):
    # 1. OCR with WebGPU (browser-based)
    text = await webgpu_ocr(file_path)
    
    # 2. Analysis with Foundry Local (private)
    summary = await foundry_local_analyze(text)
    
    # 3. Enhancement with cloud (if needed)
    if summary.confidence < 0.8:
        summary = await azure_openai_enhance(summary)
    
    return summary
```

## DÃ©ploiement en production

### ConsidÃ©rations de sÃ©curitÃ©

- **ClÃ©s API** : Utilisez des variables d'environnement, ne les codez jamais en dur
- **RÃ©seau** : Utilisez HTTPS en production, envisagez un VPN pour l'accÃ¨s en Ã©quipe
- **ContrÃ´le d'accÃ¨s** : ImplÃ©mentez l'authentification pour Open WebUI
- **ConfidentialitÃ© des donnÃ©es** : Auditez les donnÃ©es locales vs celles envoyÃ©es au cloud
- **Mises Ã  jour** : Gardez Foundry Local et les conteneurs Ã  jour

### Surveillance et maintenance

- **VÃ©rifications de santÃ©** : ImplÃ©mentez la surveillance des points de terminaison
- **Journalisation** : Centralisez les journaux de tous les composants
- **MÃ©triques** : Suivez les temps de rÃ©ponse, les taux d'erreur, l'utilisation des ressources
- **Sauvegarde** : Sauvegarde rÃ©guliÃ¨re des donnÃ©es de conversation et des configurations

## RÃ©fÃ©rences et ressources

### Documentation
- [Documentation Chainlit](https://docs.chainlit.io/) - Guide complet du framework
- [Documentation Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/) - Documentation officielle de Microsoft
- [ONNX Runtime Web](https://onnxruntime.ai/docs/get-started/with-javascript/web.html) - IntÃ©gration WebGPU
- [Documentation Open WebUI](https://docs.openwebui.com/) - Configuration avancÃ©e

### Fichiers d'exemple
- [`app.py`](../../../../../Module08/samples/04/app.py) - Application Chainlit en production
- [`chainlit_app.ipynb`](./chainlit_app.ipynb) - Notebook Ã©ducatif
- [`webgpu-demo/`](../../../../../Module08/samples/04/webgpu-demo) - InfÃ©rence IA basÃ©e sur le navigateur
- [`open-webui-guide.md`](./open-webui-guide.md) - Configuration complÃ¨te Open WebUI

### Exemples associÃ©s
- [Documentation Session 4](../../04.CuttingEdgeModels.md) - Guide complet de la session
- [Exemples Foundry Local](https://github.com/microsoft/foundry-local/tree/main/samples) - Exemples officiels

---

**Avertissement** :  
Ce document a Ã©tÃ© traduit Ã  l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisÃ©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit Ãªtre considÃ©rÃ© comme la source faisant autoritÃ©. Pour des informations critiques, il est recommandÃ© de recourir Ã  une traduction humaine professionnelle. Nous dÃ©clinons toute responsabilitÃ© en cas de malentendus ou d'interprÃ©tations erronÃ©es rÃ©sultant de l'utilisation de cette traduction.