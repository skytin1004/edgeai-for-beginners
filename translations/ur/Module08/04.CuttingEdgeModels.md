<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-09-30T23:15:44+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "ur"
}
-->
# ุณุดู 4: ฺูููน ฺฉ ุณุงุชฺพ ูพุฑูฺฺฉุดู ฺูน ุงูพูฺฉุดูุฒ ุจูุงูุง

## ุฌุงุฆุฒ

 ุณุดู ฺูููน ุงูุฑ ูุงุฆฺฉุฑูุณุงููน ูุงุคูฺุฑ ููฺฉู ฺฉ ุฐุฑุน ูพุฑูฺฺฉุดู ฺฉ ู ุชุงุฑ ฺูน ุงูพูฺฉุดูุฒ ุจูุงู ูพุฑ ูุฑฺฉูุฒ  ุขูพ ุฌุฏุฏ ูุจ ุงููนุฑูุณุฒ ุจูุงูุง ุณฺฉฺพฺบ ฺฏ ุฌู AI ฺฏูุชฺฏู ฺฉ ู ููุฒูฺบ ูฺบุ ุงุณูนุฑููฺฏ ุฌูุงุจุงุช ูุงูุฐ ฺฉุฑฺบ ฺฏุ ุงูุฑ ูุถุจูุท ฺูน ุงูพูฺฉุดูุฒ ฺฉู ุจุชุฑ ุงุฑุฑ ูฺููฺฏ ุงูุฑ ุตุงุฑู ฺฉ ุชุฌุฑุจ ฺฉ ฺุฒุงุฆู ฺฉ ุณุงุชฺพ ุชุนูุงุช ฺฉุฑฺบ ฺฏ

**ุขูพ ฺฉุง ุจูุงุฆฺบ ฺฏ:**
- **ฺูููน ฺูน ุงูพ**: ุฌุฏุฏ ูุจ UI ฺฉ ุณุงุชฺพ ุงุณูนุฑููฺฏ ุฌูุงุจุงุช
- **ูุจ ุฌ ูพ ู ฺูู**: ุจุฑุงุคุฒุฑ ูพุฑ ูุจู ุงููุฑูุณ ูพุฑุงุฆูุณ ูุฑุณูน ุงูพูฺฉุดูุฒ ฺฉ ู  
- **ุงููพู ูุจ UI ุงููนฺฏุฑุดู**: ูุงุคูฺุฑ ููฺฉู ฺฉ ุณุงุชฺพ ูพุฑููุดูู ฺูน ุงููนุฑูุณ
- **ูพุฑูฺฺฉุดู ูพูนุฑูุฒ**: ุงุฑุฑ ูฺููฺฏุ ูุงููนุฑูฺฏุ ุงูุฑ ุชุนูุงุช ฺฉ ุญฺฉูุช ุนูู

## ุณฺฉฺพู ฺฉ ููุงุตุฏ

- ฺูููน ฺฉ ุณุงุชฺพ ูพุฑูฺฺฉุดู ฺฉ ู ุชุงุฑ ฺูน ุงูพูฺฉุดูุฒ ุจูุงุฆฺบ
- ุตุงุฑู ฺฉ ุชุฌุฑุจ ฺฉู ุจุชุฑ ุจูุงู ฺฉ ู ุงุณูนุฑููฺฏ ุฌูุงุจุงุช ูุงูุฐ ฺฉุฑฺบ
- ูุงุคูฺุฑ ููฺฉู SDK ุงููนฺฏุฑุดู ูพูนุฑูุฒ ูฺบ ูุงุฑุช ุญุงุตู ฺฉุฑฺบ
- ููุงุณุจ ุงุฑุฑ ูฺููฺฏ ุงูุฑ ฺฏุฑุณูู ฺฺฏุฑฺุดู ฺฉุง ุงุทูุงู ฺฉุฑฺบ
- ูุฎุชูู ูุงุญูู ฺฉ ู ฺูน ุงูพูฺฉุดูุฒ ฺฉู ุชุนูุงุช ุงูุฑ ุชุฑุชุจ ุฏฺบ
- ฺฏูุชฺฏู AI ฺฉ ู ุฌุฏุฏ ูุจ UI ูพูนุฑูุฒ ฺฉู ุณูุฌฺพฺบ

## ุถุฑูุฑุงุช

- **ูุงุคูฺุฑ ููฺฉู**: ุงูุณูนุงู ุงูุฑ ฺู ุฑุง ู ([ุงูุณูนุงูุดู ฺฏุงุฆฺ](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **ูพุงุฆุชฺพูู**: 3.10 ุง ุงุณ ุณ ุฌุฏุฏ ูุฑฺู ฺฉ ุณุงุชฺพ ูุฑฺูุฆู ูุงุญูู ฺฉ ุตูุงุญุช
- **ูุงฺู**: ฺฉู ุงุฒ ฺฉู ุงฺฉ ูุงฺู ููฺ ฺฉุง ูุง (`foundry model run phi-4-mini`)
- **ุจุฑุงุคุฒุฑ**: ุฌุฏุฏ ูุจ ุจุฑุงุคุฒุฑ ุฌุณ ูฺบ ูุจ ุฌ ูพ ู ุณูพูุฑูน ู (ฺฉุฑูู/ุงุฌ)
- **ฺูฺฉุฑ**: ุงููพู ูุจ UI ุงููนฺฏุฑุดู ฺฉ ู (ุงุฎุชุงุฑ)

## ุญุต 1: ุฌุฏุฏ ฺูน ุงูพูฺฉุดูุฒ ฺฉู ุณูุฌฺพูุง

### ุขุฑฺฉูนฺฉฺุฑ ฺฉุง ุฌุงุฆุฒ

```
User Browser โโ Chainlit UI โโ Python Backend โโ Foundry Local โโ AI Model
      โ              โ              โ              โ            โ
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### ฺฉูุฏ ูนฺฉูุงููุฌุฒ

**ูุงุคูฺุฑ ููฺฉู SDK ูพูนุฑูุฒ:**
- `FoundryLocalManager(alias)`: ุฎูุฏฺฉุงุฑ ุณุฑูุณ ููุฌูููน
- `manager.endpoint` ุงูุฑ `manager.api_key`: ฺฉูฺฉุดู ฺฉ ุชูุตูุงุช
- `manager.get_model_info(alias).id`: ูุงฺู ฺฉ ุดูุงุฎุช

**ฺูููน ูุฑู ูุฑฺฉ:**
- `@cl.on_chat_start`: ฺูน ุณุดูุฒ ฺฉู ุดุฑูุน ฺฉุฑฺบ
- `@cl.on_message`: ุตุงุฑู ฺฉ ุขู ูุงู ูพุบุงูุงุช ฺฉู ูฺู ฺฉุฑฺบ  
- `cl.Message().stream_token()`: ุญูู ููุช ูฺบ ุงุณูนุฑููฺฏ
- ุฎูุฏฺฉุงุฑ UI ุฌูุฑุดู ุงูุฑ ูุจ ุณุงฺฉูน ููุฌูููน

## ุญุต 2: ููฺฉู ุจููุงุจู ฺฉูุงุคฺ ูุตู ููนุฑฺฉุณ

### ฺฉุงุฑฺฉุฑุฏฺฏ ฺฉ ุฎุตูุตุงุช

| ูพูู | ููฺฉู (ูุงุคูฺุฑ) | ฺฉูุงุคฺ (Azure OpenAI) |
|--------|-----------------|---------------------|
| **ููนูุณ** | ๐ 50-200ms (ฺฉูุฆ ููน ูุฑฺฉ ูฺบ) | โฑ๏ธ 200-2000ms (ููน ูุฑฺฉ ูพุฑ ููุญุตุฑ) |
| **ูพุฑุงุฆูุณ** | ๐ ฺูนุง ฺฉุจฺพ ุจฺพ ฺูุงุฆุณ ุณ ุจุงุฑ ูฺบ ุฌุงุชุง | โ๏ธ ฺูนุง ฺฉูุงุคฺ ฺฉู ุจฺพุฌุง ุฌุงุชุง  |
| **ูุงฺฏุช** | ๐ฐ ุงุฑฺูุฆุฑ ฺฉ ุจุนุฏ ููุช | ๐ธ ุฑ ูนูฺฉู ูพุฑ ุงุฏุงุฆฺฏ |
| **ุขู ูุงุฆู** | โ ุงููนุฑููน ฺฉ ุจุบุฑ ฺฉุงู ฺฉุฑุชุง  | โ ุงููนุฑููน ฺฉ ุถุฑูุฑุช  |
| **ูุงฺู ุณุงุฆุฒ** | โ๏ธ ุงุฑฺูุฆุฑ ฺฉ ุญุฏ ุชฺฉ ูุญุฏูุฏ | โ ุณุจ ุณ ุจฺ ูุงฺูุฒ ุชฺฉ ุฑุณุงุฆ |
| **ุงุณฺฉููฺฏ** | โ๏ธ ุงุฑฺูุฆุฑ ูพุฑ ููุญุตุฑ | โ ูุงูุญุฏูุฏ ุงุณฺฉููฺฏ |

### ุงุฆุจุฑฺ ุญฺฉูุช ุนูู ฺฉ ูพูนุฑูุฒ

**ููฺฉู-ูุฑุณูน ฺฉ ุณุงุชฺพ ูุงู ุจฺฉ:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**ูนุงุณฺฉ ูพุฑ ูุจู ุฑููนูฺฏ:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## ุญุต 3: ุณููพู 04 - ฺูููน ฺูน ุงูพูฺฉุดู

### ููุฑ ุขุบุงุฒ

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

ุงูพูฺฉุดู ุฎูุฏ ุจุฎูุฏ `http://localhost:8080` ูพุฑ ุงฺฉ ุฌุฏุฏ ฺูน ุงููนุฑูุณ ฺฉ ุณุงุชฺพ ฺฉฺพูุช 

### ุจูุงุฏ ููุงุฐ

ุณููพู 04 ุงูพูฺฉุดู ูพุฑูฺฺฉุดู ฺฉ ู ุชุงุฑ ูพูนุฑูุฒ ฺฉู ุธุงุฑ ฺฉุฑุช :

**ุฎูุฏฺฉุงุฑ ุณุฑูุณ ุฏุฑุงูุช:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**ุงุณูนุฑููฺฏ ฺูน ูฺูุฑ:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### ฺฉููฺฏุฑุดู ฺฉ ุงุฎุชุงุฑุงุช

**ูุงุญููุงุช ูุชุบุฑุงุช:**

| ูุชุบุฑ | ูุถุงุญุช | ฺูุงููน | ูุซุงู |
|----------|-------------|---------|----------|
| `MODEL` | ุงุณุชุนูุงู ฺฉุฑู ฺฉ ู ูุงฺู ฺฉุง ุนุฑู | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | ูุงุคูฺุฑ ููฺฉู ุงูฺ ูพูุงุฆููน | ุฎูุฏฺฉุงุฑ ุทูุฑ ูพุฑ ูพุช ูฺฏุงุง ฺฏุง | `http://localhost:51211` |
| `API_KEY` | API ฺฉูุฏ (ููฺฉู ฺฉ ู ุงุฎุชุงุฑ) | `""` | `your-api-key` |

**ุงฺูุงูุณฺ ุงุณุชุนูุงู:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## ุญุต 4: ุฌููพูนุฑ ูููน ุจฺฉุณ ุจูุงูุง ุงูุฑ ุงุณุชุนูุงู ฺฉุฑูุง

### ูููน ุจฺฉ ุณูพูุฑูน ฺฉุง ุฌุงุฆุฒ

ุณููพู 04 ูฺบ ุงฺฉ ุฌุงูุน ุฌููพูนุฑ ูููน ุจฺฉ (`chainlit_app.ipynb`) ุดุงูู  ุฌู ูุฑุงู ฺฉุฑุช :

- **๐ ุชุนูู ููุงุฏ**: ูุฑุญู ูุงุฑ ุณฺฉฺพู ฺฉ ููุงุฏ
- **๐ฌ ุงููนุฑุงฺฉูนู ุงฺฉุณูพููุฑุดู**: ฺฉูฺ ุณูุฒ ฺฉู ฺูุงุฆฺบ ุงูุฑ ุชุฌุฑุจ ฺฉุฑฺบ
- **๐ ุจุตุฑ ูุธุงุฑ**: ฺุงุฑูนุณุ ฺุงุงฺฏุฑุงูุฒุ ุงูุฑ ุขุคูน ูพูน ฺฉ ุจุตุฑ ููุงุฆูุฏฺฏ
- **๐๏ธ ุชุฑูุงุช ูนููุฒ**: ูนุณูนูฺฏ ุงูุฑ ฺุจฺฏูฺฏ ฺฉ ุตูุงุญุชฺบ

### ุงูพู ูููน ุจฺฉุณ ุจูุงูุง

#### ูุฑุญู 1: ุฌููพูนุฑ ูุงุญูู ุชุฑุชุจ ุฏฺบ

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### ูุฑุญู 2: ูุฆ ูููน ุจฺฉ ุจูุงุฆฺบ

**VS ฺฉูฺ ุงุณุชุนูุงู ฺฉุฑุช ูุฆ:**
1. VS ฺฉูฺ ฺฉู Module08 ฺุงุฆุฑฺฉูนุฑ ูฺบ ฺฉฺพููฺบ
2. `.ipynb` ุงฺฉุณูนูุดู ฺฉ ุณุงุชฺพ ูุฆ ูุงุฆู ุจูุงุฆฺบ
3. "ูุงุคูฺุฑ ููฺฉู" ฺฉุฑูู ููุชุฎุจ ฺฉุฑฺบ ุฌุจ ูพูฺฺพุง ุฌุงุฆ
4. ุงูพู ููุงุฏ ฺฉ ุณุงุชฺพ ุณูุฒ ุดุงูู ฺฉุฑูุง ุดุฑูุน ฺฉุฑฺบ

**ุฌููพูนุฑ ูุจ ุงุณุชุนูุงู ฺฉุฑุช ูุฆ:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### ูููน ุจฺฉ ฺฉ ุณุงุฎุช ฺฉ ุจุชุฑู ุทุฑู

#### ุณู ุขุฑฺฏูุงุฆุฒุดู

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("โ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### ุงููนุฑุงฺฉูนู ูุซุงูฺบ ุงูุฑ ูุดูฺบ

#### ูุดู 1: ฺฉูุงุฆููน ฺฉููฺฏุฑุดู ูนุณูนูฺฏ

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\n๐งช Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'โ Success' if result['status'] == 'ok' else 'โ Failed'}")
```

#### ูุดู 2: ุงุณูนุฑููฺฏ ุฑุณูพุงูุณ ุณูููุดู

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("๐ Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\nโ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## ุญุต 5: ูุจ ุฌ ูพ ู ุจุฑุงุคุฒุฑ ุงููุฑูุณ ฺูู

### ุฌุงุฆุฒ

ูุจ ุฌ ูพ ู AI ูุงฺูุฒ ฺฉู ุจุฑุง ุฑุงุณุช ุจุฑุงุคุฒุฑ ูฺบ ฺูุงู ฺฉ ุงุฌุงุฒุช ุฏุชุง  ุชุงฺฉ ุฒุงุฏ ุณ ุฒุงุฏ ูพุฑุงุฆูุณ ุงูุฑ ุฒุฑู ุงูุณูนุงู ุชุฌุฑุจุงุช ูุฑุงู ฺฉ ุฌุง ุณฺฉฺบ  ุณููพู ONNX ุฑู ูนุงุฆู ูุจ ฺฉ ุณุงุชฺพ ูุจ ุฌ ูพ ู ุงฺฉุฒฺฉูุดู ฺฉู ุธุงุฑ ฺฉุฑุชุง 

### ูุฑุญู 1: ูุจ ุฌ ูพ ู ุณูพูุฑูน ฺฺฉ ฺฉุฑฺบ

**ุจุฑุงุคุฒุฑ ฺฉ ุถุฑูุฑุงุช:**
- ฺฉุฑูู/ุงุฌ 113+ ูุจ ุฌ ูพ ู ฺฉ ุณุงุชฺพ ูุนุงู
- ฺฺฉ ฺฉุฑฺบ: `chrome://gpu` โ "WebGPU" ุงุณูนูนุณ ฺฉ ุชุตุฏู ฺฉุฑฺบ
- ูพุฑูฺฏุฑุงูุงุช ฺฺฉ: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### ูุฑุญู 2: ูุจ ุฌ ูพ ู ฺูู ุจูุงุฆฺบ

ฺุงุฆุฑฺฉูนุฑ ุจูุงุฆฺบ: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>๐ WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = 'โ WebGPU not available';
            return;
        }
        
        statusEl.textContent = '๐ WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('โ ONNX Runtime session created with WebGPU');
        log(`๐ Input names: ${session.inputNames.join(', ')}`);
        log(`๐ Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = 'โ WebGPU inference complete!';
        log(`๐ฏ Predicted class: ${maxIdx}`);
        log(`๐ Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `โ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### ูุฑุญู 3: ฺูู ฺูุงุฆฺบ

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## ุญุต 6: ุงููพู ูุจ UI ุงููนฺฏุฑุดู

### ุฌุงุฆุฒ

ุงููพู ูุจ UI ุงฺฉ ูพุฑููุดูู ChatGPT ุฌุณุง ุงููนุฑูุณ ูุฑุงู ฺฉุฑุชุง  ุฌู ูุงุคูฺุฑ ููฺฉู ฺฉ ุงููพู AI-ฺฉููพูนุจู API ุณ ุฌฺุชุง 

### ูุฑุญู 1: ุถุฑูุฑุงุช

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### ูุฑุญู 2: ฺูฺฉุฑ ุณูน ุงูพ (ุชุฌูุฒ ฺฉุฑุฏ)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**ูููน:** `host.docker.internal` ููฺูุฒ ูพุฑ ฺูฺฉุฑ ฺฉููนูุฑุฒ ฺฉู ูุฒุจุงู ูุดู ุชฺฉ ุฑุณุงุฆ ฺฉ ุงุฌุงุฒุช ุฏุชุง 

### ูุฑุญู 3: ฺฉููฺฏุฑุดู

1. **ุจุฑุงุคุฒุฑ ฺฉฺพููฺบ:** `http://localhost:3000` ูพุฑ ุฌุงุฆฺบ
2. **ุงุจุชุฏุงุฆ ุณูน ุงูพ:** ุงฺูู ุงฺฉุงุคููน ุจูุงุฆฺบ
3. **ูุงฺู ฺฉููฺฏุฑุดู:**
   - ุณูนูฺฏุฒ โ ูุงฺูุฒ โ ุงููพู AI API  
   - ุจุณ URL: `http://host.docker.internal:51211/v1`
   - API ฺฉูุฏ: `foundry-local-key` (ฺฉูุฆ ุจฺพ ููู ฺฉุงู ฺฉุฑ ฺฏ)
4. **ฺฉูฺฉุดู ูนุณูน ฺฉุฑฺบ:** ูุงฺูุฒ ฺุฑุงูพ ฺุงุคู ูฺบ ุธุงุฑ ูู ฺุงุฆฺบ

### ุฎุฑุงุจูฺบ ฺฉุง ุณุฑุงุบ ูฺฏุงูุง

**ุนุงู ูุณุงุฆู:**

1. **ฺฉูฺฉุดู ุฑููุฒฺ:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **ูุงฺูุฒ ุธุงุฑ ูฺบ ู ุฑ:**
   - ุชุตุฏู ฺฉุฑฺบ ฺฉ ูุงฺู ููฺ : `foundry model list`
   - API ุฑุณูพุงูุณ ฺฺฉ ฺฉุฑฺบ: `curl http://localhost:51211/v1/models`
   - ุงููพู ูุจ UI ฺฉููนูุฑ ฺฉู ุฏูุจุงุฑ ุดุฑูุน ฺฉุฑฺบ

## ุญุต 7: ูพุฑูฺฺฉุดู ุชุนูุงุช ฺฉ ุฎุงูุงุช

### ูุงุญููุงุช ฺฉููฺฏุฑุดู

**ุชุฑูุงุช ุณูน ุงูพ:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**ูพุฑูฺฺฉุดู ุชุนูุงุช:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### ุนุงู ูพูุฑูน ูุณุงุฆู ุงูุฑ ุญู

**ูพูุฑูน 51211 ุชูุงุฒุน ฺฉ ุฑูฺฉ ุชฺพุงู:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### ฺฉุงุฑฺฉุฑุฏฺฏ ฺฉ ูฺฏุฑุงู

**ูุชฺพ ฺฺฉ ููุงุฐ:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## ุฎูุงุต

ุณุดู 4 ูฺบ ฺูููน ุงูพูฺฉุดูุฒ ฺฉ ู ูพุฑูฺฺฉุดู ฺฉ ู ุชุงุฑ ฺูน ุงูพูฺฉุดูุฒ ุจูุงู ฺฉุง ุงุญุงุท ฺฉุง ฺฏุง ุขูพ ู ุณฺฉฺพุง:

- โ **ฺูููน ูุฑู ูุฑฺฉ**: ฺูน ุงูพูฺฉุดูุฒ ฺฉ ู ุฌุฏุฏ UI ุงูุฑ ุงุณูนุฑููฺฏ ุณูพูุฑูน
- โ **ูุงุคูฺุฑ ููฺฉู ุงููนฺฏุฑุดู**: SDK ุงุณุชุนูุงู ุงูุฑ ฺฉููฺฏุฑุดู ูพูนุฑูุฒ  
- โ **ูุจ ุฌ ูพ ู ุงููุฑูุณ**: ุฒุงุฏ ุณ ุฒุงุฏ ูพุฑุงุฆูุณ ฺฉ ู ุจุฑุงุคุฒุฑ ูพุฑ ูุจู AI
- โ **ุงููพู ูุจ UI ุณูน ุงูพ**: ูพุฑููุดูู ฺูน ุงููนุฑูุณ ุชุนูุงุช
- โ **ูพุฑูฺฺฉุดู ูพูนุฑูุฒ**: ุงุฑุฑ ูฺููฺฏุ ูุงููนุฑูฺฏุ ุงูุฑ ุงุณฺฉููฺฏ

ุณููพู 04 ุงูพูฺฉุดู ุจุชุฑู ุทุฑููฺบ ฺฉู ุธุงุฑ ฺฉุฑุช  ุฌู ูุถุจูุท ฺูน ุงููนุฑูุณุฒ ุจูุงู ฺฉ ู ููุงู AI ูุงฺูุฒ ฺฉู ูุงุฆฺฉุฑูุณุงููน ูุงุคูฺุฑ ููฺฉู ฺฉ ุฐุฑุน ุงุณุชุนูุงู ฺฉุฑุช  ุงูุฑ ุจุชุฑู ุตุงุฑู ุชุฌุฑุจ ูุฑุงู ฺฉุฑุช 

## ุญูุงู ุฌุงุช

- **[ุณููพู 04: ฺูููน ุงูพูฺฉุดู](samples/04/README.md)**: ูฺฉูู ุงูพูฺฉุดู ฺฉ ุณุงุชฺพ ุฏุณุชุงูุฒุงุช
- **[ฺูููน ุชุนูู ูููน ุจฺฉ](samples/04/chainlit_app.ipynb)**: ุงููนุฑุงฺฉูนู ุณฺฉฺพู ฺฉ ููุงุฏ
- **[ูุงุคูฺุฑ ููฺฉู ุฏุณุชุงูุฒุงุช](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: ูฺฉูู ูพููน ูุงุฑู ุฏุณุชุงูุฒุงุช
- **[ฺูููน ุฏุณุชุงูุฒุงุช](https://docs.chainlit.io/)**: ุขูุดู ูุฑู ูุฑฺฉ ุฏุณุชุงูุฒุงุช
- **[ุงููพู ูุจ UI ุงููนฺฏุฑุดู ฺฏุงุฆฺ](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: ุขูุดู ูนููนูุฑู

---

**ฺุณฺฉููุฑ**:  
 ุฏุณุชุงูุฒ AI ุชุฑุฌู ุณุฑูุณ [Co-op Translator](https://github.com/Azure/co-op-translator) ฺฉุง ุงุณุชุนูุงู ฺฉุฑุช ูุฆ ุชุฑุฌู ฺฉ ฺฏุฆ  ู ุฏุฑุณุชฺฏ ฺฉ ู ฺฉูุดุด ฺฉุฑุช ฺบุ ูฺฉู ุจุฑุง ฺฉุฑู ุขฺฏุง ุฑฺบ ฺฉ ุฎูุฏฺฉุงุฑ ุชุฑุฌู ูฺบ ุบูุทุงฺบ ุง ุบุฑ ุฏุฑุณุชุงฺบ ู ุณฺฉุช ฺบ ุงุตู ุฏุณุชุงูุฒ ฺฉู ุงุณ ฺฉ ุงุตู ุฒุจุงู ูฺบ ูุณุชูุฏ ุฐุฑุน ุณูุฌฺพุง ุฌุงูุง ฺุง ุงู ูุนูููุงุช ฺฉ ูุ ูพุด ูุฑ ุงูุณุงู ุชุฑุฌู ฺฉ ุณูุงุฑุด ฺฉ ุฌุงุช  ู ุงุณ ุชุฑุฌู ฺฉ ุงุณุชุนูุงู ุณ ูพุฏุง ูู ูุงู ฺฉุณ ุจฺพ ุบูุท ูู ุง ุบูุท ุชุดุฑุญ ฺฉ ุฐู ุฏุงุฑ ูฺบ ฺบ