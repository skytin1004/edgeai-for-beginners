<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6503a980cb3bf2b2de2d2bc4ac6acc4c",
  "translation_date": "2025-09-24T13:36:48+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "ur"
}
-->
# Ø³ÛŒØ´Ù† 1: ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ú©Û’ Ø³Ø§ØªÚ¾ Ø´Ø±ÙˆØ¹Ø§Øª

## Ø¬Ø§Ø¦Ø²Û

Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Azure AI Foundry Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Ùˆ Ø¨Ø±Ø§Û Ø±Ø§Ø³Øª Ø¢Ù¾ Ú©Û’ Windows 11 ÚˆÛŒÙˆÙ„Ù¾Ù…Ù†Ù¹ Ù…Ø§Ø­ÙˆÙ„ Ù…ÛŒÚº Ù„Ø§ØªØ§ ÛÛ’ØŒ Ø¬Ùˆ Ù¾Ø±Ø§Ø¦ÛŒÙˆÛŒØ³ÛŒ Ú©Ùˆ Ù…Ø­ÙÙˆØ¸ Ø±Ú©Ú¾Ù†Û’ØŒ Ú©Ù… ØªØ§Ø®ÛŒØ± ÙˆØ§Ù„Û’ AI ÚˆÛŒÙˆÙ„Ù¾Ù…Ù†Ù¹ Ú©Ùˆ Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ú¯Ø±ÛŒÚˆ Ù¹ÙˆÙ„Ø² Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ù…Ú©Ù† Ø¨Ù†Ø§ØªØ§ ÛÛ’Û” Ø§Ø³ Ø³ÛŒØ´Ù† Ù…ÛŒÚº Ù…Ú©Ù…Ù„ Ø§Ù†Ø³Ù¹Ø§Ù„ÛŒØ´Ù†ØŒ Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù†ØŒ Ø§ÙˆØ± Ù…Ø´ÛÙˆØ± Ù…Ø§ÚˆÙ„Ø² Ø¬ÛŒØ³Û’ phiØŒ qwenØŒ deepseekØŒ Ø§ÙˆØ± GPT-OSS-20B Ú©ÛŒ Ø¹Ù…Ù„ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ø´Ø§Ù…Ù„ ÛÛ’Û”

## Ø³ÛŒÚ©Ú¾Ù†Û’ Ú©Û’ Ù…Ù‚Ø§ØµØ¯

Ø§Ø³ Ø³ÛŒØ´Ù† Ú©Û’ Ø§Ø®ØªØªØ§Ù… ØªÚ©ØŒ Ø¢Ù¾:
- Windows 11 Ù¾Ø± ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ø§Ù†Ø³Ù¹Ø§Ù„ Ø§ÙˆØ± Ú©Ù†ÙÛŒÚ¯Ø± Ú©Ø±ÛŒÚº Ú¯Û’
- CLI Ú©Ù…Ø§Ù†ÚˆØ² Ø§ÙˆØ± Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù† Ø¢Ù¾Ø´Ù†Ø² Ù…ÛŒÚº Ù…ÛØ§Ø±Øª Ø­Ø§ØµÙ„ Ú©Ø±ÛŒÚº Ú¯Û’
- Ø¨ÛØªØ±ÛŒÙ† Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø§ÚˆÙ„ Ú©ÛŒ Ú©ÛŒØ´Ù†Ú¯ Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒ Ú©Ùˆ Ø³Ù…Ø¬Ú¾ÛŒÚº Ú¯Û’
- phiØŒ qwenØŒ deepseekØŒ Ø§ÙˆØ± GPT-OSS-20B Ù…Ø§ÚˆÙ„Ø² Ú©Ùˆ Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ø³Û’ Ú†Ù„Ø§Ø¦ÛŒÚº Ú¯Û’
- ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø§Ù¾Ù†ÛŒ Ù¾ÛÙ„ÛŒ AI Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù† Ø¨Ù†Ø§Ø¦ÛŒÚº Ú¯Û’

## Ø¶Ø±ÙˆØ±ÛŒØ§Øª

### Ø³Ø³Ù¹Ù… Ú©ÛŒ Ø¶Ø±ÙˆØ±ÛŒØ§Øª
- **Windows 11**: ÙˆØ±Ú˜Ù† 22H2 ÛŒØ§ Ø§Ø³ Ø³Û’ Ø¬Ø¯ÛŒØ¯
- **RAM**: Ú©Ù… Ø§Ø² Ú©Ù… 16GBØŒ 32GB ØªØ¬ÙˆÛŒØ² Ú©Ø±Ø¯Û
- **Ø§Ø³Ù¹ÙˆØ±ÛŒØ¬**: Ù…Ø§ÚˆÙ„Ø² Ø§ÙˆØ± Ú©ÛŒØ´ Ú©Û’ Ù„ÛŒÛ’ 50GB Ø®Ø§Ù„ÛŒ Ø¬Ú¯Û
- **ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø±**: NPU- ÛŒØ§ GPU-ÙØ¹Ø§Ù„ ÚˆÛŒÙˆØ§Ø¦Ø³ ØªØ±Ø¬ÛŒØ­ÛŒ (Copilot+ PC ÛŒØ§ NVIDIA GPU)
- **Ù†ÛŒÙ¹ ÙˆØ±Ú©**: Ù…Ø§ÚˆÙ„ ÚˆØ§Ø¤Ù†Ù„ÙˆÚˆØ² Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ² Ø±ÙØªØ§Ø± Ø§Ù†Ù¹Ø±Ù†ÛŒÙ¹

### ÚˆÛŒÙˆÙ„Ù¾Ù…Ù†Ù¹ Ù…Ø§Ø­ÙˆÙ„
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion

# Set up Python environment (recommended)
py -m venv .venv
.venv\Scripts\activate

# Install required dependencies
pip install openai foundry-local-sdk
```

## Ø­ØµÛ 1: Ø§Ù†Ø³Ù¹Ø§Ù„ÛŒØ´Ù† Ø§ÙˆØ± Ø³ÛŒÙ¹ Ø§Ù¾

### Ù…Ø±Ø­Ù„Û 1: ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ø§Ù†Ø³Ù¹Ø§Ù„ Ú©Ø±ÛŒÚº

Winget Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ ÛŒØ§ GitHub Ø³Û’ Ø§Ù†Ø³Ù¹Ø§Ù„Ø± ÚˆØ§Ø¤Ù†Ù„ÙˆÚˆ Ú©Ø±Ú©Û’ ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ø§Ù†Ø³Ù¹Ø§Ù„ Ú©Ø±ÛŒÚº:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### Ù…Ø±Ø­Ù„Û 2: Ø§Ù†Ø³Ù¹Ø§Ù„ÛŒØ´Ù† Ú©ÛŒ ØªØµØ¯ÛŒÙ‚ Ú©Ø±ÛŒÚº

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## Ø­ØµÛ 2: CLI Ú©Ùˆ Ø³Ù…Ø¬Ú¾Ù†Ø§

### Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ú©Ù…Ø§Ù†ÚˆØ² Ú©Ø§ ÚˆÚ¾Ø§Ù†Ú†Û

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## Ø­ØµÛ 3: Ù…Ø§ÚˆÙ„ Ú©ÛŒØ´Ù†Ú¯ Ø§ÙˆØ± Ù…ÛŒÙ†Ø¬Ù…Ù†Ù¹

ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ø§ÙˆØ± Ø§Ø³Ù¹ÙˆØ±ÛŒØ¬ Ú©Ùˆ Ø¨ÛØªØ± Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø°ÛÛŒÙ† Ù…Ø§ÚˆÙ„ Ú©ÛŒØ´Ù†Ú¯ Ú©Ùˆ Ù†Ø§ÙØ° Ú©Ø±ØªØ§ ÛÛ’:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## Ø­ØµÛ 4: Ù…Ø§ÚˆÙ„Ø² Ú©ÛŒ Ø¹Ù…Ù„ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ

### Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Phi Ù…Ø§ÚˆÙ„Ø² Ú†Ù„Ø§Ù†Ø§

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### Qwen Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ø³Ø§ØªÚ¾ Ú©Ø§Ù… Ú©Ø±Ù†Ø§

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b-instruct
foundry model run qwen2.5-14b-instruct
```

### DeepSeek Ù…Ø§ÚˆÙ„Ø² Ú†Ù„Ø§Ù†Ø§

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-distill-qwen-7b
```

### GPT-OSS-20B Ú†Ù„Ø§Ù†Ø§

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## Ø­ØµÛ 5: Ø§Ù¾Ù†ÛŒ Ù¾ÛÙ„ÛŒ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù† Ø¨Ù†Ø§Ù†Ø§

### Ø¬Ø¯ÛŒØ¯ Ú†ÛŒÙ¹ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù† (OpenAI SDK + ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„)

OpenAI SDK Ú©Û’ Ø³Ø§ØªÚ¾ ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ø§Ù†Ù¹ÛŒÚ¯Ø±ÛŒØ´Ù† Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø§ÛŒÚ© Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† Ø±ÛŒÚˆÛŒ Ú†ÛŒÙ¹ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù† Ø¨Ù†Ø§Ø¦ÛŒÚºØŒ ÛÙ…Ø§Ø±Û’ Sample 01 Ú©Û’ Ù¾ÛŒÙ¹Ø±Ù†Ø² Ú©ÛŒ Ù¾ÛŒØ±ÙˆÛŒ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’Û”

```python
# chat_quickstart.py (Sample 01 pattern)
import os
import sys
from openai import OpenAI

try:
    from foundry_local import FoundryLocalManager
    FOUNDRY_SDK_AVAILABLE = True
except ImportError:
    FOUNDRY_SDK_AVAILABLE = False
    print("âš ï¸ Install foundry-local-sdk: pip install foundry-local-sdk")

def create_client():
    """Create OpenAI client with Foundry Local or Azure OpenAI."""
    # Check for Azure OpenAI configuration
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    
    if azure_endpoint and azure_api_key:
        # Azure OpenAI path
        model = os.environ.get("MODEL", "your-deployment-name")
        client = OpenAI(
            base_url=f"{azure_endpoint}/openai",
            api_key=azure_api_key,
            default_query={"api-version": "2024-08-01-preview"},
        )
        print(f"ğŸŒ Using Azure OpenAI with model: {model}")
        return client, model
    
    # Foundry Local path with SDK management
    alias = os.environ.get("MODEL", "phi-4-mini")
    if FOUNDRY_SDK_AVAILABLE:
        try:
            # Use FoundryLocalManager for proper service management
            manager = FoundryLocalManager(alias)
            model_info = manager.get_model_info(alias)
            
            client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            model = model_info.id
            print(f"ğŸ  Using Foundry Local SDK with model: {model}")
            return client, model
        except Exception as e:
            print(f"âš ï¸ Foundry SDK failed ({e}), using manual configuration")
    
    # Fallback to manual configuration
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    api_key = os.environ.get("API_KEY", "")
    model = alias
    
    client = OpenAI(
        base_url=f"{base_url}/v1",
        api_key=api_key
    )
    print(f"ğŸ”§ Manual configuration with model: {model}")
    return client, model

def main():
    """Main chat function."""
    client, model = create_client()
    
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    conversation_history = []
    
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        
        try:
            # Add user message to history
            conversation_history.append({"role": "user", "content": user_input})
            
            # Create chat completion
            response = client.chat.completions.create(
                model=model,
                messages=conversation_history,
                max_tokens=500,
                temperature=0.7
            )
            
            assistant_message = response.choices[0].message.content
            conversation_history.append({"role": "assistant", "content": assistant_message})
            
            print(f"Assistant: {assistant_message}\n")
            
        except Exception as e:
            print(f"Error: {e}\n")

if __name__ == "__main__":
    main()
```

### Ú†ÛŒÙ¹ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù† Ú†Ù„Ø§Ø¦ÛŒÚº

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Option 1: Using FoundryLocalManager (recommended)
python chat_quickstart.py "Explain what Foundry Local is"

# Option 2: Manual configuration with environment variables
set BASE_URL=http://localhost:8000
set MODEL=phi-4-mini
set API_KEY=
python chat_quickstart.py "Write a welcome message"

# Option 3: Azure OpenAI configuration
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name
python chat_quickstart.py "Hello from Azure OpenAI"
```

## Ø­ØµÛ 6: Ù…Ø³Ø§Ø¦Ù„ Ú©Ø§ Ø­Ù„ Ø§ÙˆØ± Ø¨ÛØªØ±ÛŒÙ† Ø·Ø±ÛŒÙ‚Û’

### Ø¹Ø§Ù… Ù…Ø³Ø§Ø¦Ù„ Ø§ÙˆØ± Ø§Ù† Ú©Û’ Ø­Ù„

```powershell
# Issue: "Could not use Foundry SDK" warning
pip install foundry-local-sdk
# Or set environment variables for manual configuration

# Issue: Connection refused
foundry service status
foundry service ps  # Check loaded models

# Issue: Model not found
foundry model list
foundry model run phi-4-mini

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal

# Test API endpoint
curl http://localhost:8000/v1/models
```

### Ø³Ø³Ù¹Ù… ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ù†Ú¯Ø±Ø§Ù†ÛŒ (Windows)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### Ù…Ø§Ø­ÙˆÙ„ Ú©Û’ Ù…ØªØºÛŒØ±Ø§Øª

| Ù…ØªØºÛŒØ± | ÙˆØ¶Ø§Ø­Øª | ÚˆÛŒÙØ§Ù„Ù¹ | Ø¶Ø±ÙˆØ±ÛŒ |
|----------|-------------|---------|----------|
| `MODEL` | Ù…Ø§ÚˆÙ„ Ú©Ø§ Ø¹Ø±Ù ÛŒØ§ Ù†Ø§Ù… | `phi-4-mini` | Ù†ÛÛŒÚº |
| `BASE_URL` | ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ø¨ÛŒØ³ URL | `http://localhost:8000` | Ù†ÛÛŒÚº |
| `API_KEY` | API Ú©ÛŒ (Ø¹Ø§Ù… Ø·ÙˆØ± Ù¾Ø± Ù„ÙˆÚ©Ù„ Ú©Û’ Ù„ÛŒÛ’ Ø¶Ø±ÙˆØ±ÛŒ Ù†ÛÛŒÚº) | `""` | Ù†ÛÛŒÚº |
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI Ø§ÛŒÙ†ÚˆÙ¾ÙˆØ§Ø¦Ù†Ù¹ | - | Azure Ú©Û’ Ù„ÛŒÛ’ |
| `AZURE_OPENAI_API_KEY` | Azure OpenAI API Ú©ÛŒ | - | Azure Ú©Û’ Ù„ÛŒÛ’ |
| `AZURE_OPENAI_API_VERSION` | Azure API ÙˆØ±Ú˜Ù† | `2024-08-01-preview` | Ù†ÛÛŒÚº |

### Ø¨ÛØªØ±ÛŒÙ† Ø·Ø±ÛŒÙ‚Û’

- **OpenAI SDK Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº**: Ø¨ÛØªØ± Ù…ÛŒÙ†Ù¹ÛŒÙ† Ø§ÛŒØ¨Ù„Ù¹ÛŒ Ú©Û’ Ù„ÛŒÛ’ OpenAI SDK Ú©Ùˆ Ø®Ø§Ù… HTTP Ø¯Ø±Ø®ÙˆØ§Ø³ØªÙˆÚº Ù¾Ø± ØªØ±Ø¬ÛŒØ­ Ø¯ÛŒÚº
- **FoundryLocalManager**: Ø¬Ø¨ Ø¯Ø³ØªÛŒØ§Ø¨ ÛÙˆ ØªÙˆ Ø³Ø±ÙˆØ³ Ù…ÛŒÙ†Ø¬Ù…Ù†Ù¹ Ú©Û’ Ù„ÛŒÛ’ Ø¢ÙÛŒØ´Ù„ SDK Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº
- **ØºÙ„Ø·ÛŒÙˆÚº Ú©Ø§ Ø­Ù„**: Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ù…Ù†Ø§Ø³Ø¨ ÙØ§Ù„ Ø¨ÛŒÚ© Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒ Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº
- **Ø¨Ø§Ù‚Ø§Ø¹Ø¯Ú¯ÛŒ Ø³Û’ Ø§Ù¾ Ú¯Ø±ÛŒÚˆ Ú©Ø±ÛŒÚº**: Ù†Ø¦Û’ Ù…Ø§ÚˆÙ„Ø² Ø§ÙˆØ± ÙÚ©Ø³Ø² ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ Ú©Û’ Ù„ÛŒÛ’ ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ú©Ùˆ Ø§Ù¾ ÚˆÛŒÙ¹ Ø±Ú©Ú¾ÛŒÚº
- **Ú†Ú¾ÙˆÙ¹Û’ Ù…Ø§ÚˆÙ„Ø² Ø³Û’ Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº**: Ú†Ú¾ÙˆÙ¹Û’ Ù…Ø§ÚˆÙ„Ø² (Phi miniØŒ Qwen 7B) Ø³Û’ Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø¨Ú‘Ú¾Ø§Ø¦ÛŒÚº
- **ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ù†Ú¯Ø±Ø§Ù†ÛŒ Ú©Ø±ÛŒÚº**: CPU/GPU/Ù…ÛŒÙ…ÙˆØ±ÛŒ Ú©Ùˆ Ù¹Ø±ÛŒÚ© Ú©Ø±ÛŒÚº Ø¬Ø¨Ú©Û Ù¾Ø±Ø§Ù…Ù¾Ù¹Ø³ Ø§ÙˆØ± Ø³ÛŒÙ¹Ù†Ú¯Ø² Ú©Ùˆ Ù¹ÛŒÙˆÙ† Ú©Ø±ÛŒÚº

## Ø­ØµÛ 7: Ø¹Ù…Ù„ÛŒ Ù…Ø´Ù‚ÛŒÚº

### Ù…Ø´Ù‚ 1: ÙÙˆØ±ÛŒ Ù…Ù„Ù¹ÛŒ Ù…Ø§ÚˆÙ„ Ù¹ÛŒØ³Ù¹

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b-instruct"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### Ù…Ø´Ù‚ 2: OpenAI SDK Ø§Ù†Ù¹ÛŒÚ¯Ø±ÛŒØ´Ù† Ù¹ÛŒØ³Ù¹

```python
# sdk_integration_test.py (matching Sample 01 pattern)
import os
from openai import OpenAI
from foundry_local import FoundryLocalManager

def test_model_integration(model_alias):
    """Test OpenAI SDK integration with different models."""
    try:
        # Use FoundryLocalManager for proper setup
        manager = FoundryLocalManager(model_alias)
        model_info = manager.get_model_info(model_alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Test basic completion
        response = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Say hello and state your model name."}],
            max_tokens=50
        )
        
        print(f"âœ… {model_alias}: {response.choices[0].message.content}")
        return True
    except Exception as e:
        print(f"âŒ {model_alias}: {e}")
        return False

# Test multiple models
models_to_test = ["phi-4-mini", "qwen2.5-7b-instruct"]
for model in models_to_test:
    test_model_integration(model)
```

### Ù…Ø´Ù‚ 3: Ø¬Ø§Ù…Ø¹ Ø³Ø±ÙˆØ³ ÛÛŒÙ„ØªÚ¾ Ú†ÛŒÚ©

```python
# health_check.py
from openai import OpenAI
from foundry_local import FoundryLocalManager

def comprehensive_health_check():
    """Perform comprehensive health check of Foundry Local service."""
    try:
        # Initialize with a common model
        manager = FoundryLocalManager("phi-4-mini")
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # 1. Check service connectivity
        models_response = client.models.list()
        available_models = [model.id for model in models_response.data]
        print(f"âœ… Service healthy - {len(available_models)} models available")
        
        # 2. Test each available model
        for model_id in available_models:
            try:
                response = client.chat.completions.create(
                    model=model_id,
                    messages=[{"role": "user", "content": "Test"}],
                    max_tokens=10
                )
                print(f"âœ… {model_id}: Working")
            except Exception as e:
                print(f"âŒ {model_id}: {e}")
        
        return True
    except Exception as e:
        print(f"âŒ Service check failed: {e}")
        return False

comprehensive_health_check()
```

## Ø­ÙˆØ§Ù„Û Ø¬Ø§Øª

- **ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ú©Û’ Ø³Ø§ØªÚ¾ Ø´Ø±ÙˆØ¹Ø§Øª Ú©Ø±ÛŒÚº**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- **CLI Ø­ÙˆØ§Ù„Û Ø§ÙˆØ± Ú©Ù…Ø§Ù†ÚˆØ² Ú©Ø§ Ø¬Ø§Ø¦Ø²Û**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- **OpenAI SDK Ø§Ù†Ù¹ÛŒÚ¯Ø±ÛŒØ´Ù†**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- **Hugging Face Ù…Ø§ÚˆÙ„Ø² Ú©Ùˆ Ú©Ù…Ù¾Ø§Ø¦Ù„ Ú©Ø±ÛŒÚº**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- **Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ GitHub**: https://github.com/microsoft/Foundry-Local
- **OpenAI Python SDK**: https://github.com/openai/openai-python
- **Sample 01: OpenAI SDK Ú©Û’ Ø°Ø±ÛŒØ¹Û’ ÙÙˆØ±ÛŒ Ú†ÛŒÙ¹**: samples/01/README.md
- **Sample 02: Ø§ÛŒÚˆÙˆØ§Ù†Ø³Úˆ SDK Ø§Ù†Ù¹ÛŒÚ¯Ø±ÛŒØ´Ù†**: samples/02/README.md

---

