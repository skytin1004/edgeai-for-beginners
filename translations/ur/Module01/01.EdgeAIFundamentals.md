<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a35d3b47e6ae98ad9b3e89fb73917e90",
  "translation_date": "2025-09-17T17:43:59+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "ur"
}
-->
# سیکشن 1: ایج اے آئی کی بنیادی باتیں

ایج اے آئی مصنوعی ذہانت کی تعیناتی میں ایک بنیادی تبدیلی کی نمائندگی کرتا ہے، جو اے آئی کی صلاحیتوں کو براہ راست ایج ڈیوائسز پر لاتا ہے بجائے اس کے کہ صرف کلاؤڈ پر مبنی پروسیسنگ پر انحصار کیا جائے۔ یہ سمجھنا ضروری ہے کہ ایج اے آئی کس طرح محدود وسائل والے ڈیوائسز پر مقامی اے آئی پروسیسنگ کو فعال کرتا ہے، جبکہ کارکردگی کو برقرار رکھتا ہے اور پرائیویسی، لیٹنسی، اور آف لائن صلاحیتوں جیسے چیلنجز کو حل کرتا ہے۔

## تعارف

اس سبق میں، ہم ایج اے آئی اور اس کے بنیادی تصورات کا جائزہ لیں گے۔ ہم روایتی اے آئی کمپیوٹنگ کے طریقہ کار، ایج کمپیوٹنگ کے چیلنجز، ایج اے آئی کو فعال کرنے والی کلیدی ٹیکنالوجیز، اور مختلف صنعتوں میں عملی اطلاقات کا احاطہ کریں گے۔

## سیکھنے کے مقاصد

اس سبق کے اختتام تک، آپ قابل ہوں گے:

- روایتی کلاؤڈ پر مبنی اے آئی اور ایج اے آئی کے طریقوں کے درمیان فرق کو سمجھنا۔
- ان کلیدی ٹیکنالوجیز کی شناخت کرنا جو ایج ڈیوائسز پر اے آئی پروسیسنگ کو فعال کرتی ہیں۔
- ایج اے آئی کے نفاذ کے فوائد اور حدود کو پہچاننا۔
- ایج اے آئی کے علم کو حقیقی دنیا کے منظرناموں اور استعمال کے معاملات پر لاگو کرنا۔

## روایتی اے آئی کمپیوٹنگ کے طریقہ کار کو سمجھنا

روایتی طور پر، جنریٹو اے آئی ایپلیکیشنز بڑے لینگویج ماڈلز (LLMs) کو مؤثر طریقے سے چلانے کے لیے ہائی پرفارمنس کمپیوٹنگ انفراسٹرکچر پر انحصار کرتی ہیں۔ تنظیمیں عام طور پر ان ماڈلز کو کلاؤڈ ماحول میں GPU کلسٹرز پر تعینات کرتی ہیں اور ان کی صلاحیتوں تک API انٹرفیس کے ذریعے رسائی حاصل کرتی ہیں۔

یہ مرکزی ماڈل بہت سی ایپلیکیشنز کے لیے اچھا کام کرتا ہے لیکن ایج کمپیوٹنگ کے منظرناموں میں اندرونی حدود رکھتا ہے۔ روایتی طریقہ کار میں صارف کی درخواستوں کو دور دراز سرورز پر بھیجنا، انہیں طاقتور ہارڈویئر کے ذریعے پروسیس کرنا، اور انٹرنیٹ کے ذریعے نتائج واپس کرنا شامل ہے۔ اگرچہ یہ طریقہ جدید ترین ماڈلز تک رسائی فراہم کرتا ہے، یہ انٹرنیٹ کنیکٹیویٹی پر انحصار پیدا کرتا ہے، لیٹنسی کے خدشات کو جنم دیتا ہے، اور حساس ڈیٹا کو بیرونی سرورز پر منتقل کرنے کی ضرورت کے وقت پرائیویسی کے مسائل پیدا کرتا ہے۔

روایتی اے آئی کمپیوٹنگ کے طریقہ کار کے ساتھ کام کرتے وقت ہمیں کچھ بنیادی تصورات کو سمجھنے کی ضرورت ہے، یعنی:

- **☁️ کلاؤڈ پر مبنی پروسیسنگ**: اے آئی ماڈلز طاقتور سرور انفراسٹرکچر پر چلتے ہیں جن میں اعلیٰ کمپیوٹیشنل وسائل ہوتے ہیں۔
- **🔌 API پر مبنی رسائی**: ایپلیکیشنز مقامی پروسیسنگ کے بجائے ریموٹ API کالز کے ذریعے اے آئی صلاحیتوں تک رسائی حاصل کرتی ہیں۔
- **🎛️ مرکزی ماڈل مینجمنٹ**: ماڈلز کو مرکزی طور پر برقرار رکھا اور اپ ڈیٹ کیا جاتا ہے، جو مستقل مزاجی کو یقینی بناتا ہے لیکن نیٹ ورک کنیکٹیویٹی کی ضرورت ہوتی ہے۔
- **📈 وسائل کی توسیع پذیری**: کلاؤڈ انفراسٹرکچر مختلف کمپیوٹیشنل مطالبات کو پورا کرنے کے لیے متحرک طور پر توسیع کر سکتا ہے۔

## ایج کمپیوٹنگ کا چیلنج

ایج ڈیوائسز جیسے لیپ ٹاپ، موبائل فونز، اور انٹرنیٹ آف تھنگز (IoT) ڈیوائسز جیسے Raspberry Pi اور NVIDIA Orin Nano منفرد کمپیوٹیشنل حدود پیش کرتے ہیں۔ ان ڈیوائسز میں عام طور پر ڈیٹا سینٹر انفراسٹرکچر کے مقابلے میں محدود پروسیسنگ پاور، میموری، اور توانائی کے وسائل ہوتے ہیں۔

روایتی LLMs کو ایسے ڈیوائسز پر چلانا تاریخی طور پر ان ہارڈویئر کی حدود کی وجہ سے مشکل رہا ہے۔ تاہم، مختلف منظرناموں میں ایج اے آئی پروسیسنگ کی ضرورت بڑھتی جا رہی ہے۔ انٹرنیٹ کنیکٹیویٹی ناقابل اعتماد یا دستیاب نہ ہونے کی صورت میں، جیسے دور دراز صنعتی مقامات، سفر میں موجود گاڑیاں، یا کمزور نیٹ ورک کوریج والے علاقے، ایج اے آئی کی اہمیت بڑھ جاتی ہے۔ مزید برآں، اعلیٰ سیکیورٹی معیارات کی ضرورت والے ایپلیکیشنز، جیسے طبی آلات، مالیاتی نظام، یا حکومتی ایپلیکیشنز، حساس ڈیٹا کو مقامی طور پر پروسیس کرنے کی ضرورت ہو سکتی ہے تاکہ پرائیویسی اور تعمیل کی ضروریات کو برقرار رکھا جا سکے۔

### ایج کمپیوٹنگ کی کلیدی حدود

ایج کمپیوٹنگ کے ماحول کو کئی بنیادی حدود کا سامنا کرنا پڑتا ہے جو روایتی کلاؤڈ پر مبنی اے آئی حلوں کو نہیں ہوتا:

- **محدود پروسیسنگ پاور**: ایج ڈیوائسز میں عام طور پر سرور گریڈ ہارڈویئر کے مقابلے میں کم CPU کورز اور کم کلاک اسپیڈز ہوتی ہیں۔
- **میموری کی حدود**: ایج ڈیوائسز پر دستیاب RAM اور اسٹوریج کی گنجائش نمایاں طور پر کم ہوتی ہے۔
- **توانائی کی حدود**: بیٹری سے چلنے والے ڈیوائسز کو طویل آپریشن کے لیے کارکردگی اور توانائی کی کھپت کے درمیان توازن قائم کرنا ہوتا ہے۔
- **تھرمل مینجمنٹ**: کمپیکٹ فارم فیکٹرز کولنگ کی صلاحیتوں کو محدود کرتے ہیں، جو لوڈ کے تحت مسلسل کارکردگی کو متاثر کرتے ہیں۔

## ایج اے آئی کیا ہے؟

### تصور: ایج اے آئی کی تعریف

ایج اے آئی مصنوعی ذہانت کے الگورتھمز کو براہ راست ایج ڈیوائسز پر تعینات اور چلانے کو کہتے ہیں—وہ فزیکل ہارڈویئر جو نیٹ ورک کے "ایج" پر موجود ہوتا ہے، جہاں ڈیٹا پیدا اور جمع کیا جاتا ہے۔ ان ڈیوائسز میں اسمارٹ فونز، IoT سینسرز، اسمارٹ کیمرے، خود مختار گاڑیاں، پہننے کے قابل آلات، اور صنعتی سامان شامل ہیں۔ روایتی اے آئی سسٹمز کے برعکس جو پروسیسنگ کے لیے کلاؤڈ سرورز پر انحصار کرتے ہیں، ایج اے آئی ذہانت کو براہ راست ڈیٹا کے منبع تک لے آتا ہے۔

ایج اے آئی بنیادی طور پر اے آئی پروسیسنگ کو مرکزیت سے دور کرنے کے بارے میں ہے، اسے مرکزی ڈیٹا سینٹرز سے ہٹا کر ان ڈیوائسز کے وسیع نیٹ ورک میں تقسیم کرنا جو ہمارے ڈیجیٹل ماحولیاتی نظام کو تشکیل دیتے ہیں۔ یہ اے آئی سسٹمز کے ڈیزائن اور تعیناتی میں ایک بنیادی آرکیٹیکچرل تبدیلی کی نمائندگی کرتا ہے۔

ایج اے آئی کے کلیدی تصورات میں شامل ہیں:

- **قربت پروسیسنگ**: کمپیوٹیشن جسمانی طور پر اس جگہ کے قریب ہوتی ہے جہاں ڈیٹا پیدا ہوتا ہے۔
- **غیر مرکزی ذہانت**: فیصلہ سازی کی صلاحیتیں متعدد ڈیوائسز میں تقسیم ہوتی ہیں۔
- **ڈیٹا کی خود مختاری**: معلومات مقامی کنٹرول میں رہتی ہیں، اکثر کبھی بھی ڈیوائس سے باہر نہیں جاتی۔
- **خود مختار آپریشن**: ڈیوائسز مستقل کنیکٹیویٹی کی ضرورت کے بغیر ذہین طور پر کام کر سکتی ہیں۔
- **ایمبیڈڈ اے آئی**: ذہانت روزمرہ کے آلات کی ایک اندرونی صلاحیت بن جاتی ہے۔

### ایج اے آئی آرکیٹیکچر کی بصری وضاحت

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                  │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                      │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────────────────────────────────────────┐   Direct Response   ┌───────────┐
│              Edge Devices with Embedded AI        │───────────────────>│ End Users │
│  ┌─────────┐  ┌──────────────┐  ┌──────────────┐ │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │ │
│  └─────────┘  └──────────────┘  └──────────────┘ │
└──────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

ایج اے آئی مصنوعی ذہانت کی تعیناتی میں ایک بنیادی تبدیلی کی نمائندگی کرتا ہے، جو اے آئی کی صلاحیتوں کو براہ راست ایج ڈیوائسز پر لاتا ہے بجائے اس کے کہ صرف کلاؤڈ پر مبنی پروسیسنگ پر انحصار کیا جائے۔ یہ طریقہ محدود کمپیوٹیشنل وسائل والے ڈیوائسز پر مقامی طور پر اے آئی ماڈلز کو چلانے کے قابل بناتا ہے، مستقل انٹرنیٹ کنیکٹیویٹی کی ضرورت کے بغیر ریئل ٹائم انفرنس کی صلاحیت فراہم کرتا ہے۔

ایج اے آئی مختلف ٹیکنالوجیز اور تکنیکوں کو شامل کرتا ہے جو اے آئی ماڈلز کو زیادہ موثر اور محدود وسائل والے ڈیوائسز پر تعیناتی کے لیے موزوں بنانے کے لیے ڈیزائن کیے گئے ہیں۔ مقصد یہ ہے کہ کمپیوٹیشنل اور میموری کی ضروریات کو نمایاں طور پر کم کرتے ہوئے معقول کارکردگی کو برقرار رکھا جائے۔

آئیے ان بنیادی طریقوں پر نظر ڈالیں جو مختلف ڈیوائس کی اقسام اور استعمال کے معاملات میں ایج اے آئی کے نفاذ کو فعال کرتے ہیں۔

### ایج اے آئی کے بنیادی اصول

ایج اے آئی کئی بنیادی اصولوں پر مبنی ہے جو اسے روایتی کلاؤڈ پر مبنی اے آئی سے ممتاز کرتے ہیں:

- **مقامی پروسیسنگ**: اے آئی انفرنس براہ راست ایج ڈیوائس پر ہوتی ہے، بیرونی کنیکٹیویٹی کی ضرورت کے بغیر۔
- **وسائل کی اصلاح**: ماڈلز کو ہدف ڈیوائسز کی ہارڈویئر کی حدود کے لیے خاص طور پر بہتر بنایا جاتا ہے۔
- **ریئل ٹائم کارکردگی**: وقت کے حساس ایپلیکیشنز کے لیے کم سے کم لیٹنسی کے ساتھ پروسیسنگ ہوتی ہے۔
- **پرائیویسی بائی ڈیزائن**: حساس ڈیٹا ڈیوائس پر ہی رہتا ہے، سیکیورٹی اور تعمیل کو بڑھاتا ہے۔

## ایج اے آئی کو فعال کرنے والی کلیدی ٹیکنالوجیز

### ماڈل کوانٹائزیشن

ایج اے آئی میں سب سے اہم تکنیکوں میں سے ایک ماڈل کوانٹائزیشن ہے۔ یہ عمل ماڈل پیرامیٹرز کی درستگی کو کم کرنے پر مشتمل ہوتا ہے، عام طور پر 32 بٹ فلوٹنگ پوائنٹ نمبروں سے 8 بٹ انٹیجرز یا اس سے بھی کم درستگی کے فارمیٹس میں۔ اگرچہ درستگی میں یہ کمی پریشان کن لگ سکتی ہے، تحقیق سے پتہ چلا ہے کہ بہت سے اے آئی ماڈلز اپنی کارکردگی کو برقرار رکھ سکتے ہیں، یہاں تک کہ نمایاں طور پر کم درستگی کے ساتھ۔

کوانٹائزیشن فلوٹنگ پوائنٹ ویلیوز کی رینج کو چھوٹے سیٹ کے ڈسکریٹ ویلیوز میں میپ کرنے کے ذریعے کام کرتا ہے۔ مثال کے طور پر، ہر پیرامیٹر کی نمائندگی کے لیے 32 بٹس استعمال کرنے کے بجائے، کوانٹائزیشن صرف 8 بٹس استعمال کر سکتا ہے، جس کے نتیجے میں میموری کی ضروریات میں 4x کمی اور اکثر تیز انفرنس کے اوقات ہوتے ہیں۔

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

مختلف کوانٹائزیشن تکنیکوں میں شامل ہیں:

- **پوسٹ ٹریننگ کوانٹائزیشن (PTQ)**: ماڈل کی تربیت کے بعد لاگو کیا جاتا ہے، دوبارہ تربیت کی ضرورت کے بغیر۔
- **کوانٹائزیشن-آگاہ تربیت (QAT)**: بہتر درستگی کے لیے تربیت کے دوران کوانٹائزیشن اثرات کو شامل کرتا ہے۔
- **ڈائنامک کوانٹائزیشن**: ویٹس کو int8 میں کوانٹائز کرتا ہے لیکن ایکٹیویشنز کو متحرک طور پر حساب کرتا ہے۔
- **اسٹیک کوانٹائزیشن**: ویٹس اور ایکٹیویشنز دونوں کے لیے تمام کوانٹائزیشن پیرامیٹرز کو پہلے سے حساب کرتا ہے۔

ایج اے آئی کی تعیناتی کے لیے، مناسب کوانٹائزیشن حکمت عملی کا انتخاب ہدف ڈیوائس کی ماڈل آرکیٹیکچر، کارکردگی کی ضروریات، اور ہارڈویئر کی صلاحیتوں پر منحصر ہے۔

### ماڈل کمپریشن اور اصلاح

کوانٹائزیشن کے علاوہ، مختلف کمپریشن تکنیکیں ماڈل کے سائز اور کمپیوٹیشنل ضروریات کو کم کرنے میں مدد کرتی ہیں۔ ان میں شامل ہیں:

**پروننگ**: یہ تکنیک نیورل نیٹ ورکس سے غیر ضروری کنکشنز یا نیورونز کو ہٹاتی ہے۔ ان پیرامیٹرز کی شناخت اور ختم کرکے جو ماڈل کی کارکردگی میں کم حصہ ڈالتے ہیں، پروننگ ماڈل کے سائز کو نمایاں طور پر کم کر سکتی ہے جبکہ درستگی کو برقرار رکھتی ہے۔

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**نالج ڈسٹلیشن**: یہ طریقہ ایک چھوٹے "اسٹوڈنٹ" ماڈل کو ایک بڑے "ٹیچر" ماڈل کے رویے کی نقل کرنے کے لیے تربیت دینے پر مشتمل ہوتا ہے۔ اسٹوڈنٹ ماڈل ٹیچر کے آؤٹ پٹس کو تقریباً حاصل کرنے کے لیے سیکھتا ہے، اکثر کم پیرامیٹرز کے ساتھ اسی طرح کی کارکردگی حاصل کرتا ہے۔

**ماڈل آرکیٹیکچر کی اصلاح**: محققین نے خاص طور پر ایج تعیناتی کے لیے ڈیزائن کردہ خصوصی آرکیٹیکچرز تیار کیے ہیں، جیسے MobileNets، EfficientNets، اور دیگر ہلکے آرکیٹیکچرز جو کارکردگی کو کمپیوٹیشنل کارکردگی کے ساتھ متوازن کرتے ہیں۔

### چھوٹے لینگویج ماڈلز (SLMs)

ایج اے آئی میں ایک ابھرتا ہوا رجحان چھوٹے لینگویج ماڈلز (SLMs) کی ترقی ہے۔ یہ ماڈلز شروع سے ہی کمپیکٹ اور موثر ہونے کے لیے ڈیزائن کیے گئے ہیں جبکہ اب بھی معنی خیز قدرتی زبان کی صلاحیتیں فراہم کرتے ہیں۔ SLMs یہ حاصل کرتے ہیں محتاط آرکیٹیکچرل انتخاب، موثر تربیتی تکنیکوں، اور مخصوص ڈومینز یا کاموں پر مرکوز تربیت کے ذریعے۔

روایتی طریقوں کے برعکس جو بڑے ماڈلز کو کمپریس کرنے میں شامل ہوتے ہیں، SLMs اکثر چھوٹے ڈیٹا سیٹس اور خاص طور پر ایج تعیناتی کے لیے ڈیزائن کردہ بہتر آرکیٹیکچرز کے ساتھ تربیت یافتہ ہوتے ہیں۔ یہ طریقہ ایسے ماڈلز کا نتیجہ دے سکتا ہے جو نہ صرف چھوٹے ہوں بلکہ مخصوص استعمال کے معاملات کے لیے زیادہ موثر ہوں۔

## ایج اے آئی کے لیے ہارڈویئر ایکسیلیریشن

جدید ایج ڈیوائسز میں تیزی سے خصوصی ہارڈویئر شامل ہوتا ہے جو اے آئی ورک لوڈز کو تیز کرنے کے لیے ڈیزائن کیا گیا ہے:

### نیورل پروسیسنگ یونٹس (NPUs)

NPUs خاص طور پر نیورل نیٹ ورک کمپیوٹیشنز کے لیے ڈیزائن کردہ پروسیسرز ہیں۔ یہ چپس روایتی CPUs کے مقابلے میں اے آئی انفرنس کے کاموں کو زیادہ مؤثر طریقے سے انجام دے سکتی ہیں، اکثر کم توانائی کی کھپت کے ساتھ۔ بہت سے جدید اسمارٹ فونز، لیپ ٹاپ، اور IoT ڈیوائسز اب NPUs شامل کرتے ہیں تاکہ آن ڈیوائس اے آئی پروسیسنگ کو فعال کیا جا سکے۔

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

NPUs والے ڈیوائسز میں شامل ہیں:

- **ایپل**: A-سیریز اور M-سیریز چپس نیورل انجن کے ساتھ
- **کوالکوم**: اسنیپ ڈریگن پروسیسرز Hexagon DSP/NPU کے ساتھ
- **سام سنگ**: Exynos پروسیسرز NPU کے ساتھ
- **انٹیل**: Movidius VPUs اور Habana Labs ایکسیلیریٹرز
- **مائیکروسافٹ**: Windows Copilot+ PCs NPUs کے ساتھ

### 🎮 GPU ایکسیلیریشن

اگرچہ ایج ڈیوائسز میں ڈیٹا سینٹرز میں موجود طاقتور GPUs نہیں ہوتے، بہت سے اب بھی مربوط یا الگ GPUs شامل کرتے ہیں جو اے آئی ورک لوڈز کو تیز کر سکتے ہیں۔ جدید موبائل GPUs اور مربوط گرافکس پروسیسرز اے آئی انفرنس کے کاموں کے لیے نمایاں کارکردگی میں بہتری فراہم کر سکتے ہیں۔

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### CPU کی اصلاح

یہاں تک کہ صرف CPU والے ڈیوائسز بھی ایج اے آئی کے ذریعے بہتر نفاذ سے فائدہ اٹھا سکتے ہیں۔ جدید CPUs میں اے آئی ورک لوڈز کے لیے خصوصی ہدایات شامل ہیں، اور سافٹ ویئر فریم ورک اے آئی انفرنس کے لیے CPU کی کارکردگی کو زیادہ سے زیادہ کرنے کے لیے تیار کیے گئے ہیں۔

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

ایج اے آئی کے ساتھ کام کرنے والے سافٹ ویئر انجینئرز کے لیے، ہدف ڈیوائسز پر انفرنس کی کارکردگی اور توانائی کی کارکردگی کو بہتر بنانے کے لیے ان ہارڈویئر ایکسیلیریشن کے اختیارات کو استعمال کرنے کا طریقہ سمجھنا بہت ضروری ہے۔

## ایج اے آئی کے فوائد

### پرائیویسی اور سیکیورٹی

ایج اے آئی کا سب سے اہم فائدہ بہتر پرائیویسی اور سیکیورٹی ہے۔ ڈیٹا کو مقامی طور پر ڈیوائس پر پروسیس کرنے سے، حساس معلومات کبھی بھی صارف کے کنٹرول سے باہر نہیں جاتی۔ یہ خاص طور پر ذاتی ڈیٹا، طبی معلومات، یا خفیہ کاروباری ڈیٹا کو ہینڈل کرنے والی ایپلیکیشنز کے لیے اہم ہے۔

### کم لیٹنسی

ایج اے آئی ڈیٹا کو پروسیسنگ کے لیے دور دراز سرورز پر بھیجنے کی ضرورت کو ختم کرتا ہے، لیٹنسی کو نمایاں طور پر کم کرتا ہے۔ یہ خود مختار گاڑیاں، صنعتی آٹومیشن، یا انٹرایکٹو ایپلیکیشنز جیسے ریئل ٹائم ایپلیکیشنز کے لیے بہت اہم ہے جہاں فوری ردعمل کی ضرورت ہوتی ہے
## ➡️ آگے کیا ہے

- [02: EdgeAI ایپلیکیشنز](02.RealWorldCaseStudies.md)

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔