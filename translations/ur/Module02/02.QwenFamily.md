<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-17T17:05:41+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "ur"
}
-->
# Ø³ÛŒÚ©Ø´Ù† 2: Ù‚ÛŒÙˆÛŒÙ† ÙÛŒÙ…Ù„ÛŒ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¨Ø§ØªÛŒÚº

Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„ ÙÛŒÙ…Ù„ÛŒ Ø¹Ù„ÛŒ Ø¨Ø§Ø¨Ø§ Ú©Ù„Ø§Ø¤Úˆ Ú©Ø§ Ø¬Ø§Ù…Ø¹ Ø·Ø±ÛŒÙ‚Û ÛÛ’ Ø¬Ùˆ Ø¨Ú‘Û’ Ø²Ø¨Ø§Ù† Ù…Ø§ÚˆÙ„Ø² Ø§ÙˆØ± Ù…Ù„Ù¹ÛŒ Ù…ÙˆÚˆÙ„ Ø§Û’ Ø¢Ø¦ÛŒ Ú©Û’ Ù„ÛŒÛ’ ÛÛ’ØŒ ÛŒÛ Ø¸Ø§ÛØ± Ú©Ø±ØªØ§ ÛÛ’ Ú©Û Ø§ÙˆÙ¾Ù† Ø³ÙˆØ±Ø³ Ù…Ø§ÚˆÙ„Ø² Ù‚Ø§Ø¨Ù„ Ø°Ú©Ø± Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ø¬Ø¨Ú©Û Ù…Ø®ØªÙ„Ù ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ù…ÛŒÚº Ù‚Ø§Ø¨Ù„ Ø±Ø³Ø§Ø¦ÛŒ Ø±ÛØªÛ’ ÛÛŒÚºÛ” ÛŒÛ Ø³Ù…Ø¬Ú¾Ù†Ø§ Ø¶Ø±ÙˆØ±ÛŒ ÛÛ’ Ú©Û Ù‚ÛŒÙˆÛŒÙ† ÙÛŒÙ…Ù„ÛŒ Ø·Ø§Ù‚ØªÙˆØ± Ø§Û’ Ø¢Ø¦ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Ùˆ Ù„Ú†Ú©Ø¯Ø§Ø± ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ø§Ø®ØªÛŒØ§Ø±Ø§Øª Ú©Û’ Ø³Ø§ØªÚ¾ Ú©ÛŒØ³Û’ ÙØ¹Ø§Ù„ Ú©Ø±ØªÛŒ ÛÛ’ Ø¬Ø¨Ú©Û Ù…Ø®ØªÙ„Ù Ú©Ø§Ù…ÙˆÚº Ù…ÛŒÚº Ù…Ø³Ø§Ø¨Ù‚ØªÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Ùˆ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø±Ú©Ú¾ØªÛŒ ÛÛ’Û”

## ÚˆÙˆÛŒÙ„Ù¾Ø±Ø² Ú©Û’ Ù„ÛŒÛ’ ÙˆØ³Ø§Ø¦Ù„

### ÛÚ¯Ù†Ú¯ ÙÛŒØ³ Ù…Ø§ÚˆÙ„ Ø±ÛŒÙ¾ÙˆØ²Ù¹Ø±ÛŒ
Ù‚ÛŒÙˆÛŒÙ† ÙÛŒÙ…Ù„ÛŒ Ú©Û’ Ù…Ù†ØªØ®Ø¨ Ù…Ø§ÚˆÙ„Ø² [ÛÚ¯Ù†Ú¯ ÙÛŒØ³](https://huggingface.co/models?search=qwen) Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø¯Ø³ØªÛŒØ§Ø¨ ÛÛŒÚºØŒ Ø¬Ùˆ Ø§Ù† Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ú©Ú†Ú¾ ÙˆØ±Ú˜Ù†Ø² ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ø¢Ù¾ Ø¯Ø³ØªÛŒØ§Ø¨ ÙˆØ±Ú˜Ù†Ø² Ú©Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø§Ù†ÛÛŒÚº Ø§Ù¾Ù†Û’ Ù…Ø®ØµÙˆØµ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ± Ø¨Ù†Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± Ù…Ø®ØªÙ„Ù ÙØ±ÛŒÙ… ÙˆØ±Ú© Ú©Û’ Ø°Ø±ÛŒØ¹Û’ ØªØ¹ÛŒÙ†Ø§Øª Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

### Ù…Ù‚Ø§Ù…ÛŒ ØªØ±Ù‚ÛŒØ§ØªÛŒ Ù¹ÙˆÙ„Ø²
Ù…Ù‚Ø§Ù…ÛŒ ØªØ±Ù‚ÛŒ Ø§ÙˆØ± Ù¹ÛŒØ³Ù¹Ù†Ú¯ Ú©Û’ Ù„ÛŒÛ’ØŒ Ø¢Ù¾ [Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„](https://github.com/microsoft/foundry-local) Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº ØªØ§Ú©Û Ø¯Ø³ØªÛŒØ§Ø¨ Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ú©Ùˆ Ø§Ù¾Ù†ÛŒ ØªØ±Ù‚ÛŒØ§ØªÛŒ Ù…Ø´ÛŒÙ† Ù¾Ø± Ø¨ÛØªØ± Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ Ø³Ø§ØªÚ¾ Ú†Ù„Ø§ÛŒØ§ Ø¬Ø§ Ø³Ú©Û’Û”

### Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ú©Û’ ÙˆØ³Ø§Ø¦Ù„
- [Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Ø§ÛŒØ¬ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ú©Ùˆ Ø¨ÛØªØ± Ø¨Ù†Ø§Ù†Ø§](https://github.com/microsoft/olive)

## ØªØ¹Ø§Ø±Ù

Ø§Ø³ Ù¹ÛŒÙˆÙ¹ÙˆØ±ÛŒÙ„ Ù…ÛŒÚºØŒ ÛÙ… Ø¹Ù„ÛŒ Ø¨Ø§Ø¨Ø§ Ú©Û’ Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„ ÙÛŒÙ…Ù„ÛŒ Ø§ÙˆØ± Ø§Ø³ Ú©Û’ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ØªØµÙˆØ±Ø§Øª Ú©Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±ÛŒÚº Ú¯Û’Û” ÛÙ… Ù‚ÛŒÙˆÛŒÙ† ÙÛŒÙ…Ù„ÛŒ Ú©Û’ Ø§Ø±ØªÙ‚Ø§Ø¡ØŒ Ø¬Ø¯ÛŒØ¯ ØªØ±Ø¨ÛŒØªÛŒ Ø·Ø±ÛŒÙ‚ÙˆÚºØŒ Ø¬Ùˆ Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ú©Ùˆ Ù…Ø¤Ø«Ø± Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚºØŒ ÙÛŒÙ…Ù„ÛŒ Ù…ÛŒÚº Ú©Ù„ÛŒØ¯ÛŒ ÙˆØ±Ú˜Ù†Ø²ØŒ Ø§ÙˆØ± Ù…Ø®ØªÙ„Ù Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ù…ÛŒÚº Ø¹Ù…Ù„ÛŒ Ø§Ø·Ù„Ø§Ù‚Ø§Øª Ú©Ø§ Ø§Ø­Ø§Ø·Û Ú©Ø±ÛŒÚº Ú¯Û’Û”

## Ø³ÛŒÚ©Ú¾Ù†Û’ Ú©Û’ Ù…Ù‚Ø§ØµØ¯

Ø§Ø³ Ù¹ÛŒÙˆÙ¹ÙˆØ±ÛŒÙ„ Ú©Û’ Ø§Ø®ØªØªØ§Ù… ØªÚ©ØŒ Ø¢Ù¾ Ù‚Ø§Ø¨Ù„ ÛÙˆÚº Ú¯Û’:

- Ø¹Ù„ÛŒ Ø¨Ø§Ø¨Ø§ Ú©Û’ Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„ ÙÛŒÙ…Ù„ÛŒ Ú©Û’ ÚˆÛŒØ²Ø§Ø¦Ù† ÙÙ„Ø³ÙÛ Ø§ÙˆØ± Ø§Ø±ØªÙ‚Ø§Ø¡ Ú©Ùˆ Ø³Ù…Ø¬Ú¾Ù†Ø§
- Ú©Ù„ÛŒØ¯ÛŒ Ø¬Ø¯ØªÙˆÚº Ú©ÛŒ Ø´Ù†Ø§Ø®Øª Ú©Ø±Ù†Ø§ Ø¬Ùˆ Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ú©Ùˆ Ù…Ø®ØªÙ„Ù Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø± Ø³Ø§Ø¦Ø² Ù…ÛŒÚº Ø§Ø¹Ù„ÛŒÙ° Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ù‚Ø§Ø¨Ù„ Ø¨Ù†Ø§ØªÛŒ ÛÛŒÚº
- Ù…Ø®ØªÙ„Ù Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„ ÙˆØ±Ú˜Ù†Ø² Ú©Û’ ÙÙˆØ§Ø¦Ø¯ Ø§ÙˆØ± Ø­Ø¯ÙˆØ¯ Ú©Ùˆ Ù¾ÛÚ†Ø§Ù†Ù†Ø§
- Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ Ú©Û’ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ù…Ù†Ø§Ø³Ø¨ ÙˆØ±Ú˜Ù†Ø² Ù…Ù†ØªØ®Ø¨ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ø¹Ù„Ù… Ú©Ùˆ Ù„Ø§Ú¯Ùˆ Ú©Ø±Ù†Ø§

## Ø¬Ø¯ÛŒØ¯ Ø§Û’ Ø¢Ø¦ÛŒ Ù…Ø§ÚˆÙ„ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…Û’ Ú©Ùˆ Ø³Ù…Ø¬Ú¾Ù†Ø§

Ø§Û’ Ø¢Ø¦ÛŒ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…Û Ù†Ù…Ø§ÛŒØ§Úº Ø·ÙˆØ± Ù¾Ø± ØªØ±Ù‚ÛŒ Ú©Ø± Ú†Ú©Ø§ ÛÛ’ØŒ Ù…Ø®ØªÙ„Ù ØªÙ†Ø¸ÛŒÙ…ÛŒÚº Ø²Ø¨Ø§Ù† Ù…Ø§ÚˆÙ„ Ú©ÛŒ ØªØ±Ù‚ÛŒ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø®ØªÙ„Ù Ø·Ø±ÛŒÙ‚Û’ Ø§Ø®ØªÛŒØ§Ø± Ú©Ø± Ø±ÛÛŒ ÛÛŒÚºÛ” Ú©Ú†Ú¾ Ù…Ù„Ú©ÛŒØªÛŒ Ø¨Ù†Ø¯ Ø³ÙˆØ±Ø³ Ù…Ø§ÚˆÙ„Ø² Ù¾Ø± ØªÙˆØ¬Û Ù…Ø±Ú©ÙˆØ² Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø¬Ø¨Ú©Û Ø¯ÛŒÚ¯Ø± Ø§ÙˆÙ¾Ù† Ø³ÙˆØ±Ø³ Ø±Ø³Ø§Ø¦ÛŒ Ø§ÙˆØ± Ø´ÙØ§ÙÛŒØª Ù¾Ø± Ø²ÙˆØ± Ø¯ÛŒØªÛ’ ÛÛŒÚºÛ” Ø±ÙˆØ§ÛŒØªÛŒ Ø·Ø±ÛŒÙ‚Û ÛŒØ§ ØªÙˆ Ø¨Ú‘Û’ Ù…Ù„Ú©ÛŒØªÛŒ Ù…Ø§ÚˆÙ„Ø² Ù¾Ø± Ù…Ø´ØªÙ…Ù„ ÛÙˆØªØ§ ÛÛ’ Ø¬Ùˆ ØµØ±Ù APIs Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ù‚Ø§Ø¨Ù„ Ø±Ø³Ø§Ø¦ÛŒ ÛÙˆØªÛ’ ÛÛŒÚº ÛŒØ§ Ø§ÙˆÙ¾Ù† Ø³ÙˆØ±Ø³ Ù…Ø§ÚˆÙ„Ø² Ø¬Ùˆ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ù…ÛŒÚº Ù¾ÛŒÚ†Ú¾Û’ Ø±Û Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

ÛŒÛ Ù†Ù‚Ø·Û Ù†Ø¸Ø± Ø§Ù† ØªÙ†Ø¸ÛŒÙ…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ú†ÛŒÙ„Ù†Ø¬Ø² Ù¾ÛŒØ¯Ø§ Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ùˆ Ø·Ø§Ù‚ØªÙˆØ± Ø§Û’ Ø¢Ø¦ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©ÛŒ ØªÙ„Ø§Ø´ Ù…ÛŒÚº ÛÛŒÚº Ø¬Ø¨Ú©Û Ø§Ù¾Ù†Û’ ÚˆÛŒÙ¹Ø§ØŒ Ø§Ø®Ø±Ø§Ø¬Ø§ØªØŒ Ø§ÙˆØ± ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©ÛŒ Ù„Ú†Ú© Ù¾Ø± Ú©Ù†Ù¹Ø±ÙˆÙ„ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚºÛ” Ø±ÙˆØ§ÛŒØªÛŒ Ø·Ø±ÛŒÙ‚Û Ø§Ú©Ø«Ø± Ø¬Ø¯ÛŒØ¯ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ø§ÙˆØ± Ø¹Ù…Ù„ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ ØªØ­ÙØ¸Ø§Øª Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† Ø§Ù†ØªØ®Ø§Ø¨ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆØªÛŒ ÛÛ’Û”

## Ù‚Ø§Ø¨Ù„ Ø±Ø³Ø§Ø¦ÛŒ Ø§Û’ Ø¢Ø¦ÛŒ Ú©ÛŒ Ø¹Ù…Ø¯Ú¯ÛŒ Ú©Ø§ Ú†ÛŒÙ„Ù†Ø¬

Ø§Ø¹Ù„ÛŒÙ° Ù…Ø¹ÛŒØ§Ø±ØŒ Ù‚Ø§Ø¨Ù„ Ø±Ø³Ø§Ø¦ÛŒ Ø§Û’ Ø¢Ø¦ÛŒ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª Ù…Ø®ØªÙ„Ù Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ù…ÛŒÚº Ø¨Ú‘Ú¾ØªÛŒ Ø¬Ø§ Ø±ÛÛŒ ÛÛ’Û” Ø§Ù† Ø§Ø·Ù„Ø§Ù‚Ø§Øª Ù¾Ø± ØºÙˆØ± Ú©Ø±ÛŒÚº Ø¬Ù† Ú©Û’ Ù„ÛŒÛ’ Ù…Ø®ØªÙ„Ù ØªÙ†Ø¸ÛŒÙ…ÛŒ Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ú©Û’ Ù„ÛŒÛ’ Ù„Ú†Ú©Ø¯Ø§Ø± ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ø§Ø®ØªÛŒØ§Ø±Ø§ØªØŒ Ù„Ø§Ú¯Øª Ù…Ø¤Ø«Ø± Ù†ÙØ§Ø° Ø¬ÛØ§Úº API Ú©Û’ Ø§Ø®Ø±Ø§Ø¬Ø§Øª Ø§ÛÙ… ÛÙˆ Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø¹Ø§Ù„Ù…ÛŒ Ø§Ø·Ù„Ø§Ù‚Ø§Øª Ú©Û’ Ù„ÛŒÛ’ Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ ØµÙ„Ø§Ø­ÛŒØªÛŒÚºØŒ ÛŒØ§ Ú©ÙˆÚˆÙ†Ú¯ Ø§ÙˆØ± Ø±ÛŒØ§Ø¶ÛŒ Ø¬ÛŒØ³Û’ Ø´Ø¹Ø¨ÙˆÚº Ù…ÛŒÚº Ø®ØµÙˆØµÛŒ Ù…ÛØ§Ø±Øª Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆØªÛŒ ÛÛ’Û”

### Ú©Ù„ÛŒØ¯ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ø¶Ø±ÙˆØ±ÛŒØ§Øª

Ø¬Ø¯ÛŒØ¯ Ø§Û’ Ø¢Ø¦ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒÙˆÚº Ú©Ùˆ Ú©Ø¦ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ú©Ø§ Ø³Ø§Ù…Ù†Ø§ Ú©Ø±Ù†Ø§ Ù¾Ú‘ØªØ§ ÛÛ’ Ø¬Ùˆ Ø¹Ù…Ù„ÛŒ Ø§Ø·Ù„Ø§Ù‚ Ú©Ùˆ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±ØªÛŒ ÛÛŒÚº:

- **Ø±Ø³Ø§Ø¦ÛŒ**: Ø´ÙØ§ÙÛŒØª Ø§ÙˆØ± Ø­Ø³Ø¨ Ø¶Ø±ÙˆØ±Øª Ú©Û’ Ù„ÛŒÛ’ Ø§ÙˆÙ¾Ù† Ø³ÙˆØ±Ø³ Ø¯Ø³ØªÛŒØ§Ø¨ÛŒ
- **Ù„Ø§Ú¯Øª Ù…Ø¤Ø«Ø±**: Ù…Ø®ØªÙ„Ù Ø¨Ø¬Ù¹ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø¹Ù‚ÙˆÙ„ Ú©Ù…Ù¾ÛŒÙˆÙ¹ÛŒØ´Ù†Ù„ Ø¶Ø±ÙˆØ±ÛŒØ§Øª
- **Ù„Ú†Ú©**: Ù…Ø®ØªÙ„Ù ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ù…ØªØ¹Ø¯Ø¯ Ù…Ø§ÚˆÙ„ Ø³Ø§Ø¦Ø²
- **Ø¹Ø§Ù„Ù…ÛŒ Ø±Ø³Ø§Ø¦ÛŒ**: Ù…Ø¶Ø¨ÙˆØ· Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ Ø§ÙˆØ± Ø¨ÛŒÙ† Ø§Ù„Ø«Ù‚Ø§ÙØªÛŒ ØµÙ„Ø§Ø­ÛŒØªÛŒÚº
- **Ø®ØµÙˆØµÛŒØª**: Ù…Ø®ØµÙˆØµ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ú©Û’ Ù„ÛŒÛ’ ÚˆÙˆÙ…ÛŒÙ† Ù…Ø®ØµÙˆØµ ÙˆØ±Ú˜Ù†Ø²

## Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„ ÙÙ„Ø³ÙÛ

Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„ ÙÛŒÙ…Ù„ÛŒ Ø§Û’ Ø¢Ø¦ÛŒ Ù…Ø§ÚˆÙ„ Ú©ÛŒ ØªØ±Ù‚ÛŒ Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© Ø¬Ø§Ù…Ø¹ Ù†Ù‚Ø·Û Ù†Ø¸Ø± Ú©ÛŒ Ù†Ù…Ø§Ø¦Ù†Ø¯Ú¯ÛŒ Ú©Ø±ØªÛŒ ÛÛ’ØŒ Ø¬Ùˆ Ø§ÙˆÙ¾Ù† Ø³ÙˆØ±Ø³ Ø±Ø³Ø§Ø¦ÛŒØŒ Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚºØŒ Ø§ÙˆØ± Ø¹Ù…Ù„ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Ùˆ ØªØ±Ø¬ÛŒØ­ Ø¯ÛŒØªÛŒ ÛÛ’ Ø¬Ø¨Ú©Û Ù…Ø³Ø§Ø¨Ù‚ØªÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©ÛŒ Ø®ØµÙˆØµÛŒØ§Øª Ú©Ùˆ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø±Ú©Ú¾ØªÛŒ ÛÛ’Û” Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ù…Ø®ØªÙ„Ù Ù…Ø§ÚˆÙ„ Ø³Ø§Ø¦Ø²ØŒ Ø§Ø¹Ù„ÛŒÙ° Ù…Ø¹ÛŒØ§Ø± Ú©ÛŒ ØªØ±Ø¨ÛŒØªÛŒ Ø·Ø±ÛŒÙ‚ÙˆÚºØŒ Ø§ÙˆØ± Ù…Ø®ØªÙ„Ù ÚˆÙˆÙ…ÛŒÙ†Ø² Ú©Û’ Ù„ÛŒÛ’ Ø®ØµÙˆØµÛŒ ÙˆØ±Ú˜Ù†Ø² Ú©Û’ Ø°Ø±ÛŒØ¹Û’ ÛŒÛ Ø­Ø§ØµÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

Ù‚ÛŒÙˆÛŒÙ† ÙÛŒÙ…Ù„ÛŒ Ù…Ø®ØªÙ„Ù Ù†Ù‚Ø·Û Ù†Ø¸Ø± Ú©Ùˆ Ø´Ø§Ù…Ù„ Ú©Ø±ØªÛŒ ÛÛ’ Ø¬Ùˆ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ-Ø§ÙØ§Ø¯ÛŒØª Ú©Û’ Ø§Ø³Ù¾ÛŒÚ©Ù¹Ø±Ù… Ù…ÛŒÚº Ø§Ø®ØªÛŒØ§Ø±Ø§Øª ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛŒ ÛÛ’ØŒ Ù…ÙˆØ¨Ø§Ø¦Ù„ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ø³Û’ Ù„Û’ Ú©Ø± Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ø³Ø±ÙˆØ±Ø² ØªÚ© ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Ùˆ ÙØ¹Ø§Ù„ Ú©Ø±ØªÛŒ ÛÛ’ Ø¬Ø¨Ú©Û Ø¨Ø§Ù…Ø¹Ù†ÛŒ Ø§Û’ Ø¢Ø¦ÛŒ ØµÙ„Ø§Ø­ÛŒØªÛŒÚº ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛŒ ÛÛ’Û” Ù…Ù‚ØµØ¯ Ø§Ø¹Ù„ÛŒÙ° Ù…Ø¹ÛŒØ§Ø± Ú©Û’ Ø§Û’ Ø¢Ø¦ÛŒ ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ Ú©Ùˆ Ø¬Ù…ÛÙˆØ±ÛŒ Ø¨Ù†Ø§Ù†Ø§ ÛÛ’ Ø¬Ø¨Ú©Û ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒÚº Ù„Ú†Ú© ÙØ±Ø§ÛÙ… Ú©Ø±Ù†Ø§ ÛÛ’Û”

### Ù‚ÛŒÙˆÛŒÙ† Ú©Û’ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÚˆÛŒØ²Ø§Ø¦Ù† Ø§ØµÙˆÙ„

Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ú©Ø¦ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø§ØµÙˆÙ„ÙˆÚº Ù¾Ø± Ù…Ø¨Ù†ÛŒ ÛÛŒÚº Ø¬Ùˆ Ø§Ù†ÛÛŒÚº Ø¯ÛŒÚ¯Ø± Ø²Ø¨Ø§Ù† Ù…Ø§ÚˆÙ„ ÙÛŒÙ…Ù„ÛŒØ² Ø³Û’ Ù…Ù…ØªØ§Ø² Ú©Ø±ØªÛ’ ÛÛŒÚº:

- **Ø§ÙˆÙ¾Ù† Ø³ÙˆØ±Ø³ Ù¾ÛÙ„Û’**: ØªØ­Ù‚ÛŒÙ‚ Ø§ÙˆØ± ØªØ¬Ø§Ø±ØªÛŒ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ Ù…Ú©Ù…Ù„ Ø´ÙØ§ÙÛŒØª Ø§ÙˆØ± Ø±Ø³Ø§Ø¦ÛŒ
- **Ø¬Ø§Ù…Ø¹ ØªØ±Ø¨ÛŒØª**: Ù…ØªØ¹Ø¯Ø¯ Ø²Ø¨Ø§Ù†ÙˆÚº Ø§ÙˆØ± ÚˆÙˆÙ…ÛŒÙ†Ø² Ú©Ø§ Ø§Ø­Ø§Ø·Û Ú©Ø±Ù†Û’ ÙˆØ§Ù„Û’ Ø¨Ú‘Û’ØŒ Ù…ØªÙ†ÙˆØ¹ ÚˆÛŒÙ¹Ø§ Ø³ÛŒÙ¹Ø³ Ù¾Ø± ØªØ±Ø¨ÛŒØª
- **Ù‚Ø§Ø¨Ù„ ØªÙˆØ³ÛŒØ¹ Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø±**: Ù…Ø®ØªÙ„Ù Ú©Ù…Ù¾ÛŒÙˆÙ¹ÛŒØ´Ù†Ù„ Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ø³Û’ Ù…Ø·Ø§Ø¨Ù‚Øª Ø±Ú©Ú¾Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ù…ØªØ¹Ø¯Ø¯ Ù…Ø§ÚˆÙ„ Ø³Ø§Ø¦Ø²
- **Ø®ØµÙˆØµÛŒ Ø¹Ù…Ø¯Ú¯ÛŒ**: Ù…Ø®ØµÙˆØµ Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ± ÚˆÙˆÙ…ÛŒÙ† Ù…Ø®ØµÙˆØµ ÙˆØ±Ú˜Ù†Ø²

## Ù‚ÛŒÙˆÛŒÙ† ÙÛŒÙ…Ù„ÛŒ Ú©Ùˆ ÙØ¹Ø§Ù„ Ú©Ø±Ù†Û’ ÙˆØ§Ù„ÛŒ Ú©Ù„ÛŒØ¯ÛŒ Ù¹ÛŒÚ©Ù†Ø§Ù„ÙˆØ¬ÛŒØ²

### Ø¨Ú‘Û’ Ù¾ÛŒÙ…Ø§Ù†Û’ Ù¾Ø± ØªØ±Ø¨ÛŒØª

Ù‚ÛŒÙˆÛŒÙ† ÙÛŒÙ…Ù„ÛŒ Ú©ÛŒ Ø§ÛŒÚ© Ù†Ù…Ø§ÛŒØ§Úº Ø®ØµÙˆØµÛŒØª ØªØ±Ø¨ÛŒØªÛŒ ÚˆÛŒÙ¹Ø§ Ø§ÙˆØ± Ú©Ù…Ù¾ÛŒÙˆÙ¹ÛŒØ´Ù†Ù„ ÙˆØ³Ø§Ø¦Ù„ Ú©Ø§ Ø¨Ú‘Ø§ Ù¾ÛŒÙ…Ø§Ù†Û ÛÛ’ Ø¬Ùˆ Ù…Ø§ÚˆÙ„ Ú©ÛŒ ØªØ±Ù‚ÛŒ Ù…ÛŒÚº Ù„Ú¯Ø§ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’Û” Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ø§Ø­ØªÛŒØ§Ø· Ø³Û’ ØªÛŒØ§Ø± Ú©Ø±Ø¯ÛØŒ Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ ÚˆÛŒÙ¹Ø§ Ø³ÛŒÙ¹Ø³ Ú©Ø§ ÙØ§Ø¦Ø¯Û Ø§Ù¹Ú¾Ø§ØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ú©Ú¾Ø±Ø¨ÙˆÚº Ù¹ÙˆÚ©Ù†Ø² Ù¾Ø± Ù…Ø´ØªÙ…Ù„ ÛÛŒÚºØŒ Ø¬Ùˆ Ø¬Ø§Ù…Ø¹ Ø¹Ø§Ù„Ù…ÛŒ Ø¹Ù„Ù… Ø§ÙˆØ± Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÛŒÚº ÙØ±Ø§ÛÙ… Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ÚˆÛŒØ²Ø§Ø¦Ù† Ú©ÛŒÛ’ Ú¯Ø¦Û’ ÛÛŒÚºÛ”

ÛŒÛ Ø·Ø±ÛŒÙ‚Û Ø§Ø¹Ù„ÛŒÙ° Ù…Ø¹ÛŒØ§Ø± Ú©Û’ ÙˆÛŒØ¨ Ù…ÙˆØ§Ø¯ØŒ ØªØ¹Ù„ÛŒÙ…ÛŒ Ø§Ø¯Ø¨ØŒ Ú©ÙˆÚˆ Ø±ÛŒÙ¾ÙˆØ²Ù¹Ø±ÛŒØ²ØŒ Ø§ÙˆØ± Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ ÙˆØ³Ø§Ø¦Ù„ Ú©Ùˆ ÛŒÚ©Ø¬Ø§ Ú©Ø± Ú©Û’ Ú©Ø§Ù… Ú©Ø±ØªØ§ ÛÛ’Û” ØªØ±Ø¨ÛŒØªÛŒ Ø·Ø±ÛŒÙ‚Û Ú©Ø§Ø± Ù…Ø®ØªÙ„Ù ÚˆÙˆÙ…ÛŒÙ†Ø² Ø§ÙˆØ± Ø²Ø¨Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ø¹Ù„Ù… Ú©ÛŒ ÙˆØ³Ø¹Øª Ø§ÙˆØ± ØªÙÛÛŒÙ… Ú©ÛŒ Ú¯ÛØ±Ø§Ø¦ÛŒ Ø¯ÙˆÙ†ÙˆÚº Ù¾Ø± Ø²ÙˆØ± Ø¯ÛŒØªØ§ ÛÛ’Û”

### Ø¬Ø¯ÛŒØ¯ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§ÙˆØ± Ø³ÙˆÚ†

Ø­Ø§Ù„ÛŒÛ Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ù¾ÛŒÚ†ÛŒØ¯Û Ú©Ø«ÛŒØ± Ù…Ø±Ø­Ù„Û Ù…Ø³Ø¦Ù„Û Ø­Ù„ Ú©Ø±Ù†Û’ Ú©Ùˆ ÙØ¹Ø§Ù„ Ú©Ø±Ù†Û’ ÙˆØ§Ù„ÛŒ Ù†ÙÛŒØ³ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Ùˆ Ø´Ø§Ù…Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº:

**Ø³ÙˆÚ†Ù†Û’ Ú©Ø§ Ù…ÙˆÚˆ (Ù‚ÛŒÙˆÛŒÙ†3)**: Ù…Ø§ÚˆÙ„Ø² ØªÙØµÛŒÙ„ÛŒ Ù…Ø±Ø­Ù„Û ÙˆØ§Ø± Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…ÛŒÚº Ù…Ø´ØºÙˆÙ„ ÛÙˆ Ø³Ú©ØªÛ’ ÛÛŒÚº Ø§Ø³ Ø³Û’ Ù¾ÛÙ„Û’ Ú©Û Ø­ØªÙ…ÛŒ Ø¬ÙˆØ§Ø¨Ø§Øª ÙØ±Ø§ÛÙ… Ú©Ø±ÛŒÚºØŒ Ø§Ù†Ø³Ø§Ù†ÛŒ Ù…Ø³Ø¦Ù„Û Ø­Ù„ Ú©Ø±Ù†Û’ Ú©Û’ Ø·Ø±ÛŒÙ‚ÙˆÚº Ú©ÛŒ Ø·Ø±Ø­Û”

**Ø¯ÙˆÛØ±ÛŒ Ù…ÙˆÚˆ Ø¢Ù¾Ø±ÛŒØ´Ù†**: Ø³Ø§Ø¯Û Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ù„ÛŒÛ’ ÙÙˆØ±ÛŒ Ø¬ÙˆØ§Ø¨ Ù…ÙˆÚˆ Ø§ÙˆØ± Ù¾ÛŒÚ†ÛŒØ¯Û Ù…Ø³Ø§Ø¦Ù„ Ú©Û’ Ù„ÛŒÛ’ Ú¯ÛØ±ÛŒ Ø³ÙˆÚ† Ú©Û’ Ù…ÙˆÚˆ Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† Ø³ÙˆØ¦Ú† Ú©Ø±Ù†Û’ Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÛ”

**Ú†ÛŒÙ† Ø¢Ù ØªÚ¾ÙˆÙ¹ Ø§Ù†Ø¶Ù…Ø§Ù…**: Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©Û’ Ù…Ø±Ø§Ø­Ù„ Ú©Ø§ Ù‚Ø¯Ø±ØªÛŒ Ø·ÙˆØ± Ù¾Ø± Ø§Ù†Ø¶Ù…Ø§Ù… Ø¬Ùˆ Ù¾ÛŒÚ†ÛŒØ¯Û Ú©Ø§Ù…ÙˆÚº Ù…ÛŒÚº Ø´ÙØ§ÙÛŒØª Ø§ÙˆØ± Ø¯Ø±Ø³ØªÚ¯ÛŒ Ú©Ùˆ Ø¨ÛØªØ± Ø¨Ù†Ø§ØªØ§ ÛÛ’Û”

### Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø±Ù„ Ø¬Ø¯ØªÛŒÚº

Ù‚ÛŒÙˆÛŒÙ† ÙÛŒÙ…Ù„ÛŒ Ú©Ø¦ÛŒ Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø±Ù„ Ø§ØµÙ„Ø§Ø­Ø§Øª Ú©Ùˆ Ø´Ø§Ù…Ù„ Ú©Ø±ØªÛŒ ÛÛ’ Ø¬Ùˆ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ø§ÙˆØ± Ø§ÙØ§Ø¯ÛŒØª Ø¯ÙˆÙ†ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ ÚˆÛŒØ²Ø§Ø¦Ù† Ú©ÛŒ Ú¯Ø¦ÛŒ ÛÛŒÚº:

**Ù‚Ø§Ø¨Ù„ ØªÙˆØ³ÛŒØ¹ ÚˆÛŒØ²Ø§Ø¦Ù†**: Ù…Ø§ÚˆÙ„ Ø³Ø§Ø¦Ø²Ø² Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† Ù…Ø³ØªÙ‚Ù„ Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø± Ø¬Ùˆ Ø¢Ø³Ø§Ù† ØªÙˆØ³ÛŒØ¹ Ø§ÙˆØ± Ù…ÙˆØ§Ø²Ù†Û Ú©Ùˆ ÙØ¹Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’Û”

**Ù…Ù„Ù¹ÛŒ Ù…ÙˆÚˆÙ„ Ø§Ù†Ø¶Ù…Ø§Ù…**: Ù…ØªÙ†ØŒ ÙˆÚ˜Ù†ØŒ Ø§ÙˆØ± Ø¢ÚˆÛŒÙˆ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Ø§ Ø§ÛŒÚ© Ù…ØªØ­Ø¯ Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø± Ù…ÛŒÚº ÛÙ…ÙˆØ§Ø± Ø§Ù†Ø¶Ù…Ø§Ù…Û”

**ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©ÛŒ Ø§ØµÙ„Ø§Ø­**: Ù…Ø®ØªÙ„Ù ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ù…ØªØ¹Ø¯Ø¯ Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø²ÛŒØ´Ù† Ø§Ø®ØªÛŒØ§Ø±Ø§Øª Ø§ÙˆØ± ØªØ¹ÛŒÙ†Ø§ØªÛŒ ÙØ§Ø±Ù…ÛŒÙ¹Ø³Û”

## Ù…Ø§ÚˆÙ„ Ø³Ø§Ø¦Ø² Ø§ÙˆØ± ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ø§Ø®ØªÛŒØ§Ø±Ø§Øª

Ø¬Ø¯ÛŒØ¯ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ù…Ø§Ø­ÙˆÙ„ Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ú©ÛŒ Ù„Ú†Ú© Ø³Û’ ÙØ§Ø¦Ø¯Û Ø§Ù¹Ú¾Ø§ØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ù…Ø®ØªÙ„Ù Ú©Ù…Ù¾ÛŒÙˆÙ¹ÛŒØ´Ù†Ù„ Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ ÛÛŒÚº:

### Ú†Ú¾ÙˆÙ¹Û’ Ù…Ø§ÚˆÙ„Ø² (0.5B-3B)

Ù‚ÛŒÙˆÛŒÙ† Ú†Ú¾ÙˆÙ¹Û’ Ù…Ø§ÚˆÙ„Ø² ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ùˆ Ø§ÛŒØ¬ ØªØ¹ÛŒÙ†Ø§ØªÛŒØŒ Ù…ÙˆØ¨Ø§Ø¦Ù„ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø²ØŒ Ø§ÙˆØ± ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ù…Ø§Ø­ÙˆÙ„ Ú©Û’ Ù„ÛŒÛ’ Ù…ÙˆØ²ÙˆÚº ÛÛŒÚº Ø¬Ø¨Ú©Û Ù…ØªØ§Ø«Ø± Ú©Ù† ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Ùˆ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚºÛ”

### Ø¯Ø±Ù…ÛŒØ§Ù†Û’ Ù…Ø§ÚˆÙ„Ø² (7B-32B)

Ø¯Ø±Ù…ÛŒØ§Ù†Û’ Ø¯Ø±Ø¬Û’ Ú©Û’ Ù…Ø§ÚˆÙ„Ø² Ù¾ÛŒØ´Û ÙˆØ±Ø§Ù†Û Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ± ØµÙ„Ø§Ø­ÛŒØªÛŒÚº ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ø§ÙˆØ± Ú©Ù…Ù¾ÛŒÙˆÙ¹ÛŒØ´Ù†Ù„ Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† Ø¨ÛØªØ±ÛŒÙ† ØªÙˆØ§Ø²Ù† ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

### Ø¨Ú‘Û’ Ù…Ø§ÚˆÙ„Ø² (72B+)

ÙÙ„ Ø§Ø³Ú©ÛŒÙ„ Ù…Ø§ÚˆÙ„Ø² Ø¬Ø¯ÛŒØ¯ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ø²ÛŒØ§Ø¯Û Ù…Ø·Ø§Ù„Ø¨Û Ú©Ø±Ù†Û’ ÙˆØ§Ù„Û’ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø²ØŒ ØªØ­Ù‚ÛŒÙ‚ØŒ Ø§ÙˆØ± Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² ØªØ¹ÛŒÙ†Ø§ØªÛŒÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û ØµÙ„Ø§Ø­ÛŒØª Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆØªÛŒ ÛÛ’Û”

## Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„ ÙÛŒÙ…Ù„ÛŒ Ú©Û’ ÙÙˆØ§Ø¦Ø¯

### Ø§ÙˆÙ¾Ù† Ø³ÙˆØ±Ø³ Ø±Ø³Ø§Ø¦ÛŒ

Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ù…Ú©Ù…Ù„ Ø´ÙØ§ÙÛŒØª Ø§ÙˆØ± Ø­Ø³Ø¨ Ø¶Ø±ÙˆØ±Øª ØµÙ„Ø§Ø­ÛŒØªÛŒÚº ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ ØªÙ†Ø¸ÛŒÙ…ÙˆÚº Ú©Ùˆ Ø§Ù† Ú©Û’ Ù…Ø®ØµÙˆØµ Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ Ù…Ø§ÚˆÙ„Ø² Ú©Ùˆ Ø³Ù…Ø¬Ú¾Ù†Û’ØŒ ØªØ±Ù…ÛŒÙ… Ú©Ø±Ù†Û’ØŒ Ø§ÙˆØ± Ø§Ù¾Ù†Ø§Ù†Û’ Ú©Û’ Ù‚Ø§Ø¨Ù„ Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚº Ø¨ØºÛŒØ± ÙˆÛŒÙ†ÚˆØ± Ù„Ø§Ú© Ø§Ù† Ú©Û’Û”

### ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©ÛŒ Ù„Ú†Ú©

Ù…Ø§ÚˆÙ„ Ø³Ø§Ø¦Ø²Ø² Ú©ÛŒ Ø±ÛŒÙ†Ø¬ Ù…Ø®ØªÙ„Ù ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù†Ø² Ù¾Ø± ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Ùˆ ÙØ¹Ø§Ù„ Ú©Ø±ØªÛŒ ÛÛ’ØŒ Ù…ÙˆØ¨Ø§Ø¦Ù„ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ø³Û’ Ù„Û’ Ú©Ø± ÛØ§Ø¦ÛŒ Ø§ÛŒÙ†Úˆ Ø³Ø±ÙˆØ±Ø² ØªÚ©ØŒ ØªÙ†Ø¸ÛŒÙ…ÙˆÚº Ú©Ùˆ Ø§Ù† Ú©Û’ Ø§Û’ Ø¢Ø¦ÛŒ Ø§Ù†ÙØ±Ø§Ø³Ù¹Ø±Ú©Ú†Ø± Ú©Û’ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒÚº Ù„Ú†Ú© ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛŒ ÛÛ’Û”

### Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ Ø¹Ù…Ø¯Ú¯ÛŒ

Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ ØªÙÛÛŒÙ… Ø§ÙˆØ± Ø¬Ù†Ø±ÛŒØ´Ù† Ù…ÛŒÚº Ø¨ÛØªØ±ÛŒÙ† ÛÛŒÚºØŒ Ø¯Ø±Ø¬Ù†ÙˆÚº Ø²Ø¨Ø§Ù†ÙˆÚº Ú©Ùˆ Ø³Ù¾ÙˆØ±Ù¹ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø®Ø§Øµ Ø·ÙˆØ± Ù¾Ø± Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ Ø§ÙˆØ± Ú†ÛŒÙ†ÛŒ Ù…ÛŒÚº Ù…Ø¶Ø¨ÙˆØ·ØŒ Ø¬Ùˆ Ø§Ù†ÛÛŒÚº Ø¹Ø§Ù„Ù…ÛŒ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ù…ÙˆØ²ÙˆÚº Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚºÛ”

### Ù…Ø³Ø§Ø¨Ù‚ØªÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ

Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ù…Ø³Ù„Ø³Ù„ Ø¨ÛŒÙ†Ú† Ù…Ø§Ø±Ú©Ø³ Ù¾Ø± Ù…Ø³Ø§Ø¨Ù‚ØªÛŒ Ù†ØªØ§Ø¦Ø¬ Ø­Ø§ØµÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬Ø¨Ú©Û Ø§ÙˆÙ¾Ù† Ø³ÙˆØ±Ø³ Ø±Ø³Ø§Ø¦ÛŒ ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ ÛŒÛ Ø¸Ø§ÛØ± Ú©Ø±ØªÛ’ ÛÛŒÚº Ú©Û Ø§ÙˆÙ¾Ù† Ù…Ø§ÚˆÙ„Ø² Ù…Ù„Ú©ÛŒØªÛŒ Ù…ØªØ¨Ø§Ø¯Ù„Ø§Øª Ø³Û’ Ù…ÛŒÙ„ Ú©Ú¾Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

### Ø®ØµÙˆØµÛŒ ØµÙ„Ø§Ø­ÛŒØªÛŒÚº

Ù‚ÛŒÙˆÛŒÙ†-Ú©ÙˆÚˆØ± Ø§ÙˆØ± Ù‚ÛŒÙˆÛŒÙ†-Ù…ÛŒÙ¹Ú¾ Ø¬ÛŒØ³Û’ ÚˆÙˆÙ…ÛŒÙ† Ù…Ø®ØµÙˆØµ ÙˆØ±Ú˜Ù†Ø² Ø¹Ù…ÙˆÙ…ÛŒ Ø²Ø¨Ø§Ù† Ú©ÛŒ ØªÙÛÛŒÙ… Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Ùˆ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø±Ú©Ú¾ØªÛ’ ÛÙˆØ¦Û’ Ø®ØµÙˆØµÛŒ Ù…ÛØ§Ø±Øª ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

## Ø¹Ù…Ù„ÛŒ Ù…Ø«Ø§Ù„ÛŒÚº Ø§ÙˆØ± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ù…Ø¹Ø§Ù…Ù„Ø§Øª

ØªÚ©Ù†ÛŒÚ©ÛŒ ØªÙØµÛŒÙ„Ø§Øª Ù…ÛŒÚº Ø¬Ø§Ù†Û’ Ø³Û’ Ù¾ÛÙ„Û’ØŒ Ø¢Ø¦ÛŒÛ’ Ú©Ú†Ú¾ Ù¹Ú¾ÙˆØ³ Ù…Ø«Ø§Ù„ÙˆÚº Ú©Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±ÛŒÚº Ú©Û Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ú©ÛŒØ§ Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

### Ø±ÛŒØ§Ø¶ÛŒØ§ØªÛŒ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©ÛŒ Ù…Ø«Ø§Ù„

Ù‚ÛŒÙˆÛŒÙ†-Ù…ÛŒÙ¹Ú¾ Ù…Ø±Ø­Ù„Û ÙˆØ§Ø± Ø±ÛŒØ§Ø¶ÛŒØ§ØªÛŒ Ù…Ø³Ø¦Ù„Û Ø­Ù„ Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ø¨ÛØªØ±ÛŒÙ† ÛÛ’Û” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ Ø¬Ø¨ Ù¾ÛŒÚ†ÛŒØ¯Û Ú©ÛŒÙ„Ú©ÙˆÙ„Ø³ Ù…Ø³Ø¦Ù„Û Ø­Ù„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©ÛØ§ Ú¯ÛŒØ§:

```
User: Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = xÂ³ + 2xÂ² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹

Breaking down each term:
- d/dx(xÂ³) = 3xÂ²
- d/dx(2xÂ²) = 2 Â· 2xÂ¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3xÂ² + 4x - 5
```

### Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ Ø³Ù¾ÙˆØ±Ù¹ Ú©ÛŒ Ù…Ø«Ø§Ù„

Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ù…Ø®ØªÙ„Ù Ø²Ø¨Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ù…Ø¶Ø¨ÙˆØ· Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Ø§ Ù…Ø¸Ø§ÛØ±Û Ú©Ø±ØªÛ’ ÛÛŒÚº:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**ä¸­æ–‡:**
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡æœºå™¨ï¼ˆç‰¹åˆ«æ˜¯è®¡ç®—æœºç³»ç»Ÿï¼‰å¯¹äººç±»æ™ºèƒ½è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚è¿™äº›è¿‡ç¨‹åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€è§£å†³é—®é¢˜ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚äººå·¥æ™ºèƒ½ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»è®¤çŸ¥èƒ½åŠ›çš„ä»»åŠ¡ã€‚
```

### Ù…Ù„Ù¹ÛŒ Ù…ÙˆÚˆÙ„ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©ÛŒ Ù…Ø«Ø§Ù„

Ù‚ÛŒÙˆÛŒÙ†-VL Ù…ØªÙ† Ø§ÙˆØ± ØªØµØ§ÙˆÛŒØ± Ø¯ÙˆÙ†ÙˆÚº Ú©Ùˆ Ø¨ÛŒÚ© ÙˆÙ‚Øª Ù¾Ø±ÙˆØ³ÛŒØ³ Ú©Ø± Ø³Ú©ØªØ§ ÛÛ’:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Ú©ÙˆÚˆ Ø¬Ù†Ø±ÛŒØ´Ù† Ú©ÛŒ Ù…Ø«Ø§Ù„

Ù‚ÛŒÙˆÛŒÙ†-Ú©ÙˆÚˆØ± Ù…ØªØ¹Ø¯Ø¯ Ù¾Ø±ÙˆÚ¯Ø±Ø§Ù…Ù†Ú¯ Ø²Ø¨Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©ÙˆÚˆ Ø¬Ù†Ø±ÛŒÙ¹ Ú©Ø±Ù†Û’ Ø§ÙˆØ± ÙˆØ¶Ø§Ø­Øª Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ø¨ÛØªØ±ÛŒÙ† ÛÛ’:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### Ø§ÛŒØ¬ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©ÛŒ Ù…Ø«Ø§Ù„

Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ù…Ø®ØªÙ„Ù Ø§ÛŒØ¬ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ù¾Ø± Ø¨ÛØªØ± Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù†Ø² Ú©Û’ Ø³Ø§ØªÚ¾ ØªØ¹ÛŒÙ†Ø§Øª Ú©ÛŒÛ’ Ø¬Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚº:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Ù‚ÛŒÙˆÛŒÙ† ÙÛŒÙ…Ù„ÛŒ Ú©Ø§ Ø§Ø±ØªÙ‚Ø§Ø¡

### Ù‚ÛŒÙˆÛŒÙ† 1.0 Ø§ÙˆØ± 1.5: Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ù…Ø§ÚˆÙ„Ø²

Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ù†Û’ Ø¬Ø§Ù…Ø¹ ØªØ±Ø¨ÛŒØª Ø§ÙˆØ± Ø§ÙˆÙ¾Ù† Ø³ÙˆØ±Ø³ Ø±Ø³Ø§Ø¦ÛŒ Ú©Û’ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø§ØµÙˆÙ„ÙˆÚº Ú©Ùˆ Ù‚Ø§Ø¦Ù… Ú©ÛŒØ§:

- **Ù‚ÛŒÙˆÛŒÙ†-7B (7B Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø²)**: Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ Ø±ÛŒÙ„ÛŒØ² Ø¬Ùˆ Ú†ÛŒÙ†ÛŒ Ø§ÙˆØ± Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ Ø²Ø¨Ø§Ù† Ú©ÛŒ ØªÙÛÛŒÙ… Ù¾Ø± Ù…Ø±Ú©ÙˆØ² ÛÛ’
- **Ù‚ÛŒÙˆÛŒÙ†-14B (14B Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø²)**: Ø¨ÛØªØ± Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§ÙˆØ± Ø¹Ù„Ù… Ú©Û’ Ø³Ø§ØªÚ¾ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ
- **Ù‚ÛŒÙˆÛŒÙ†-72B (72B Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø²)**: Ø¨Ú‘Û’ Ù¾ÛŒÙ…Ø§Ù†Û’ Ù¾Ø± Ù…Ø§ÚˆÙ„ Ø¬Ùˆ Ø¬Ø¯ÛŒØ¯ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’
- **Ù‚ÛŒÙˆÛŒÙ†1.5 Ø³ÛŒØ±ÛŒØ²**: Ø¨ÛØªØ± Ø·ÙˆÛŒÙ„ Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ ÛÛŒÙ†ÚˆÙ„Ù†Ú¯ Ú©Û’ Ø³Ø§ØªÚ¾ Ù…ØªØ¹Ø¯Ø¯ Ø³Ø§Ø¦Ø² (0.5B Ø³Û’ 110B) ØªÚ© ØªÙˆØ³ÛŒØ¹

### Ù‚ÛŒÙˆÛŒÙ†2 ÙÛŒÙ…Ù„ÛŒ: Ù…Ù„Ù¹ÛŒ Ù…ÙˆÚˆÙ„ ØªÙˆØ³ÛŒØ¹

Ù‚ÛŒÙˆÛŒÙ†2 Ø³ÛŒØ±ÛŒØ² Ù†Û’ Ø²Ø¨Ø§Ù† Ø§ÙˆØ± Ù…Ù„Ù¹ÛŒ Ù…ÙˆÚˆÙ„ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ù…ÛŒÚº Ù†Ù…Ø§ÛŒØ§Úº Ù¾ÛŒØ´ Ø±ÙØª Ú©ÛŒ:

- **Ù‚ÛŒÙˆÛŒÙ†2-0.5B Ø³Û’ 72B**: Ù…Ø®ØªÙ„Ù ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ú©Û’ Ù„ÛŒÛ’ Ø¬Ø§Ù…Ø¹ Ø²Ø¨Ø§Ù† Ù…Ø§ÚˆÙ„Ø² Ú©ÛŒ Ø±ÛŒÙ†Ø¬
- **Ù‚ÛŒÙˆÛŒÙ†2-57B-A14B (MoE)**: Ù…Ø¤Ø«Ø± Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø§ÛØ±ÛŒÙ† Ú©Û’ Ù…Ø±Ú©Ø¨ Ú©ÛŒ Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø±
- **Ù‚ÛŒÙˆÛŒÙ†2-VL**: Ø§Ù…ÛŒØ¬ ØªÙÛÛŒÙ… Ú©Û’ Ù„ÛŒÛ’ Ø¬Ø¯ÛŒØ¯ ÙˆÚ˜Ù†-Ø²Ø¨Ø§Ù† Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÛŒÚº
- **Ù‚ÛŒÙˆÛŒÙ†2-Ø¢ÚˆÛŒÙˆ**: Ø¢ÚˆÛŒÙˆ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ø§ÙˆØ± ØªÙÛÛŒÙ… Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÛŒÚº
- **Ù‚ÛŒÙˆÛŒÙ†2-Ù…ÛŒÙ¹Ú¾**: Ø®ØµÙˆØµÛŒ Ø±ÛŒØ§Ø¶ÛŒØ§ØªÛŒ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§ÙˆØ± Ù…Ø³Ø¦Ù„Û Ø­Ù„ Ú©Ø±Ù†Û’ Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÛŒÚº

### Ù‚ÛŒÙˆÛŒÙ†2.5 ÙÛŒÙ…Ù„ÛŒ: Ø¨ÛØªØ± Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ

Ù‚ÛŒÙˆÛŒÙ†2.5 Ø³ÛŒØ±ÛŒØ² Ù†Û’ ØªÙ…Ø§Ù… Ø¬ÛØªÙˆÚº Ù…ÛŒÚº Ù†Ù…Ø§ÛŒØ§Úº Ø¨ÛØªØ±ÛŒ Ù„Ø§Ø¦ÛŒ:

- **ØªÙˆØ³ÛŒØ¹ Ø´Ø¯Û ØªØ±Ø¨ÛŒØª**: 18 Ú©Ú¾Ø±Ø¨ Ù¹ÙˆÚ©Ù†Ø² Ú©Û’ ØªØ±Ø¨ÛŒØªÛŒ ÚˆÛŒÙ¹Ø§ Ú©Û’ Ø³Ø§ØªÚ¾ Ø¨ÛØªØ± ØµÙ„Ø§Ø­ÛŒØªÛŒÚº
- **ÙˆØ³ÛŒØ¹ Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚**: 128K Ù¹ÙˆÚ©Ù†Ø² Ø³ÛŒØ§Ù‚ Ú©ÛŒ Ù„Ù…Ø¨Ø§Ø¦ÛŒØŒ Ù¹Ø±Ø¨Ùˆ ÙˆØ±Ú˜Ù† 1M Ù¹ÙˆÚ©Ù†Ø² Ú©Ùˆ Ø³Ù¾ÙˆØ±Ù¹ Ú©Ø±ØªØ§ ÛÛ’
- **Ø¨ÛØªØ± Ø®ØµÙˆØµÛŒØª**: Ø¨ÛØªØ± Ù‚ÛŒÙˆÛŒÙ†2.5-Ú©ÙˆÚˆØ± Ø§ÙˆØ± Ù‚ÛŒÙˆÛŒÙ†2.5-Ù…ÛŒÙ¹Ú¾ ÙˆØ±Ú˜Ù†Ø²
- **Ø¨ÛØªØ± Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ Ø³Ù¾ÙˆØ±Ù¹**: 27+ Ø²Ø¨Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ø¨ÛØªØ± Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ

### Ù‚ÛŒÙˆÛŒÙ†3 ÙÛŒÙ…Ù„ÛŒ: Ø¬Ø¯ÛŒØ¯ Ø§Ø³ØªØ¯Ù„Ø§Ù„

ØªØ§Ø²Û ØªØ±ÛŒÙ† Ù†Ø³Ù„ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§ÙˆØ± Ø³ÙˆÚ†Ù†Û’ Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©ÛŒ Ø­Ø¯ÙˆØ¯ Ú©Ùˆ Ø¢Ú¯Û’ Ø¨Ú‘Ú¾Ø§ØªÛŒ ÛÛ’:

- **Ù‚ÛŒÙˆÛŒÙ†3-235B-A22B**: 235B Ú©Ù„ Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø² Ú©Û’ Ø³Ø§ØªÚ¾ ÙÙ„ÛŒÚ¯ Ø´Ù¾ Ù…Ø§ÛØ±ÛŒÙ† Ú©Û’ Ù…Ø±Ú©Ø¨ Ù…Ø§ÚˆÙ„
- **Ù‚ÛŒÙˆÛŒÙ†3-30B-A3B**: Ù…Ø¤Ø«Ø± MoE Ù…Ø§ÚˆÙ„ Ø¬Ùˆ ÙØ¹Ø§Ù„ Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø² Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ Ù…Ø¶Ø¨ÙˆØ· Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’
- **ÚˆÛŒÙ†Ø³ Ù…Ø§ÚˆÙ„Ø²**: Ù…Ø®ØªÙ„Ù ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ù‚ÛŒÙˆÛŒÙ†3-32BØŒ 14BØŒ 8BØŒ 4BØŒ 1.7BØŒ 0.6B
- **Ø³ÙˆÚ†Ù†Û’ Ú©Ø§ Ù…ÙˆÚˆ**: ÙÙˆØ±ÛŒ Ø¬ÙˆØ§Ø¨Ø§Øª Ø§ÙˆØ± Ú¯ÛØ±ÛŒ Ø³ÙˆÚ† Ø¯ÙˆÙ†ÙˆÚº Ú©Ùˆ Ø³Ù¾ÙˆØ±Ù¹ Ú©Ø±Ù†Û’ ÙˆØ§Ù„Ø§ ÛØ§Ø¦Ø¨Ø±Úˆ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©Ø§ Ø·Ø±ÛŒÙ‚Û
- **Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ Ø¹Ù…Ø¯Ú¯ÛŒ**: 119 Ø²Ø¨Ø§Ù†ÙˆÚº Ø§ÙˆØ± Ø¨ÙˆÙ„ÛŒÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø³Ù¾ÙˆØ±Ù¹
- **Ø¨ÛØªØ± ØªØ±Ø¨ÛŒØª**: 36 Ú©Ú¾Ø±Ø¨ Ù¹ÙˆÚ©Ù†Ø² Ú©Û’ Ù…ØªÙ†ÙˆØ¹ØŒ Ø§Ø¹Ù„ÛŒÙ° Ù…Ø¹ÛŒØ§Ø± Ú©Û’ ØªØ±Ø¨ÛŒØªÛŒ ÚˆÛŒÙ¹Ø§

## Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ø§Ø·Ù„Ø§Ù‚Ø§Øª

### Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ø§Ø·Ù„Ø§Ù‚Ø§Øª

ØªÙ†Ø¸ÛŒÙ…ÛŒÚº Ù‚ÛŒÙˆÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ú©Ùˆ Ø¯Ø³ØªØ§ÙˆÛŒØ² ØªØ¬Ø²ÛŒÛØŒ Ú©Ø³Ù¹Ù…Ø± Ø³Ø±ÙˆØ³ Ø¢Ù¹ÙˆÙ…ÛŒØ´Ù†ØŒ Ú©ÙˆÚˆ Ø¬Ù†Ø±ÛŒØ´Ù† Ø§Ø³Ø³Ù¹Ù†Ø³ØŒ Ø§ÙˆØ± Ø¨Ø²Ù†Ø³ Ø§Ù†
ÛŒÛØ§Úº ÛŒÛ Ø¨ØªØ§ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’ Ú©Û Hugging Face Transformers Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Qwen Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ø³Ø§ØªÚ¾ Ú©ÛŒØ³Û’ Ø´Ø±ÙˆØ¹Ø§Øª Ú©Ø±ÛŒÚº:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Qwen2.5 Ù…Ø§ÚˆÙ„Ø² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### Ø®ØµÙˆØµÛŒ Ù…Ø§ÚˆÙ„Ø² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„

**Qwen-Coder Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ú©ÙˆÚˆ Ø¬Ù†Ø±ÛŒØ´Ù†:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**Ø±ÛŒØ§Ø¶ÛŒ Ú©Û’ Ù…Ø³Ø§Ø¦Ù„ Ø­Ù„ Ú©Ø±Ù†Ø§:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**ÙˆÛŒÚ˜Ù†-Ù„ÛŒÙ†Ú¯ÙˆÛŒØ¬ Ù¹Ø§Ø³Ú©Ø³:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### ØªÚ¾Ù†Ú©Ù†Ú¯ Ù…ÙˆÚˆ (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### ğŸ“± Ù…ÙˆØ¨Ø§Ø¦Ù„ Ø§ÙˆØ± Ø§ÛŒØ¬ ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### API ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ú©ÛŒ Ù…Ø«Ø§Ù„

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ Ø¨ÛŒÙ†Ú† Ù…Ø§Ø±Ú©Ø³ Ø§ÙˆØ± Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒØ§Úº

Qwen Ù…Ø§ÚˆÙ„ ÙÛŒÙ…Ù„ÛŒ Ù†Û’ Ù…Ø®ØªÙ„Ù Ø¨ÛŒÙ†Ú† Ù…Ø§Ø±Ú©Ø³ Ù¾Ø± Ø´Ø§Ù†Ø¯Ø§Ø± Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ø­Ø§ØµÙ„ Ú©ÛŒ ÛÛ’ØŒ Ø¬Ø¨Ú©Û Ø§ÙˆÙ¾Ù† Ø³ÙˆØ±Ø³ Ø±Ø³Ø§Ø¦ÛŒ Ú©Ùˆ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø±Ú©Ú¾Ø§ ÛÛ’:

### Ø§ÛÙ… Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©ÛŒ Ø¬Ú¾Ù„Ú©ÛŒØ§Úº

**Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…ÛŒÚº Ù…ÛØ§Ø±Øª:**
- Qwen3-235B-A22B Ú©ÙˆÚˆÙ†Ú¯ØŒ Ø±ÛŒØ§Ø¶ÛŒØŒ Ø§ÙˆØ± Ø¹Ù…ÙˆÙ…ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Û’ Ø¨ÛŒÙ†Ú† Ù…Ø§Ø±Ú© Ù…ÛŒÚº Ø¯ÛŒÚ¯Ø± Ø§Ø¹Ù„ÛŒÙ° Ù…Ø§ÚˆÙ„Ø² Ø¬ÛŒØ³Û’ DeepSeek-R1ØŒ o1ØŒ o3-miniØŒ Grok-3ØŒ Ø§ÙˆØ± Gemini-2.5-Pro Ú©Û’ Ù…Ù‚Ø§Ø¨Ù„Û’ Ù…ÛŒÚº Ù…Ø³Ø§Ø¨Ù‚ØªÛŒ Ù†ØªØ§Ø¦Ø¬ Ø­Ø§ØµÙ„ Ú©Ø±ØªØ§ ÛÛ’Û”
- Qwen3-30B-A3B Ù†Û’ QwQ-32B Ú©Ùˆ 10 Ú¯Ù†Ø§ Ø²ÛŒØ§Ø¯Û ÙØ¹Ø§Ù„ Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø² Ú©Û’ Ø³Ø§ØªÚ¾ Ù¾ÛŒÚ†Ú¾Û’ Ú†Ú¾ÙˆÚ‘ Ø¯ÛŒØ§Û”
- Qwen3-4B Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Qwen2.5-72B-Instruct Ú©Û’ Ø¨Ø±Ø§Ø¨Ø± ÛÙˆ Ø³Ú©ØªÛŒ ÛÛ’Û”

**Ù…ÙˆØ«Ø±ÛŒØª Ú©ÛŒ Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒØ§Úº:**
- Qwen3-MoE Ø¨ÛŒØ³ Ù…Ø§ÚˆÙ„Ø² Ù†Û’ Qwen2.5 Ú©Û’ ÚˆÛŒÙ†Ø³ Ø¨ÛŒØ³ Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ø¨Ø±Ø§Ø¨Ø± Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ø­Ø§ØµÙ„ Ú©ÛŒØŒ Ø¬Ø¨Ú©Û ØµØ±Ù 10% ÙØ¹Ø§Ù„ Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø² Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒÛ’Û”
- ÚˆÛŒÙ†Ø³ Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ù…Ù‚Ø§Ø¨Ù„Û’ Ù…ÛŒÚº ØªØ±Ø¨ÛŒØª Ø§ÙˆØ± Ø§Ù†ÙØ±Ù†Ø³ Ù…ÛŒÚº Ù†Ù…Ø§ÛŒØ§Úº Ù„Ø§Ú¯Øª Ú©ÛŒ Ø¨Ú†ØªÛ”

**Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ ØµÙ„Ø§Ø­ÛŒØªÛŒÚº:**
- Qwen3 Ù…Ø§ÚˆÙ„Ø² 119 Ø²Ø¨Ø§Ù†ÙˆÚº Ø§ÙˆØ± Ø¨ÙˆÙ„ÛŒÙˆÚº Ú©ÛŒ Ø­Ù…Ø§ÛŒØª Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”
- Ù…Ø®ØªÙ„Ù Ù„Ø³Ø§Ù†ÛŒ Ø§ÙˆØ± Ø«Ù‚Ø§ÙØªÛŒ Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ù…ÛŒÚº Ù…Ø¶Ø¨ÙˆØ· Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒÛ”

**ØªØ±Ø¨ÛŒØªÛŒ Ù¾ÛŒÙ…Ø§Ù†Û:**
- Qwen3 ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ 36 Ù¹Ø±ÛŒÙ„ÛŒÙ† Ù¹ÙˆÚ©Ù†Ø² Ú©Û’ Ø³Ø§ØªÚ¾ 119 Ø²Ø¨Ø§Ù†ÙˆÚº Ø§ÙˆØ± Ø¨ÙˆÙ„ÛŒÙˆÚº Ú©Ø§ Ø§Ø­Ø§Ø·Û Ú©Ø±ØªØ§ ÛÛ’ØŒ Ø¬Ùˆ Qwen2.5 Ú©Û’ 18 Ù¹Ø±ÛŒÙ„ÛŒÙ† Ù¹ÙˆÚ©Ù†Ø² Ø³Û’ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ø¯ÙˆÚ¯Ù†Ø§ ÛÛ’Û”

### Ù…Ø§ÚˆÙ„ Ù…ÙˆØ§Ø²Ù†Û Ù…ÛŒÙ¹Ø±Ú©Ø³

| Ù…Ø§ÚˆÙ„ Ø³ÛŒØ±ÛŒØ² | Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø² Ú©ÛŒ Ø­Ø¯ | Ú©Ø§Ù†Ù¹ÛŒÚ©Ø³Ù¹ Ú©ÛŒ Ù„Ù…Ø¨Ø§Ø¦ÛŒ | Ø§ÛÙ… Ø·Ø§Ù‚ØªÛŒÚº | Ø¨ÛØªØ±ÛŒÙ† Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ú©ÛŒØ³Ø² |
|--------------|------------------|----------------|---------------|----------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | Ù…ØªÙˆØ§Ø²Ù† Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒØŒ Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ | Ø¹Ù…ÙˆÙ…ÛŒ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø²ØŒ Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | Ú©ÙˆÚˆ Ø¬Ù†Ø±ÛŒØ´Ù†ØŒ Ù¾Ø±ÙˆÚ¯Ø±Ø§Ù…Ù†Ú¯ | Ø³Ø§ÙÙ¹ ÙˆÛŒØ¦Ø± ÚˆÛŒÙˆÙ„Ù¾Ù…Ù†Ù¹ØŒ Ú©ÙˆÚˆÙ†Ú¯ Ø§Ø³Ø³Ù¹Ù†Ø³ |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | Ø±ÛŒØ§Ø¶ÛŒØ§ØªÛŒ Ø§Ø³ØªØ¯Ù„Ø§Ù„ | ØªØ¹Ù„ÛŒÙ…ÛŒ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù…Ø²ØŒ STEM Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² |
| **Qwen2.5-VL** | Ù…Ø®ØªÙ„Ù | Ù…ØªØºÛŒØ± | ÙˆÛŒÚ˜Ù†-Ù„ÛŒÙ†Ú¯ÙˆÛŒØ¬ Ø³Ù…Ø¬Ú¾Ø¯Ø§Ø±ÛŒ | Ù…Ù„Ù¹ÛŒ Ù…ÙˆÚˆÙ„ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø²ØŒ Ø§Ù…ÛŒØ¬ ØªØ¬Ø²ÛŒÛ |
| **Qwen3** | 0.6B-235B | Ù…ØªØºÛŒØ± | Ø§Ø¹Ù„ÛŒÙ° Ø§Ø³ØªØ¯Ù„Ø§Ù„ØŒ ØªÚ¾Ù†Ú©Ù†Ú¯ Ù…ÙˆÚˆ | Ù¾ÛŒÚ†ÛŒØ¯Û Ø§Ø³ØªØ¯Ù„Ø§Ù„ØŒ ØªØ­Ù‚ÛŒÙ‚ÛŒ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² |
| **Qwen3 MoE** | 30B-235B Ú©Ù„ | Ù…ØªØºÛŒØ± | Ù…ÙˆØ«Ø± Ø¨Ú‘Û’ Ù¾ÛŒÙ…Ø§Ù†Û’ Ù¾Ø± Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ | Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø²ØŒ Ø§Ø¹Ù„ÛŒÙ° Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©ÛŒ Ø¶Ø±ÙˆØ±ÛŒØ§Øª |

## Ù…Ø§ÚˆÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ú¯Ø§Ø¦ÛŒÚˆ

### Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’
- **Qwen2.5-0.5B/1.5B**: Ù…ÙˆØ¨Ø§Ø¦Ù„ Ø§ÛŒÙ¾Ø³ØŒ Ø§ÛŒØ¬ ÚˆÛŒÙˆØ§Ø¦Ø³Ø²ØŒ Ø±ÛŒØ¦Ù„ Ù¹Ø§Ø¦Ù… Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø²
- **Qwen2.5-3B/7B**: Ø¹Ù…ÙˆÙ…ÛŒ Ú†ÛŒÙ¹ Ø¨ÙˆÙ¹Ø³ØŒ Ù…ÙˆØ§Ø¯ Ú©ÛŒ ØªØ®Ù„ÛŒÙ‚ØŒ Ø³ÙˆØ§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨ Ú©Û’ Ù†Ø¸Ø§Ù…

### Ø±ÛŒØ§Ø¶ÛŒØ§ØªÛŒ Ø§ÙˆØ± Ø§Ø³ØªØ¯Ù„Ø§Ù„ÛŒ Ù¹Ø§Ø³Ú©Ø³ Ú©Û’ Ù„ÛŒÛ’
- **Qwen2.5-Math**: Ø±ÛŒØ§Ø¶ÛŒØ§ØªÛŒ Ù…Ø³Ø§Ø¦Ù„ Ø­Ù„ Ú©Ø±Ù†Ø§ Ø§ÙˆØ± STEM ØªØ¹Ù„ÛŒÙ…
- **Qwen3 ØªÚ¾Ù†Ú©Ù†Ú¯ Ù…ÙˆÚˆ Ú©Û’ Ø³Ø§ØªÚ¾**: Ù¾ÛŒÚ†ÛŒØ¯Û Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¬Ø³ Ù…ÛŒÚº Ù…Ø±Ø­Ù„Û ÙˆØ§Ø± ØªØ¬Ø²ÛŒÛ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆ

### Ù¾Ø±ÙˆÚ¯Ø±Ø§Ù…Ù†Ú¯ Ø§ÙˆØ± ÚˆÛŒÙˆÙ„Ù¾Ù…Ù†Ù¹ Ú©Û’ Ù„ÛŒÛ’
- **Qwen2.5-Coder**: Ú©ÙˆÚˆ Ø¬Ù†Ø±ÛŒØ´Ù†ØŒ ÚˆÛŒØ¨Ú¯Ù†Ú¯ØŒ Ù¾Ø±ÙˆÚ¯Ø±Ø§Ù…Ù†Ú¯ Ø§Ø³Ø³Ù¹Ù†Ø³
- **Qwen3**: Ø§Ø³ØªØ¯Ù„Ø§Ù„ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Û’ Ø³Ø§ØªÚ¾ Ø§Ø¹Ù„ÛŒÙ° Ù¾Ø±ÙˆÚ¯Ø±Ø§Ù…Ù†Ú¯ Ù¹Ø§Ø³Ú©Ø³

### Ù…Ù„Ù¹ÛŒ Ù…ÙˆÚˆÙ„ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’
- **Qwen2.5-VL**: Ø§Ù…ÛŒØ¬ Ø³Ù…Ø¬Ú¾Ø¯Ø§Ø±ÛŒØŒ Ø¨ØµØ±ÛŒ Ø³ÙˆØ§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨
- **Qwen-Audio**: Ø¢ÚˆÛŒÙˆ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ø§ÙˆØ± ØªÙ‚Ø±ÛŒØ± Ú©ÛŒ Ø³Ù…Ø¬Ú¾Ø¯Ø§Ø±ÛŒ

### Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ú©Û’ Ù„ÛŒÛ’
- **Qwen2.5-32B/72B**: Ø§Ø¹Ù„ÛŒÙ° Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©ÛŒ Ø²Ø¨Ø§Ù† Ø³Ù…Ø¬Ú¾Ø¯Ø§Ø±ÛŒ
- **Qwen3-235B-A22B**: Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û ØµÙ„Ø§Ø­ÛŒØª Ú©Û’ Ù„ÛŒÛ’ Ø³Ø®Øª Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø²

## ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù…Ø² Ø§ÙˆØ± Ø±Ø³Ø§Ø¦ÛŒ

### Ú©Ù„Ø§Ø¤Úˆ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù…Ø²
- **Hugging Face Hub**: Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ Ø³Ù¾ÙˆØ±Ù¹ Ú©Û’ Ø³Ø§ØªÚ¾ Ø¬Ø§Ù…Ø¹ Ù…Ø§ÚˆÙ„ Ø±ÛŒÙ¾ÙˆØ²Ù¹Ø±ÛŒ
- **ModelScope**: Ø¹Ù„ÛŒ Ø¨Ø§Ø¨Ø§ Ú©Ø§ Ù…Ø§ÚˆÙ„ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù… Ø¬Ø³ Ù…ÛŒÚº Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù† Ù¹ÙˆÙ„Ø² Ø´Ø§Ù…Ù„ ÛÛŒÚº
- **Ù…Ø®ØªÙ„Ù Ú©Ù„Ø§Ø¤Úˆ ÙØ±Ø§ÛÙ… Ú©Ù†Ù†Ø¯Ú¯Ø§Ù†**: Ù…Ø¹ÛŒØ§Ø±ÛŒ ML Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù…Ø² Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø³Ù¾ÙˆØ±Ù¹

### Ù„ÙˆÚ©Ù„ ÚˆÛŒÙˆÙ„Ù¾Ù…Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú©Ø³
- **Transformers**: Ø¢Ø³Ø§Ù† ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø¹ÛŒØ§Ø±ÛŒ Hugging Face Ø§Ù†Ù¹ÛŒÚ¯Ø±ÛŒØ´Ù†
- **vLLM**: Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† Ù…Ø§Ø­ÙˆÙ„ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø¹Ù„ÛŒÙ° Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©ÛŒ Ø³Ø±ÙˆØ³Ù†Ú¯
- **Ollama**: Ù„ÙˆÚ©Ù„ ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ø§ÙˆØ± Ù…ÛŒÙ†Ø¬Ù…Ù†Ù¹ Ú©Ùˆ Ø¢Ø³Ø§Ù† Ø¨Ù†Ø§ØªØ§ ÛÛ’
- **ONNX Runtime**: Ù…Ø®ØªÙ„Ù ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ú©Û’ Ù„ÛŒÛ’ Ú©Ø±Ø§Ø³ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù… Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†
- **llama.cpp**: Ù…Ø®ØªÙ„Ù Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù…Ø² Ú©Û’ Ù„ÛŒÛ’ Ù…ÙˆØ«Ø± C++ Ø§Ù…Ù¾Ù„ÛŒÙ…Ù†Ù¹ÛŒØ´Ù†

### Ø³ÛŒÚ©Ú¾Ù†Û’ Ú©Û’ ÙˆØ³Ø§Ø¦Ù„
- **Qwen Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª**: Ø¢ÙÛŒØ´Ù„ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ø§ÙˆØ± Ù…Ø§ÚˆÙ„ Ú©Ø§Ø±ÚˆØ²
- **Hugging Face Model Hub**: Ø§Ù†Ù¹Ø±Ø§ÛŒÚ©Ù¹Ùˆ ÚˆÛŒÙ…ÙˆØ² Ø§ÙˆØ± Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ Ú©ÛŒ Ù…Ø«Ø§Ù„ÛŒÚº
- **Ø±ÛŒØ³Ø±Ú† Ù¾ÛŒÙ¾Ø±Ø²**: arxiv Ù¾Ø± ØªÚ©Ù†ÛŒÚ©ÛŒ Ù¾ÛŒÙ¾Ø±Ø² Ú¯ÛØ±Ø§Ø¦ÛŒ Ø³Û’ Ø³Ù…Ø¬Ú¾Ù†Û’ Ú©Û’ Ù„ÛŒÛ’
- **Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ ÙÙˆØ±Ù…Ø²**: ÙØ¹Ø§Ù„ Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ Ø³Ù¾ÙˆØ±Ù¹ Ø§ÙˆØ± Ù…Ø¨Ø§Ø­Ø«Û’

### Qwen Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ø³Ø§ØªÚ¾ Ø´Ø±ÙˆØ¹Ø§Øª

#### ÚˆÛŒÙˆÙ„Ù¾Ù…Ù†Ù¹ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù…Ø²
1. **Hugging Face Transformers**: Ù…Ø¹ÛŒØ§Ø±ÛŒ Python Ø§Ù†Ù¹ÛŒÚ¯Ø±ÛŒØ´Ù† Ú©Û’ Ø³Ø§ØªÚ¾ Ø´Ø±ÙˆØ¹Ø§Øª Ú©Ø±ÛŒÚº
2. **ModelScope**: Ø¹Ù„ÛŒ Ø¨Ø§Ø¨Ø§ Ú©Û’ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²Úˆ ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ù¹ÙˆÙ„Ø² Ú©Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±ÛŒÚº
3. **Ù„ÙˆÚ©Ù„ ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹**: Ollama ÛŒØ§ ÚˆØ§Ø¦Ø±ÛŒÚ©Ù¹ Transformers Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ù„ÙˆÚ©Ù„ Ù¹ÛŒØ³Ù¹Ù†Ú¯ Ú©Û’ Ù„ÛŒÛ’

#### Ø³ÛŒÚ©Ú¾Ù†Û’ Ú©Ø§ Ø±Ø§Ø³ØªÛ
1. **Ø¨Ù†ÛŒØ§Ø¯ÛŒ ØªØµÙˆØ±Ø§Øª Ú©Ùˆ Ø³Ù…Ø¬Ú¾ÛŒÚº**: Qwen ÙÛŒÙ…Ù„ÛŒ Ú©ÛŒ Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø± Ø§ÙˆØ± ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Ø§ Ù…Ø·Ø§Ù„Ø¹Û Ú©Ø±ÛŒÚº
2. **ÙˆÛŒØ±ÛŒØ¦Ù†Ù¹Ø³ Ú©Û’ Ø³Ø§ØªÚ¾ ØªØ¬Ø±Ø¨Û Ú©Ø±ÛŒÚº**: Ù…Ø®ØªÙ„Ù Ù…Ø§ÚˆÙ„ Ø³Ø§Ø¦Ø² Ø¢Ø²Ù…Ø§Ø¦ÛŒÚº ØªØ§Ú©Û Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ ÙØ±Ù‚ Ú©Ùˆ Ø³Ù…Ø¬Ú¾ Ø³Ú©ÛŒÚº
3. **Ø¹Ù…Ù„ÛŒ Ù†ÙØ§Ø° Ú©Ø±ÛŒÚº**: ÚˆÛŒÙˆÙ„Ù¾Ù…Ù†Ù¹ Ù…Ø§Ø­ÙˆÙ„ Ù…ÛŒÚº Ù…Ø§ÚˆÙ„Ø² Ú©Ùˆ ÚˆÙ¾Ù„Ø§Ø¦Û’ Ú©Ø±ÛŒÚº
4. **ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ú©Ùˆ Ø¨ÛØªØ± Ø¨Ù†Ø§Ø¦ÛŒÚº**: Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† Ú©Û’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ú©ÛŒØ³Ø² Ú©Û’ Ù„ÛŒÛ’ ÙØ§Ø¦Ù† Ù¹ÛŒÙˆÙ† Ú©Ø±ÛŒÚº

#### Ø¨ÛØªØ±ÛŒÙ† Ø·Ø±ÛŒÙ‚Û’
- **Ú†Ú¾ÙˆÙ¹Û’ Ø³Û’ Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº**: Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ ÚˆÛŒÙˆÙ„Ù¾Ù…Ù†Ù¹ Ú©Û’ Ù„ÛŒÛ’ Ú†Ú¾ÙˆÙ¹Û’ Ù…Ø§ÚˆÙ„Ø² (1.5B-7B) Ø³Û’ Ø¢ØºØ§Ø² Ú©Ø±ÛŒÚº
- **Ú†ÛŒÙ¹ Ù¹ÛŒÙ…Ù¾Ù„ÛŒÙ¹Ø³ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº**: Ø¨ÛØªØ±ÛŒÙ† Ù†ØªØ§Ø¦Ø¬ Ú©Û’ Ù„ÛŒÛ’ Ù…Ù†Ø§Ø³Ø¨ ÙØ§Ø±Ù…ÛŒÙ¹Ù†Ú¯ Ø§Ù¾Ù„Ø§Ø¦ÛŒ Ú©Ø±ÛŒÚº
- **ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ù†Ú¯Ø±Ø§Ù†ÛŒ Ú©Ø±ÛŒÚº**: Ù…ÛŒÙ…ÙˆØ±ÛŒ Ú©Û’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ø§ÙˆØ± Ø§Ù†ÙØ±Ù†Ø³ Ú©ÛŒ Ø±ÙØªØ§Ø± Ú©Ùˆ Ù¹Ø±ÛŒÚ© Ú©Ø±ÛŒÚº
- **Ø®ØµÙˆØµÛŒØª Ù¾Ø± ØºÙˆØ± Ú©Ø±ÛŒÚº**: Ø¬Ø¨ Ù…Ù†Ø§Ø³Ø¨ ÛÙˆ ØªÙˆ ÚˆÙˆÙ…ÛŒÙ† Ù…Ø®ØµÙˆØµ ÙˆÛŒØ±ÛŒØ¦Ù†Ù¹Ø³ Ú©Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±ÛŒÚº

## Ø§Ø¹Ù„ÛŒÙ° Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ù¾ÛŒÙ¹Ø±Ù†Ø²

### ÙØ§Ø¦Ù† Ù¹ÛŒÙˆÙ†Ù†Ú¯ Ú©ÛŒ Ù…Ø«Ø§Ù„ÛŒÚº

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### Ø®ØµÙˆØµÛŒ Ù¾Ø±Ø§Ù…Ù¾Ù¹ Ø§Ù†Ø¬ÛŒÙ†Ø¦Ø±Ù†Ú¯

**Ù¾ÛŒÚ†ÛŒØ¯Û Ø§Ø³ØªØ¯Ù„Ø§Ù„ÛŒ Ù¹Ø§Ø³Ú©Ø³ Ú©Û’ Ù„ÛŒÛ’:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**Ú©Ø§Ù†Ù¹ÛŒÚ©Ø³Ù¹ Ú©Û’ Ø³Ø§ØªÚ¾ Ú©ÙˆÚˆ Ø¬Ù†Ø±ÛŒØ´Ù† Ú©Û’ Ù„ÛŒÛ’:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø²

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ä¸­æ–‡)",
        "es": "Spanish (EspaÃ±ol)",
        "fr": "French (FranÃ§ais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (æ—¥æœ¬èª)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ğŸ”§ Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ù¾ÛŒÙ¹Ø±Ù†Ø²

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©ÛŒ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù† Ú©ÛŒ Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒ

### Ù…ÛŒÙ…ÙˆØ±ÛŒ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### Ø§Ù†ÙØ±Ù†Ø³ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## Ø¨ÛØªØ±ÛŒÙ† Ø·Ø±ÛŒÙ‚Û’ Ø§ÙˆØ± Ø±ÛÙ†Ù…Ø§ Ø§ØµÙˆÙ„

### Ø³ÛŒÚ©ÛŒÙˆØ±Ù¹ÛŒ Ø§ÙˆØ± Ù¾Ø±Ø§Ø¦ÛŒÙˆÛŒØ³ÛŒ

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### Ù†Ú¯Ø±Ø§Ù†ÛŒ Ø§ÙˆØ± ØªØ´Ø®ÛŒØµ

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## Ù†ØªÛŒØ¬Û

Qwen Ù…Ø§ÚˆÙ„ ÙÛŒÙ…Ù„ÛŒ Ø§ÛŒÚ© Ø¬Ø§Ù…Ø¹ Ø·Ø±ÛŒÙ‚Û Ù¾ÛŒØ´ Ú©Ø±ØªÛŒ ÛÛ’ AI Ù¹ÛŒÚ©Ù†Ø§Ù„ÙˆØ¬ÛŒ Ú©Ùˆ Ø¬Ù…ÛÙˆØ±ÛŒ Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ Ø¬Ø¨Ú©Û Ù…Ø®ØªÙ„Ù Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ù…ÛŒÚº Ù…Ø³Ø§Ø¨Ù‚ØªÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Ùˆ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø±Ú©Ú¾ØªÛŒ ÛÛ’Û” Ø§ÙˆÙ¾Ù† Ø³ÙˆØ±Ø³ Ø±Ø³Ø§Ø¦ÛŒØŒ Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚºØŒ Ø§ÙˆØ± Ù„Ú†Ú©Ø¯Ø§Ø± ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ø¢Ù¾Ø´Ù†Ø² Ú©Û’ Ø°Ø±ÛŒØ¹Û’ØŒ Qwen ØªÙ†Ø¸ÛŒÙ…ÙˆÚº Ø§ÙˆØ± ÚˆÛŒÙˆÙ„Ù¾Ø±Ø² Ú©Ùˆ Ø·Ø§Ù‚ØªÙˆØ± AI ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ø³Û’ ÙØ§Ø¦Ø¯Û Ø§Ù¹Ú¾Ø§Ù†Û’ Ú©ÛŒ Ø§Ø¬Ø§Ø²Øª Ø¯ÛŒØªØ§ ÛÛ’ØŒ Ú†Ø§ÛÛ’ Ø§Ù† Ú©Û’ ÙˆØ³Ø§Ø¦Ù„ ÛŒØ§ Ù…Ø®ØµÙˆØµ Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ú©Ú†Ú¾ Ø¨Ú¾ÛŒ ÛÙˆÚºÛ”

### Ø§ÛÙ… Ù†Ú©Ø§Øª

**Ø§ÙˆÙ¾Ù† Ø³ÙˆØ±Ø³ Ù…ÛØ§Ø±Øª**: Qwen ÛŒÛ Ø¸Ø§ÛØ± Ú©Ø±ØªØ§ ÛÛ’ Ú©Û Ø§ÙˆÙ¾Ù† Ø³ÙˆØ±Ø³ Ù…Ø§ÚˆÙ„Ø² Ù…Ù„Ú©ÛŒØªÛŒ Ù…ØªØ¨Ø§Ø¯Ù„Ø§Øª Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ø³Ø§Ø¨Ù‚ØªÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø¬Ø¨Ú©Û Ø´ÙØ§ÙÛŒØªØŒ Ø­Ø³Ø¨ Ø¶Ø±ÙˆØ±ØªØŒ Ø§ÙˆØ± Ú©Ù†Ù¹Ø±ÙˆÙ„ ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

**Ù‚Ø§Ø¨Ù„ ØªÙˆØ³ÛŒØ¹ Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø±**: 0.5B Ø³Û’ 235B Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø² Ú©ÛŒ Ø­Ø¯ Ù…Ø®ØªÙ„Ù Ú©Ù…Ù¾ÛŒÙˆÙ¹ÛŒØ´Ù†Ù„ Ù…Ø§Ø­ÙˆÙ„ Ù…ÛŒÚº ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ú©Ùˆ Ù…Ù…Ú©Ù† Ø¨Ù†Ø§ØªÛŒ ÛÛ’ØŒ Ù…ÙˆØ¨Ø§Ø¦Ù„ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ø³Û’ Ù„Û’ Ú©Ø± Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ú©Ù„Ø³Ù¹Ø±Ø² ØªÚ©Û”

**Ø®ØµÙˆØµÛŒ ØµÙ„Ø§Ø­ÛŒØªÛŒÚº**: ÚˆÙˆÙ…ÛŒÙ† Ù…Ø®ØµÙˆØµ ÙˆÛŒØ±ÛŒØ¦Ù†Ù¹Ø³ Ø¬ÛŒØ³Û’ Qwen-CoderØŒ Qwen-MathØŒ Ø§ÙˆØ± Qwen-VL Ø¹Ù…ÙˆÙ…ÛŒ Ø²Ø¨Ø§Ù† Ú©ÛŒ Ø³Ù…Ø¬Ú¾Ø¯Ø§Ø±ÛŒ Ú©Ùˆ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø±Ú©Ú¾ØªÛ’ ÛÙˆØ¦Û’ Ø®ØµÙˆØµÛŒ Ù…ÛØ§Ø±Øª ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

**Ø¹Ø§Ù„Ù…ÛŒ Ø±Ø³Ø§Ø¦ÛŒ**: 119+ Ø²Ø¨Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ù…Ø¶Ø¨ÙˆØ· Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ Ø³Ù¾ÙˆØ±Ù¹ Qwen Ú©Ùˆ Ø¨ÛŒÙ† Ø§Ù„Ø§Ù‚ÙˆØ§Ù…ÛŒ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ø§ÙˆØ± Ù…ØªÙ†ÙˆØ¹ ØµØ§Ø±ÙÛŒÙ† Ú©Û’ Ù„ÛŒÛ’ Ù…ÙˆØ²ÙˆÚº Ø¨Ù†Ø§ØªÛŒ ÛÛ’Û”

**Ù…Ø³Ù„Ø³Ù„ Ø¬Ø¯Øª**: Qwen 1.0 Ø³Û’ Qwen3 ØªÚ© Ø§Ø±ØªÙ‚Ø§Ø¡ ØµÙ„Ø§Ø­ÛŒØªÙˆÚºØŒ Ù…ÙˆØ«Ø±ÛŒØªØŒ Ø§ÙˆØ± ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ø¢Ù¾Ø´Ù†Ø² Ù…ÛŒÚº Ù…Ø³Ù„Ø³Ù„ Ø¨ÛØªØ±ÛŒ Ú©Ùˆ Ø¸Ø§ÛØ± Ú©Ø±ØªØ§ ÛÛ’Û”

### Ù…Ø³ØªÙ‚Ø¨Ù„ Ú©Ø§ Ù…Ù†Ø¸Ø±

Ø¬ÛŒØ³Û’ Ø¬ÛŒØ³Û’ Qwen ÙÛŒÙ…Ù„ÛŒ ØªØ±Ù‚ÛŒ Ú©Ø±ØªÛŒ ÛÛ’ØŒ ÛÙ… ØªÙˆÙ‚Ø¹ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

- **Ù…ÙˆØ«Ø±ÛŒØª Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ**: Ø¨ÛØªØ± Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ-Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø± ØªÙ†Ø§Ø³Ø¨ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø³Ù„Ø³Ù„ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†
- **Ù…Ù„Ù¹ÛŒ Ù…ÙˆÚˆÙ„ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ù…ÛŒÚº ØªÙˆØ³ÛŒØ¹**: Ø²ÛŒØ§Ø¯Û Ù†ÙÛŒØ³ ÙˆÛŒÚ˜Ù†ØŒ Ø¢ÚˆÛŒÙˆØŒ Ø§ÙˆØ± Ù¹ÛŒÚ©Ø³Ù¹ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ú©Ø§ Ø§Ù†Ø¶Ù…Ø§Ù…
- **Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…ÛŒÚº Ø¨ÛØªØ±ÛŒ**: Ø§Ø¹Ù„ÛŒÙ° ØªÚ¾Ù†Ú©Ù†Ú¯ Ù…ÛŒÚ©Ø§Ù†Ø²Ù… Ø§ÙˆØ± Ù…Ù„Ù¹ÛŒ-Ø§Ø³Ù¹ÛŒÙ¾ Ù…Ø³Ø¦Ù„Û Ø­Ù„ Ú©Ø±Ù†Û’ Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÛŒÚº
- **Ø¨ÛØªØ± ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ù¹ÙˆÙ„Ø²**: Ù…Ø®ØªÙ„Ù ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ± ÙØ±ÛŒÙ… ÙˆØ±Ú©Ø³ Ø§ÙˆØ± Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù† Ù¹ÙˆÙ„Ø²
- **Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ Ú©ÛŒ ØªØ±Ù‚ÛŒ**: Ù¹ÙˆÙ„Ø²ØŒ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø²ØŒ Ø§ÙˆØ± Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ Ú©ÛŒ Ø´Ø±Ø§Ú©ØªÙˆÚº Ú©Û’ ÙˆØ³ÛŒØ¹ ØªØ± Ù…Ø§Ø­ÙˆÙ„ÛŒØ§ØªÛŒ Ù†Ø¸Ø§Ù…

### Ø§Ú¯Ù„Û’ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª

Ú†Ø§ÛÛ’ Ø¢Ù¾ Ú†ÛŒÙ¹ Ø¨ÙˆÙ¹ Ø¨Ù†Ø§ Ø±ÛÛ’ ÛÙˆÚºØŒ ØªØ¹Ù„ÛŒÙ…ÛŒ Ù¹ÙˆÙ„Ø² ØªÛŒØ§Ø± Ú©Ø± Ø±ÛÛ’ ÛÙˆÚºØŒ Ú©ÙˆÚˆÙ†Ú¯ Ø§Ø³Ø³Ù¹Ù†Ø³ ØªØ®Ù„ÛŒÙ‚ Ú©Ø± Ø±ÛÛ’ ÛÙˆÚºØŒ ÛŒØ§ Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ù¾Ø± Ú©Ø§Ù… Ú©Ø± Ø±ÛÛ’ ÛÙˆÚºØŒ Qwen ÙÛŒÙ…Ù„ÛŒ Ù…Ø¶Ø¨ÙˆØ· Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ Ø³Ù¾ÙˆØ±Ù¹ Ø§ÙˆØ± Ø¬Ø§Ù…Ø¹ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ú©Û’ Ø³Ø§ØªÚ¾ Ù‚Ø§Ø¨Ù„ ØªÙˆØ³ÛŒØ¹ Ø­Ù„ ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛŒ ÛÛ’Û”

ØªØ§Ø²Û ØªØ±ÛŒÙ† Ø§Ù¾ ÚˆÛŒÙ¹Ø³ØŒ Ù…Ø§ÚˆÙ„ Ø±ÛŒÙ„ÛŒØ²Ø²ØŒ Ø§ÙˆØ± ØªÙØµÛŒÙ„ÛŒ ØªÚ©Ù†ÛŒÚ©ÛŒ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ú©Û’ Ù„ÛŒÛ’ØŒ Hugging Face Ù¾Ø± Ø¢ÙÛŒØ´Ù„ Qwen Ø±ÛŒÙ¾ÙˆØ²Ù¹Ø±ÛŒØ² Ù…Ù„Ø§Ø­Ø¸Û Ú©Ø±ÛŒÚº Ø§ÙˆØ± ÙØ¹Ø§Ù„ Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ Ù…Ø¨Ø§Ø­Ø«Û’ Ø§ÙˆØ± Ù…Ø«Ø§Ù„ÙˆÚº Ú©Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±ÛŒÚºÛ”

AI ÚˆÛŒÙˆÙ„Ù¾Ù…Ù†Ù¹ Ú©Ø§ Ù…Ø³ØªÙ‚Ø¨Ù„ Ù‚Ø§Ø¨Ù„ Ø±Ø³Ø§Ø¦ÛŒØŒ Ø´ÙØ§ÙØŒ Ø§ÙˆØ± Ø·Ø§Ù‚ØªÙˆØ± Ù¹ÙˆÙ„Ø² Ù…ÛŒÚº ÛÛ’ Ø¬Ùˆ ØªÙ…Ø§Ù… Ø´Ø¹Ø¨ÙˆÚº Ø§ÙˆØ± Ù¾ÛŒÙ…Ø§Ù†ÙˆÚº Ù¾Ø± Ø¬Ø¯Øª Ú©Ùˆ Ù…Ù…Ú©Ù† Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚºÛ” Qwen ÙÛŒÙ…Ù„ÛŒ Ø§Ø³ ÙˆÚ˜Ù† Ú©ÛŒ Ù…Ø«Ø§Ù„ Ù¾ÛŒØ´ Ú©Ø±ØªÛŒ ÛÛ’ØŒ ØªÙ†Ø¸ÛŒÙ…ÙˆÚº Ø§ÙˆØ± ÚˆÛŒÙˆÙ„Ù¾Ø±Ø² Ú©Ùˆ Ø§Ú¯Ù„ÛŒ Ù†Ø³Ù„ Ú©ÛŒ AI Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„ÛŒ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø¨Ù†ÛŒØ§Ø¯ ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛŒ ÛÛ’Û”

## Ø§Ø¶Ø§ÙÛŒ ÙˆØ³Ø§Ø¦Ù„

- **Ø¢ÙÛŒØ´Ù„ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª**: [Qwen Documentation](https://qwen.readthedocs.io/)
- **Ù…Ø§ÚˆÙ„ ÛØ¨**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **ØªÚ©Ù†ÛŒÚ©ÛŒ Ù¾ÛŒÙ¾Ø±Ø²**: [Qwen Research Publications](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ**: [GitHub Discussions and Issues](https://github.com/QwenLM/)
- **ModelScope Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù…**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## Ø³ÛŒÚ©Ú¾Ù†Û’ Ú©Û’ Ù†ØªØ§Ø¦Ø¬

Ø§Ø³ Ù…Ø§ÚˆÛŒÙˆÙ„ Ú©Ùˆ Ù…Ú©Ù…Ù„ Ú©Ø±Ù†Û’ Ú©Û’ Ø¨Ø¹Ø¯ØŒ Ø¢Ù¾ Ù‚Ø§Ø¨Ù„ ÛÙˆÚº Ú¯Û’:

1. Qwen Ù…Ø§ÚˆÙ„ ÙÛŒÙ…Ù„ÛŒ Ú©Û’ Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø±Ù„ ÙÙˆØ§Ø¦Ø¯ Ø§ÙˆØ± Ø§Ø³ Ú©Û’ Ø§ÙˆÙ¾Ù† Ø³ÙˆØ±Ø³ Ø·Ø±ÛŒÙ‚Û Ú©Ø§Ø± Ú©ÛŒ ÙˆØ¶Ø§Ø­Øª Ú©Ø±ÛŒÚº
2. Ù…Ø®ØµÙˆØµ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù† Ú©ÛŒ Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ø§ÙˆØ± ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ø­Ø¯ÙˆØ¯ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ù…Ù†Ø§Ø³Ø¨ Qwen ÙˆÛŒØ±ÛŒØ¦Ù†Ù¹ Ú©Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±ÛŒÚº
3. Ù…Ø®ØªÙ„Ù ÚˆÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ù…ÛŒÚº Qwen Ù…Ø§ÚˆÙ„Ø² Ú©Ùˆ Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²Úˆ Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù†Ø² Ú©Û’ Ø³Ø§ØªÚ¾
4. Qwen Ù…Ø§ÚˆÙ„ Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Ùˆ Ø¨ÛØªØ± Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø²ÛŒØ´Ù† Ø§ÙˆØ± Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù† ØªÚ©Ù†ÛŒÚ©ÙˆÚº Ú©Ø§ Ø§Ø·Ù„Ø§Ù‚ Ú©Ø±ÛŒÚº
5. Qwen ÙÛŒÙ…Ù„ÛŒ Ù…ÛŒÚº Ù…Ø§ÚˆÙ„ Ø³Ø§Ø¦Ø²ØŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒØŒ Ø§ÙˆØ± ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† ØªØ¬Ø§Ø±ØªÛŒ ØªØ¹Ù„Ù‚Ø§Øª Ú©Ø§ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒÚº

## Ø¢Ú¯Û’ Ú©ÛŒØ§ ÛÛ’

- [03: Gemma Family Fundamentals](03.GemmaFamily.md)

---

**ÚˆØ³Ú©Ù„ÛŒÙ…Ø±**:  
ÛŒÛ Ø¯Ø³ØªØ§ÙˆÛŒØ² AI ØªØ±Ø¬Ù…Û Ø³Ø±ÙˆØ³ [Co-op Translator](https://github.com/Azure/co-op-translator) Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ ØªØ±Ø¬Ù…Û Ú©ÛŒ Ú¯Ø¦ÛŒ ÛÛ’Û” ÛÙ… Ø¯Ø±Ø³ØªÚ¯ÛŒ Ú©Û’ Ù„ÛŒÛ’ Ú©ÙˆØ´Ø´ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ù„ÛŒÚ©Ù† Ø¨Ø±Ø§Û Ú©Ø±Ù… Ø¢Ú¯Ø§Û Ø±ÛÛŒÚº Ú©Û Ø®ÙˆØ¯Ú©Ø§Ø± ØªØ±Ø¬Ù…Û’ Ù…ÛŒÚº ØºÙ„Ø·ÛŒØ§Úº ÛŒØ§ ØºÛŒØ± Ø¯Ø±Ø³ØªÛŒØ§Úº ÛÙˆ Ø³Ú©ØªÛŒ ÛÛŒÚºÛ” Ø§ØµÙ„ Ø¯Ø³ØªØ§ÙˆÛŒØ² Ú©Ùˆ Ø§Ø³ Ú©ÛŒ Ø§ØµÙ„ Ø²Ø¨Ø§Ù† Ù…ÛŒÚº Ù…Ø³ØªÙ†Ø¯ Ø°Ø±ÛŒØ¹Û Ø³Ù…Ø¬Ú¾Ø§ Ø¬Ø§Ù†Ø§ Ú†Ø§ÛÛŒÛ’Û” Ø§ÛÙ… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ú©Û’ Ù„ÛŒÛ’ØŒ Ù¾ÛŒØ´Û ÙˆØ± Ø§Ù†Ø³Ø§Ù†ÛŒ ØªØ±Ø¬Ù…Û Ú©ÛŒ Ø³ÙØ§Ø±Ø´ Ú©ÛŒ Ø¬Ø§ØªÛŒ ÛÛ’Û” ÛÙ… Ø§Ø³ ØªØ±Ø¬Ù…Û’ Ú©Û’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ø³Û’ Ù¾ÛŒØ¯Ø§ ÛÙˆÙ†Û’ ÙˆØ§Ù„ÛŒ Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ ØºÙ„Ø· ÙÛÙ…ÛŒ ÛŒØ§ ØºÙ„Ø· ØªØ´Ø±ÛŒØ­ Ú©Û’ Ø°Ù…Û Ø¯Ø§Ø± Ù†ÛÛŒÚº ÛÛŒÚºÛ”