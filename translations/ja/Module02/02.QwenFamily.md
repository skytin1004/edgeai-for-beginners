<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-07-22T03:22:48+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "ja"
}
-->
# ã‚»ã‚¯ã‚·ãƒ§ãƒ³2: Qwenãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®åŸºæœ¬

Qwenãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã¯ã€Alibaba CloudãŒæä¾›ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã«å¯¾ã™ã‚‹åŒ…æ‹¬çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è¡¨ã—ã¦ãŠã‚Šã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒå„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã—ã¤ã¤ã€ã•ã¾ã–ã¾ãªå±•é–‹ã‚·ãƒŠãƒªã‚ªã§åˆ©ç”¨å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚Qwenãƒ•ã‚¡ãƒŸãƒªãƒ¼ãŒæŸ”è»Ÿãªå±•é–‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æä¾›ã—ãªãŒã‚‰ã€å¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã§ç«¶äº‰åŠ›ã®ã‚ã‚‹æ€§èƒ½ã‚’ç¶­æŒã™ã‚‹æ–¹æ³•ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

## é–‹ç™ºè€…å‘ã‘ãƒªã‚½ãƒ¼ã‚¹

### Hugging Faceãƒ¢ãƒ‡ãƒ«ãƒªãƒã‚¸ãƒˆãƒª
é¸æŠã•ã‚ŒãŸQwenãƒ•ã‚¡ãƒŸãƒªãƒ¼ãƒ¢ãƒ‡ãƒ«ã¯ã€[Hugging Face](https://huggingface.co/models?search=qwen)ã‚’é€šã˜ã¦åˆ©ç”¨å¯èƒ½ã§ã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã®ã„ãã¤ã‹ã®ãƒãƒªã‚¢ãƒ³ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚åˆ©ç”¨å¯èƒ½ãªãƒãƒªã‚¢ãƒ³ãƒˆã‚’æ¢ç´¢ã—ã€ç‰¹å®šã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«åˆã‚ã›ã¦å¾®èª¿æ•´ã—ã€ã•ã¾ã–ã¾ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é€šã˜ã¦å±•é–‹ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

### ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºãƒ„ãƒ¼ãƒ«
ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®é–‹ç™ºã¨ãƒ†ã‚¹ãƒˆã«ã¯ã€[Microsoft Foundry Local](https://github.com/microsoft/foundry-local)ã‚’ä½¿ç”¨ã—ã¦ã€é–‹ç™ºãƒã‚·ãƒ³ä¸Šã§æœ€é©åŒ–ã•ã‚ŒãŸæ€§èƒ½ã§Qwenãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚½ãƒ¼ã‚¹
- [Qwenãƒ¢ãƒ‡ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://huggingface.co/docs/transformers/model_doc/qwen)
- [ã‚¨ãƒƒã‚¸å±•é–‹å‘ã‘Qwenãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–](https://github.com/microsoft/olive)

## ã¯ã˜ã‚ã«

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€Alibabaã®Qwenãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã¨ãã®åŸºæœ¬æ¦‚å¿µã‚’æ¢ã‚Šã¾ã™ã€‚Qwenãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®é€²åŒ–ã€åŠ¹æœçš„ãªQwenãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿç¾ã™ã‚‹é©æ–°çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã€ãƒ•ã‚¡ãƒŸãƒªãƒ¼å†…ã®ä¸»è¦ãªãƒãƒªã‚¢ãƒ³ãƒˆã€ãŠã‚ˆã³ã•ã¾ã–ã¾ãªã‚·ãƒŠãƒªã‚ªã§ã®å®Ÿç”¨çš„ãªå¿œç”¨ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚

## å­¦ç¿’ç›®æ¨™

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’çµ‚ãˆã‚‹ã¾ã§ã«ã€æ¬¡ã®ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

- Alibabaã®Qwenãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®è¨­è¨ˆå“²å­¦ã¨é€²åŒ–ã‚’ç†è§£ã™ã‚‹
- ã•ã¾ã–ã¾ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã§Qwenãƒ¢ãƒ‡ãƒ«ãŒé«˜æ€§èƒ½ã‚’é”æˆã™ã‚‹ãŸã‚ã®ä¸»è¦ãªé©æ–°ã‚’ç‰¹å®šã™ã‚‹
- Qwenãƒ¢ãƒ‡ãƒ«ã®ç•°ãªã‚‹ãƒãƒªã‚¢ãƒ³ãƒˆã®åˆ©ç‚¹ã¨åˆ¶é™ã‚’èªè­˜ã™ã‚‹
- Qwenãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’å¿œç”¨ã—ã¦ã€å®Ÿéš›ã®ã‚·ãƒŠãƒªã‚ªã«é©ã—ãŸãƒãƒªã‚¢ãƒ³ãƒˆã‚’é¸æŠã™ã‚‹

## ç¾ä»£ã®AIãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ³ã‚’ç†è§£ã™ã‚‹

AIã®çŠ¶æ³ã¯å¤§ããé€²åŒ–ã—ã¦ãŠã‚Šã€ã•ã¾ã–ã¾ãªçµ„ç¹”ãŒè¨€èªãƒ¢ãƒ‡ãƒ«é–‹ç™ºã«ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è¿½æ±‚ã—ã¦ã„ã¾ã™ã€‚ä¸€éƒ¨ã¯ç‹¬è‡ªã®ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ä¸€æ–¹ã§ã€ä»–ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ã‚¢ã‚¯ã‚»ã‚¹æ€§ã¨é€æ˜æ€§ã‚’é‡è¦–ã—ã¦ã„ã¾ã™ã€‚å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã¯ã€APIã‚’é€šã˜ã¦ã®ã¿ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªå¤§è¦æ¨¡ãªç‹¬è‡ªãƒ¢ãƒ‡ãƒ«ã‹ã€èƒ½åŠ›ãŒåŠ£ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã„ãšã‚Œã‹ã‚’é¸æŠã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã—ãŸã€‚

ã“ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¯ã€ãƒ‡ãƒ¼ã‚¿ã€ã‚³ã‚¹ãƒˆã€å±•é–‹ã®æŸ”è»Ÿæ€§ã‚’ç®¡ç†ã—ãªãŒã‚‰å¼·åŠ›ãªAIæ©Ÿèƒ½ã‚’æ±‚ã‚ã‚‹çµ„ç¹”ã«ã¨ã£ã¦èª²é¡Œã‚’ç”Ÿã¿å‡ºã—ã¾ã™ã€‚å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã¯ã€æœ€å…ˆç«¯ã®æ€§èƒ½ã¨å®Ÿç”¨çš„ãªå±•é–‹ã®è€ƒæ…®äº‹é …ã®é–“ã§é¸æŠã‚’è¿«ã‚‰ã‚Œã‚‹ã“ã¨ãŒã‚ˆãã‚ã‚Šã¾ã™ã€‚

## ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªAIã®å“è¶Šæ€§ã®èª²é¡Œ

é«˜å“è³ªã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªAIã®å¿…è¦æ€§ã¯ã€ã•ã¾ã–ã¾ãªã‚·ãƒŠãƒªã‚ªã§ã¾ã™ã¾ã™é‡è¦ã«ãªã£ã¦ã„ã¾ã™ã€‚ãŸã¨ãˆã°ã€ç•°ãªã‚‹çµ„ç¹”ã®ãƒ‹ãƒ¼ã‚ºã«åˆã‚ã›ãŸæŸ”è»Ÿãªå±•é–‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€APIã‚³ã‚¹ãƒˆãŒé‡è¦ã«ãªã‚‹ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®é«˜ã„å®Ÿè£…ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‘ã‘ã®å¤šè¨€èªå¯¾å¿œã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚„æ•°å­¦ã®ã‚ˆã†ãªå°‚é–€åˆ†é‡ã®å°‚é–€çŸ¥è­˜ãŒå¿…è¦ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è€ƒãˆã¦ã¿ã¦ãã ã•ã„ã€‚

### ä¸»ãªå±•é–‹è¦ä»¶

ç¾ä»£ã®AIå±•é–‹ã¯ã€å®Ÿç”¨æ€§ã‚’åˆ¶é™ã™ã‚‹ã„ãã¤ã‹ã®åŸºæœ¬çš„ãªè¦ä»¶ã«ç›´é¢ã—ã¦ã„ã¾ã™ï¼š

- **ã‚¢ã‚¯ã‚»ã‚¹æ€§**: é€æ˜æ€§ã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã®ãŸã‚ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®åˆ©ç”¨å¯èƒ½æ€§
- **ã‚³ã‚¹ãƒˆåŠ¹ç‡**: ã•ã¾ã–ã¾ãªäºˆç®—ã«å¯¾å¿œã™ã‚‹åˆç†çš„ãªè¨ˆç®—è¦ä»¶
- **æŸ”è»Ÿæ€§**: ç•°ãªã‚‹å±•é–‹ã‚·ãƒŠãƒªã‚ªã«å¯¾å¿œã™ã‚‹è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º
- **ã‚°ãƒ­ãƒ¼ãƒãƒ«å¯¾å¿œ**: å¼·åŠ›ãªå¤šè¨€èªãŠã‚ˆã³ç•°æ–‡åŒ–å¯¾å¿œèƒ½åŠ›
- **å°‚é–€æ€§**: ç‰¹å®šã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«å¯¾å¿œã™ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®ãƒãƒªã‚¢ãƒ³ãƒˆ

## Qwenãƒ¢ãƒ‡ãƒ«ã®å“²å­¦

Qwenãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ã‚¢ã‚¯ã‚»ã‚¹æ€§ã€å¤šè¨€èªå¯¾å¿œã€å®Ÿç”¨çš„ãªå±•é–‹ã‚’å„ªå…ˆã—ãªãŒã‚‰ã€ç«¶äº‰åŠ›ã®ã‚ã‚‹æ€§èƒ½ç‰¹æ€§ã‚’ç¶­æŒã™ã‚‹AIãƒ¢ãƒ‡ãƒ«é–‹ç™ºã¸ã®åŒ…æ‹¬çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚Qwenãƒ¢ãƒ‡ãƒ«ã¯ã€å¤šæ§˜ãªãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€é«˜å“è³ªãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã€ãŠã‚ˆã³ç•°ãªã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³å‘ã‘ã®å°‚é–€ãƒãƒªã‚¢ãƒ³ãƒˆã‚’é€šã˜ã¦ã“ã‚Œã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚

Qwenãƒ•ã‚¡ãƒŸãƒªãƒ¼ã¯ã€æ€§èƒ½ã¨åŠ¹ç‡ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«å…¨ä½“ã«ã‚ãŸã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æä¾›ã™ã‚‹ã•ã¾ã–ã¾ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’åŒ…å«ã—ã¦ãŠã‚Šã€ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹ã‹ã‚‰ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚µãƒ¼ãƒãƒ¼ã¾ã§ã®å±•é–‹ã‚’å¯èƒ½ã«ã—ã€æœ‰æ„ç¾©ãªAIæ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚ãã®ç›®æ¨™ã¯ã€é«˜å“è³ªãªAIã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’æ°‘ä¸»åŒ–ã—ã€å±•é–‹ã®é¸æŠè‚¢ã«æŸ”è»Ÿæ€§ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã™ã€‚

### Qwenã®è¨­è¨ˆã®åŸºæœ¬åŸå‰‡

Qwenãƒ¢ãƒ‡ãƒ«ã¯ã€ä»–ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã¨åŒºåˆ¥ã•ã‚Œã‚‹ã„ãã¤ã‹ã®åŸºæœ¬åŸå‰‡ã«åŸºã¥ã„ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ï¼š

- **ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹å„ªå…ˆ**: ç ”ç©¶ãŠã‚ˆã³å•†æ¥­åˆ©ç”¨ã®ãŸã‚ã®å®Œå…¨ãªé€æ˜æ€§ã¨ã‚¢ã‚¯ã‚»ã‚¹æ€§
- **åŒ…æ‹¬çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°**: è¤‡æ•°ã®è¨€èªã¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ã‚«ãƒãƒ¼ã™ã‚‹å¤§è¦æ¨¡ã§å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: ç•°ãªã‚‹è¨ˆç®—è¦ä»¶ã«å¯¾å¿œã™ã‚‹è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º
- **å°‚é–€çš„ãªå“è¶Šæ€§**: ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®ãƒãƒªã‚¢ãƒ³ãƒˆ

## Qwenãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’æ”¯ãˆã‚‹ä¸»è¦æŠ€è¡“

### å¤§è¦æ¨¡ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

Qwenãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ç‰¹å¾´ã®1ã¤ã¯ã€ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã«æŠ•è³‡ã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã®å¤§è¦æ¨¡ã•ã§ã™ã€‚Qwenãƒ¢ãƒ‡ãƒ«ã¯ã€æ•°å…†ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ã‚ãŸã‚‹æ…é‡ã«ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚ŒãŸå¤šè¨€èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ´»ç”¨ã—ã€åŒ…æ‹¬çš„ãªä¸–ç•ŒçŸ¥è­˜ã¨æ¨è«–èƒ½åŠ›ã‚’æä¾›ã—ã¾ã™ã€‚

ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€é«˜å“è³ªãªã‚¦ã‚§ãƒ–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€å­¦è¡“æ–‡çŒ®ã€ã‚³ãƒ¼ãƒ‰ãƒªãƒã‚¸ãƒˆãƒªã€å¤šè¨€èªãƒªã‚½ãƒ¼ã‚¹ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§æ©Ÿèƒ½ã—ã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã¯ã€çŸ¥è­˜ã®å¹…ã¨ã•ã¾ã–ã¾ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã‚„è¨€èªã«ã‚ãŸã‚‹ç†è§£ã®æ·±ã•ã®ä¸¡æ–¹ã‚’é‡è¦–ã—ã¦ã„ã¾ã™ã€‚

### é«˜åº¦ãªæ¨è«–ã¨æ€è€ƒ

æœ€æ–°ã®Qwenãƒ¢ãƒ‡ãƒ«ã¯ã€è¤‡é›‘ãªãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—å•é¡Œè§£æ±ºã‚’å¯èƒ½ã«ã™ã‚‹é«˜åº¦ãªæ¨è«–èƒ½åŠ›ã‚’çµ„ã¿è¾¼ã‚“ã§ã„ã¾ã™ï¼š

**Thinking Mode (Qwen3)**: äººé–“ã®å•é¡Œè§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ä¼¼ãŸè©³ç´°ãªã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®æ¨è«–ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚

**ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ¼ãƒ‰æ“ä½œ**: å˜ç´”ãªã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹è¿…é€Ÿãªå¿œç­”ãƒ¢ãƒ¼ãƒ‰ã¨ã€è¤‡é›‘ãªå•é¡Œã«å¯¾ã™ã‚‹æ·±ã„æ€è€ƒãƒ¢ãƒ¼ãƒ‰ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹èƒ½åŠ›ã€‚

**Chain-of-Thoughtçµ±åˆ**: è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹é€æ˜æ€§ã¨æ­£ç¢ºæ€§ã‚’å‘ä¸Šã•ã›ã‚‹æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã®è‡ªç„¶ãªçµ„ã¿è¾¼ã¿ã€‚

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é©æ–°

Qwenãƒ•ã‚¡ãƒŸãƒªãƒ¼ã¯ã€æ€§èƒ½ã¨åŠ¹ç‡ã®ä¸¡æ–¹ã‚’ç›®çš„ã¨ã—ãŸã„ãã¤ã‹ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æœ€é©åŒ–ã‚’çµ„ã¿è¾¼ã‚“ã§ã„ã¾ã™ï¼š

**ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªè¨­è¨ˆ**: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºå…¨ä½“ã§ä¸€è²«ã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ã—ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨æ¯”è¼ƒã‚’å®¹æ˜“ã«ã—ã¾ã™ã€‚

**ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çµ±åˆ**: ãƒ†ã‚­ã‚¹ãƒˆã€ãƒ“ã‚¸ãƒ§ãƒ³ã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå‡¦ç†æ©Ÿèƒ½ã‚’çµ±ä¸€ã•ã‚ŒãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å†…ã§ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«çµ±åˆã€‚

**å±•é–‹æœ€é©åŒ–**: ã•ã¾ã–ã¾ãªãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ§‹æˆã«å¯¾å¿œã™ã‚‹è¤‡æ•°ã®é‡å­åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨å±•é–‹å½¢å¼ã€‚

## ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨å±•é–‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³

ç¾ä»£ã®å±•é–‹ç’°å¢ƒã¯ã€ã•ã¾ã–ã¾ãªè¨ˆç®—è¦ä»¶ã«å¯¾å¿œã™ã‚‹Qwenãƒ¢ãƒ‡ãƒ«ã®æŸ”è»Ÿæ€§ã®æ©æµã‚’å—ã‘ã¦ã„ã¾ã™ï¼š

### å°å‹ãƒ¢ãƒ‡ãƒ« (0.5B-3B)

Qwenã¯ã€ã‚¨ãƒƒã‚¸å±•é–‹ã€ãƒ¢ãƒã‚¤ãƒ«ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ã®ã‚ã‚‹ç’°å¢ƒã«é©ã—ãŸåŠ¹ç‡çš„ãªå°å‹ãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã—ã€å„ªã‚ŒãŸèƒ½åŠ›ã‚’ç¶­æŒã—ã¾ã™ã€‚

### ä¸­å‹ãƒ¢ãƒ‡ãƒ« (7B-32B)

ä¸­å‹ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‘ã‘ã«å¼·åŒ–ã•ã‚ŒãŸèƒ½åŠ›ã‚’æä¾›ã—ã€æ€§èƒ½ã¨è¨ˆç®—è¦ä»¶ã®å„ªã‚ŒãŸãƒãƒ©ãƒ³ã‚¹ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

### å¤§å‹ãƒ¢ãƒ‡ãƒ« (72B+)

ãƒ•ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ãƒ¢ãƒ‡ãƒ«ã¯ã€æœ€å¤§é™ã®èƒ½åŠ›ã‚’å¿…è¦ã¨ã™ã‚‹è¦æ±‚ã®å³ã—ã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€ç ”ç©¶ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºå±•é–‹å‘ã‘ã«æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

## Qwenãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®åˆ©ç‚¹

### ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ã‚¢ã‚¯ã‚»ã‚¹æ€§

Qwenãƒ¢ãƒ‡ãƒ«ã¯å®Œå…¨ãªé€æ˜æ€§ã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ©Ÿèƒ½ã‚’æä¾›ã—ã€çµ„ç¹”ãŒãƒ¢ãƒ‡ãƒ«ã‚’ç†è§£ã€å¤‰æ›´ã€é©å¿œã•ã›ã‚‹ã“ã¨ã§ã€ç‰¹å®šã®ãƒ‹ãƒ¼ã‚ºã«å¯¾å¿œã—ãªãŒã‚‰ãƒ™ãƒ³ãƒ€ãƒ¼ãƒ­ãƒƒã‚¯ã‚¤ãƒ³ã‚’å›é¿ã§ãã¾ã™ã€‚

### å±•é–‹ã®æŸ”è»Ÿæ€§

ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®ç¯„å›²ã¯ã€ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹ã‹ã‚‰é«˜æ€§èƒ½ã‚µãƒ¼ãƒãƒ¼ã¾ã§ã€ã•ã¾ã–ã¾ãªãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ§‹æˆã«ã‚ãŸã‚‹å±•é–‹ã‚’å¯èƒ½ã«ã—ã€çµ„ç¹”ã«AIã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã®é¸æŠè‚¢ã‚’æä¾›ã—ã¾ã™ã€‚

### å¤šè¨€èªã®å“è¶Šæ€§

Qwenãƒ¢ãƒ‡ãƒ«ã¯ã€å¤šè¨€èªã®ç†è§£ã¨ç”Ÿæˆã«å„ªã‚Œã¦ãŠã‚Šã€ç‰¹ã«è‹±èªã¨ä¸­å›½èªã§å¼·åŠ›ãªèƒ½åŠ›ã‚’ç™ºæ®ã—ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«é©ã—ã¦ã„ã¾ã™ã€‚

### ç«¶äº‰åŠ›ã®ã‚ã‚‹æ€§èƒ½

Qwenãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ã‚¢ã‚¯ã‚»ã‚¹æ€§ã‚’æä¾›ã—ãªãŒã‚‰ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ä¸€è²«ã—ã¦ç«¶äº‰åŠ›ã®ã‚ã‚‹çµæœã‚’é”æˆã—ã¦ãŠã‚Šã€ã‚ªãƒ¼ãƒ—ãƒ³ãƒ¢ãƒ‡ãƒ«ãŒç‹¬è‡ªãƒ¢ãƒ‡ãƒ«ã«åŒ¹æ•µã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

### å°‚é–€çš„ãªèƒ½åŠ›

Qwen-Coderã‚„Qwen-Mathã®ã‚ˆã†ãªãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®ãƒãƒªã‚¢ãƒ³ãƒˆã¯ã€ä¸€èˆ¬çš„ãªè¨€èªç†è§£èƒ½åŠ›ã‚’ç¶­æŒã—ãªãŒã‚‰ã€å°‚é–€çš„ãªå°‚é–€çŸ¥è­˜ã‚’æä¾›ã—ã¾ã™ã€‚

## å®Ÿç”¨ä¾‹ã¨ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹

æŠ€è¡“çš„ãªè©³ç´°ã«å…¥ã‚‹å‰ã«ã€Qwenãƒ¢ãƒ‡ãƒ«ãŒé”æˆã§ãã‚‹å…·ä½“çš„ãªä¾‹ã‚’ã„ãã¤ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

### æ•°å­¦çš„æ¨è«–ã®ä¾‹

Qwen-Mathã¯ã€ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®æ•°å­¦çš„å•é¡Œè§£æ±ºã«å„ªã‚Œã¦ã„ã¾ã™ã€‚ãŸã¨ãˆã°ã€è¤‡é›‘ãªå¾®ç©åˆ†ã®å•é¡Œã‚’è§£ãéš›ï¼š

```
User: Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = xÂ³ + 2xÂ² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹

Breaking down each term:
- d/dx(xÂ³) = 3xÂ²
- d/dx(2xÂ²) = 2 Â· 2xÂ¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3xÂ² + 4x - 5
```

### å¤šè¨€èªã‚µãƒãƒ¼ãƒˆã®ä¾‹

Qwenãƒ¢ãƒ‡ãƒ«ã¯ã€ã•ã¾ã–ã¾ãªè¨€èªã§å¼·åŠ›ãªå¤šè¨€èªèƒ½åŠ›ã‚’ç¤ºã—ã¾ã™ï¼š

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**ä¸­æ–‡:**
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡æœºå™¨ï¼ˆç‰¹åˆ«æ˜¯è®¡ç®—æœºç³»ç»Ÿï¼‰å¯¹äººç±»æ™ºèƒ½è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚è¿™äº›è¿‡ç¨‹åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€è§£å†³é—®é¢˜ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚äººå·¥æ™ºèƒ½ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»è®¤çŸ¥èƒ½åŠ›çš„ä»»åŠ¡ã€‚
```

### ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«èƒ½åŠ›ã®ä¾‹

Qwen-VLã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’åŒæ™‚ã«å‡¦ç†ã§ãã¾ã™ï¼š

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã®ä¾‹

Qwen-Coderã¯ã€è¤‡æ•°ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã®ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã¨èª¬æ˜ã«å„ªã‚Œã¦ã„ã¾ã™ï¼š

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸé…åˆ—ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã®ãƒã‚¤ãƒŠãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    
    Args:
        arr (list): ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸæ¯”è¼ƒå¯èƒ½ãªè¦ç´ ã®ãƒªã‚¹ãƒˆ
        target: æ¤œç´¢ã™ã‚‹å€¤
        
    Returns:
        int: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯-1
        
    æ™‚é–“è¨ˆç®—é‡: O(log n)
    ç©ºé–“è¨ˆç®—é‡: O(1)
    """
    # å·¦å³ã®ãƒã‚¤ãƒ³ã‚¿ã‚’åˆæœŸåŒ–
    left, right = 0, len(arr) - 1
    
    # æ¤œç´¢ç¯„å›²ãŒæœ‰åŠ¹ãªé–“ã€æ¤œç´¢ã‚’ç¶šã‘ã‚‹
    while left <= right:
        # æ•´æ•°ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚ã«ä¸­é–“ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
        mid = left + (right - left) // 2
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒè¦‹ã¤ã‹ã£ãŸã‹ç¢ºèª
        if arr[mid] == target:
            return mid
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒå°ã•ã„å ´åˆã€å·¦åŠåˆ†ã‚’æ¤œç´¢
        elif arr[mid] > target:
            right = mid - 1
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒå¤§ãã„å ´åˆã€å³åŠåˆ†ã‚’æ¤œç´¢
        else:
            left = mid + 1
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
    return -1

# ä½¿ç”¨ä¾‹:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # å‡ºåŠ›: Index of 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### ã‚¨ãƒƒã‚¸å±•é–‹ã®ä¾‹

Qwenãƒ¢ãƒ‡ãƒ«ã¯ã€æœ€é©åŒ–ã•ã‚ŒãŸæ§‹æˆã§ã•ã¾ã–ã¾ãªã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã«å±•é–‹ã§ãã¾ã™ï¼š

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Qwenãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®é€²åŒ–

### Qwen 1.0ã¨1.5: åŸºç›¤ãƒ¢ãƒ‡ãƒ«

åˆæœŸã®Qwenãƒ¢ãƒ‡ãƒ«ã¯ã€åŒ…æ‹¬çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ã‚¢ã‚¯ã‚»ã‚¹æ€§ã®åŸºæœ¬åŸå‰‡ã‚’ç¢ºç«‹ã—ã¾ã—ãŸï¼š

- **Qwen-7B (7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)**: ä¸­å›½èªã¨è‹±èªã®è¨€èªç†è§£ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸåˆæœŸãƒªãƒªãƒ¼ã‚¹
- **Qwen-14B (14Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)**: æ¨è«–ã¨çŸ¥è­˜ãŒå‘ä¸Šã—ãŸèƒ½åŠ›
- **Qwen-72B (72Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)**: æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’æä¾›ã™ã‚‹å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«
- **Qwen1.5ã‚·ãƒªãƒ¼ã‚º**: é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ãŒæ”¹å–„ã•ã‚ŒãŸè¤‡æ•°ã‚µã‚¤ã‚ºï¼ˆ0.5Bã‹ã‚‰110Bï¼‰ã«æ‹¡å¼µ

### Qwen2ãƒ•ã‚¡ãƒŸãƒªãƒ¼: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã®æ‹¡å¼µ

Qwen2ã‚·ãƒªãƒ¼ã‚ºã¯ã€è¨€èªã¨ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«èƒ½åŠ›ã®ä¸¡æ–¹ã§å¤§ããªé€²æ­©ã‚’é‚ã’ã¾ã—ãŸï¼š

- **Qwen2-0.5Bã‹ã‚‰72B**: ã•ã¾ã–ã¾ãªå±•é–‹ãƒ‹ãƒ¼ã‚ºã«å¯¾å¿œã™ã‚‹åŒ…æ‹¬çš„ãªè¨€èªãƒ¢ãƒ‡ãƒ«
- **Qwen2-57B-A14B (MoE)**: åŠ¹ç‡çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä½¿ç”¨ã®ãŸã‚ã®Mixture-of-Expertsã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **Qwen2-VL**: ç”»åƒç†è§£ã®ãŸã‚ã®é«˜åº¦ãªãƒ“ã‚¸ãƒ§ãƒ³ã¨è¨€èªã®èƒ½åŠ›
- **Qwen2-Audio**: éŸ³å£°å‡¦ç†ã¨ç†è§£èƒ½åŠ›
- **Qwen2-Math**: æ•°å­¦çš„æ¨è«–ã¨å•é¡Œè§£æ±ºã«ç‰¹åŒ–

### Qwen2.5ãƒ•ã‚¡ãƒŸãƒªãƒ¼: æ€§èƒ½ã®å‘ä¸Š

Qwen2.5ã‚·ãƒªãƒ¼ã‚ºã¯ã€ã™ã¹ã¦ã®å´é¢ã§å¤§å¹…ãªæ”¹å–„ã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸï¼š

- **æ‹¡å¼µãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°**: 18å…†ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã§èƒ½åŠ›ã‚’å‘ä¸Š
- **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ‹¡å¼µ**: æœ€å¤§128Kãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã€Turboãƒãƒªã‚¢ãƒ³ãƒˆã¯1Mãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚µãƒãƒ¼ãƒˆ
- **å°‚é–€æ€§ã®å‘ä¸Š**: æ”¹å–„ã•ã‚ŒãŸQwen2.5-CoderãŠã‚ˆã³Qwen2.5-Mathãƒãƒªã‚¢ãƒ³ãƒˆ
- **å¤šè¨€èªã‚µãƒãƒ¼ãƒˆã®å‘ä¸Š**: 27ä»¥ä¸Šã®è¨€èªã§ã®æ€§èƒ½å‘ä¸Š

### Qwen3ãƒ•ã‚¡ãƒŸãƒªãƒ¼: é«˜åº¦ãªæ¨è«–

æœ€æ–°ä¸–ä»£ã¯ã€æ¨è«–ã¨æ€è€ƒèƒ½åŠ›ã®é™ç•Œã‚’æŠ¼ã—åºƒã’ã¦ã„ã¾ã™ï¼š

- **Qwen3-235B-A22B**: 235Bã®ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ãƒ•ãƒ©ãƒƒã‚°ã‚·ãƒƒãƒ—Mixture-of-Expertsãƒ¢ãƒ‡ãƒ«
- **Qwen3-30B-A3B**: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚ãŸã‚Šã®æ€§èƒ½ãŒé«˜ã„åŠ¹ç‡çš„ãªMoEãƒ¢ãƒ‡ãƒ«
- **å¯†é›†ãƒ¢ãƒ‡ãƒ«**: Qwen3-32Bã€14Bã€8Bã€4Bã€1.7Bã€0.6Bãªã©ã€ã•ã¾ã–ã¾ãªå±•é–‹ã‚·ãƒŠãƒªã‚ªã«å¯¾å¿œ
- **Thinking Mode**: ç°¡å˜ãªå¿œç­”ã¨æ·±ã„æ€è€ƒã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¨è«–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
- **å¤šè¨€èªã®å“è¶Šæ€§**: 119ã®è¨€èªã¨æ–¹è¨€ã‚’ã‚µãƒãƒ¼ãƒˆ
-
Qwenãƒ¢ãƒ‡ãƒ«ã‚’Hugging Face Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ä½¿ç”¨ã™ã‚‹æ–¹æ³•:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Qwen2.5ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨

**Qwen-Coderã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**æ•°å­¦å•é¡Œã®è§£æ±º:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**ãƒ“ã‚¸ãƒ§ãƒ³ã¨è¨€èªã®ã‚¿ã‚¹ã‚¯:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### æ€è€ƒãƒ¢ãƒ¼ãƒ‰ (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### ğŸ“± ãƒ¢ãƒã‚¤ãƒ«ãŠã‚ˆã³ã‚¨ãƒƒã‚¸ã§ã®å±•é–‹

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### APIå±•é–‹ã®ä¾‹

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨æˆæœ

Qwenãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ã‚¢ã‚¯ã‚»ã‚¹æ€§ã‚’ç¶­æŒã—ãªãŒã‚‰ã€ã•ã¾ã–ã¾ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚

### ä¸»ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ

**æ¨è«–èƒ½åŠ›ã®å“è¶Šæ€§:**
- Qwen3-235B-A22Bã¯ã€DeepSeek-R1ã€o1ã€o3-miniã€Grok-3ã€Gemini-2.5-Proãªã©ã®ãƒˆãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€æ•°å­¦ã€ä¸€èˆ¬çš„ãªèƒ½åŠ›ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡ã§ç«¶äº‰åŠ›ã®ã‚ã‚‹çµæœã‚’é”æˆ
- Qwen3-30B-A3Bã¯ã€10å€ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤QwQ-32Bã‚’ä¸Šå›ã‚‹
- Qwen3-4Bã¯ã€Qwen2.5-72B-Instructã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«åŒ¹æ•µ

**åŠ¹ç‡æ€§ã®æˆæœ:**
- Qwen3-MoEãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¯ã€Qwen2.5ã®å¯†é›†ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®10%ã§é”æˆ
- å¯†é›†ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã®ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›

**å¤šè¨€èªå¯¾å¿œ:**
- Qwen3ãƒ¢ãƒ‡ãƒ«ã¯119ã®è¨€èªã¨æ–¹è¨€ã‚’ã‚µãƒãƒ¼ãƒˆ
- å¤šæ§˜ãªè¨€èªçš„ãƒ»æ–‡åŒ–çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

**ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¦æ¨¡:**
- Qwen3ã¯ç´„36å…†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã—ã€119ã®è¨€èªã¨æ–¹è¨€ã‚’ã‚«ãƒãƒ¼ã€‚ã“ã‚Œã¯Qwen2.5ã®18å…†ãƒˆãƒ¼ã‚¯ãƒ³ã®ç´„2å€

### ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒãƒˆãƒªãƒƒã‚¯ã‚¹

| ãƒ¢ãƒ‡ãƒ«ã‚·ãƒªãƒ¼ã‚º | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›² | ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•· | ä¸»ãªå¼·ã¿ | æœ€é©ãªãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ |
|----------------|----------------|----------------|----------|--------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸæ€§èƒ½ã€å¤šè¨€èªå¯¾å¿œ | ä¸€èˆ¬çš„ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å±•é–‹ |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° | ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ”¯æ´ |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | æ•°å­¦çš„æ¨è«– | æ•™è‚²ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã€STEMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ |
| **Qwen2.5-VL** | Various | Variable | ãƒ“ã‚¸ãƒ§ãƒ³ã¨è¨€èªã®ç†è§£ | ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€ç”»åƒè§£æ |
| **Qwen3** | 0.6B-235B | Variable | é«˜åº¦ãªæ¨è«–ã€æ€è€ƒãƒ¢ãƒ¼ãƒ‰ | è¤‡é›‘ãªæ¨è«–ã€ç ”ç©¶ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ |
| **Qwen3 MoE** | 30B-235B total | Variable | åŠ¹ç‡çš„ãªå¤§è¦æ¨¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ | ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€é«˜æ€§èƒ½ãƒ‹ãƒ¼ã‚º |

## ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¬ã‚¤ãƒ‰

### åŸºæœ¬çš„ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‘ã‘
- **Qwen2.5-0.5B/1.5B**: ãƒ¢ãƒã‚¤ãƒ«ã‚¢ãƒ—ãƒªã€ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
- **Qwen2.5-3B/7B**: ä¸€èˆ¬çš„ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã€Q&Aã‚·ã‚¹ãƒ†ãƒ 

### æ•°å­¦ãŠã‚ˆã³æ¨è«–ã‚¿ã‚¹ã‚¯å‘ã‘
- **Qwen2.5-Math**: æ•°å­¦å•é¡Œã®è§£æ±ºã¨STEMæ•™è‚²
- **Qwen3ã®æ€è€ƒãƒ¢ãƒ¼ãƒ‰**: ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®åˆ†æã‚’å¿…è¦ã¨ã™ã‚‹è¤‡é›‘ãªæ¨è«–

### ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¨é–‹ç™ºå‘ã‘
- **Qwen2.5-Coder**: ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã€ãƒ‡ãƒãƒƒã‚°ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°æ”¯æ´
- **Qwen3**: æ¨è«–èƒ½åŠ›ã‚’ä¼´ã†é«˜åº¦ãªãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚¿ã‚¹ã‚¯

### ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‘ã‘
- **Qwen2.5-VL**: ç”»åƒç†è§£ã€è¦–è¦šçš„è³ªå•å¿œç­”
- **Qwen-Audio**: éŸ³å£°å‡¦ç†ã¨éŸ³å£°ç†è§£

### ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºå±•é–‹å‘ã‘
- **Qwen2.5-32B/72B**: é«˜æ€§èƒ½ãªè¨€èªç†è§£
- **Qwen3-235B-A22B**: é«˜åº¦ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‘ã‘ã®æœ€å¤§èƒ½åŠ›

## å±•é–‹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¨ã‚¢ã‚¯ã‚»ã‚¹æ€§
### ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
- **Hugging Face Hub**: ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚µãƒãƒ¼ãƒˆä»˜ãã®åŒ…æ‹¬çš„ãªãƒ¢ãƒ‡ãƒ«ãƒªãƒã‚¸ãƒˆãƒª
- **ModelScope**: Alibabaã®ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¨æœ€é©åŒ–ãƒ„ãƒ¼ãƒ«
- **ãã®ä»–ã®ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼**: æ¨™æº–çš„ãªMLãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’é€šã˜ãŸã‚µãƒãƒ¼ãƒˆ

### ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- **Transformers**: ç°¡å˜ã«å±•é–‹ã§ãã‚‹Hugging Faceã®æ¨™æº–çµ±åˆ
- **vLLM**: ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒå‘ã‘ã®é«˜æ€§èƒ½ã‚µãƒ¼ãƒ“ãƒ³ã‚°
- **Ollama**: ãƒ­ãƒ¼ã‚«ãƒ«å±•é–‹ã¨ç®¡ç†ã®ç°¡ç´ åŒ–
- **ONNX Runtime**: å¤šæ§˜ãªãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å‘ã‘ã®ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æœ€é©åŒ–
- **llama.cpp**: å¤šæ§˜ãªãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å‘ã‘ã®åŠ¹ç‡çš„ãªC++å®Ÿè£…

### å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹
- **Qwenãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰
- **Hugging Face Model Hub**: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ‡ãƒ¢ã¨ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ä¾‹
- **ç ”ç©¶è«–æ–‡**: arxivä¸Šã®æŠ€è¡“è«–æ–‡ã§è©³ç´°ã‚’ç†è§£
- **ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ•ã‚©ãƒ¼ãƒ©ãƒ **: æ´»ç™ºãªã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚µãƒãƒ¼ãƒˆã¨è­°è«–

### Qwenãƒ¢ãƒ‡ãƒ«ã®å§‹ã‚æ–¹

#### é–‹ç™ºãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
1. **Hugging Face Transformers**: æ¨™æº–çš„ãªPythonçµ±åˆã§å§‹ã‚ã‚‹
2. **ModelScope**: Alibabaã®æœ€é©åŒ–ã•ã‚ŒãŸå±•é–‹ãƒ„ãƒ¼ãƒ«ã‚’æ¢ç´¢
3. **ãƒ­ãƒ¼ã‚«ãƒ«å±•é–‹**: Ollamaã¾ãŸã¯Transformersã‚’ç›´æ¥ä½¿ç”¨ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆ

#### å­¦ç¿’ãƒ‘ã‚¹
1. **ã‚³ã‚¢ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’ç†è§£**: Qwenãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨èƒ½åŠ›ã‚’å­¦ã¶
2. **ãƒãƒªã‚¢ãƒ³ãƒˆã‚’è©¦ã™**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’è©¦ã™
3. **å®Ÿè£…ã‚’ç·´ç¿’**: é–‹ç™ºç’°å¢ƒã§ãƒ¢ãƒ‡ãƒ«ã‚’å±•é–‹
4. **å±•é–‹ã‚’æœ€é©åŒ–**: ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹å‘ã‘ã«å¾®èª¿æ•´

#### ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
- **å°è¦æ¨¡ã‹ã‚‰å§‹ã‚ã‚‹**: åˆæœŸé–‹ç™ºã«ã¯å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ1.5B-7Bï¼‰ã‚’ä½¿ç”¨
- **ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨**: æœ€é©ãªçµæœã‚’å¾—ã‚‹ãŸã‚ã«é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’é©ç”¨
- **ãƒªã‚½ãƒ¼ã‚¹ã‚’ç›£è¦–**: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨æ¨è«–é€Ÿåº¦ã‚’è¿½è·¡
- **ç‰¹åŒ–ã‚’æ¤œè¨**: å¿…è¦ã«å¿œã˜ã¦ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒãƒªã‚¢ãƒ³ãƒˆã‚’é¸æŠ

## é«˜åº¦ãªä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³

### ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ä¾‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### ç‰¹åŒ–ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

**è¤‡é›‘ãªæ¨è«–ã‚¿ã‚¹ã‚¯å‘ã‘:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¼´ã†ã‚³ãƒ¼ãƒ‰ç”Ÿæˆå‘ã‘:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### å¤šè¨€èªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ä¸­æ–‡)",
        "es": "Spanish (EspaÃ±ol)",
        "fr": "French (FranÃ§ais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (æ—¥æœ¬èª)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ğŸ”§ ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å±•é–‹ãƒ‘ã‚¿ãƒ¼ãƒ³

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–æˆ¦ç•¥

### ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### æ¨è«–æœ€é©åŒ–

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨è©•ä¾¡

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## çµè«–

Qwenãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®é€æ˜æ€§ã‚’ç¶­æŒã—ãªãŒã‚‰ã€ã•ã¾ã–ã¾ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ç«¶äº‰åŠ›ã®ã‚ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æä¾›ã™ã‚‹åŒ…æ‹¬çš„ãªAIæŠ€è¡“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚å¤šè¨€èªå¯¾å¿œã‚„æŸ”è»Ÿãªå±•é–‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’é€šã˜ã¦ã€Qwenã¯ãƒªã‚½ãƒ¼ã‚¹ã‚„ç‰¹å®šã®è¦ä»¶ã«é–¢ä¿‚ãªãã€çµ„ç¹”ã‚„é–‹ç™ºè€…ãŒå¼·åŠ›ãªAIæ©Ÿèƒ½ã‚’æ´»ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

### ä¸»ãªãƒã‚¤ãƒ³ãƒˆ

**ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®å“è¶Šæ€§**: Qwenã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒãƒ—ãƒ­ãƒ—ãƒ©ã‚¤ã‚¨ã‚¿ãƒªãªä»£æ›¿å“ã¨ç«¶äº‰ã§ãã‚‹æ€§èƒ½ã‚’æŒã¡ãªãŒã‚‰ã€é€æ˜æ€§ã€ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ€§ã€åˆ¶å¾¡æ€§ã‚’æä¾›ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

**ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: 0.5Bã‹ã‚‰235Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¾ã§ã®ç¯„å›²ã«ã‚ˆã‚Šã€ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹ã‹ã‚‰ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¾ã§ã€ã‚ã‚‰ã‚†ã‚‹è¨ˆç®—ç’°å¢ƒã§ã®å±•é–‹ãŒå¯èƒ½ã§ã™ã€‚

**ç‰¹åŒ–ã—ãŸèƒ½åŠ›**: Qwen-Coderã€Qwen-Mathã€Qwen-VLãªã©ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒãƒªã‚¢ãƒ³ãƒˆã¯ã€ä¸€èˆ¬çš„ãªè¨€èªç†è§£ã‚’ç¶­æŒã—ãªãŒã‚‰å°‚é–€çš„ãªã‚¹ã‚­ãƒ«ã‚’æä¾›ã—ã¾ã™ã€‚

**ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚¢ã‚¯ã‚»ã‚¹æ€§**: 119ä»¥ä¸Šã®è¨€èªã«å¯¾å¿œã—ãŸå¼·åŠ›ãªå¤šè¨€èªã‚µãƒãƒ¼ãƒˆã«ã‚ˆã‚Šã€å›½éš›çš„ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚„å¤šæ§˜ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ãƒ¼ã‚¹ã«é©ã—ã¦ã„ã¾ã™ã€‚

**ç¶™ç¶šçš„ãªé©æ–°**: Qwen 1.0ã‹ã‚‰Qwen3ã¸ã®é€²åŒ–ã¯ã€èƒ½åŠ›ã€åŠ¹ç‡æ€§ã€å±•é–‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ä¸€è²«ã—ãŸæ”¹å–„ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

### ä»Šå¾Œã®å±•æœ›

Qwenãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®é€²åŒ–ã«ä¼´ã„ã€ä»¥ä¸‹ãŒæœŸå¾…ã•ã‚Œã¾ã™:

- **åŠ¹ç‡æ€§ã®å‘ä¸Š**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯”ç‡ã®ã•ã‚‰ãªã‚‹æœ€é©åŒ–
- **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«èƒ½åŠ›ã®æ‹¡å¤§**: ã‚ˆã‚Šé«˜åº¦ãªãƒ“ã‚¸ãƒ§ãƒ³ã€éŸ³å£°ã€ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã®çµ±åˆ
- **æ¨è«–èƒ½åŠ›ã®å‘ä¸Š**: é«˜åº¦ãªæ€è€ƒãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¨å¤šæ®µéšå•é¡Œè§£æ±ºèƒ½åŠ›
- **å±•é–‹ãƒ„ãƒ¼ãƒ«ã®æ”¹å–„**: å¤šæ§˜ãªå±•é–‹ã‚·ãƒŠãƒªã‚ªå‘ã‘ã®å¼·åŒ–ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨æœ€é©åŒ–ãƒ„ãƒ¼ãƒ«
- **ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®æˆé•·**: ãƒ„ãƒ¼ãƒ«ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£è²¢çŒ®ã®ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ æ‹¡å¤§

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®æ§‹ç¯‰ã€æ•™è‚²ãƒ„ãƒ¼ãƒ«ã®é–‹ç™ºã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ä½œæˆã€å¤šè¨€èªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æ§‹ç¯‰ãªã©ã€Qwenãƒ•ã‚¡ãƒŸãƒªãƒ¼ã¯ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æä¾›ã—ã€å¼·åŠ›ãªã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚µãƒãƒ¼ãƒˆã¨åŒ…æ‹¬çš„ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚™ãˆã¦ã„ã¾ã™ã€‚

æœ€æ–°ã®æ›´æ–°æƒ…å ±ã€ãƒ¢ãƒ‡ãƒ«ãƒªãƒªãƒ¼ã‚¹ã€è©³ç´°ãªæŠ€è¡“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¤ã„ã¦ã¯ã€Hugging Faceã®å…¬å¼Qwenãƒªãƒã‚¸ãƒˆãƒªã‚’è¨ªå•ã—ã€æ´»ç™ºãªã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚„ä¾‹ã‚’æ¢ç´¢ã—ã¦ãã ã•ã„ã€‚

AIé–‹ç™ºã®æœªæ¥ã¯ã€ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§é€æ˜æ€§ãŒã‚ã‚Šã€å¼·åŠ›ãªãƒ„ãƒ¼ãƒ«ã«ã‚ˆã£ã¦ã€ã‚ã‚‰ã‚†ã‚‹åˆ†é‡ã‚„è¦æ¨¡ã§ã®ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¯èƒ½ã«ã™ã‚‹ã“ã¨ã«ã‚ã‚Šã¾ã™ã€‚Qwenãƒ•ã‚¡ãƒŸãƒªãƒ¼ã¯ã“ã®ãƒ“ã‚¸ãƒ§ãƒ³ã‚’ä½“ç¾ã—ã€æ¬¡ä¸–ä»£ã®AIé§†å‹•ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®åŸºç›¤ã‚’æä¾›ã—ã¾ã™ã€‚

## è¿½åŠ ãƒªã‚½ãƒ¼ã‚¹

- **å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: [Qwen Documentation](https://qwen.readthedocs.io/)
- **ãƒ¢ãƒ‡ãƒ«ãƒãƒ–**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **æŠ€è¡“è«–æ–‡**: [Qwen Research Publications](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£**: [GitHub Discussions and Issues](https://github.com/QwenLM/)
- **ModelScopeãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ **: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## å­¦ç¿’æˆæœ

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ãŒå¯èƒ½ã«ãªã‚Šã¾ã™:

1. Qwenãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åˆ©ç‚¹ã¨ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’èª¬æ˜ã™ã‚‹
2. ç‰¹å®šã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¦ä»¶ã¨ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ã«åŸºã¥ã„ã¦é©åˆ‡ãªQwenãƒãƒªã‚¢ãƒ³ãƒˆã‚’é¸æŠã™ã‚‹
3. ã•ã¾ã–ã¾ãªå±•é–‹ã‚·ãƒŠãƒªã‚ªã§Qwenãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã€æœ€é©åŒ–ã•ã‚ŒãŸæ§‹æˆã‚’ä½¿ç”¨ã™ã‚‹
4. é‡å­åŒ–ã¨æœ€é©åŒ–æŠ€è¡“ã‚’é©ç”¨ã—ã¦Qwenãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹
5. ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€èƒ½åŠ›ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è©•ä¾¡ã™ã‚‹

## æ¬¡ã«é€²ã‚€

- [03: Gemma Family Fundamentals](03.GemmaFamily.md)

**å…è²¬äº‹é …**:  
ã“ã®æ–‡æ›¸ã¯ã€AIç¿»è¨³ã‚µãƒ¼ãƒ“ã‚¹ [Co-op Translator](https://github.com/Azure/co-op-translator) ã‚’ä½¿ç”¨ã—ã¦ç¿»è¨³ã•ã‚Œã¦ã„ã¾ã™ã€‚æ­£ç¢ºæ€§ã‚’è¿½æ±‚ã—ã¦ãŠã‚Šã¾ã™ãŒã€è‡ªå‹•ç¿»è¨³ã«ã¯èª¤ã‚Šã‚„ä¸æ­£ç¢ºã•ãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’ã”æ‰¿çŸ¥ãŠããã ã•ã„ã€‚å…ƒã®è¨€èªã§è¨˜è¼‰ã•ã‚ŒãŸåŸæ–‡ãŒæ­£å¼ãªæƒ…å ±æºã¨ã¿ãªã•ã‚Œã‚‹ã¹ãã§ã™ã€‚é‡è¦ãªæƒ…å ±ã«ã¤ã„ã¦ã¯ã€å°‚é–€ã®äººé–“ã«ã‚ˆã‚‹ç¿»è¨³ã‚’æ¨å¥¨ã—ã¾ã™ã€‚ã“ã®ç¿»è¨³ã®åˆ©ç”¨ã«èµ·å› ã™ã‚‹èª¤è§£ã‚„èª¤èªã«ã¤ã„ã¦ã€å½“ç¤¾ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚