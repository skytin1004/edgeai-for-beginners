<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddaad917d0c16fc3d498a6b4eabc8088",
  "translation_date": "2025-10-08T19:35:44+00:00",
  "source_file": "Workshop/notebooks/quickstart.md",
  "language_code": "ja"
}
-->
# ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ - ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰

## ç›®æ¬¡

- [å‰ææ¡ä»¶](../../../../Workshop/notebooks)
- [åˆæœŸè¨­å®š](../../../../Workshop/notebooks)
- [ã‚»ãƒƒã‚·ãƒ§ãƒ³04: ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ](../../../../Workshop/notebooks)
- [ã‚»ãƒƒã‚·ãƒ§ãƒ³05: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼](../../../../Workshop/notebooks)
- [ã‚»ãƒƒã‚·ãƒ§ãƒ³06: æ„å›³ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](../../../../Workshop/notebooks)
- [ç’°å¢ƒå¤‰æ•°](../../../../Workshop/notebooks)
- [å…±é€šã‚³ãƒãƒ³ãƒ‰](../../../../Workshop/notebooks)

---

## å‰ææ¡ä»¶

### 1. Foundry Localã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

**Windows:**
```bash
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª:**
```bash
foundry --version
```

### 2. Pythonä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
cd Workshop
pip install -r requirements.txt
```

ã¾ãŸã¯å€‹åˆ¥ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:
```bash
pip install foundry-local-sdk openai numpy requests
```

---

## åˆæœŸè¨­å®š

### Foundry Localã‚µãƒ¼ãƒ“ã‚¹ã®é–‹å§‹

**ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹å‰ã«å¿…é ˆ:**

```bash
# Start the service
foundry service start

# Verify it's running
foundry service status
```

æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
```
âœ… Service started successfully
Endpoint: http://localhost:59959
```

### ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ãƒ­ãƒ¼ãƒ‰

ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä½¿ç”¨ã•ã‚Œã¾ã™:

```bash
# Download models (first time only - may take several minutes)
foundry model download phi-4-mini
foundry model download qwen2.5-3b
foundry model download phi-3.5-mini
foundry model download qwen2.5-0.5b

# Load models into memory
foundry model run phi-4-mini
foundry model run qwen2.5-3b
foundry model run phi-3.5-mini
```

### è¨­å®šç¢ºèª

```bash
# List loaded models
foundry model ls

# Check service health
curl http://localhost:59959/v1/models
```

---

## ã‚»ãƒƒã‚·ãƒ§ãƒ³04: ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ

### ç›®çš„
Small Language Models (SLM) ã¨ Large Language Models (LLM) ã®æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚

### ã‚¯ã‚¤ãƒƒã‚¯ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Start service (if not already running)
foundry service start

# Load required models
foundry model run phi-4-mini
foundry model run qwen2.5-3b
```

### ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å®Ÿè¡Œ

1. **é–‹ã** `session04_model_compare.ipynb` ã‚’VS Codeã¾ãŸã¯Jupyterã§
2. **ã‚«ãƒ¼ãƒãƒ«ã‚’å†èµ·å‹•** (Kernel â†’ Restart Kernel)
3. **ã™ã¹ã¦ã®ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ**

### ä¸»ãªè¨­å®š

**ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«:**
- **SLM:** `phi-4-mini` (~4GB RAMã€é«˜é€Ÿ)
- **LLM:** `qwen2.5-3b` (~3GB RAMã€ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–)

**ç’°å¢ƒå¤‰æ•° (ã‚ªãƒ—ã‚·ãƒ§ãƒ³):**
```python
import os
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'
```

### æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›

```
================================================================================
COMPARISON SUMMARY
================================================================================
Alias                Latency(s)      Tokens     Route               
--------------------------------------------------------------------------------
phi-4-mini           1.234           150        chat.completions    
qwen2.5-3b           2.456           180        chat.completions    
================================================================================

ğŸ’¡ SLM is 1.99x faster than LLM for this prompt
```

### ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

**ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨:**
```python
os.environ['SLM_ALIAS'] = 'phi-3.5-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-1.5b'
```

**ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:**
```python
os.environ['COMPARE_PROMPT'] = 'Explain quantum computing in simple terms'
```

### æ¤œè¨¼ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] ã‚»ãƒ«12ã«æ­£ã—ã„ãƒ¢ãƒ‡ãƒ«ãŒè¡¨ç¤ºã•ã‚Œã‚‹ (phi-4-mini, qwen2.5-3b)
- [ ] ã‚»ãƒ«12ã«æ­£ã—ã„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒè¡¨ç¤ºã•ã‚Œã‚‹ (ãƒãƒ¼ãƒˆ59959)
- [ ] ã‚»ãƒ«16ã®è¨ºæ–­ãŒæˆåŠŸã™ã‚‹ (âœ… ã‚µãƒ¼ãƒ“ã‚¹ãŒç¨¼åƒä¸­)
- [ ] ã‚»ãƒ«20ã®äº‹å‰ãƒã‚§ãƒƒã‚¯ãŒæˆåŠŸã™ã‚‹ (ä¸¡ãƒ¢ãƒ‡ãƒ«OK)
- [ ] ã‚»ãƒ«22ã®æ¯”è¼ƒãŒé…å»¶å€¤ã¨ã¨ã‚‚ã«å®Œäº†ã™ã‚‹
- [ ] ã‚»ãƒ«24ã®æ¤œè¨¼ã§ ğŸ‰ ALL CHECKS PASSED! ãŒè¡¨ç¤ºã•ã‚Œã‚‹

### æ‰€è¦æ™‚é–“
- **åˆå›å®Ÿè¡Œ:** 5-10åˆ† (ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å«ã‚€)
- **2å›ç›®ä»¥é™:** 1-2åˆ†

---

## ã‚»ãƒƒã‚·ãƒ§ãƒ³05: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼

### ç›®çš„
Foundry Local SDKã‚’ä½¿ç”¨ã—ã¦ã€è¤‡æ•°ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå”åŠ›ã—ã¦æ´—ç·´ã•ã‚ŒãŸå‡ºåŠ›ã‚’ç”Ÿæˆã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚

### ã‚¯ã‚¤ãƒƒã‚¯ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Start service
foundry service start

# Load models
foundry model run phi-4-mini  # Primary model
foundry model run qwen2.5-7b  # Optional: higher quality editor
```

### ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å®Ÿè¡Œ

1. **é–‹ã** `session05_agents_orchestrator.ipynb`
2. **ã‚«ãƒ¼ãƒãƒ«ã‚’å†èµ·å‹•**
3. **ã™ã¹ã¦ã®ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ**

### ä¸»ãªè¨­å®š

**ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š (ä¸¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«åŒã˜ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨):**
```python
PRIMARY_ALIAS = 'phi-4-mini'
EDITOR_ALIAS = 'phi-4-mini'  # Uses same model
```

**é«˜åº¦ãªè¨­å®š (ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨):**
```python
import os
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'     # Fast for research
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'      # High quality for editing
```

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
User Question
    â†“
Researcher Agent (phi-4-mini)
  â†’ Gathers bullet points
    â†“
Editor Agent (phi-4-mini or qwen2.5-7b)
  â†’ Refines into executive summary
    â†“
Final Output
```

### æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›

```
================================================================================
[Pipeline] Question: Explain why edge AI matters for compliance.
================================================================================

[Stage 1: Research]
Output: â€¢ Edge AI processes data locally, reducing transmission...

[Stage 2: Editorial Refinement]
Output: Executive Summary: Edge AI enhances compliance by keeping data...

[FINAL OUTPUT]
Executive Summary: Edge AI enhances compliance by keeping sensitive data 
on-premises and reduces latency through local processing.

[METADATA]
Models used: {'researcher': 'phi-4-mini', 'editor': 'phi-4-mini'}
```

### æ‹¡å¼µ

**ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¿½åŠ :**
```python
critic = Agent(
    name='Critic',
    system='Review content for accuracy',
    client=client,
    model_id=model_id
)
```

**ãƒãƒƒãƒãƒ†ã‚¹ãƒˆ:**
```python
test_questions = [
    "What are benefits of local AI?",
    "How does RAG improve accuracy?",
]

for q in test_questions:
    result = pipeline(q, verbose=False)
    print(result['final'])
```

### æ‰€è¦æ™‚é–“
- **åˆå›å®Ÿè¡Œ:** 3-5åˆ†
- **2å›ç›®ä»¥é™:** è³ªå•ã”ã¨ã«1-2åˆ†

---

## ã‚»ãƒƒã‚·ãƒ§ãƒ³06: æ„å›³ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ç›®çš„
æ¤œå‡ºã•ã‚ŒãŸæ„å›³ã«åŸºã¥ã„ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å°‚é–€ãƒ¢ãƒ‡ãƒ«ã«ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã—ã¾ã™ã€‚

### ã‚¯ã‚¤ãƒƒã‚¯ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Start service
foundry service start

# Load all routing models (CPU variants recommended)
foundry model run phi-4-mini-cpu
foundry model run qwen2.5-0.5b-cpu
foundry model run phi-3.5-mini-cpu
```

**æ³¨æ„:** ã‚»ãƒƒã‚·ãƒ§ãƒ³06ã¯æœ€å¤§é™ã®äº’æ›æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«CPUãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã—ã¦ã„ã¾ã™ã€‚

### ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å®Ÿè¡Œ

1. **é–‹ã** `session06_models_router.ipynb`
2. **ã‚«ãƒ¼ãƒãƒ«ã‚’å†èµ·å‹•**
3. **ã™ã¹ã¦ã®ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ**

### ä¸»ãªè¨­å®š

**ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚«ã‚¿ãƒ­ã‚° (CPUãƒ¢ãƒ‡ãƒ«):**
```python
CATALOG = {
    'phi-4-mini-cpu': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b-cpu': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini-cpu': {'capabilities':['code','refactor'],'priority':3},
}
```

**ä»£æ›¿ (GPUãƒ¢ãƒ‡ãƒ«):**
```python
# Uncomment GPU catalog in Cell #6 if you have sufficient VRAM (8GB+)
CATALOG = {
    'phi-4-mini': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini': {'capabilities':['code','refactor'],'priority':3},
}
```

### æ„å›³æ¤œå‡º

ãƒ«ãƒ¼ã‚¿ãƒ¼ã¯æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã—ã¦æ„å›³ã‚’æ¤œå‡ºã—ã¾ã™:

| æ„å›³ | ãƒ‘ã‚¿ãƒ¼ãƒ³ä¾‹ | ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å…ˆ |
|------|------------|----------------|
| `code` | "refactor", "implement function" | phi-3.5-mini-cpu |
| `classification` | "categorize", "classify this" | qwen2.5-0.5b-cpu |
| `summarize` | "summarize", "tl;dr" | phi-4-mini-cpu |
| `general` | ãã®ä»–ã™ã¹ã¦ | phi-4-mini-cpu |

### æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›

```
âœ“ Using CPU-optimized models (default configuration)
  Models: phi-4-mini-cpu, qwen2.5-0.5b-cpu, phi-3.5-mini-cpu

Routing prompts to specialized models...
============================================================

Prompt: Refactor this Python function for readability
  Intent: code           | Model: phi-3.5-mini-cpu
  Output: Here's a refactored version...
  Tokens: 156

Prompt: Categorize this email as urgent or normal
  Intent: classification | Model: qwen2.5-0.5b-cpu
  Output: Category: Normal
  Tokens: 45

âœ“ Success! All prompts routed correctly.
```

### ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

**ã‚«ã‚¹ã‚¿ãƒ æ„å›³ã‚’è¿½åŠ :**
```python
import re

# Add to RULES
RULES.append((re.compile('translate|ç¿»è¯‘', re.I), 'translation'))

# Add capability to catalog
CATALOG['phi-4-mini-cpu']['capabilities'].append('translation')
```

**ãƒˆãƒ¼ã‚¯ãƒ³ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–:**
```python
import os
os.environ['SHOW_USAGE'] = '1'
```

### GPUãƒ¢ãƒ‡ãƒ«ã¸ã®åˆ‡ã‚Šæ›¿ãˆ

8GBä»¥ä¸Šã®VRAMãŒã‚ã‚‹å ´åˆ:

1. **ã‚»ãƒ«#6**ã§CPUã‚«ã‚¿ãƒ­ã‚°ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
2. GPUã‚«ã‚¿ãƒ­ã‚°ã‚’ã‚¢ãƒ³ã‚³ãƒ¡ãƒ³ãƒˆ
3. GPUãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰:
   ```bash
   foundry model run phi-4-mini
   foundry model run qwen2.5-0.5b
   foundry model run phi-3.5-mini
   ```
4. ã‚«ãƒ¼ãƒãƒ«ã‚’å†èµ·å‹•ã—ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å†å®Ÿè¡Œ

### æ‰€è¦æ™‚é–“
- **åˆå›å®Ÿè¡Œ:** 5-10åˆ† (ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰)
- **2å›ç›®ä»¥é™:** ãƒ†ã‚¹ãƒˆã”ã¨ã«30-60ç§’

---

## ç’°å¢ƒå¤‰æ•°

### ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š

Jupyter/VS Codeã‚’é–‹å§‹ã™ã‚‹å‰ã«è¨­å®š:

**Windows (ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ):**
```cmd
set FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
set SHOW_USAGE=1
set RETRY_ON_FAIL=1
```

**Windows (PowerShell):**
```powershell
$env:FOUNDRY_LOCAL_ENDPOINT="http://localhost:59959/v1"
$env:SHOW_USAGE="1"
$env:RETRY_ON_FAIL="1"
```

**macOS/Linux:**
```bash
export FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
export SHOW_USAGE=1
export RETRY_ON_FAIL=1
```

### ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…è¨­å®š

ä»»æ„ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®é–‹å§‹æ™‚ã«è¨­å®š:

```python
import os

# Foundry Local configuration
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'

# Model selection
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'

# Agent models
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'

# Debugging
os.environ['SHOW_USAGE'] = '1'       # Show token usage
os.environ['RETRY_ON_FAIL'] = '1'    # Enable retries
os.environ['RETRY_BACKOFF'] = '2.0'  # Retry delay
```

---

## å…±é€šã‚³ãƒãƒ³ãƒ‰

### ã‚µãƒ¼ãƒ“ã‚¹ç®¡ç†

```bash
# Start service
foundry service start

# Check status
foundry service status

# Stop service
foundry service stop

# View logs
foundry service logs
```

### ãƒ¢ãƒ‡ãƒ«ç®¡ç†

```bash
# List all available models in catalog
foundry model catalog

# List loaded models
foundry model ls

# Download a model
foundry model download phi-4-mini

# Load a model
foundry model run phi-4-mini

# Unload a model
foundry model unload phi-4-mini

# Remove a model
foundry model remove phi-4-mini

# Get model info
foundry model info phi-4-mini
```

### ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ

```bash
# Check service health
curl http://localhost:59959/health

# List available models via API
curl http://localhost:59959/v1/models

# Test model completion
curl http://localhost:59959/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role":"user","content":"Hello"}],
    "max_tokens": 50
  }'
```

### è¨ºæ–­ã‚³ãƒãƒ³ãƒ‰

```bash
# Check everything
foundry --version
foundry service status
foundry model ls
foundry device info

# GPU status (NVIDIA)
nvidia-smi

# NPU status (Qualcomm)
foundry device info
```

---

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’é–‹å§‹ã™ã‚‹å‰ã«

1. **ã‚µãƒ¼ãƒ“ã‚¹ãŒç¨¼åƒä¸­ã‹ç¢ºèª:**
   ```bash
   foundry service status
   ```

2. **ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª:**
   ```bash
   foundry model ls
   ```

3. **ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚«ãƒ¼ãƒãƒ«ã‚’å†èµ·å‹•** å†å®Ÿè¡Œã™ã‚‹å ´åˆ
4. **ã™ã¹ã¦ã®å‡ºåŠ›ã‚’ã‚¯ãƒªã‚¢** ã‚¯ãƒªãƒ¼ãƒ³ãªå®Ÿè¡Œã®ãŸã‚

### ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†

1. **ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§CPUãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨** äº’æ›æ€§ã®ãŸã‚
2. **GPUãƒ¢ãƒ‡ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆ** 8GBä»¥ä¸Šã®VRAMãŒã‚ã‚‹å ´åˆã®ã¿
3. **ä»–ã®GPUã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‰ã˜ã‚‹** å®Ÿè¡Œå‰ã«
4. **ã‚µãƒ¼ãƒ“ã‚¹ã‚’ç¨¼åƒçŠ¶æ…‹ã«ä¿ã¤** ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚»ãƒƒã‚·ãƒ§ãƒ³é–“ã§
5. **ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³ã‚’ç›£è¦–** ã‚¿ã‚¹ã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ / nvidia-smiã§

### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

1. **ã¾ãšã‚µãƒ¼ãƒ“ã‚¹ã‚’ç¢ºèª** ã‚³ãƒ¼ãƒ‰ã‚’ãƒ‡ãƒãƒƒã‚°ã™ã‚‹å‰ã«
2. **ã‚«ãƒ¼ãƒãƒ«ã‚’å†èµ·å‹•** å¤ã„è¨­å®šãŒè¡¨ç¤ºã•ã‚Œã‚‹å ´åˆ
3. **è¨ºæ–­ã‚»ãƒ«ã‚’å†å®Ÿè¡Œ** å¤‰æ›´å¾Œ
4. **ãƒ¢ãƒ‡ãƒ«åã‚’ç¢ºèª** ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã¨ä¸€è‡´ã™ã‚‹ã‹
5. **ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒãƒ¼ãƒˆã‚’ç¢ºèª** ã‚µãƒ¼ãƒ“ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã¨ä¸€è‡´ã™ã‚‹ã‹

---

## ã‚¯ã‚¤ãƒƒã‚¯ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹: ãƒ¢ãƒ‡ãƒ«ã‚¨ã‚¤ãƒªã‚¢ã‚¹

### ä¸€èˆ¬çš„ãªãƒ¢ãƒ‡ãƒ«

| ã‚¨ã‚¤ãƒªã‚¢ã‚¹ | ã‚µã‚¤ã‚º | æœ€é©ç”¨é€” | RAM/VRAM | ãƒãƒªã‚¢ãƒ³ãƒˆ |
|------------|--------|----------|----------|------------|
| `phi-4-mini` | ~4B | ä¸€èˆ¬çš„ãªãƒãƒ£ãƒƒãƒˆã€è¦ç´„ | 4-6GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `phi-3.5-mini` | ~3.5B | ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã€ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚° | 3-5GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `qwen2.5-3b` | ~3B | ä¸€èˆ¬çš„ãªã‚¿ã‚¹ã‚¯ã€åŠ¹ç‡çš„ | 3-4GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-1.5b` | ~1.5B | é«˜é€Ÿã€ä½ãƒªã‚½ãƒ¼ã‚¹ | 2-3GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-0.5b` | ~0.5B | åˆ†é¡ã€æœ€å°ãƒªã‚½ãƒ¼ã‚¹ | 1-2GB | `-cpu`, `-cuda-gpu` |

### ãƒãƒªã‚¢ãƒ³ãƒˆå‘½å

- **åŸºæœ¬å** (ä¾‹: `phi-4-mini`): ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«æœ€é©ãªãƒãƒªã‚¢ãƒ³ãƒˆã‚’è‡ªå‹•é¸æŠ
- **`-cpu`**: CPUæœ€é©åŒ–ã€ã©ã“ã§ã‚‚å‹•ä½œ
- **`-cuda-gpu`**: NVIDIA GPUæœ€é©åŒ–ã€8GBä»¥ä¸Šã®VRAMãŒå¿…è¦
- **`-npu`**: Qualcomm NPUæœ€é©åŒ–ã€NPUãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãŒå¿…è¦

**æ¨å¥¨:** åŸºæœ¬å (ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãªã—) ã‚’ä½¿ç”¨ã—ã€Foundry Localã«æœ€é©ãªãƒãƒªã‚¢ãƒ³ãƒˆã‚’è‡ªå‹•é¸æŠã•ã›ã‚‹ã€‚

---

## æˆåŠŸã®æŒ‡æ¨™

ä»¥ä¸‹ãŒç¢ºèªã§ãã‚Œã°æº–å‚™å®Œäº†ã§ã™:

âœ… `foundry service status` ãŒ "running" ã‚’è¡¨ç¤º  
âœ… `foundry model ls` ãŒå¿…è¦ãªãƒ¢ãƒ‡ãƒ«ã‚’è¡¨ç¤º  
âœ… ã‚µãƒ¼ãƒ“ã‚¹ãŒæ­£ã—ã„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½  
âœ… ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãŒ200 OKã‚’è¿”ã™  
âœ… ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯è¨ºæ–­ã‚»ãƒ«ãŒæˆåŠŸ  
âœ… å‡ºåŠ›ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒãªã„  

---

## ãƒ˜ãƒ«ãƒ—ã‚’å¾—ã‚‹

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- **ãƒ¡ã‚¤ãƒ³ãƒªãƒã‚¸ãƒˆãƒª**: https://github.com/microsoft/Foundry-Local
- **Python SDK**: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python
- **CLIãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹**: https://github.com/microsoft/Foundry-Local/blob/main/docs/reference/reference-cli.md
- **ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**: ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã® `troubleshooting.md` ã‚’å‚ç…§

### GitHub Issues
- https://github.com/microsoft/Foundry-Local/issues
- https://github.com/microsoft/edgeai-for-beginners/issues

---

**æœ€çµ‚æ›´æ–°æ—¥:** 2025å¹´10æœˆ8æ—¥  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³:** ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ 2.0

---

**å…è²¬äº‹é …**:  
ã“ã®æ–‡æ›¸ã¯ã€AIç¿»è¨³ã‚µãƒ¼ãƒ“ã‚¹[Co-op Translator](https://github.com/Azure/co-op-translator)ã‚’ä½¿ç”¨ã—ã¦ç¿»è¨³ã•ã‚Œã¦ã„ã¾ã™ã€‚æ­£ç¢ºæ€§ã‚’è¿½æ±‚ã—ã¦ãŠã‚Šã¾ã™ãŒã€è‡ªå‹•ç¿»è¨³ã«ã¯èª¤ã‚Šã‚„ä¸æ­£ç¢ºãªéƒ¨åˆ†ãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å…ƒã®è¨€èªã§è¨˜è¼‰ã•ã‚ŒãŸæ–‡æ›¸ã‚’æ­£å¼ãªæƒ…å ±æºã¨ã—ã¦ãŠè€ƒãˆãã ã•ã„ã€‚é‡è¦ãªæƒ…å ±ã«ã¤ã„ã¦ã¯ã€å°‚é–€ã®äººé–“ã«ã‚ˆã‚‹ç¿»è¨³ã‚’æ¨å¥¨ã—ã¾ã™ã€‚ã“ã®ç¿»è¨³ã®ä½¿ç”¨ã«èµ·å› ã™ã‚‹èª¤è§£ã‚„èª¤è§£é‡ˆã«ã¤ã„ã¦ã€å½“æ–¹ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚