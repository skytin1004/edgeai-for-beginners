<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a534c7d108d29f908a8f9f693d694664",
  "translation_date": "2025-09-24T10:29:38+00:00",
  "source_file": "Module08/05.AIPoweredAgents.md",
  "language_code": "ja"
}
-->
# ã‚»ãƒƒã‚·ãƒ§ãƒ³5: Foundry Localã§AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¿…é€Ÿã«æ§‹ç¯‰

æ³¨: Foundry Localã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ã¯é€²åŒ–ã—ã¦ã„ã¾ã™ã€‚é«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®Ÿè£…ã™ã‚‹å‰ã«ã€æœ€æ–°ã®ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆã§ã‚µãƒãƒ¼ãƒˆçŠ¶æ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

## æ¦‚è¦

Foundry Localã‚’ä½¿ç”¨ã—ã¦ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã€ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ´»ç”¨ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¿…é€Ÿã«ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—åŒ–ã§ãã¾ã™ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã€OpenAIäº’æ›ã®é–¢æ•°å‘¼ã³å‡ºã—ã‚’æ¨™æº–åŒ–ã™ã‚‹ã‹ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è¨­è¨ˆã§ã‚¯ãƒ©ã‚¦ãƒ‰å´ã®Azure AI Agentsã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

> **ğŸ”„ æœ€æ–°SDKã«å¯¾å¿œ**: ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯æœ€æ–°ã®Microsoft Foundry-Localãƒªãƒã‚¸ãƒˆãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆã‚ã›ã¦æ›´æ–°ã•ã‚Œã¦ãŠã‚Šã€`samples/05/`ã®åŒ…æ‹¬çš„ãªå®Ÿè£…ã¨ä¸€è‡´ã—ã¦ã„ã¾ã™ã€‚ä¾‹ã¯ã€æ‰‹å‹•ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä»£ã‚ã‚Šã«æœ€æ–°ã®`foundry-local-sdk`ã¨`OpenAI`ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

**ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ:**
- **å°‚é–€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ**: æ¤œç´¢ã€æ¨è«–ã€å®Ÿè¡Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒãã‚Œãã‚Œç•°ãªã‚‹èƒ½åŠ›ã‚’æŒã¤
- **ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³**: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã‚’ä¼´ã†ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- **æœ€æ–°SDKçµ±åˆ**: `FoundryLocalManager`ã¨OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
- **æœ¬ç•ªå¯¾å¿œ**: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã€ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚’å«ã‚€
- **åŒ…æ‹¬çš„ãªä¾‹**: é«˜åº¦ãªæ©Ÿèƒ½ã‚’å‚™ãˆãŸã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªJupyterãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯

**ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè£…:**
- `samples/05/multi_agent_orchestration.ipynb` - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªä¾‹ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
- `samples/05/agents/specialists.py` - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…
- `samples/05/agents/coordinator.py` - ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒã‚¯

å‚è€ƒè³‡æ–™:
- Foundry Local ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Azure AI Foundry Agents: https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- é–¢æ•°å‘¼ã³å‡ºã—ã‚µãƒ³ãƒ—ãƒ« (Foundry Local samples): https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling

## å­¦ç¿’ç›®æ¨™
- ä¿¡é ¼æ€§ã®é«˜ã„å‹•ä½œã®ãŸã‚ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥ã‚’è¨­è¨ˆã™ã‚‹
- é–¢æ•°å‘¼ã³å‡ºã—ï¼ˆãƒ„ãƒ¼ãƒ«ä½¿ç”¨ï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®Ÿè£…ã™ã‚‹
- ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãŠã‚ˆã³ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼‰ã‚’ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹
- å¯è¦³æ¸¬æ€§ã¨å®‰å…¨æ€§ã‚’è¨ˆç”»ã™ã‚‹

## ãƒ‘ãƒ¼ãƒˆ1: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°

- å³å¯†ãªå½¹å‰²ã€åˆ¶ç´„ã€å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©ã™ã‚‹
- ãƒ­ãƒ¼ã‚«ãƒ«ã¾ãŸã¯ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã§å¿œç­”ã‚’ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹
- ä¸‹æµã®è‡ªå‹•åŒ–ã®ãŸã‚ã«JSONå‡ºåŠ›ã‚’å¼·åˆ¶ã™ã‚‹

## ãƒ‘ãƒ¼ãƒˆ2: é–¢æ•°å‘¼ã³å‡ºã—ï¼ˆæœ€æ–°SDKã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰

```python
# tools.py
import json
from typing import List, Dict, Any

def get_weather(city: str) -> str:
    return f"Weather in {city}: Sunny, 25C"

# Modern tools format for OpenAI API
TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current weather for a city",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {"type": "string", "description": "City name"}
                },
                "required": ["city"]
            }
        }
    }
]
```

```python
# agent.py
from foundry_local import FoundryLocalManager
from openai import OpenAI
import json
from tools import TOOLS, get_weather

# Initialize Foundry Local Manager
alias = "phi-4-mini"
manager = FoundryLocalManager(alias)

# Create OpenAI client using Foundry Local endpoint
client = OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key
)

SYSTEM_PROMPT = "You are a helpful assistant. Use tools when needed."

def process_function_call(messages: List[Dict], tools: List[Dict]) -> str:
    """Process function calling with modern OpenAI API."""
    try:
        response = client.chat.completions.create(
            model=manager.get_model_info(alias).id,
            messages=messages,
            tools=tools,
            tool_choice="auto"
        )
        
        message = response.choices[0].message
        
        if message.tool_calls:
            # Handle function calls
            messages.append(message)
            
            for tool_call in message.tool_calls:
                if tool_call.function.name == "get_weather":
                    args = json.loads(tool_call.function.arguments)
                    result = get_weather(args["city"])
                    
                    # Add function result to messages
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": result
                    })
            
            # Get final response
            final_response = client.chat.completions.create(
                model=manager.get_model_info(alias).id,
                messages=messages
            )
            return final_response.choices[0].message.content
        else:
            return message.content
            
    except Exception as e:
        return f"Error: {str(e)}"

# Example usage
messages = [
    {"role": "system", "content": SYSTEM_PROMPT},
    {"role": "user", "content": "What's the weather in Paris?"}
]

result = process_function_call(messages, TOOLS)
print(result)
```

å®Ÿè¡Œ:
```powershell
# Ensure Foundry Local is running with a model
foundry model run phi-4-mini
python agent.py
```


## ãƒ‘ãƒ¼ãƒˆ3: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰

Foundry Localã®OpenAIäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€ã‚¿ã‚¹ã‚¯ã‚’å°‚é–€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆæ¤œç´¢ã€æ¨è«–ã€å®Ÿè¡Œï¼‰ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ã‚’è¨­è¨ˆã—ã¾ã™ã€‚

ã‚¹ãƒ†ãƒƒãƒ—1) æœ€æ–°SDKã‚’ä½¿ç”¨ã—ã¦å°‚é–€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®šç¾©ã™ã‚‹ï¼ˆ`samples/05/agents/specialists.py`ã‚’å‚ç…§ï¼‰
```python
# agents/specialists.py
from foundry_local import FoundryLocalManager
from openai import OpenAI
from typing import List, Dict, Any

class FoundryClient:
    """Shared client for all specialist agents."""
    
    def __init__(self, model_alias: str = "phi-4-mini"):
        self.client = None
        self.model_name = None
        self.model_alias = model_alias
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize OpenAI client with Foundry Local."""
        try:
            manager = FoundryLocalManager(self.model_alias)
            model_info = manager.get_model_info(self.model_alias)
            
            self.client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            self.model_name = model_info.id
            print(f"âœ… Foundry Local initialized with model: {self.model_name}")
        except Exception as e:
            print(f"âŒ Error initializing Foundry Local: {e}")
            raise
    
    def chat(self, messages: List[Dict[str, str]], max_tokens: int = 300, temperature: float = 0.4) -> str:
        """Send chat completion request to the model."""
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error generating response: {str(e)}"

# Global client instance
_client = FoundryClient()

class RetrievalAgent:
    """Agent specialized in retrieving relevant information from knowledge sources."""
    
    SYSTEM = """You are a specialized retrieval agent. Your job is to extract and retrieve 
    the most relevant information from knowledge sources based on a given query. Focus on key facts, 
    data points, and contextual information that would be useful for decision-making."""
    
    def run(self, query: str) -> str:
        """Retrieve relevant information based on the query."""
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Query: {query}\n\nRetrieve the most relevant key facts, data points, and contextual information that would help answer this query or support decision-making around it."}
        ]
        return _client.chat(messages)

class ReasoningAgent:
    """Agent specialized in step-by-step analysis and reasoning."""
    
    SYSTEM = """You are a specialized reasoning agent. Your job is to analyze inputs 
    step-by-step and produce structured, logical conclusions. Break down complex problems 
    into manageable parts and provide clear reasoning for your conclusions."""
    
    def run(self, context: str, question: str) -> str:
        """Analyze context and question to produce structured conclusions."""
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {question}\n\nAnalyze this step-by-step and provide a structured, logical conclusion with clear reasoning."}
        ]
        return _client.chat(messages, max_tokens=400)

class ExecutionAgent:
    """Agent specialized in creating actionable execution plans."""
    
    SYSTEM = """You are a specialized execution agent. Your job is to transform decisions 
    and conclusions into concrete, actionable steps. Always format your response as valid JSON 
    with an array of action items. Each action should be specific, measurable, and achievable."""
    
    def run(self, decision: str) -> str:
        """Transform decision into actionable steps in JSON format."""
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Decision/Conclusion:\n{decision}\n\nCreate 3-5 specific, actionable steps to implement this decision. Format as JSON with this structure:\n{{\"actions\": [{{\"step\": 1, \"description\": \"...\", \"priority\": \"high/medium/low\", \"timeline\": \"...\"}}]}}"}
        ]
        return _client.chat(messages, max_tokens=400, temperature=0.3)
```

ã‚¹ãƒ†ãƒƒãƒ—2) é«˜åº¦ãªæ©Ÿèƒ½ã‚’å‚™ãˆãŸã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ã‚’æ§‹ç¯‰ã™ã‚‹
```python
# agents/coordinator.py
from .specialists import RetrievalAgent, ReasoningAgent, ExecutionAgent
from typing import Dict, Any
import time
import json

class Coordinator:
    """Multi-agent coordinator that orchestrates specialist agents to handle complex tasks."""
    
    def __init__(self):
        """Initialize the coordinator with specialist agents."""
        self.retrieval = RetrievalAgent()
        self.reasoning = ReasoningAgent()
        self.execution = ExecutionAgent()
    
    def handle(self, user_goal: str) -> Dict[str, Any]:
        """
        Orchestrate multiple agents to handle a complex user goal.
        
        Args:
            user_goal: The user's high-level goal or request
            
        Returns:
            Dictionary containing the goal, context, decision, and actions
        """
        print(f"ğŸ¯ **Coordinator:** Processing goal: {user_goal}")
        print("=" * 60)
        
        start_time = time.time()
        
        # Step 1: Retrieve relevant context
        print("ğŸ“š **Step 1:** Retrieving context...")
        context = self.retrieval.run(user_goal)
        print(f"   âœ… Context retrieved ({len(context)} chars)")
        
        # Step 2: Analyze and reason about the context
        print("ğŸ§  **Step 2:** Analyzing and reasoning...")
        decision = self.reasoning.run(context, user_goal)
        print(f"   âœ… Analysis completed ({len(decision)} chars)")
        
        # Step 3: Create actionable execution plan
        print("âš¡ **Step 3:** Creating execution plan...")
        actions = self.execution.run(decision)
        print(f"   âœ… Execution plan created ({len(actions)} chars)")
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        result = {
            "goal": user_goal,
            "context": context,
            "decision": decision,
            "actions": actions,
            "agent_flow": ["retrieval", "reasoning", "execution"],
            "processing_time": processing_time,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        print(f"âœ… **Coordination Complete** (â±ï¸ {processing_time:.2f}s)")
        return result
    
    def handle_with_feedback(self, user_goal: str, feedback_rounds: int = 1) -> Dict[str, Any]:
        """
        Handle a goal with multiple feedback rounds for refinement.
        
        Args:
            user_goal: The user's high-level goal or request
            feedback_rounds: Number of feedback rounds to perform
            
        Returns:
            Dictionary containing the refined result
        """
        result = self.handle(user_goal)
        
        for round_num in range(feedback_rounds):
            print(f"\nğŸ”„ **Feedback Round {round_num + 1}:**")
            print("-" * 40)
            
            # Use reasoning agent to refine the execution plan
            refinement_prompt = f"""
            Original Goal: {user_goal}
            Current Decision: {result['decision']}
            Current Actions: {result['actions']}
            
            Review the above and suggest improvements or refinements to make the execution plan more effective.
            """
            
            refined_decision = self.reasoning.run(result['context'], refinement_prompt)
            refined_actions = self.execution.run(refined_decision)
            
            result['decision'] = refined_decision
            result['actions'] = refined_actions
            result['refinement_rounds'] = round_num + 1
            
            print(f"   âœ… Round {round_num + 1} refinement completed")
        
        return result

def main():
    """Main function demonstrating the multi-agent coordinator."""
    print("ğŸ¤– **Multi-Agent Coordinator Demo**")
    print("=" * 50)
    
    # Create coordinator
    coord = Coordinator()
    
    # Example goals
    example_goals = [
        "Create a plan to onboard 5 new customers this month",
        "Develop a strategy to improve team productivity by 20%",
        "Design a customer feedback collection system"
    ]
    
    # Process example with feedback
    goal = example_goals[0]
    print(f"ğŸ¯ **Processing Goal:** {goal}")
    print("-" * 50)
    
    try:
        # Basic processing
        result = coord.handle(goal)
        
        # With feedback refinement
        refined_result = coord.handle_with_feedback(goal, feedback_rounds=1)
        
        print("\nğŸ“Š **Final Result:**")
        print("=" * 50)
        print(f"**Goal:** {refined_result['goal']}")
        print(f"**Processing Time:** {refined_result['processing_time']:.2f}s")
        
        # Try to parse actions as JSON
        try:
            actions_json = json.loads(refined_result['actions'])
            print(f"\n**Formatted Actions:**")
            print(json.dumps(actions_json, indent=2))
        except (json.JSONDecodeError, TypeError):
            print(f"\n**Actions:** {refined_result['actions']}")
            
    except Exception as e:
        print(f"âŒ **Error:** {e}")
        print("\nPlease ensure Foundry Local is running with a model loaded.")

if __name__ == "__main__":
    main()
```

ã‚¹ãƒ†ãƒƒãƒ—3) Foundry Localã§æ¤œè¨¼ã—ã€ã‚µãƒ³ãƒ—ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹
```powershell
REM Confirm the local endpoint and model are available
foundry model list
foundry model run phi-4-mini
curl http://localhost:8000/v1/models

REM Run the coordinator from Module08 directory
cd Module08
python -m samples.05.agents.coordinator

REM Or explore the comprehensive Jupyter notebook
jupyter notebook samples/05/multi_agent_orchestration.ipynb
```


> **ğŸ“š ãƒ­ãƒ¼ã‚«ãƒ«ã‚µãƒ³ãƒ—ãƒ«å‚ç…§:**
> - **ãƒ¡ã‚¤ãƒ³å®Ÿè£…**: `samples/05/agents/specialists.py` ãŠã‚ˆã³ `samples/05/agents/coordinator.py`
> - **åŒ…æ‹¬çš„ãªä¾‹**: `samples/05/multi_agent_orchestration.ipynb`
> - **ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †**: `samples/05/README.md`
> 
> **ğŸ”— é–¢é€£Foundry Localã‚µãƒ³ãƒ—ãƒ«:**
> - [é–¢æ•°å‘¼ã³å‡ºã—ã‚µãƒ³ãƒ—ãƒ«](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling)
> - [Hello Foundry Local](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/hello-foundry-local)

ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³:
- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®ãƒªãƒˆãƒ©ã‚¤ã¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å®Ÿè£…ã™ã‚‹
- ä¼šè©±/ã‚¹ãƒ¬ãƒƒãƒ‰çŠ¶æ…‹ã®ãŸã‚ã®å°ã•ãªã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‚¹ãƒˆã‚¢ï¼ˆè¾æ›¸ï¼‰ã‚’è¿½åŠ ã™ã‚‹
- è¤‡æ•°ã®å‘¼ã³å‡ºã—ã‚’é€£é–ã™ã‚‹éš›ã«ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å°å…¥ã™ã‚‹

## ãƒ‘ãƒ¼ãƒˆ4: å¯è¦³æ¸¬æ€§ã¨å®‰å…¨æ€§

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€å¿œç­”ã€ã‚¨ãƒ©ãƒ¼ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§è¿½è·¡ã—ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¹ã‚¿ãƒƒã‚¯å†…ã§ãƒ‡ãƒ¼ã‚¿ã®å¥å…¨æ€§ã‚’ç¢ºä¿ã—ã¾ã™ã€‚

ã‚¹ãƒ†ãƒƒãƒ—1) è»½é‡ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚®ãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

æ³¨: ä»¥ä¸‹ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å®Ÿé¨“ç”¨ã«ãƒ­ãƒ¼ã‚«ãƒ«JSONãƒ­ã‚®ãƒ³ã‚°ã‚’è¡Œã„ãŸã„å ´åˆã¯ã€`infra/obs.py`ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
```python
# infra/obs.py
import time, json, os
from datetime import datetime

LOG_DIR = os.getenv("FOUNDRY_AGENT_LOG_DIR", "./agent_logs")
os.makedirs(LOG_DIR, exist_ok=True)

def log_event(kind: str, payload: dict):
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    path = os.path.join(LOG_DIR, f"{ts}_{kind}.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
```

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ãƒ­ã‚®ãƒ³ã‚°ã‚’çµ±åˆã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰:
```python
# in agents/specialists.py after receiving content
from infra.obs import log_event
# ... inside chat(...)
resp = r.json()
log_event("chat_request", {"endpoint": f"{BASE_URL}/v1/chat/completions"})
log_event("chat_response", resp)
return resp["choices"][0]["message"]["content"]
```

ã‚¹ãƒ†ãƒƒãƒ—2) CLIã‚’ä½¿ç”¨ã—ã¦å¯ç”¨æ€§ã¨åŸºæœ¬çš„ãªãƒ˜ãƒ«ã‚¹ã‚’æ¤œè¨¼ã™ã‚‹
```powershell
REM Ensure Foundry Local is running a model
foundry model list
foundry model run phi-4-mini

REM Validate the OpenAI-compatible endpoint
curl http://localhost:8000/v1/models
```

ã‚¹ãƒ†ãƒƒãƒ—3) æƒ…å ±ã®å‰Šé™¤ã¨PIIã®å¥å…¨æ€§
- ãƒ¢ãƒ‡ãƒ«ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã™ã‚‹å‰ã«ã€æ©Ÿå¯†ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã€é›»è©±ç•ªå·ã€IDï¼‰ã‚’å‰Šé™¤ã¾ãŸã¯ãƒãƒƒã‚·ãƒ¥åŒ–ã™ã‚‹
- ç”Ÿã®ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¯ãƒ‡ãƒã‚¤ã‚¹ä¸Šã«ä¿æŒã—ã€å¿…è¦ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã®ã¿ã‚’æ¸¡ã™

ä¾‹: æƒ…å ±å‰Šé™¤ãƒ˜ãƒ«ãƒ‘ãƒ¼
```python
# infra/redact.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã®ä½¿ç”¨:
```python
from infra.redact import sanitize
# user_goal = sanitize(user_goal)
# context = sanitize(context)
```

ã‚¹ãƒ†ãƒƒãƒ—4) ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‘¼ã³å‡ºã—ã‚’try/exceptã¨æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§ãƒ©ãƒƒãƒ—ã™ã‚‹
- ç¹°ã‚Šè¿”ã—å¤±æ•—ã—ãŸå ´åˆã«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’çŸ­çµ¡ã™ã‚‹

```python
import time

def with_retry(func, retries=3, base_delay=0.5):
    for i in range(retries):
        try:
            return func()
        except Exception as e:
            if i == retries - 1:
                raise
            time.sleep(base_delay * (2 ** i))
```

ã‚¹ãƒ†ãƒƒãƒ—5) ãƒ­ãƒ¼ã‚«ãƒ«ç›£æŸ»ãƒ­ã‚°ã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
- JSONãƒ­ã‚°ã‚’`./agent_logs`ã«ä¿å­˜ã™ã‚‹
- å®šæœŸçš„ã«ãƒ­ã‚°ã‚’åœ§ç¸®ã—ã¦ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹
- ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã«ã‚µãƒãƒªãƒ¼ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ï¼ˆä»¶æ•°ã€å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã€ã‚¨ãƒ©ãƒ¼ç‡ï¼‰

ã‚¹ãƒ†ãƒƒãƒ—6) Microsoft Learnãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¯ãƒ­ã‚¹ãƒã‚§ãƒƒã‚¯
- Foundry Localã¯OpenAIäº’æ›APIã‚’æä¾›ã—ã¾ã™ï¼ˆ`curl /v1/models`ã§æ¤œè¨¼æ¸ˆã¿ï¼‰
- `foundry model run <name>`ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®å¯ç”¨æ€§ã‚’ç¢ºèªã™ã‚‹
- ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆçµ±åˆã¨ã‚µãƒ³ãƒ—ãƒ«ã‚¢ãƒ—ãƒªã®å…¬å¼ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã«å¾“ã†ï¼ˆOpen WebUI/ãƒã‚¦ãƒ„ãƒ¼ï¼‰

å‚è€ƒè³‡æ–™:
- **Foundry Local ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- **Azure AI Agents**: https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- **ãƒ­ãƒ¼ã‚«ãƒ«ã‚µãƒ³ãƒ—ãƒ«**:
  - ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: `Module08/samples/05/multi_agent_orchestration.ipynb`
  - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…: `Module08/samples/05/agents/`
  - ã‚µãƒ³ãƒ—ãƒ«README: `Module08/samples/05/README.md`
- **å…¬å¼Microsoftã‚µãƒ³ãƒ—ãƒ«**:
  - [é–¢æ•°å‘¼ã³å‡ºã—](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling)
  - [Hello Foundry Local](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/hello-foundry-local)
  - [Foundry Local Python SDK](https://github.com/microsoft/Foundry-Local/tree/main/sdk/python)
- **çµ±åˆä¾‹**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
- ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ›ã‚¹ãƒˆå‹ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã«Azure AI Agentsã‚’æ¢ç´¢ã™ã‚‹
- ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚³ãƒã‚¯ã‚¿ï¼ˆMicrosoft Graphã€æ¤œç´¢ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰ã‚’è¿½åŠ ã™ã‚‹

---

