<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6503a980cb3bf2b2de2d2bc4ac6acc4c",
  "translation_date": "2025-09-24T10:30:37+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "ja"
}
-->
# ã‚»ãƒƒã‚·ãƒ§ãƒ³ 1: Foundry Local ã®å§‹ã‚æ–¹

## æ¦‚è¦

Microsoft Foundry Local ã¯ã€Azure AI Foundry ã®æ©Ÿèƒ½ã‚’ç›´æ¥ Windows 11 é–‹ç™ºç’°å¢ƒã«æä¾›ã—ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã‚’ä¿è­·ã—ãªãŒã‚‰ä½é…å»¶ã®AIé–‹ç™ºã‚’å¯èƒ½ã«ã™ã‚‹ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºå‘ã‘ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã¯ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€è¨­å®šã€ãã—ã¦ phiã€qwenã€deepseekã€GPT-OSS-20B ãªã©ã®äººæ°—ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè·µçš„ãªãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚’ç¶²ç¾…ã—ã¾ã™ã€‚

## å­¦ç¿’ç›®æ¨™

ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«ã¯ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™:
- Windows 11 ã« Foundry Local ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—è¨­å®šã™ã‚‹
- CLI ã‚³ãƒãƒ³ãƒ‰ã¨è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹
- æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã‚’ç†è§£ã™ã‚‹
- phiã€qwenã€deepseekã€GPT-OSS-20B ãƒ¢ãƒ‡ãƒ«ã‚’æˆåŠŸè£ã«å®Ÿè¡Œã™ã‚‹
- Foundry Local ã‚’ä½¿ç”¨ã—ã¦åˆã‚ã¦ã®AIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹

## å‰ææ¡ä»¶

### ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶
- **Windows 11**: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ 22H2 ä»¥é™
- **RAM**: æœ€ä½ 16GBã€æ¨å¥¨ 32GB
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: ãƒ¢ãƒ‡ãƒ«ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ã« 50GB ã®ç©ºãå®¹é‡
- **ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢**: NPU ã¾ãŸã¯ GPU å¯¾å¿œãƒ‡ãƒã‚¤ã‚¹æ¨å¥¨ (Copilot+ PC ã¾ãŸã¯ NVIDIA GPU)
- **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**: ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã®é«˜é€Ÿã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ

### é–‹ç™ºç’°å¢ƒ
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion

# Set up Python environment (recommended)
py -m venv .venv
.venv\Scripts\activate

# Install required dependencies
pip install openai foundry-local-sdk
```

## ãƒ‘ãƒ¼ãƒˆ 1: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### ã‚¹ãƒ†ãƒƒãƒ— 1: Foundry Local ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

Winget ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€GitHub ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ Foundry Local ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### ã‚¹ãƒ†ãƒƒãƒ— 2: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®ç¢ºèª

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## ãƒ‘ãƒ¼ãƒˆ 2: CLI ã®ç†è§£

### ã‚³ã‚¢ã‚³ãƒãƒ³ãƒ‰æ§‹é€ 

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## ãƒ‘ãƒ¼ãƒˆ 3: ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ç®¡ç†

Foundry Local ã¯ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## ãƒ‘ãƒ¼ãƒˆ 4: ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®å®Ÿè·µ

### Microsoft Phi ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œ

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### Qwen ãƒ¢ãƒ‡ãƒ«ã®æ“ä½œ

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b-instruct
foundry model run qwen2.5-14b-instruct
```

### DeepSeek ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œ

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-distill-qwen-7b
```

### GPT-OSS-20B ã®å®Ÿè¡Œ

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## ãƒ‘ãƒ¼ãƒˆ 5: åˆã‚ã¦ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ

### ãƒ¢ãƒ€ãƒ³ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ (OpenAI SDK + Foundry Local)

OpenAI SDK ã¨ Foundry Local ã®çµ±åˆã‚’æ´»ç”¨ã—ã€Sample 01 ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¾“ã£ã¦ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å¯¾å¿œã®ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¾ã™ã€‚

```python
# chat_quickstart.py (Sample 01 pattern)
import os
import sys
from openai import OpenAI

try:
    from foundry_local import FoundryLocalManager
    FOUNDRY_SDK_AVAILABLE = True
except ImportError:
    FOUNDRY_SDK_AVAILABLE = False
    print("âš ï¸ Install foundry-local-sdk: pip install foundry-local-sdk")

def create_client():
    """Create OpenAI client with Foundry Local or Azure OpenAI."""
    # Check for Azure OpenAI configuration
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    
    if azure_endpoint and azure_api_key:
        # Azure OpenAI path
        model = os.environ.get("MODEL", "your-deployment-name")
        client = OpenAI(
            base_url=f"{azure_endpoint}/openai",
            api_key=azure_api_key,
            default_query={"api-version": "2024-08-01-preview"},
        )
        print(f"ğŸŒ Using Azure OpenAI with model: {model}")
        return client, model
    
    # Foundry Local path with SDK management
    alias = os.environ.get("MODEL", "phi-4-mini")
    if FOUNDRY_SDK_AVAILABLE:
        try:
            # Use FoundryLocalManager for proper service management
            manager = FoundryLocalManager(alias)
            model_info = manager.get_model_info(alias)
            
            client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            model = model_info.id
            print(f"ğŸ  Using Foundry Local SDK with model: {model}")
            return client, model
        except Exception as e:
            print(f"âš ï¸ Foundry SDK failed ({e}), using manual configuration")
    
    # Fallback to manual configuration
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    api_key = os.environ.get("API_KEY", "")
    model = alias
    
    client = OpenAI(
        base_url=f"{base_url}/v1",
        api_key=api_key
    )
    print(f"ğŸ”§ Manual configuration with model: {model}")
    return client, model

def main():
    """Main chat function."""
    client, model = create_client()
    
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    conversation_history = []
    
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        
        try:
            # Add user message to history
            conversation_history.append({"role": "user", "content": user_input})
            
            # Create chat completion
            response = client.chat.completions.create(
                model=model,
                messages=conversation_history,
                max_tokens=500,
                temperature=0.7
            )
            
            assistant_message = response.choices[0].message.content
            conversation_history.append({"role": "assistant", "content": assistant_message})
            
            print(f"Assistant: {assistant_message}\n")
            
        except Exception as e:
            print(f"Error: {e}\n")

if __name__ == "__main__":
    main()
```

### ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œ

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Option 1: Using FoundryLocalManager (recommended)
python chat_quickstart.py "Explain what Foundry Local is"

# Option 2: Manual configuration with environment variables
set BASE_URL=http://localhost:8000
set MODEL=phi-4-mini
set API_KEY=
python chat_quickstart.py "Write a welcome message"

# Option 3: Azure OpenAI configuration
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name
python chat_quickstart.py "Hello from Azure OpenAI"
```

## ãƒ‘ãƒ¼ãƒˆ 6: ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–

```powershell
# Issue: "Could not use Foundry SDK" warning
pip install foundry-local-sdk
# Or set environment variables for manual configuration

# Issue: Connection refused
foundry service status
foundry service ps  # Check loaded models

# Issue: Model not found
foundry model list
foundry model run phi-4-mini

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal

# Test API endpoint
curl http://localhost:8000/v1/models
```

### ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®ç›£è¦– (Windows)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### ç’°å¢ƒå¤‰æ•°

| å¤‰æ•° | èª¬æ˜ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | å¿…é ˆ |
|------|------|------------|------|
| `MODEL` | ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¾ãŸã¯åå‰ | `phi-4-mini` | ã„ã„ãˆ |
| `BASE_URL` | Foundry Local ã®ãƒ™ãƒ¼ã‚¹ URL | `http://localhost:8000` | ã„ã„ãˆ |
| `API_KEY` | API ã‚­ãƒ¼ (é€šå¸¸ãƒ­ãƒ¼ã‚«ãƒ«ã§ã¯ä¸è¦) | `""` | ã„ã„ãˆ |
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ | - | Azure ç”¨ |
| `AZURE_OPENAI_API_KEY` | Azure OpenAI API ã‚­ãƒ¼ | - | Azure ç”¨ |
| `AZURE_OPENAI_API_VERSION` | Azure API ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | `2024-08-01-preview` | ã„ã„ãˆ |

### ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

- **OpenAI SDK ã‚’ä½¿ç”¨**: ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ€§å‘ä¸Šã®ãŸã‚ã«ç”Ÿã® HTTP ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ˆã‚Š OpenAI SDK ã‚’æ¨å¥¨
- **FoundryLocalManager**: ã‚µãƒ¼ãƒ“ã‚¹ç®¡ç†ã«ã¯å…¬å¼ SDK ã‚’ä½¿ç”¨
- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: æœ¬ç•ªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥ã‚’å®Ÿè£…
- **å®šæœŸçš„ãªã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰**: æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚„ä¿®æ­£ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã« Foundry Local ã‚’æœ€æ–°ã«ä¿ã¤
- **å°è¦æ¨¡ã‹ã‚‰å§‹ã‚ã‚‹**: å°å‹ãƒ¢ãƒ‡ãƒ« (Phi mini, Qwen 7B) ã‹ã‚‰å§‹ã‚ã¦ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
- **ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„è¨­å®šã‚’èª¿æ•´ã—ãªãŒã‚‰ CPU/GPU/ãƒ¡ãƒ¢ãƒªã‚’è¿½è·¡

## ãƒ‘ãƒ¼ãƒˆ 7: å®Ÿè·µæ¼”ç¿’

### æ¼”ç¿’ 1: ã‚¯ã‚¤ãƒƒã‚¯ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b-instruct"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### æ¼”ç¿’ 2: OpenAI SDK çµ±åˆãƒ†ã‚¹ãƒˆ

```python
# sdk_integration_test.py (matching Sample 01 pattern)
import os
from openai import OpenAI
from foundry_local import FoundryLocalManager

def test_model_integration(model_alias):
    """Test OpenAI SDK integration with different models."""
    try:
        # Use FoundryLocalManager for proper setup
        manager = FoundryLocalManager(model_alias)
        model_info = manager.get_model_info(model_alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Test basic completion
        response = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Say hello and state your model name."}],
            max_tokens=50
        )
        
        print(f"âœ… {model_alias}: {response.choices[0].message.content}")
        return True
    except Exception as e:
        print(f"âŒ {model_alias}: {e}")
        return False

# Test multiple models
models_to_test = ["phi-4-mini", "qwen2.5-7b-instruct"]
for model in models_to_test:
    test_model_integration(model)
```

### æ¼”ç¿’ 3: åŒ…æ‹¬çš„ãªã‚µãƒ¼ãƒ“ã‚¹ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

```python
# health_check.py
from openai import OpenAI
from foundry_local import FoundryLocalManager

def comprehensive_health_check():
    """Perform comprehensive health check of Foundry Local service."""
    try:
        # Initialize with a common model
        manager = FoundryLocalManager("phi-4-mini")
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # 1. Check service connectivity
        models_response = client.models.list()
        available_models = [model.id for model in models_response.data]
        print(f"âœ… Service healthy - {len(available_models)} models available")
        
        # 2. Test each available model
        for model_id in available_models:
            try:
                response = client.chat.completions.create(
                    model=model_id,
                    messages=[{"role": "user", "content": "Test"}],
                    max_tokens=10
                )
                print(f"âœ… {model_id}: Working")
            except Exception as e:
                print(f"âŒ {model_id}: {e}")
        
        return True
    except Exception as e:
        print(f"âŒ Service check failed: {e}")
        return False

comprehensive_health_check()
```

## å‚è€ƒè³‡æ–™

- **Foundry Local ã®å§‹ã‚æ–¹**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- **CLI ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã¨ã‚³ãƒãƒ³ãƒ‰æ¦‚è¦**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- **OpenAI SDK çµ±åˆ**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- **Hugging Face ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- **Microsoft Foundry Local GitHub**: https://github.com/microsoft/Foundry-Local
- **OpenAI Python SDK**: https://github.com/openai/openai-python
- **Sample 01: OpenAI SDK ã‚’ä½¿ç”¨ã—ãŸã‚¯ã‚¤ãƒƒã‚¯ãƒãƒ£ãƒƒãƒˆ**: samples/01/README.md
- **Sample 02: é«˜åº¦ãª SDK çµ±åˆ**: samples/02/README.md

---

