<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-09-30T23:33:31+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "ja"
}
-->
# ã‚»ãƒƒã‚·ãƒ§ãƒ³ 4: Chainlit ã‚’ä½¿ã£ãŸæœ¬ç•ªãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æ§‹ç¯‰

## æ¦‚è¦

ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã¯ã€Chainlit ã¨ Microsoft Foundry Local ã‚’ä½¿ç”¨ã—ã¦æœ¬ç•ªå¯¾å¿œã®ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚AIä¼šè©±ã®ãŸã‚ã®æœ€æ–°ã®ã‚¦ã‚§ãƒ–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½œæˆã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’å®Ÿè£…ã—ã€ã‚¨ãƒ©ãƒ¼å‡¦ç†ã‚„ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“è¨­è¨ˆã‚’å‚™ãˆãŸå …ç‰¢ãªãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å±•é–‹ã™ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚

**æ§‹ç¯‰ã™ã‚‹å†…å®¹:**
- **Chainlit ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª**: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’å‚™ãˆãŸæœ€æ–°ã®ã‚¦ã‚§ãƒ–UI
- **WebGPU ãƒ‡ãƒ¢**: ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼é‡è¦–ã®ãƒ–ãƒ©ã‚¦ã‚¶ãƒ™ãƒ¼ã‚¹æ¨è«–
- **Open WebUI çµ±åˆ**: Foundry Local ã‚’ä½¿ç”¨ã—ãŸãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
- **æœ¬ç•ªãƒ‘ã‚¿ãƒ¼ãƒ³**: ã‚¨ãƒ©ãƒ¼å‡¦ç†ã€ç›£è¦–ã€å±•é–‹æˆ¦ç•¥

## å­¦ç¿’ç›®æ¨™

- Chainlit ã‚’ä½¿ç”¨ã—ã¦æœ¬ç•ªå¯¾å¿œã®ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã‚’å‘ä¸Šã•ã›ã‚‹ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’å®Ÿè£…ã™ã‚‹
- Foundry Local SDK ã®çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¿’å¾—ã™ã‚‹
- é©åˆ‡ãªã‚¨ãƒ©ãƒ¼å‡¦ç†ã¨å„ªé›…ãªåŠ£åŒ–ã‚’é©ç”¨ã™ã‚‹
- ç•°ãªã‚‹ç’°å¢ƒå‘ã‘ã«ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å±•é–‹ãŠã‚ˆã³æ§‹æˆã™ã‚‹
- ä¼šè©±å‹AIã®ãŸã‚ã®æœ€æ–°ã‚¦ã‚§ãƒ–UIãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç†è§£ã™ã‚‹

## å‰ææ¡ä»¶

- **Foundry Local**: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã§ç¨¼åƒä¸­ ([ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¬ã‚¤ãƒ‰](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: 3.10ä»¥é™ã€ä»®æƒ³ç’°å¢ƒæ©Ÿèƒ½ä»˜ã
- **ãƒ¢ãƒ‡ãƒ«**: å°‘ãªãã¨ã‚‚1ã¤ã®ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ (`foundry model run phi-4-mini`)
- **ãƒ–ãƒ©ã‚¦ã‚¶**: WebGPUå¯¾å¿œã®æœ€æ–°ã‚¦ã‚§ãƒ–ãƒ–ãƒ©ã‚¦ã‚¶ (Chrome/Edge)
- **Docker**: Open WebUI çµ±åˆç”¨ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)

## ãƒ‘ãƒ¼ãƒˆ 1: æœ€æ–°ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ç†è§£

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```
User Browser â†â†’ Chainlit UI â†â†’ Python Backend â†â†’ Foundry Local â†â†’ AI Model
      â†“              â†“              â†“              â†“            â†“
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### ä¸»è¦æŠ€è¡“

**Foundry Local SDK ãƒ‘ã‚¿ãƒ¼ãƒ³:**
- `FoundryLocalManager(alias)`: è‡ªå‹•ã‚µãƒ¼ãƒ“ã‚¹ç®¡ç†
- `manager.endpoint` ã¨ `manager.api_key`: æ¥ç¶šè©³ç´°
- `manager.get_model_info(alias).id`: ãƒ¢ãƒ‡ãƒ«è­˜åˆ¥

**Chainlit ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯:**
- `@cl.on_chat_start`: ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–
- `@cl.on_message`: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†
- `cl.Message().stream_token()`: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
- è‡ªå‹•UIç”Ÿæˆã¨WebSocketç®¡ç†

## ãƒ‘ãƒ¼ãƒˆ 2: ãƒ­ãƒ¼ã‚«ãƒ« vs ã‚¯ãƒ©ã‚¦ãƒ‰ã®æ„æ€æ±ºå®šãƒãƒˆãƒªãƒƒã‚¯ã‚¹

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰¹æ€§

| é …ç›® | ãƒ­ãƒ¼ã‚«ãƒ« (Foundry) | ã‚¯ãƒ©ã‚¦ãƒ‰ (Azure OpenAI) |
|------|-------------------|-----------------------|
| **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·** | ğŸš€ 50-200ms (ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸è¦) | â±ï¸ 200-2000ms (ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¾å­˜) |
| **ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼** | ğŸ”’ ãƒ‡ãƒ¼ã‚¿ãŒãƒ‡ãƒã‚¤ã‚¹ã‚’é›¢ã‚Œãªã„ | âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒã‚¯ãƒ©ã‚¦ãƒ‰ã«é€ä¿¡ã•ã‚Œã‚‹ |
| **ã‚³ã‚¹ãƒˆ** | ğŸ’° ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ä»¥å¤–ç„¡æ–™ | ğŸ’¸ ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«èª²é‡‘ |
| **ã‚ªãƒ•ãƒ©ã‚¤ãƒ³** | âœ… ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸è¦ã§å‹•ä½œ | âŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆå¿…é ˆ |
| **ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º** | âš ï¸ ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢åˆ¶é™ã‚ã‚Š | âœ… æœ€å¤§ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ |
| **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°** | âš ï¸ ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ä¾å­˜ | âœ… ç„¡åˆ¶é™ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° |

### ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥ãƒ‘ã‚¿ãƒ¼ãƒ³

**ãƒ­ãƒ¼ã‚«ãƒ«å„ªå…ˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**ã‚¿ã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹ã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## ãƒ‘ãƒ¼ãƒˆ 3: ã‚µãƒ³ãƒ—ãƒ« 04 - Chainlit ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

### ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯è‡ªå‹•çš„ã« `http://localhost:8080` ã§æœ€æ–°ã®ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’é–‹ãã¾ã™ã€‚

### ã‚³ã‚¢å®Ÿè£…

ã‚µãƒ³ãƒ—ãƒ« 04 ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯æœ¬ç•ªå¯¾å¿œã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¤ºã—ã¾ã™:

**è‡ªå‹•ã‚µãƒ¼ãƒ“ã‚¹æ¤œå‡º:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### æ§‹æˆã‚ªãƒ—ã‚·ãƒ§ãƒ³

**ç’°å¢ƒå¤‰æ•°:**

| å¤‰æ•° | èª¬æ˜ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | ä¾‹ |
|------|------|-----------|----|
| `MODEL` | ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Foundry Local ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ | è‡ªå‹•æ¤œå‡º | `http://localhost:51211` |
| `API_KEY` | APIã‚­ãƒ¼ (ãƒ­ãƒ¼ã‚«ãƒ«ã§ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³) | `""` | `your-api-key` |

**é«˜åº¦ãªä½¿ç”¨æ³•:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## ãƒ‘ãƒ¼ãƒˆ 4: Jupyter ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ä½œæˆã¨ä½¿ç”¨

### ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚µãƒãƒ¼ãƒˆã®æ¦‚è¦

ã‚µãƒ³ãƒ—ãƒ« 04 ã«ã¯åŒ…æ‹¬çš„ãª Jupyter ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ (`chainlit_app.ipynb`) ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šä»¥ä¸‹ãŒå¯èƒ½ã§ã™:

- **ğŸ“š æ•™è‚²ã‚³ãƒ³ãƒ†ãƒ³ãƒ„**: ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®å­¦ç¿’è³‡æ–™
- **ğŸ”¬ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªæ¢ç´¢**: ã‚³ãƒ¼ãƒ‰ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦å®Ÿé¨“
- **ğŸ“Š è¦–è¦šçš„ãƒ‡ãƒ¢**: ãƒãƒ£ãƒ¼ãƒˆã€å›³ã€å‡ºåŠ›ã®è¦–è¦šåŒ–
- **ğŸ› ï¸ é–‹ç™ºãƒ„ãƒ¼ãƒ«**: ãƒ†ã‚¹ãƒˆã¨ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½

### ç‹¬è‡ªã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆã™ã‚‹

#### ã‚¹ãƒ†ãƒƒãƒ— 1: Jupyter ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### ã‚¹ãƒ†ãƒƒãƒ— 2: æ–°ã—ã„ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆ

**VS Code ã‚’ä½¿ç”¨:**
1. Module08 ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ VS Code ã‚’é–‹ã
2. `.ipynb` æ‹¡å¼µå­ã®æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ŒFoundry Localã€ã‚«ãƒ¼ãƒãƒ«ã‚’é¸æŠ
4. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿½åŠ ã™ã‚‹ã‚»ãƒ«ã‚’ä½œæˆ

**Jupyter Lab ã‚’ä½¿ç”¨:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯æ§‹é€ ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### ã‚»ãƒ«ã®æ•´ç†

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("âœ… Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªä¾‹ã¨æ¼”ç¿’

#### æ¼”ç¿’ 1: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ§‹æˆãƒ†ã‚¹ãƒˆ

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\nğŸ§ª Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'âœ… Success' if result['status'] == 'ok' else 'âŒ Failed'}")
```

#### æ¼”ç¿’ 2: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("ğŸŒŠ Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\nâœ… Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## ãƒ‘ãƒ¼ãƒˆ 5: WebGPU ãƒ–ãƒ©ã‚¦ã‚¶æ¨è«–ãƒ‡ãƒ¢

### æ¦‚è¦

WebGPU ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€AIãƒ¢ãƒ‡ãƒ«ã‚’ãƒ–ãƒ©ã‚¦ã‚¶å†…ã§ç›´æ¥å®Ÿè¡Œã§ãã€æœ€å¤§é™ã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã¨ã‚¼ãƒ­ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä½“é¨“ã‚’æä¾›ã—ã¾ã™ã€‚ã“ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã¯ ONNX Runtime Web ã‚’ä½¿ç”¨ã—ãŸ WebGPU å®Ÿè¡Œã‚’ç¤ºã—ã¾ã™ã€‚

### ã‚¹ãƒ†ãƒƒãƒ— 1: WebGPU ã‚µãƒãƒ¼ãƒˆã‚’ç¢ºèª

**ãƒ–ãƒ©ã‚¦ã‚¶è¦ä»¶:**
- WebGPU ãŒæœ‰åŠ¹ãª Chrome/Edge 113+
- ç¢ºèª: `chrome://gpu` â†’ ã€ŒWebGPUã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ç¢ºèª
- ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«ã‚ˆã‚‹ç¢ºèª: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### ã‚¹ãƒ†ãƒƒãƒ— 2: WebGPU ãƒ‡ãƒ¢ã‚’ä½œæˆ

ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>ğŸš€ WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = 'âŒ WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'ğŸ” WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('âœ… ONNX Runtime session created with WebGPU');
        log(`ğŸ“Š Input names: ${session.inputNames.join(', ')}`);
        log(`ğŸ“Š Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = 'âœ… WebGPU inference complete!';
        log(`ğŸ¯ Predicted class: ${maxIdx}`);
        log(`ğŸ“ˆ Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `âŒ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### ã‚¹ãƒ†ãƒƒãƒ— 3: ãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## ãƒ‘ãƒ¼ãƒˆ 6: Open WebUI çµ±åˆ

### æ¦‚è¦

Open WebUI ã¯ã€Foundry Local ã® OpenAI äº’æ› API ã«æ¥ç¶šã™ã‚‹ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãª ChatGPT ãƒ©ã‚¤ã‚¯ãªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚

### ã‚¹ãƒ†ãƒƒãƒ— 1: å‰ææ¡ä»¶

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### ã‚¹ãƒ†ãƒƒãƒ— 2: Docker ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— (æ¨å¥¨)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**æ³¨æ„:** `host.docker.internal` ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€Windows ä¸Šã® Docker ã‚³ãƒ³ãƒ†ãƒŠãŒãƒ›ã‚¹ãƒˆãƒã‚·ãƒ³ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚

### ã‚¹ãƒ†ãƒƒãƒ— 3: æ§‹æˆ

1. **ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‹ã:** `http://localhost:3000` ã«ç§»å‹•
2. **åˆæœŸã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—:** ç®¡ç†è€…ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½œæˆ
3. **ãƒ¢ãƒ‡ãƒ«æ§‹æˆ:**
   - è¨­å®š â†’ ãƒ¢ãƒ‡ãƒ« â†’ OpenAI API  
   - ãƒ™ãƒ¼ã‚¹URL: `http://host.docker.internal:51211/v1`
   - APIã‚­ãƒ¼: `foundry-local-key` (ä»»æ„ã®å€¤ã§å¯)
4. **æ¥ç¶šãƒ†ã‚¹ãƒˆ:** ãƒ¢ãƒ‡ãƒ«ãŒãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã«è¡¨ç¤ºã•ã‚Œã‚‹ã¯ãšã§ã™

### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

**ä¸€èˆ¬çš„ãªå•é¡Œ:**

1. **æ¥ç¶šæ‹’å¦:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **ãƒ¢ãƒ‡ãƒ«ãŒè¡¨ç¤ºã•ã‚Œãªã„:**
   - ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª: `foundry model list`
   - APIå¿œç­”ã‚’ç¢ºèª: `curl http://localhost:51211/v1/models`
   - Open WebUI ã‚³ãƒ³ãƒ†ãƒŠã‚’å†èµ·å‹•

## ãƒ‘ãƒ¼ãƒˆ 7: æœ¬ç•ªå±•é–‹ã®è€ƒæ…®äº‹é …

### ç’°å¢ƒæ§‹æˆ

**é–‹ç™ºã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**æœ¬ç•ªå±•é–‹:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### ä¸€èˆ¬çš„ãªãƒãƒ¼ãƒˆå•é¡Œã¨è§£æ±ºç­–

**ãƒãƒ¼ãƒˆ 51211 ã®ç«¶åˆé˜²æ­¢:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–

**ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã®å®Ÿè£…:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## ã¾ã¨ã‚

ã‚»ãƒƒã‚·ãƒ§ãƒ³ 4 ã§ã¯ã€ä¼šè©±å‹AIã®ãŸã‚ã®æœ¬ç•ªå¯¾å¿œ Chainlit ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æ§‹ç¯‰ã«ã¤ã„ã¦å­¦ã³ã¾ã—ãŸã€‚ä»¥ä¸‹ã‚’ç¿’å¾—ã—ã¾ã—ãŸ:

- âœ… **Chainlit ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‘ã‘ã®æœ€æ–°UIã¨ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚µãƒãƒ¼ãƒˆ
- âœ… **Foundry Local çµ±åˆ**: SDK ã®ä½¿ç”¨æ³•ã¨æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³
- âœ… **WebGPU æ¨è«–**: æœ€å¤§é™ã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã‚’å®Ÿç¾ã™ã‚‹ãƒ–ãƒ©ã‚¦ã‚¶ãƒ™ãƒ¼ã‚¹AI
- âœ… **Open WebUI ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®å±•é–‹
- âœ… **æœ¬ç•ªãƒ‘ã‚¿ãƒ¼ãƒ³**: ã‚¨ãƒ©ãƒ¼å‡¦ç†ã€ç›£è¦–ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

ã‚µãƒ³ãƒ—ãƒ« 04 ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€Microsoft Foundry Local ã‚’é€šã˜ã¦ãƒ­ãƒ¼ã‚«ãƒ«AIãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãªãŒã‚‰ã€å„ªã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã‚’æä¾›ã™ã‚‹å …ç‰¢ãªãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

## å‚è€ƒè³‡æ–™

- **[ã‚µãƒ³ãƒ—ãƒ« 04: Chainlit ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³](samples/04/README.md)**: å®Œå…¨ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- **[Chainlit æ•™è‚²ç”¨ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](samples/04/chainlit_app.ipynb)**: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªå­¦ç¿’è³‡æ–™
- **[Foundry Local ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®å®Œå…¨ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- **[Chainlit ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.chainlit.io/)**: å…¬å¼ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- **[Open WebUI çµ±åˆã‚¬ã‚¤ãƒ‰](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: å…¬å¼ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«

---

**å…è²¬äº‹é …**:  
ã“ã®æ–‡æ›¸ã¯ã€AIç¿»è¨³ã‚µãƒ¼ãƒ“ã‚¹ [Co-op Translator](https://github.com/Azure/co-op-translator) ã‚’ä½¿ç”¨ã—ã¦ç¿»è¨³ã•ã‚Œã¦ã„ã¾ã™ã€‚æ­£ç¢ºæ€§ã‚’è¿½æ±‚ã—ã¦ãŠã‚Šã¾ã™ãŒã€è‡ªå‹•ç¿»è¨³ã«ã¯èª¤ã‚Šã‚„ä¸æ­£ç¢ºãªéƒ¨åˆ†ãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å…ƒã®è¨€èªã§è¨˜è¼‰ã•ã‚ŒãŸæ–‡æ›¸ã‚’æ­£å¼ãªæƒ…å ±æºã¨ã—ã¦ãŠè€ƒãˆãã ã•ã„ã€‚é‡è¦ãªæƒ…å ±ã«ã¤ã„ã¦ã¯ã€å°‚é–€ã®äººé–“ã«ã‚ˆã‚‹ç¿»è¨³ã‚’æ¨å¥¨ã—ã¾ã™ã€‚ã“ã®ç¿»è¨³ã®ä½¿ç”¨ã«èµ·å› ã™ã‚‹èª¤è§£ã‚„èª¤è§£ã«ã¤ã„ã¦ã€å½“æ–¹ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚