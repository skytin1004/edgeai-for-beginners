<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-07-22T03:24:18+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "ko"
}
-->
# ì„¹ì…˜ 2: Qwen ëª¨ë¸ íŒ¨ë°€ë¦¬ ê¸°ë³¸ ê°œë…

Qwen ëª¨ë¸ íŒ¨ë°€ë¦¬ëŠ” Alibaba Cloudì˜ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ë° ë©€í‹°ëª¨ë‹¬ AIì— ëŒ€í•œ í¬ê´„ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ ë‚˜íƒ€ë‚´ë©°, ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë¸ë„ ë‹¤ì–‘í•œ ë°°í¬ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Qwen íŒ¨ë°€ë¦¬ê°€ ìœ ì—°í•œ ë°°í¬ ì˜µì…˜ì„ í†µí•´ ê°•ë ¥í•œ AI ê¸°ëŠ¥ì„ ì œê³µí•˜ë©´ì„œë„ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ë°©ë²•ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

## ê°œë°œìë¥¼ ìœ„í•œ ë¦¬ì†ŒìŠ¤

### Hugging Face ëª¨ë¸ ì €ì¥ì†Œ
ì„ íƒëœ Qwen ëª¨ë¸ íŒ¨ë°€ë¦¬ ëª¨ë¸ì€ [Hugging Face](https://huggingface.co/models?search=qwen)ë¥¼ í†µí•´ ì œê³µë˜ë©°, ì´ ëª¨ë¸ì˜ ì¼ë¶€ ë³€í˜•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ë³€í˜•ì„ íƒìƒ‰í•˜ê³ , íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ì— ë§ê²Œ ë¯¸ì„¸ ì¡°ì •í•˜ë©°, ë‹¤ì–‘í•œ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ë°°í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë¡œì»¬ ê°œë°œ ë„êµ¬
ë¡œì»¬ ê°œë°œ ë° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ [Microsoft Foundry Local](https://github.com/microsoft/foundry-local)ì„ ì‚¬ìš©í•˜ì—¬ ê°œë°œ ë¨¸ì‹ ì—ì„œ ìµœì í™”ëœ ì„±ëŠ¥ìœ¼ë¡œ Qwen ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë¬¸ì„œ ë¦¬ì†ŒìŠ¤
- [Qwen ëª¨ë¸ ë¬¸ì„œ](https://huggingface.co/docs/transformers/model_doc/qwen)
- [ì—£ì§€ ë°°í¬ë¥¼ ìœ„í•œ Qwen ëª¨ë¸ ìµœì í™”](https://github.com/microsoft/olive)

## ì†Œê°œ

ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Alibabaì˜ Qwen ëª¨ë¸ íŒ¨ë°€ë¦¬ì™€ ê·¸ ê¸°ë³¸ ê°œë…ì„ íƒêµ¬í•©ë‹ˆë‹¤. Qwen íŒ¨ë°€ë¦¬ì˜ ì§„í™”, Qwen ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ë§Œë“œëŠ” í˜ì‹ ì ì¸ í•™ìŠµ ë°©ë²•ë¡ , íŒ¨ë°€ë¦¬ ë‚´ ì£¼ìš” ë³€í˜•, ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œì˜ ì‹¤ì§ˆì ì¸ ì‘ìš© ì‚¬ë¡€ë¥¼ ë‹¤ë£° ê²ƒì…ë‹ˆë‹¤.

## í•™ìŠµ ëª©í‘œ

ì´ íŠœí† ë¦¬ì–¼ì„ ë§ˆì¹˜ë©´ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- Alibabaì˜ Qwen ëª¨ë¸ íŒ¨ë°€ë¦¬ì˜ ì„¤ê³„ ì² í•™ê³¼ ì§„í™”ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
- ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° í¬ê¸°ì—ì„œ Qwen ëª¨ë¸ì´ ë†’ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì£¼ìš” í˜ì‹ ì„ ì‹ë³„í•©ë‹ˆë‹¤.
- Qwen ëª¨ë¸ì˜ ë‹¤ì–‘í•œ ë³€í˜•ì˜ ì¥ì ê³¼ í•œê³„ë¥¼ ì¸ì‹í•©ë‹ˆë‹¤.
- Qwen ëª¨ë¸ì— ëŒ€í•œ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ì— ì í•©í•œ ë³€í˜•ì„ ì„ íƒí•©ë‹ˆë‹¤.

## í˜„ëŒ€ AI ëª¨ë¸ í™˜ê²½ ì´í•´

AI í™˜ê²½ì€ ì–¸ì–´ ëª¨ë¸ ê°œë°œì— ìˆì–´ ë‹¤ì–‘í•œ ì ‘ê·¼ ë°©ì‹ì„ ì¶”êµ¬í•˜ëŠ” ì—¬ëŸ¬ ì¡°ì§ê³¼ í•¨ê»˜ í¬ê²Œ ë°œì „í•´ ì™”ìŠµë‹ˆë‹¤. ì¼ë¶€ëŠ” ë…ì ì ì¸ íì‡„í˜• ëª¨ë¸ì— ì¤‘ì ì„ ë‘ëŠ” ë°˜ë©´, ë‹¤ë¥¸ ì¼ë¶€ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ì ‘ê·¼ì„±ê³¼ íˆ¬ëª…ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ì „í†µì ì¸ ì ‘ê·¼ ë°©ì‹ì€ ëŒ€ê·œëª¨ ë…ì  ëª¨ë¸(ì˜¤ì§ APIë¥¼ í†µí•´ì„œë§Œ ì ‘ê·¼ ê°€ëŠ¥) ë˜ëŠ” ê¸°ëŠ¥ì´ ë’¤ì²˜ì§ˆ ìˆ˜ ìˆëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì„ í¬í•¨í•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ íŒ¨ëŸ¬ë‹¤ì„ì€ ë°ì´í„°, ë¹„ìš©, ë°°í¬ ìœ ì—°ì„±ì— ëŒ€í•œ í†µì œë¥¼ ìœ ì§€í•˜ë©´ì„œ ê°•ë ¥í•œ AI ê¸°ëŠ¥ì„ ì¶”êµ¬í•˜ëŠ” ì¡°ì§ì— ë„ì „ ê³¼ì œë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ì€ ìµœì²¨ë‹¨ ì„±ëŠ¥ê³¼ ì‹¤ì§ˆì ì¸ ë°°í¬ ê³ ë ¤ ì‚¬í•­ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì•¼ í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.

## ì ‘ê·¼ ê°€ëŠ¥í•œ AI ìš°ìˆ˜ì„±ì˜ ê³¼ì œ

ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ê³ í’ˆì§ˆì˜ ì ‘ê·¼ ê°€ëŠ¥í•œ AIì— ëŒ€í•œ í•„ìš”ì„±ì´ ì ì  ë” ì¤‘ìš”í•´ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ì–‘í•œ ì¡°ì§ì˜ ìš”êµ¬ì— ë§ëŠ” ìœ ì—°í•œ ë°°í¬ ì˜µì…˜, API ë¹„ìš©ì´ ì¤‘ìš”í•œ ë¹„ìš© ì ˆê° êµ¬í˜„, ê¸€ë¡œë²Œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ë‹¤êµ­ì–´ ê¸°ëŠ¥, ì½”ë”© ë° ìˆ˜í•™ê³¼ ê°™ì€ íŠ¹ì • ë„ë©”ì¸ ì „ë¬¸ ì§€ì‹ì´ í•„ìš”í•œ ì‘ìš© í”„ë¡œê·¸ë¨ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì£¼ìš” ë°°í¬ ìš”êµ¬ ì‚¬í•­

í˜„ëŒ€ AI ë°°í¬ëŠ” ì‹¤ì§ˆì ì¸ ì ìš© ê°€ëŠ¥ì„±ì„ ì œí•œí•˜ëŠ” ëª‡ ê°€ì§€ ê¸°ë³¸ ìš”êµ¬ ì‚¬í•­ì— ì§ë©´í•©ë‹ˆë‹¤:

- **ì ‘ê·¼ì„±**: íˆ¬ëª…ì„±ê³¼ ì‚¬ìš©ì ì •ì˜ë¥¼ ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ ê°€ìš©ì„±
- **ë¹„ìš© íš¨ìœ¨ì„±**: ë‹¤ì–‘í•œ ì˜ˆì‚°ì— ë§ëŠ” í•©ë¦¬ì ì¸ ê³„ì‚° ìš”êµ¬ ì‚¬í•­
- **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ ë°°í¬ ì‹œë‚˜ë¦¬ì˜¤ì— ë§ëŠ” ì—¬ëŸ¬ ëª¨ë¸ í¬ê¸°
- **ê¸€ë¡œë²Œ ë„ë‹¬ì„±**: ê°•ë ¥í•œ ë‹¤êµ­ì–´ ë° ë‹¤ë¬¸í™” ê¸°ëŠ¥
- **ì „ë¬¸í™”**: íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ë¥¼ ìœ„í•œ ë„ë©”ì¸ë³„ ë³€í˜•

## Qwen ëª¨ë¸ ì² í•™

Qwen ëª¨ë¸ íŒ¨ë°€ë¦¬ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ì ‘ê·¼ì„±, ë‹¤êµ­ì–´ ê¸°ëŠ¥, ì‹¤ì§ˆì ì¸ ë°°í¬ë¥¼ ìš°ì„ ì‹œí•˜ë©´ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ íŠ¹ì„±ì„ ìœ ì§€í•˜ëŠ” AI ëª¨ë¸ ê°œë°œì— ëŒ€í•œ í¬ê´„ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Qwen ëª¨ë¸ì€ ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°, ê³ í’ˆì§ˆ í•™ìŠµ ë°©ë²•ë¡ , ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ ë„ë©”ì¸ì— ëŒ€í•œ íŠ¹í™”ëœ ë³€í˜•ì„ í†µí•´ ì´ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.

Qwen íŒ¨ë°€ë¦¬ëŠ” ì„±ëŠ¥-íš¨ìœ¨ì„± ìŠ¤í™íŠ¸ëŸ¼ ì „ë°˜ì— ê±¸ì³ ì˜µì…˜ì„ ì œê³µí•˜ë„ë¡ ì„¤ê³„ëœ ë‹¤ì–‘í•œ ì ‘ê·¼ ë°©ì‹ì„ í¬í•¨í•˜ë©°, ëª¨ë°”ì¼ ì¥ì¹˜ì—ì„œ ì—”í„°í”„ë¼ì´ì¦ˆ ì„œë²„ì— ì´ë¥´ê¸°ê¹Œì§€ ë°°í¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ë©´ì„œ ì˜ë¯¸ ìˆëŠ” AI ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ëª©í‘œëŠ” ê³ í’ˆì§ˆ AIì— ëŒ€í•œ ì ‘ê·¼ì„ ë¯¼ì£¼í™”í•˜ë©´ì„œ ë°°í¬ ì„ íƒì˜ ìœ ì—°ì„±ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

### Qwen ì„¤ê³„ì˜ í•µì‹¬ ì›ì¹™

Qwen ëª¨ë¸ì€ ë‹¤ë¥¸ ì–¸ì–´ ëª¨ë¸ íŒ¨ë°€ë¦¬ì™€ ì°¨ë³„í™”ë˜ëŠ” ëª‡ ê°€ì§€ ê¸°ë³¸ ì›ì¹™ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤:

- **ì˜¤í”ˆ ì†ŒìŠ¤ ìš°ì„ **: ì—°êµ¬ ë° ìƒì—…ì  ì‚¬ìš©ì„ ìœ„í•œ ì™„ì „í•œ íˆ¬ëª…ì„±ê³¼ ì ‘ê·¼ì„±
- **í¬ê´„ì  í•™ìŠµ**: ì—¬ëŸ¬ ì–¸ì–´ì™€ ë„ë©”ì¸ì„ ì•„ìš°ë¥´ëŠ” ë°©ëŒ€í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµ
- **í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: ë‹¤ì–‘í•œ ê³„ì‚° ìš”êµ¬ ì‚¬í•­ì— ë§ëŠ” ì—¬ëŸ¬ ëª¨ë¸ í¬ê¸°
- **ì „ë¬¸ì  ìš°ìˆ˜ì„±**: íŠ¹ì • ì‘ì—…ì— ìµœì í™”ëœ ë„ë©”ì¸ë³„ ë³€í˜•

## Qwen íŒ¨ë°€ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì£¼ìš” ê¸°ìˆ 

### ëŒ€ê·œëª¨ í•™ìŠµ

Qwen íŒ¨ë°€ë¦¬ì˜ ì •ì˜ì ì¸ ì¸¡ë©´ ì¤‘ í•˜ë‚˜ëŠ” ëª¨ë¸ ê°œë°œì— íˆ¬ìëœ ë°©ëŒ€í•œ ê·œëª¨ì˜ í•™ìŠµ ë°ì´í„°ì™€ ê³„ì‚° ìì›ì…ë‹ˆë‹¤. Qwen ëª¨ë¸ì€ ìˆ˜ì¡° ê°œì˜ í† í°ì— ê±¸ì¹œ ì‹ ì¤‘í•˜ê²Œ ì„ ë³„ëœ ë‹¤êµ­ì–´ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ í¬ê´„ì ì¸ ì„¸ê³„ ì§€ì‹ê³¼ ì¶”ë¡  ëŠ¥ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì´ ì ‘ê·¼ ë°©ì‹ì€ ê³ í’ˆì§ˆ ì›¹ ì½˜í…ì¸ , í•™ìˆ  ë¬¸í—Œ, ì½”ë“œ ì €ì¥ì†Œ, ë‹¤êµ­ì–´ ë¦¬ì†ŒìŠ¤ë¥¼ ê²°í•©í•˜ì—¬ ì‘ë™í•©ë‹ˆë‹¤. í•™ìŠµ ë°©ë²•ë¡ ì€ ë‹¤ì–‘í•œ ë„ë©”ì¸ê³¼ ì–¸ì–´ ì „ë°˜ì— ê±¸ì³ ì§€ì‹ì˜ í­ê³¼ ê¹Šì´ë¥¼ ëª¨ë‘ ê°•ì¡°í•©ë‹ˆë‹¤.

### ê³ ê¸‰ ì¶”ë¡  ë° ì‚¬ê³ 

ìµœì‹  Qwen ëª¨ë¸ì€ ë³µì¡í•œ ë‹¤ë‹¨ê³„ ë¬¸ì œ í•´ê²°ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì •êµí•œ ì¶”ë¡  ê¸°ëŠ¥ì„ í†µí•©í•©ë‹ˆë‹¤:

**Thinking Mode (Qwen3)**: ëª¨ë¸ì´ ì¸ê°„ì˜ ë¬¸ì œ í•´ê²° ì ‘ê·¼ ë°©ì‹ê³¼ ìœ ì‚¬í•˜ê²Œ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ê¸° ì „ì— ì„¸ë¶€ì ì¸ ë‹¨ê³„ë³„ ì¶”ë¡ ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Dual-Mode Operation**: ê°„ë‹¨í•œ ì§ˆë¬¸ì— ëŒ€í•œ ë¹ ë¥¸ ì‘ë‹µ ëª¨ë“œì™€ ë³µì¡í•œ ë¬¸ì œì— ëŒ€í•œ ì‹¬ì¸µ ì‚¬ê³  ëª¨ë“œ ê°„ ì „í™˜ ëŠ¥ë ¥.

**Chain-of-Thought Integration**: ë³µì¡í•œ ì‘ì—…ì—ì„œ íˆ¬ëª…ì„±ê³¼ ì •í™•ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ì¶”ë¡  ë‹¨ê³„ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©.

### ì•„í‚¤í…ì²˜ í˜ì‹ 

Qwen íŒ¨ë°€ë¦¬ëŠ” ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì„ ëª¨ë‘ ìœ„í•´ ì„¤ê³„ëœ ì—¬ëŸ¬ ì•„í‚¤í…ì²˜ ìµœì í™”ë¥¼ í†µí•©í•©ë‹ˆë‹¤:

**í™•ì¥ ê°€ëŠ¥í•œ ì„¤ê³„**: ëª¨ë¸ í¬ê¸° ì „ë°˜ì— ê±¸ì³ ì¼ê´€ëœ ì•„í‚¤í…ì²˜ë¡œ í™•ì¥ ë° ë¹„êµ ìš©ì´.

**ë©€í‹°ëª¨ë‹¬ í†µí•©**: í†µí•©ëœ ì•„í‚¤í…ì²˜ ë‚´ì—ì„œ í…ìŠ¤íŠ¸, ë¹„ì „, ì˜¤ë””ì˜¤ ì²˜ë¦¬ ê¸°ëŠ¥ì˜ ì›í™œí•œ í†µí•©.

**ë°°í¬ ìµœì í™”**: ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ êµ¬ì„±ì— ë§ëŠ” ì—¬ëŸ¬ ì–‘ìí™” ì˜µì…˜ ë° ë°°í¬ í˜•ì‹.

## ëª¨ë¸ í¬ê¸° ë° ë°°í¬ ì˜µì…˜

í˜„ëŒ€ ë°°í¬ í™˜ê²½ì€ ë‹¤ì–‘í•œ ê³„ì‚° ìš”êµ¬ ì‚¬í•­ì— ê±¸ì¹œ Qwen ëª¨ë¸ì˜ ìœ ì—°ì„±ìœ¼ë¡œë¶€í„° ì´ì ì„ ì–»ìŠµë‹ˆë‹¤:

### ì†Œí˜• ëª¨ë¸ (0.5B-3B)

Qwenì€ ì—£ì§€ ë°°í¬, ëª¨ë°”ì¼ ì• í”Œë¦¬ì¼€ì´ì…˜, ìì›ì´ ì œí•œëœ í™˜ê²½ì— ì í•©í•œ íš¨ìœ¨ì ì¸ ì†Œí˜• ëª¨ë¸ì„ ì œê³µí•˜ë©°, ë›°ì–´ë‚œ ê¸°ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤.

### ì¤‘í˜• ëª¨ë¸ (7B-32B)

ì¤‘ê°„ í¬ê¸° ëª¨ë¸ì€ ì „ë¬¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ í–¥ìƒëœ ê¸°ëŠ¥ì„ ì œê³µí•˜ë©°, ì„±ëŠ¥ê³¼ ê³„ì‚° ìš”êµ¬ ì‚¬í•­ ê°„ì˜ í›Œë¥­í•œ ê· í˜•ì„ ì œê³µí•©ë‹ˆë‹¤.

### ëŒ€í˜• ëª¨ë¸ (72B ì´ìƒ)

ëŒ€ê·œëª¨ ëª¨ë¸ì€ ìµœëŒ€ ì„±ëŠ¥ì´ í•„ìš”í•œ ì—°êµ¬ ë° ì—”í„°í”„ë¼ì´ì¦ˆ ë°°í¬ë¥¼ ìœ„í•œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## Qwen ëª¨ë¸ íŒ¨ë°€ë¦¬ì˜ ì¥ì 

### ì˜¤í”ˆ ì†ŒìŠ¤ ì ‘ê·¼ì„±

Qwen ëª¨ë¸ì€ ì™„ì „í•œ íˆ¬ëª…ì„±ê³¼ ì‚¬ìš©ì ì •ì˜ ê¸°ëŠ¥ì„ ì œê³µí•˜ì—¬ ì¡°ì§ì´ íŠ¹ì • ìš”êµ¬ ì‚¬í•­ì— ë§ê²Œ ëª¨ë¸ì„ ì´í•´, ìˆ˜ì • ë° ì ì‘í•  ìˆ˜ ìˆë„ë¡ í•˜ë©°, ê³µê¸‰ì—…ì²´ ì¢…ì†ì„±ì„ ë°©ì§€í•©ë‹ˆë‹¤.

### ë°°í¬ ìœ ì—°ì„±

ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°ëŠ” ëª¨ë°”ì¼ ì¥ì¹˜ì—ì„œ ê³ ê¸‰ ì„œë²„ì— ì´ë¥´ê¸°ê¹Œì§€ ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ êµ¬ì„±ì— ê±¸ì³ ë°°í¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ì¡°ì§ì— AI ì¸í”„ë¼ ì„ íƒì˜ ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

### ë‹¤êµ­ì–´ ìš°ìˆ˜ì„±

Qwen ëª¨ë¸ì€ ì˜ì–´ì™€ ì¤‘êµ­ì–´ì—ì„œ íŠ¹íˆ ê°•ì ì„ ë³´ì´ë©°, ìˆ˜ì‹­ ê°œ ì–¸ì–´ë¥¼ ì§€ì›í•˜ëŠ” ë‹¤êµ­ì–´ ì´í•´ ë° ìƒì„±ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ì—¬ ê¸€ë¡œë²Œ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì í•©í•©ë‹ˆë‹¤.

### ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥

Qwen ëª¨ë¸ì€ ì˜¤í”ˆ ì†ŒìŠ¤ ì ‘ê·¼ì„±ì„ ì œê³µí•˜ë©´ì„œë„ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì¼ê´€ë˜ê²Œ ê²½ìŸë ¥ ìˆëŠ” ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ì—¬ ì˜¤í”ˆ ëª¨ë¸ë„ ë…ì  ëª¨ë¸ê³¼ ê²¬ì¤„ ìˆ˜ ìˆìŒì„ ì…ì¦í•©ë‹ˆë‹¤.

### ì „ë¬¸í™”ëœ ê¸°ëŠ¥

Qwen-Coder ë° Qwen-Mathì™€ ê°™ì€ ë„ë©”ì¸ë³„ ë³€í˜•ì€ ì¼ë°˜ ì–¸ì–´ ì´í•´ ê¸°ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ ì „ë¬¸ì ì¸ ì „ë¬¸ ì§€ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.

## ì‹¤ì§ˆì ì¸ ì˜ˆì œ ë° ì‚¬ìš© ì‚¬ë¡€

ê¸°ìˆ ì  ì„¸ë¶€ ì‚¬í•­ì„ ì‚´í´ë³´ê¸° ì „ì— Qwen ëª¨ë¸ì´ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ êµ¬ì²´ì ì¸ ì˜ˆë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:

### ìˆ˜í•™ì  ì¶”ë¡  ì˜ˆì œ

Qwen-MathëŠ” ë‹¨ê³„ë³„ ìˆ˜í•™ ë¬¸ì œ í•´ê²°ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë³µì¡í•œ ë¯¸ì ë¶„ ë¬¸ì œë¥¼ í•´ê²°í•  ë•Œ:

```
User: Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = xÂ³ + 2xÂ² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹

Breaking down each term:
- d/dx(xÂ³) = 3xÂ²
- d/dx(2xÂ²) = 2 Â· 2xÂ¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3xÂ² + 4x - 5
```

### ë‹¤êµ­ì–´ ì§€ì› ì˜ˆì œ

Qwen ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì–¸ì–´ì—ì„œ ê°•ë ¥í•œ ë‹¤êµ­ì–´ ê¸°ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**ä¸­æ–‡:**
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡æœºå™¨ï¼ˆç‰¹åˆ«æ˜¯è®¡ç®—æœºç³»ç»Ÿï¼‰å¯¹äººç±»æ™ºèƒ½è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚è¿™äº›è¿‡ç¨‹åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€è§£å†³é—®é¢˜ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚äººå·¥æ™ºèƒ½ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»è®¤çŸ¥èƒ½åŠ›çš„ä»»åŠ¡ã€‚
```

### ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ ì˜ˆì œ

Qwen-VLì€ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### ì½”ë“œ ìƒì„± ì˜ˆì œ

Qwen-CoderëŠ” ì—¬ëŸ¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì—ì„œ ì½”ë“œ ìƒì„± ë° ì„¤ëª…ì— ë›°ì–´ë‚©ë‹ˆë‹¤:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    ì •ë ¬ëœ ë°°ì—´ì—ì„œ ì´ì§„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì—¬ ëŒ€ìƒ ê°’ì„ ì°¾ìŠµë‹ˆë‹¤.
    
    Args:
        arr (list): ë¹„êµ ê°€ëŠ¥í•œ ìš”ì†Œë¡œ êµ¬ì„±ëœ ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸
        target: ê²€ìƒ‰í•  ê°’
        
    Returns:
        int: ëŒ€ìƒì´ ë°œê²¬ë˜ë©´ ì¸ë±ìŠ¤, ë°œê²¬ë˜ì§€ ì•Šìœ¼ë©´ -1
        
    ì‹œê°„ ë³µì¡ë„: O(log n)
    ê³µê°„ ë³µì¡ë„: O(1)
    """
    # ì™¼ìª½ ë° ì˜¤ë¥¸ìª½ í¬ì¸í„° ì´ˆê¸°í™”
    left, right = 0, len(arr) - 1
    
    # ê²€ìƒ‰ ê³µê°„ì´ ìœ íš¨í•œ ë™ì•ˆ ê³„ì† ê²€ìƒ‰
    while left <= right:
        # ì •ìˆ˜ ì˜¤ë²„í”Œë¡œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì¤‘ê°„ ì¸ë±ìŠ¤ ê³„ì‚°
        mid = left + (right - left) // 2
        
        # ëŒ€ìƒì„ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
        if arr[mid] == target:
            return mid
        
        # ëŒ€ìƒì´ ë” ì‘ìœ¼ë©´ ì™¼ìª½ ì ˆë°˜ ê²€ìƒ‰
        elif arr[mid] > target:
            right = mid - 1
        
        # ëŒ€ìƒì´ ë” í¬ë©´ ì˜¤ë¥¸ìª½ ì ˆë°˜ ê²€ìƒ‰
        else:
            left = mid + 1
    
    # ëŒ€ìƒì´ ë°œê²¬ë˜ì§€ ì•ŠìŒ
    return -1

# ì˜ˆì œ ì‚¬ìš©ë²•:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # ì¶œë ¥: Index of 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### ì—£ì§€ ë°°í¬ ì˜ˆì œ

Qwen ëª¨ë¸ì€ ìµœì í™”ëœ êµ¬ì„±ìœ¼ë¡œ ë‹¤ì–‘í•œ ì—£ì§€ ì¥ì¹˜ì— ë°°í¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Qwen íŒ¨ë°€ë¦¬ì˜ ì§„í™”

### Qwen 1.0 ë° 1.5: ê¸°ì´ˆ ëª¨ë¸

ì´ˆê¸° Qwen ëª¨ë¸ì€ í¬ê´„ì ì¸ í•™ìŠµ ë° ì˜¤í”ˆ ì†ŒìŠ¤ ì ‘ê·¼ì„±ì˜ ê¸°ë³¸ ì›ì¹™ì„ í™•ë¦½í–ˆìŠµë‹ˆë‹¤:

- **Qwen-7B (7B íŒŒë¼ë¯¸í„°)**: ì¤‘êµ­ì–´ ë° ì˜ì–´ ì–¸ì–´ ì´í•´ì— ì¤‘ì ì„ ë‘” ì´ˆê¸° ë¦´ë¦¬ìŠ¤
- **Qwen-14B (14B íŒŒë¼ë¯¸í„°)**: í–¥ìƒëœ ì¶”ë¡  ë° ì§€ì‹ì„ ê°–ì¶˜ ê°œì„ ëœ ê¸°ëŠ¥
- **Qwen-72B (72B íŒŒë¼ë¯¸í„°)**: ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ì œê³µí•˜ëŠ” ëŒ€ê·œëª¨ ëª¨ë¸
- **Qwen1.5 ì‹œë¦¬ì¦ˆ**: í–¥ìƒëœ ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ ê¸°ëŠ¥ì„ ê°–ì¶˜ ë‹¤ì–‘í•œ í¬ê¸°(0.5B~110B)ë¡œ í™•ì¥

### Qwen2 íŒ¨ë°€ë¦¬: ë©€í‹°ëª¨ë‹¬ í™•ì¥

Qwen2 ì‹œë¦¬ì¦ˆëŠ” ì–¸ì–´ ë° ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ ëª¨ë‘ì—ì„œ ì¤‘ìš”í•œ ë°œì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤:

- **Qwen2-0.5B~72B**: ë‹¤ì–‘í•œ ë°°í¬ ìš”êµ¬ë¥¼ ìœ„í•œ í¬ê´„ì ì¸ ì–¸ì–´ ëª¨ë¸ ë²”ìœ„
- **Qwen2-57B-A14B (MoE)**: íš¨ìœ¨ì ì¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©ì„ ìœ„í•œ ì „ë¬¸ê°€ í˜¼í•© ì•„í‚¤í…ì²˜
- **Qwen2-VL**: ì´ë¯¸ì§€ ì´í•´ë¥¼ ìœ„í•œ ê³ ê¸‰ ë¹„ì „-ì–¸ì–´ ê¸°ëŠ¥
- **Qwen2-Audio**: ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë° ì´í•´ ê¸°ëŠ¥
- **Qwen2-Math**: ì „ë¬¸ì ì¸ ìˆ˜í•™ì  ì¶”ë¡  ë° ë¬¸ì œ í•´ê²°

### Qwen2.5 íŒ¨ë°€ë¦¬: ì„±ëŠ¥ í–¥ìƒ

Qwen2.5 ì‹œë¦¬ì¦ˆëŠ” ëª¨ë“  ì°¨ì›ì—ì„œ ìƒë‹¹í•œ ê°œì„ ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤:

- **í™•ì¥ëœ í•™ìŠµ**: 18ì¡° ê°œì˜ í•™ìŠµ ë°ì´í„° í† í°ìœ¼ë¡œ í–¥ìƒëœ ê¸°ëŠ¥
- **í™•ì¥ëœ ë¬¸ë§¥**: ìµœëŒ€ 128K í† í° ë¬¸ë§¥ ê¸¸ì´, Turbo ë³€í˜•ì€ 1M í† í° ì§€ì›
- **í–¥ìƒëœ ì „ë¬¸í™”**: ê°œì„ ëœ Qwen2.5-Coder ë° Qwen2.5-Math ë³€í˜•
- **ë” ë‚˜ì€ ë‹¤êµ­ì–´ ì§€ì›**: 27ê°œ ì´ìƒì˜ ì–¸ì–´ì—ì„œ í–¥ìƒëœ ì„±ëŠ¥

### Qwen3 íŒ¨ë°€ë¦¬: ê³ ê¸‰ ì¶”ë¡ 

ìµœì‹  ì„¸ëŒ€ëŠ” ì¶”ë¡  ë° ì‚¬ê³  ëŠ¥ë ¥ì˜ í•œê³„ë¥¼ í™•ì¥í•©ë‹ˆë‹¤:

- **Qwen3-235B-A22B**: 235B ì´ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ í”Œë˜ê·¸ì‹­ ì „ë¬¸ê°€ í˜¼í•© ëª¨ë¸
- **Qwen3-30B-A3B**: í™œì„± íŒŒë¼ë¯¸í„°ë‹¹ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ê°€ì§„ íš¨ìœ¨ì ì¸ MoE ëª¨ë¸
- **ë°€ì§‘ ëª¨ë¸**: ë‹¤ì–‘í•œ ë°°í¬ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìœ„í•œ Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B
- **Thinking Mode**: ë¹ ë¥¸ ì‘ë‹µê³¼ ì‹¬ì¸µ ì‚¬ê³ ë¥¼ ì§€ì›í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡  ì ‘ê·¼ ë°©ì‹
- **ë‹¤êµ­ì–´ ìš°ìˆ˜ì„±**: 119ê°œ ì–¸ì–´ ë° ë°©ì–¸ ì§€ì›
- **í–¥ìƒëœ í•™ìŠµ**: 36ì¡° ê°œì˜ ë‹¤ì–‘í•œ ê³ í’ˆì§ˆ í•™ìŠµ ë°ì´í„°

## Qwen ëª¨ë¸ì˜ ì‘ìš©

### ì—”í„°í”„ë¼ì´ì¦ˆ ì• í”Œë¦¬ì¼€ì´ì…˜

ì¡°ì§ì€ ë¬¸ì„œ ë¶„ì„, ê³ ê° ì„œë¹„ìŠ¤ ìë™í™”, ì½”ë“œ ìƒì„± ì§€ì›, ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤ ì• í”Œë¦¬ì¼€ì´ì…˜ì— Qwen ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜¤í”ˆ ì†ŒìŠ¤ íŠ¹ì„±ì€ ë°ì´í„° í”„ë¼ì´ë²„ì‹œì™€ í†µì œë¥¼ ìœ ì§€í•˜ë©´ì„œ íŠ¹ì • ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì— ë§ê²Œ ì‚¬ìš©ì ì •ì˜í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

### ëª¨ë°”ì¼ ë° ì—£ì§€ ì»´í“¨íŒ…

ëª¨ë°”ì¼ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ì‹¤ì‹œê°„ ë²ˆì—­, ì§€ëŠ¥í˜• ë¹„ì„œ, ì½˜í…ì¸  ìƒì„±, ê°œì¸í™”ëœ ì¶”ì²œì„ ìœ„í•´ Qwen ëª¨ë¸ì„ í™œìš©í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°ëŠ” ëª¨ë°”ì¼ ì¥ì¹˜ì—ì„œ ì—£ì§€ ì„œë²„ì— ì´ë¥´ê¸°ê¹Œì§€ ë°°í¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

### êµìœ¡ ê¸°ìˆ 

êµìœ¡ í”Œë«í¼ì€ ê°œì¸í™”ëœ íŠœí„°ë§, ìë™ ì½˜í…ì¸  ìƒì„±, ì–¸ì–´ í•™ìŠµ ì§€ì›, ëŒ€í™”í˜• êµìœ¡ ê²½í—˜ì„ ìœ„í•´ Qwen ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Qwen-Mathì™€ ê°™ì€ íŠ¹í™”ëœ ëª¨ë¸ì€ ë„ë©”ì¸ë³„ ì „ë¬¸ ì§€ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.

### ê¸€ë¡œë²Œ ì• í”Œë¦¬ì¼€ì´ì…˜

êµ­ì œ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ Qwen ëª¨ë¸ì˜ ê°•ë ¥í•œ ë‹¤êµ­ì–´ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì–¸ì–´ì™€ ë¬¸í™”ì  ë§¥ë½ì—ì„œ ì¼ê´€ëœ AI ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ê³¼ì œì™€ í•œê³„

### ê³„ì‚° ìš”êµ¬ ì‚¬í•­

Qwenì€ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ëª¨ë¸ì„ ì œê³µí•˜ì§€ë§Œ, ë” í° ë³€í˜•ì€ ìµœì ì˜ ì„±ëŠ¥ì„ ìœ„í•´ ì—¬ì „íˆ ìƒë‹¹í•œ ê³„ì‚° ìì›ì„ í•„ìš”ë¡œ í•˜ë©°, ì´ëŠ” ì¼ë¶€ ì¡°ì§ì˜ ë°°í¬ ì˜µì…˜ì„ ì œí•œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì „ë¬¸ ë„ë©”ì¸ ì„±ëŠ¥

Qwen ëª¨ë¸ì€ ì¼ë°˜ ë„ë©”ì¸ì—ì„œ ì˜ ì‘ë™í•˜ì§€ë§Œ, ê³ ë„ë¡œ ì „ë¬¸í™”ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ë„ë©”ì¸ë³„ ë¯¸ì„¸ ì¡°ì • ë˜ëŠ” íŠ¹í™”ëœ ëª¨ë¸ì—ì„œ ì´ì ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ëª¨ë¸ ì„ íƒì˜ ë³µì¡ì„±

ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ê³¼ ë³€í˜•ì˜ í­ë„“ì€ ë²”ìœ„ëŠ” Qwen ìƒíƒœê³„ì— ìµìˆ™í•˜ì§€ ì•Šì€ ì‚¬ìš©ìì—ê²Œ ì„ íƒì„ ì–´ë µê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì–¸ì–´ ë¶ˆê· í˜•

ë§ì€ ì–¸ì–´ë¥¼ ì§€ì›í•˜ì§€ë§Œ, ì–¸ì–´ë³„ ì„±ëŠ¥ì€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë©°, ì˜ì–´ì™€ ì¤‘êµ­ì–´ì—ì„œ ê°€ì¥ ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## Qwen ëª¨ë¸ íŒ¨ë°€ë¦¬ì˜ ë¯¸ë˜

Qwen ëª¨ë¸ íŒ¨ë°€ë¦¬ëŠ” ë¯¼ì£¼í™”ëœ ê³ í’ˆì§ˆ AIë¡œì˜ ì§€ì†ì ì¸ ì§„í™”ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. í–¥í›„ ê°œë°œì—ëŠ” íš¨ìœ¨ì„± ìµœì í™”, í™•ì¥ëœ ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥, í–¥ìƒëœ ì¶”ë¡  ë©”ì»¤ë‹ˆì¦˜, ë‹¤ì–‘í•œ ë°°í¬ ì‹œë‚˜ë¦¬ì˜¤ ê°„ì˜ ë” ë‚˜ì€ í†µí•©ì´ í¬í•¨ë©ë‹ˆë‹¤.

ê¸°ìˆ ì´ ê³„ì† ë°œì „í•¨ì— ë”°ë¼ Qwen ëª¨ë¸ì€ ì˜¤í”ˆ ì†ŒìŠ¤ ì ‘ê·¼ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ì ì  ë” ê°•ë ¥í•´ì ¸ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì™€ ì‚¬ìš© ì‚¬ë¡€ì— ê±¸ì³ AI ë°°í¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•  ê²ƒì…ë‹ˆë‹¤.

Qwen íŒ¨ë°€ë¦¬ëŠ” ìµœì²¨ë‹¨ ì„±ëŠ¥ê³¼ ì˜¤í”ˆ ì ‘ê·¼ì„±ì„ ëª¨ë‘ í¬ìš©í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ë©°, ì¡°ì§ì— íˆ¬ëª…ì„±ê³¼ í†µì œë¥¼ ìœ ì§€í•˜ë©´ì„œë„ ê°•ë ¥í•œ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ê°œë°œ ë° í†µí•© ì˜ˆì œ

### Transformersë¡œ ë¹ ë¥´ê²Œ ì‹œì‘í•˜ê¸°
Qwen ëª¨ë¸ì„ Hugging Face Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œì‘í•˜ëŠ” ë°©ë²•:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Qwen2.5 ëª¨ë¸ ì‚¬ìš©

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### íŠ¹í™”ëœ ëª¨ë¸ ì‚¬ìš©ë²•

**Qwen-Coderë¥¼ í™œìš©í•œ ì½”ë“œ ìƒì„±:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**ìˆ˜í•™ ë¬¸ì œ í•´ê²°:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**ë¹„ì „-ì–¸ì–´ ì‘ì—…:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### ì‚¬ê³  ëª¨ë“œ (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### ğŸ“± ëª¨ë°”ì¼ ë° ì—£ì§€ ë°°í¬

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### API ë°°í¬ ì˜ˆì œ

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ì„±ê³¼

Qwen ëª¨ë¸ íŒ¨ë°€ë¦¬ëŠ” ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©´ì„œë„ ì˜¤í”ˆ ì†ŒìŠ¤ ì ‘ê·¼ì„±ì„ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤.

### ì£¼ìš” ì„±ëŠ¥ í•˜ì´ë¼ì´íŠ¸

**ì¶”ë¡  ëŠ¥ë ¥ì˜ ìš°ìˆ˜ì„±:**
- Qwen3-235B-A22BëŠ” DeepSeek-R1, o1, o3-mini, Grok-3, Gemini-2.5-Proì™€ ê°™ì€ ìµœìƒìœ„ ëª¨ë¸ê³¼ ë¹„êµí•˜ì—¬ ì½”ë”©, ìˆ˜í•™, ì¼ë°˜ ëŠ¥ë ¥ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ê²°ê³¼ë¥¼ ë‹¬ì„±
- Qwen3-30B-A3BëŠ” í™œì„±í™”ëœ ë§¤ê°œë³€ìˆ˜ê°€ 10ë°° ë§ì€ QwQ-32Bë¥¼ ëŠ¥ê°€
- Qwen3-4BëŠ” Qwen2.5-72B-Instructì˜ ì„±ëŠ¥ì— í•„ì 

**íš¨ìœ¨ì„± ì„±ê³¼:**
- Qwen3-MoE ê¸°ë³¸ ëª¨ë¸ì€ Qwen2.5 ë°€ì§‘ ê¸°ë³¸ ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì´ë©´ì„œ í™œì„± ë§¤ê°œë³€ìˆ˜ì˜ 10%ë§Œ ì‚¬ìš©
- ë°€ì§‘ ëª¨ë¸ì— ë¹„í•´ í›ˆë ¨ ë° ì¶”ë¡  ë¹„ìš© ëŒ€í­ ì ˆê°

**ë‹¤êµ­ì–´ ì§€ì›:**
- Qwen3 ëª¨ë¸ì€ 119ê°œ ì–¸ì–´ì™€ ë°©ì–¸ì„ ì§€ì›
- ë‹¤ì–‘í•œ ì–¸ì–´ì , ë¬¸í™”ì  ë§¥ë½ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ ë°œíœ˜

**í›ˆë ¨ ê·œëª¨:**
- Qwen3ëŠ” ì•½ 36ì¡° ê°œì˜ í† í°ì„ ì‚¬ìš©í•˜ì—¬ 119ê°œ ì–¸ì–´ì™€ ë°©ì–¸ì„ ë‹¤ë£¨ë©°, Qwen2.5ì˜ 18ì¡° ê°œ í† í° ëŒ€ë¹„ ê±°ì˜ ë‘ ë°°ì˜ ê·œëª¨

### ëª¨ë¸ ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤

| ëª¨ë¸ ì‹œë¦¬ì¦ˆ | ë§¤ê°œë³€ìˆ˜ ë²”ìœ„ | ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ | ì£¼ìš” ê°•ì  | ìµœì  ì‚¬ìš© ì‚¬ë¡€ |
|--------------|------------------|----------------|---------------|----------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | ê· í˜• ì¡íŒ ì„±ëŠ¥, ë‹¤êµ­ì–´ ì§€ì› | ì¼ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜, í”„ë¡œë•ì…˜ ë°°í¬ |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | ì½”ë“œ ìƒì„±, í”„ë¡œê·¸ë˜ë° | ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ, ì½”ë”© ì§€ì› |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | ìˆ˜í•™ì  ì¶”ë¡  | êµìœ¡ í”Œë«í¼, STEM ì• í”Œë¦¬ì¼€ì´ì…˜ |
| **Qwen2.5-VL** | ë‹¤ì–‘í•œ | ê°€ë³€ì  | ë¹„ì „-ì–¸ì–´ ì´í•´ | ë©€í‹°ëª¨ë‹¬ ì• í”Œë¦¬ì¼€ì´ì…˜, ì´ë¯¸ì§€ ë¶„ì„ |
| **Qwen3** | 0.6B-235B | ê°€ë³€ì  | ê³ ê¸‰ ì¶”ë¡ , ì‚¬ê³  ëª¨ë“œ | ë³µì¡í•œ ì¶”ë¡ , ì—°êµ¬ ì• í”Œë¦¬ì¼€ì´ì…˜ |
| **Qwen3 MoE** | 30B-235B ì´í•© | ê°€ë³€ì  | íš¨ìœ¨ì ì¸ ëŒ€ê·œëª¨ ì„±ëŠ¥ | ì—”í„°í”„ë¼ì´ì¦ˆ ì• í”Œë¦¬ì¼€ì´ì…˜, ê³ ì„±ëŠ¥ ìš”êµ¬ |

## ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

### ê¸°ë³¸ ì• í”Œë¦¬ì¼€ì´ì…˜ìš©
- **Qwen2.5-0.5B/1.5B**: ëª¨ë°”ì¼ ì•±, ì—£ì§€ ë””ë°”ì´ìŠ¤, ì‹¤ì‹œê°„ ì• í”Œë¦¬ì¼€ì´ì…˜
- **Qwen2.5-3B/7B**: ì¼ë°˜ ì±—ë´‡, ì½˜í…ì¸  ìƒì„±, Q&A ì‹œìŠ¤í…œ

### ìˆ˜í•™ ë° ì¶”ë¡  ì‘ì—…ìš©
- **Qwen2.5-Math**: ìˆ˜í•™ ë¬¸ì œ í•´ê²° ë° STEM êµìœ¡
- **Qwen3 ì‚¬ê³  ëª¨ë“œ**: ë‹¨ê³„ë³„ ë¶„ì„ì´ í•„ìš”í•œ ë³µì¡í•œ ì¶”ë¡ 

### í”„ë¡œê·¸ë˜ë° ë° ê°œë°œìš©
- **Qwen2.5-Coder**: ì½”ë“œ ìƒì„±, ë””ë²„ê¹…, í”„ë¡œê·¸ë˜ë° ì§€ì›
- **Qwen3**: ì¶”ë¡  ëŠ¥ë ¥ì´ í•„ìš”í•œ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì‘ì—…

### ë©€í‹°ëª¨ë‹¬ ì• í”Œë¦¬ì¼€ì´ì…˜ìš©
- **Qwen2.5-VL**: ì´ë¯¸ì§€ ì´í•´, ì‹œê°ì  ì§ˆë¬¸ ì‘ë‹µ
- **Qwen-Audio**: ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë° ìŒì„± ì´í•´

### ì—”í„°í”„ë¼ì´ì¦ˆ ë°°í¬ìš©
- **Qwen2.5-32B/72B**: ê³ ì„±ëŠ¥ ì–¸ì–´ ì´í•´
- **Qwen3-235B-A22B**: ê¹Œë‹¤ë¡œìš´ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ìµœëŒ€ ì„±ëŠ¥

## ë°°í¬ í”Œë«í¼ ë° ì ‘ê·¼ì„±
### í´ë¼ìš°ë“œ í”Œë«í¼
- **Hugging Face Hub**: ì»¤ë®¤ë‹ˆí‹° ì§€ì›ì´ í¬í•¨ëœ ì¢…í•© ëª¨ë¸ ì €ì¥ì†Œ
- **ModelScope**: ìµœì í™” ë„êµ¬ë¥¼ ì œê³µí•˜ëŠ” Alibabaì˜ ëª¨ë¸ í”Œë«í¼
- **ë‹¤ì–‘í•œ í´ë¼ìš°ë“œ ì œê³µì—…ì²´**: í‘œì¤€ ML í”Œë«í¼ì„ í†µí•œ ì§€ì›

### ë¡œì»¬ ê°œë°œ í”„ë ˆì„ì›Œí¬
- **Transformers**: ê°„í¸í•œ ë°°í¬ë¥¼ ìœ„í•œ í‘œì¤€ Hugging Face í†µí•©
- **vLLM**: í”„ë¡œë•ì…˜ í™˜ê²½ì„ ìœ„í•œ ê³ ì„±ëŠ¥ ì„œë¹™
- **Ollama**: ê°„ì†Œí™”ëœ ë¡œì»¬ ë°°í¬ ë° ê´€ë¦¬
- **ONNX Runtime**: ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ë¥¼ ìœ„í•œ í¬ë¡œìŠ¤ í”Œë«í¼ ìµœì í™”
- **llama.cpp**: ë‹¤ì–‘í•œ í”Œë«í¼ì„ ìœ„í•œ íš¨ìœ¨ì ì¸ C++ êµ¬í˜„

### í•™ìŠµ ìë£Œ
- **Qwen ë¬¸ì„œ**: ê³µì‹ ë¬¸ì„œ ë° ëª¨ë¸ ì¹´ë“œ
- **Hugging Face Model Hub**: ì¸í„°ë™í‹°ë¸Œ ë°ëª¨ ë° ì»¤ë®¤ë‹ˆí‹° ì˜ˆì œ
- **ì—°êµ¬ ë…¼ë¬¸**: ì‹¬ì¸µ ì´í•´ë¥¼ ìœ„í•œ arxivì˜ ê¸°ìˆ  ë…¼ë¬¸
- **ì»¤ë®¤ë‹ˆí‹° í¬ëŸ¼**: í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹° ì§€ì› ë° í† ë¡ 

### Qwen ëª¨ë¸ ì‹œì‘í•˜ê¸°

#### ê°œë°œ í”Œë«í¼
1. **Hugging Face Transformers**: í‘œì¤€ Python í†µí•©ìœ¼ë¡œ ì‹œì‘
2. **ModelScope**: Alibabaì˜ ìµœì í™”ëœ ë°°í¬ ë„êµ¬ íƒìƒ‰
3. **ë¡œì»¬ ë°°í¬**: Ollama ë˜ëŠ” ì§ì ‘ Transformersë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ í…ŒìŠ¤íŠ¸

#### í•™ìŠµ ê²½ë¡œ
1. **í•µì‹¬ ê°œë… ì´í•´**: Qwen íŒ¨ë°€ë¦¬ ì•„í‚¤í…ì²˜ì™€ ê¸°ëŠ¥ í•™ìŠµ
2. **ë‹¤ì–‘í•œ ë³€í˜• ì‹¤í—˜**: ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸° ì‹œë„
3. **êµ¬í˜„ ì—°ìŠµ**: ê°œë°œ í™˜ê²½ì—ì„œ ëª¨ë¸ ë°°í¬
4. **ë°°í¬ ìµœì í™”**: í”„ë¡œë•ì…˜ ì‚¬ìš© ì‚¬ë¡€ì— ë§ê²Œ ë¯¸ì„¸ ì¡°ì •

#### ëª¨ë²” ì‚¬ë¡€
- **ì‘ê²Œ ì‹œì‘**: ì´ˆê¸° ê°œë°œì„ ìœ„í•´ ì†Œí˜• ëª¨ë¸(1.5B-7B)ë¡œ ì‹œì‘
- **ì±— í…œí”Œë¦¿ ì‚¬ìš©**: ìµœì ì˜ ê²°ê³¼ë¥¼ ìœ„í•œ ì ì ˆí•œ í¬ë§· ì ìš©
- **ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ì¶”ë¡  ì†ë„ ì¶”ì 
- **íŠ¹í™” ê³ ë ¤**: ì ì ˆí•  ê²½ìš° ë„ë©”ì¸ íŠ¹í™” ë³€í˜• ì„ íƒ

## ê³ ê¸‰ ì‚¬ìš© íŒ¨í„´

### íŒŒì¸íŠœë‹ ì˜ˆì œ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

**ë³µì¡í•œ ì¶”ë¡  ì‘ì—…ìš©:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ ì½”ë“œ ìƒì„±ìš©:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### ë‹¤êµ­ì–´ ì• í”Œë¦¬ì¼€ì´ì…˜

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ä¸­æ–‡)",
        "es": "Spanish (EspaÃ±ol)",
        "fr": "French (FranÃ§ais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (æ—¥æœ¬èª)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ğŸ”§ í”„ë¡œë•ì…˜ ë°°í¬ íŒ¨í„´

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## ì„±ëŠ¥ ìµœì í™” ì „ëµ

### ë©”ëª¨ë¦¬ ìµœì í™”

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### ì¶”ë¡  ìµœì í™”

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## ëª¨ë²” ì‚¬ë¡€ ë° ê°€ì´ë“œë¼ì¸

### ë³´ì•ˆ ë° í”„ë¼ì´ë²„ì‹œ

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### ëª¨ë‹ˆí„°ë§ ë° í‰ê°€

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## ê²°ë¡ 

Qwen ëª¨ë¸ íŒ¨ë°€ë¦¬ëŠ” ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ AI ê¸°ìˆ ì˜ ë¯¼ì£¼í™”ë¥¼ ìœ„í•œ í¬ê´„ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜¤í”ˆ ì†ŒìŠ¤ ì ‘ê·¼ì„±, ë‹¤êµ­ì–´ ì§€ì›, ìœ ì—°í•œ ë°°í¬ ì˜µì…˜ì— ëŒ€í•œ í—Œì‹ ì„ í†µí•´ Qwenì€ ì¡°ì§ê³¼ ê°œë°œìê°€ ìì›ì´ë‚˜ íŠ¹ì • ìš”êµ¬ ì‚¬í•­ì— ê´€ê³„ì—†ì´ ê°•ë ¥í•œ AI ê¸°ëŠ¥ì„ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

### ì£¼ìš” ìš”ì 

**ì˜¤í”ˆ ì†ŒìŠ¤ ìš°ìˆ˜ì„±**: Qwenì€ ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë¸ì´ íˆ¬ëª…ì„±, ë§ì¶¤í™”, ì œì–´ë¥¼ ì œê³µí•˜ë©´ì„œë„ ë…ì  ëŒ€ì•ˆê³¼ ê²½ìŸí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

**í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: 0.5Bì—ì„œ 235B ë§¤ê°œë³€ìˆ˜ê¹Œì§€ì˜ ë²”ìœ„ëŠ” ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ì—ì„œ ì—”í„°í”„ë¼ì´ì¦ˆ í´ëŸ¬ìŠ¤í„°ì— ì´ë¥´ëŠ” ëª¨ë“  ì»´í“¨íŒ… í™˜ê²½ì—ì„œ ë°°í¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

**íŠ¹í™”ëœ ê¸°ëŠ¥**: Qwen-Coder, Qwen-Math, Qwen-VLê³¼ ê°™ì€ ë„ë©”ì¸ íŠ¹í™” ë³€í˜•ì€ ì¼ë°˜ ì–¸ì–´ ì´í•´ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì „ë¬¸ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

**ê¸€ë¡œë²Œ ì ‘ê·¼ì„±**: 119ê°œ ì´ìƒì˜ ì–¸ì–´ë¥¼ ì§€ì›í•˜ëŠ” ê°•ë ¥í•œ ë‹¤êµ­ì–´ ì§€ì›ì€ Qwenì„ êµ­ì œì  ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ ë‹¤ì–‘í•œ ì‚¬ìš©ì ê¸°ë°˜ì— ì í•©í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.

**ì§€ì†ì ì¸ í˜ì‹ **: Qwen 1.0ì—ì„œ Qwen3ë¡œì˜ ì§„í™”ëŠ” ê¸°ëŠ¥, íš¨ìœ¨ì„±, ë°°í¬ ì˜µì…˜ì—ì„œ ì§€ì†ì ì¸ ê°œì„ ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

### ë¯¸ë˜ ì „ë§

Qwen íŒ¨ë°€ë¦¬ê°€ ê³„ì† ë°œì „í•¨ì— ë”°ë¼ ë‹¤ìŒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- **í–¥ìƒëœ íš¨ìœ¨ì„±**: ë” ë‚˜ì€ ì„±ëŠ¥-ë§¤ê°œë³€ìˆ˜ ë¹„ìœ¨ì„ ìœ„í•œ ì§€ì†ì ì¸ ìµœì í™”
- **í™•ì¥ëœ ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥**: ë” ì •êµí•œ ë¹„ì „, ì˜¤ë””ì˜¤, í…ìŠ¤íŠ¸ ì²˜ë¦¬ í†µí•©
- **ê°œì„ ëœ ì¶”ë¡ **: ê³ ê¸‰ ì‚¬ê³  ë©”ì»¤ë‹ˆì¦˜ ë° ë‹¤ë‹¨ê³„ ë¬¸ì œ í•´ê²° ê¸°ëŠ¥
- **ë” ë‚˜ì€ ë°°í¬ ë„êµ¬**: ë‹¤ì–‘í•œ ë°°í¬ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìœ„í•œ í–¥ìƒëœ í”„ë ˆì„ì›Œí¬ ë° ìµœì í™” ë„êµ¬
- **ì»¤ë®¤ë‹ˆí‹° ì„±ì¥**: ë„êµ¬, ì• í”Œë¦¬ì¼€ì´ì…˜, ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬ì˜ í™•ì¥ëœ ìƒíƒœê³„

### ë‹¤ìŒ ë‹¨ê³„

ì±—ë´‡ êµ¬ì¶•, êµìœ¡ ë„êµ¬ ê°œë°œ, ì½”ë”© ì§€ì› ìƒì„±, ë‹¤êµ­ì–´ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‘ì—… ë“± ë¬´ì—‡ì„ í•˜ë“  Qwen íŒ¨ë°€ë¦¬ëŠ” ê°•ë ¥í•œ ì»¤ë®¤ë‹ˆí‹° ì§€ì›ê³¼ í¬ê´„ì ì¸ ë¬¸ì„œë¥¼ ê°–ì¶˜ í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

ìµœì‹  ì—…ë°ì´íŠ¸, ëª¨ë¸ ë¦´ë¦¬ìŠ¤, ìì„¸í•œ ê¸°ìˆ  ë¬¸ì„œë¥¼ ë³´ë ¤ë©´ Hugging Faceì˜ ê³µì‹ Qwen ì €ì¥ì†Œë¥¼ ë°©ë¬¸í•˜ê³  í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹° í† ë¡ ê³¼ ì˜ˆì œë¥¼ íƒìƒ‰í•˜ì„¸ìš”.

AI ê°œë°œì˜ ë¯¸ë˜ëŠ” ëª¨ë“  ë¶€ë¬¸ê³¼ ê·œëª¨ì—ì„œ í˜ì‹ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì ‘ê·¼ ê°€ëŠ¥í•˜ê³  íˆ¬ëª…í•˜ë©° ê°•ë ¥í•œ ë„êµ¬ì— ë‹¬ë ¤ ìˆìŠµë‹ˆë‹¤. Qwen íŒ¨ë°€ë¦¬ëŠ” ì´ ë¹„ì „ì„ êµ¬í˜„í•˜ë©°, ì¡°ì§ê³¼ ê°œë°œìì—ê²Œ ì°¨ì„¸ëŒ€ AI ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ìˆ˜ ìˆëŠ” ê¸°ë°˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ì¶”ê°€ ìë£Œ

- **ê³µì‹ ë¬¸ì„œ**: [Qwen Documentation](https://qwen.readthedocs.io/)
- **ëª¨ë¸ í—ˆë¸Œ**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **ê¸°ìˆ  ë…¼ë¬¸**: [Qwen Research Publications](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **ì»¤ë®¤ë‹ˆí‹°**: [GitHub Discussions and Issues](https://github.com/QwenLM/)
- **ModelScope í”Œë«í¼**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## í•™ìŠµ ê²°ê³¼

ì´ ëª¨ë“ˆì„ ì™„ë£Œí•œ í›„, ë‹¤ìŒì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
1. Qwen ëª¨ë¸ íŒ¨ë°€ë¦¬ì˜ ì•„í‚¤í…ì²˜ì  ì¥ì ê³¼ ì˜¤í”ˆ ì†ŒìŠ¤ ì ‘ê·¼ ë°©ì‹ì„ ì„¤ëª…
2. íŠ¹ì • ì• í”Œë¦¬ì¼€ì´ì…˜ ìš”êµ¬ ì‚¬í•­ ë° ë¦¬ì†ŒìŠ¤ ì œì•½ì— ë”°ë¼ ì í•©í•œ Qwen ë³€í˜• ì„ íƒ
3. ë‹¤ì–‘í•œ ë°°í¬ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ Qwen ëª¨ë¸ êµ¬í˜„ ë° ìµœì í™”ëœ êµ¬ì„± ì‚¬ìš©
4. Qwen ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì–‘ìí™” ë° ìµœì í™” ê¸°ìˆ  ì ìš©
5. Qwen íŒ¨ë°€ë¦¬ ì „ë°˜ì˜ ëª¨ë¸ í¬ê¸°, ì„±ëŠ¥, ê¸°ëŠ¥ ê°„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ í‰ê°€

## ë‹¤ìŒ ë‹¨ê³„

- [03: Gemma Family Fundamentals](03.GemmaFamily.md)

**ë©´ì±… ì¡°í•­**:  
ì´ ë¬¸ì„œëŠ” AI ë²ˆì—­ ì„œë¹„ìŠ¤ [Co-op Translator](https://github.com/Azure/co-op-translator)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤. ì •í™•ì„±ì„ ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ê³  ìˆìœ¼ë‚˜, ìë™ ë²ˆì—­ì—ëŠ” ì˜¤ë¥˜ë‚˜ ë¶€ì •í™•ì„±ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›ë³¸ ë¬¸ì„œë¥¼ í•´ë‹¹ ì–¸ì–´ë¡œ ì‘ì„±ëœ ìƒíƒœì—ì„œ ê¶Œìœ„ ìˆëŠ” ìë£Œë¡œ ê°„ì£¼í•´ì•¼ í•©ë‹ˆë‹¤. ì¤‘ìš”í•œ ì •ë³´ì˜ ê²½ìš°, ì „ë¬¸ì ì¸ ì¸ê°„ ë²ˆì—­ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ì´ ë²ˆì—­ ì‚¬ìš©ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ì˜¤í•´ë‚˜ ì˜ëª»ëœ í•´ì„ì— ëŒ€í•´ ë‹¹ì‚¬ëŠ” ì±…ì„ì„ ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.