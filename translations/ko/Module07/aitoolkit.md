<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "65a22ed38b95f334dd8a893bf2c55806",
  "translation_date": "2025-10-02T11:46:43+00:00",
  "source_file": "Module07/aitoolkit.md",
  "language_code": "ko"
}
-->
# AI Toolkit for Visual Studio Code - 엣지 AI 개발 가이드

## 소개

Visual Studio Code용 AI Toolkit을 활용한 엣지 AI 개발에 대한 종합 가이드에 오신 것을 환영합니다. 인공지능이 중앙 집중식 클라우드 컴퓨팅에서 분산된 엣지 디바이스로 이동함에 따라, 개발자는 자원 제약부터 오프라인 작동 요구사항까지 엣지 배포의 고유한 과제를 처리할 수 있는 강력하고 통합된 도구가 필요합니다.

Visual Studio Code용 AI Toolkit은 엣지 디바이스에서 효율적으로 실행되는 AI 애플리케이션을 구축, 테스트 및 최적화할 수 있는 완전한 개발 환경을 제공하여 이러한 격차를 해소합니다. IoT 센서, 모바일 디바이스, 임베디드 시스템 또는 엣지 서버를 대상으로 개발하든, 이 툴킷은 익숙한 VS Code 환경 내에서 전체 개발 워크플로를 간소화합니다.

이 가이드는 초기 모델 선택부터 프로덕션 배포까지, 엣지 AI 프로젝트에서 AI Toolkit을 활용하기 위한 필수 개념, 도구 및 모범 사례를 안내합니다.

## 개요

Visual Studio Code용 AI Toolkit은 에이전트 개발 및 AI 애플리케이션 생성 과정을 간소화하는 강력한 확장 기능입니다. 이 툴킷은 Anthropic, OpenAI, GitHub, Google을 포함한 다양한 제공업체의 AI 모델을 탐색, 평가 및 배포할 수 있는 포괄적인 기능을 제공하며, ONNX 및 Ollama를 사용한 로컬 모델 실행도 지원합니다.

AI Toolkit의 차별화된 점은 AI 개발 생애 주기 전체를 포괄하는 종합적인 접근 방식입니다. 기존의 AI 개발 도구가 단일 측면에 초점을 맞추는 것과 달리, AI Toolkit은 모델 탐색, 실험, 에이전트 개발, 평가 및 배포를 포함한 통합 환경을 제공하며, 모두 익숙한 VS Code 환경 내에서 이루어집니다.

이 플랫폼은 신속한 프로토타이핑과 프로덕션 배포를 위해 설계되었으며, 프롬프트 생성, 빠른 시작, 원활한 MCP(Model Context Protocol) 도구 통합 및 광범위한 평가 기능과 같은 기능을 제공합니다. 엣지 AI 개발의 경우, VS Code 내에서 전체 개발 워크플로를 유지하면서 엣지 배포 시나리오에 맞는 AI 애플리케이션을 효율적으로 개발, 테스트 및 최적화할 수 있습니다.

## 학습 목표

이 가이드를 완료하면 다음을 수행할 수 있습니다:

### 핵심 역량
- Visual Studio Code용 AI Toolkit을 설치하고 엣지 AI 개발 워크플로에 맞게 구성
- AI Toolkit 인터페이스(모델 카탈로그, 플레이그라운드, 에이전트 빌더)를 탐색하고 활용
- 성능 및 자원 제약을 기반으로 엣지 배포에 적합한 AI 모델 선택 및 평가
- ONNX 형식 및 양자화 기술을 사용하여 엣지 디바이스용 모델 변환 및 최적화

### 엣지 AI 개발 기술
- 통합 개발 환경을 사용하여 엣지 AI 애플리케이션 설계 및 구현
- 로컬 추론 및 자원 모니터링을 통해 엣지와 유사한 조건에서 모델 테스트
- 엣지 배포 시나리오에 최적화된 AI 에이전트 생성 및 커스터마이징
- 엣지 컴퓨팅 관련 지표(지연 시간, 메모리 사용량, 정확도)를 사용하여 모델 성능 평가

### 최적화 및 배포
- 모델 크기를 줄이면서 성능을 유지하기 위한 양자화 및 가지치기 기술 적용
- CPU, GPU, NPU 가속을 포함한 특정 엣지 하드웨어 플랫폼에 모델 최적화
- 자원 관리 및 대체 전략을 포함한 엣지 AI 개발 모범 사례 구현
- 엣지 디바이스에 모델 및 애플리케이션을 프로덕션 배포 준비

### 고급 엣지 AI 개념
- ONNX Runtime, Windows ML, TensorFlow Lite를 포함한 엣지 AI 프레임워크와 통합
- 엣지 환경을 위한 다중 모델 아키텍처 및 연합 학습 시나리오 구현
- 메모리 제약, 추론 속도, 하드웨어 호환성 등 일반적인 엣지 AI 문제 해결
- 프로덕션 엣지 AI 애플리케이션을 위한 모니터링 및 로깅 전략 설계

### 실용적 응용
- 모델 선택부터 배포까지 엔드 투 엔드 엣지 AI 솔루션 구축
- 엣지 특화 개발 워크플로 및 최적화 기술에 대한 숙련도 입증
- IoT, 모바일 및 임베디드 애플리케이션을 포함한 실제 엣지 AI 사용 사례에 학습한 개념 적용
- 다양한 엣지 AI 배포 전략 및 그에 따른 트레이드오프 평가 및 비교

## 엣지 AI 개발을 위한 주요 기능

### 1. 모델 카탈로그 및 탐색
- **다중 제공업체 지원**: Anthropic, OpenAI, GitHub, Google 및 기타 제공업체의 AI 모델 탐색 및 액세스
- **로컬 모델 통합**: 엣지 배포를 위한 ONNX 및 Ollama 모델의 간소화된 탐색
- **GitHub 모델**: GitHub의 모델 호스팅과 직접 통합으로 간소화된 액세스
- **모델 비교**: 엣지 디바이스 제약에 최적화된 균형을 찾기 위한 모델 간 비교

### 2. 인터랙티브 플레이그라운드
- **인터랙티브 테스트 환경**: 제어된 환경에서 모델 기능을 빠르게 실험
- **다중 모달 지원**: 엣지 시나리오에서 일반적인 이미지, 텍스트 및 기타 입력으로 테스트
- **실시간 실험**: 모델 응답 및 성능에 대한 즉각적인 피드백
- **매개변수 최적화**: 엣지 배포 요구사항에 맞는 모델 매개변수 세부 조정

### 3. 프롬프트(에이전트) 빌더
- **자연어 생성**: 자연어 설명을 사용하여 시작 프롬프트 생성
- **반복적 개선**: 모델 응답 및 성능을 기반으로 프롬프트 개선
- **작업 분해**: 프롬프트 체인 및 구조화된 출력을 통해 복잡한 작업 분해
- **변수 지원**: 프롬프트에 변수를 사용하여 동적 에이전트 동작 구현
- **프로덕션 코드 생성**: 신속한 앱 개발을 위한 프로덕션 준비 코드 생성

### 4. 대량 실행 및 평가
- **다중 모델 테스트**: 선택한 모델에서 여러 프롬프트를 동시에 실행
- **효율적인 대규모 테스트**: 다양한 입력 및 구성 효율적으로 테스트
- **맞춤형 테스트 케이스**: 에이전트를 테스트 케이스로 실행하여 기능 검증
- **성능 비교**: 다양한 모델 및 구성 간 결과 비교

### 5. 데이터셋을 활용한 모델 평가
- **표준 지표**: 내장 평가자(F1 점수, 관련성, 유사성, 일관성)를 사용하여 AI 모델 테스트
- **맞춤형 평가자**: 특정 사용 사례에 맞는 평가 지표 생성
- **데이터셋 통합**: 포괄적인 데이터셋을 사용하여 모델 테스트
- **성능 측정**: 엣지 배포 결정을 위한 모델 성능 정량화

### 6. 파인튜닝 기능
- **모델 커스터마이징**: 특정 사용 사례 및 도메인에 맞게 모델 커스터마이징
- **특화된 적응**: 특화된 도메인 및 요구사항에 모델 적응
- **엣지 최적화**: 엣지 배포 제약에 맞게 모델 파인튜닝
- **도메인 특화 학습**: 특정 엣지 사용 사례에 맞춘 모델 생성

### 7. MCP 도구 통합
- **외부 도구 연결성**: Model Context Protocol 서버를 통해 에이전트를 외부 도구에 연결
- **실제 작업 수행**: 에이전트가 데이터베이스를 쿼리하거나 API에 액세스하거나 사용자 정의 로직을 실행할 수 있도록 지원
- **기존 MCP 서버**: 명령(stdio) 또는 HTTP(서버 전송 이벤트) 프로토콜의 도구 사용
- **맞춤형 MCP 개발**: 에이전트 빌더에서 테스트를 통해 새로운 MCP 서버 구축 및 스캐폴딩

### 8. 에이전트 개발 및 테스트
- **함수 호출 지원**: 에이전트가 외부 함수를 동적으로 호출할 수 있도록 지원
- **실시간 통합 테스트**: 실시간 실행 및 도구 사용을 통한 통합 테스트
- **에이전트 버전 관리**: 평가 결과 비교 기능을 갖춘 에이전트 버전 관리
- **디버깅 및 추적**: 에이전트 개발을 위한 로컬 추적 및 디버깅 기능

## 엣지 AI 개발 워크플로

### 1단계: 모델 탐색 및 선택
1. **모델 카탈로그 탐색**: 모델 카탈로그를 사용하여 엣지 배포에 적합한 모델 찾기
2. **성능 비교**: 크기, 정확도 및 추론 속도를 기준으로 모델 평가
3. **로컬 테스트**: Ollama 또는 ONNX 모델을 사용하여 엣지 배포 전에 로컬 테스트
4. **자원 요구사항 평가**: 대상 엣지 디바이스의 메모리 및 계산 요구사항 결정

### 2단계: 모델 최적화
1. **ONNX로 변환**: 선택한 모델을 엣지 호환성을 위해 ONNX 형식으로 변환
2. **양자화 적용**: INT8 또는 INT4 양자화를 통해 모델 크기 축소
3. **하드웨어 최적화**: 대상 엣지 하드웨어(ARM, x86, 특화된 가속기)에 최적화
4. **성능 검증**: 최적화된 모델이 허용 가능한 정확도를 유지하는지 검증

### 3단계: 애플리케이션 개발
1. **에이전트 설계**: 에이전트 빌더를 사용하여 엣지 최적화 AI 에이전트 생성
2. **프롬프트 엔지니어링**: 작은 엣지 모델과 효과적으로 작동하는 프롬프트 개발
3. **통합 테스트**: 시뮬레이션된 엣지 조건에서 에이전트 테스트
4. **코드 생성**: 엣지 배포에 최적화된 프로덕션 코드 생성

### 4단계: 평가 및 테스트
1. **배치 평가**: 최적의 엣지 설정을 찾기 위해 여러 구성 테스트
2. **성능 프로파일링**: 추론 속도, 메모리 사용량 및 정확도 분석
3. **엣지 시뮬레이션**: 대상 엣지 배포 환경과 유사한 조건에서 테스트
4. **스트레스 테스트**: 다양한 부하 조건에서 성능 평가

### 5단계: 배포 준비
1. **최종 최적화**: 테스트 결과를 기반으로 최종 최적화 적용
2. **배포 패키징**: 엣지 배포를 위한 모델 및 코드 패키징
3. **문서화**: 배포 요구사항 및 구성 문서화
4. **모니터링 설정**: 엣지 배포를 위한 모니터링 및 로깅 준비

## 엣지 AI 개발 대상 사용자

### 엣지 AI 개발자
- AI 기반 엣지 디바이스 및 IoT 솔루션을 구축하는 애플리케이션 개발자
- 자원 제약 디바이스에 AI 기능을 통합하는 임베디드 시스템 개발자
- 스마트폰 및 태블릿용 온디바이스 AI 애플리케이션을 개발하는 모바일 개발자

### 엣지 AI 엔지니어
- 엣지 배포를 위해 모델을 최적화하고 추론 파이프라인을 관리하는 AI 엔지니어
- 분산된 엣지 인프라에서 AI 모델을 배포 및 관리하는 DevOps 엔지니어
- 엣지 하드웨어 제약에 맞게 AI 워크로드를 최적화하는 성능 엔지니어

### 연구자 및 교육자
- 엣지 컴퓨팅을 위한 효율적인 모델 및 알고리즘을 개발하는 AI 연구자
- 엣지 AI 개념을 가르치고 최적화 기술을 시연하는 교육자
- 엣지 AI 배포의 과제와 솔루션을 배우는 학생

## 엣지 AI 사용 사례

### 스마트 IoT 디바이스
- **실시간 이미지 인식**: IoT 카메라 및 센서에서 컴퓨터 비전 모델 배포
- **음성 처리**: 스마트 스피커에서 음성 인식 및 자연어 처리 구현
- **예측 유지보수**: 산업 엣지 디바이스에서 이상 탐지 모델 실행
- **환경 모니터링**: 환경 애플리케이션을 위한 센서 데이터 분석 모델 배포

### 모바일 및 임베디드 애플리케이션
- **온디바이스 번역**: 오프라인으로 작동하는 언어 번역 모델 구현
- **증강 현실**: AR 애플리케이션을 위한 실시간 객체 인식 및 추적 배포
- **건강 모니터링**: 웨어러블 디바이스 및 의료 장비에서 건강 분석 모델 실행
- **자율 시스템**: 드론, 로봇 및 차량을 위한 의사 결정 모델 구현

### 엣지 컴퓨팅 인프라
- **엣지 데이터 센터**: 저지연 애플리케이션을 위한 엣지 데이터 센터에서 AI 모델 배포
- **CDN 통합**: 콘텐츠 전송 네트워크에 AI 처리 기능 통합
- **5G 엣지**: AI 기반 애플리케이션을 위한 5G 엣지 컴퓨팅 활용
- **포그 컴퓨팅**: 포그 컴퓨팅 환경에서 AI 처리 구현

## 설치 및 설정

### 확장 설치
Visual Studio Code Marketplace에서 AI Toolkit 확장을 직접 설치하세요:

**확장 ID**: `ms-windows-ai-studio.windows-ai-studio`

**설치 방법**:
1. **VS Code Marketplace**: 확장 보기에서 "AI Toolkit" 검색
2. **명령줄**: `code --install-extension ms-windows-ai-studio.windows-ai-studio`
3. **직접 설치**: [VS Code Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)에서 다운로드

### 엣지 AI 개발을 위한 사전 요구사항
- **Visual Studio Code**: 최신 버전 권장
- **Python 환경**: Python 3.8+ 및 필요한 AI 라이브러리
- **ONNX Runtime**(선택 사항): ONNX 모델 추론용
- **Ollama**(선택 사항): 로컬 모델 제공용
- **하드웨어 가속 도구**: CUDA, OpenVINO 또는 플랫폼별 가속기

### 초기 설정
1. **확장 활성화**: VS Code를 열고 AI Toolkit이 활동 표시줄에 나타나는지 확인
2. **모델 제공업체 설정**: GitHub, OpenAI, Anthropic 또는 기타 모델 제공업체에 대한 액세스 구성
3. **로컬 환경**: Python 환경 설정 및 필요한 패키지 설치
4. **하드웨어 가속**: GPU/NPU 가속이 가능한 경우 구성
5. **MCP 통합**: 필요한 경우 Model Context Protocol 서버 설정

### 첫 설정 체크리스트
- [ ] AI Toolkit 확장 설치 및 활성화 완료
- [ ] 모델 카탈로그 접근 가능 및 모델 탐색 가능
- [ ] 플레이그라운드에서 모델 테스트 가능
- [ ] 에이전트 빌더에서 프롬프트 개발 가능
- [ ] 로컬 개발 환경 구성 완료
- [ ] 하드웨어 가속(가능한 경우) 올바르게 구성

## AI Toolkit 시작하기

### 빠른 시작 가이드

GitHub에서 호스팅하는 모델로 시작하는 것이 가장 간소화된 경험을 제공합니다:

1. **설치**: [설치 가이드](https://code.visualstudio.com/docs/intelligentapps/overview#_install-and-setup)를 따라 AI Toolkit을 디바이스에 설정
2. **모델 탐색**: 확장 트리 보기에서 **CATALOG > Models**를 선택하여 사용 가능한 모델 탐색
3. **GitHub 모델**: GitHub에서 호스팅하는 모델로 시작하여 최적의 통합 경험 제공
4. **플레이그라운드 테스트**: 모델 카드에서 **Try in Playground**를 선택하여 모델 기능 실험 시작

### 단계별 엣지 AI 개발

#### 1단계: 모델 탐색 및 선택
1. VS Code 활동 표시줄에서 AI Toolkit 보기 열기
2. 엣지 배포에 적합한 모델을 찾기 위해 모델 카탈로그 탐색
3. 제공업체(GitHub, ONNX, Ollama)별로 필터링하여 엣지 요구사항에 맞는 모델 선택
4. **Try in Playground**를 사용하여 모델 기능 즉시 테스트

#### 2단계: 에이전트 개발
1. **프롬프트(에이전트) 빌더**를 사용하여 엣지 최적화 AI 에이전트 생성
2. 자연어 설명을 사용하여 시작 프롬프트 생성  
3. 모델 응답을 기반으로 프롬프트를 반복적으로 수정 및 개선  
4. MCP 도구를 통합하여 에이전트 기능 강화  

#### 단계 3: 테스트 및 평가  
1. **Bulk Run**을 사용하여 선택한 모델에서 여러 프롬프트를 테스트  
2. 테스트 케이스로 에이전트를 실행하여 기능 검증  
3. 내장 또는 사용자 정의 메트릭을 사용하여 정확도와 성능 평가  
4. 다양한 모델 및 구성 비교  

#### 단계 4: 미세 조정 및 최적화  
1. 특정 엣지 사용 사례에 맞게 모델 맞춤화  
2. 도메인별 미세 조정 적용  
3. 엣지 배포 제약 조건에 맞게 최적화  
4. 다양한 에이전트 구성 버전 관리 및 비교  

#### 단계 5: 배포 준비  
1. Agent Builder를 사용하여 프로덕션 준비 코드 생성  
2. 프로덕션 사용을 위한 MCP 서버 연결 설정  
3. 엣지 디바이스용 배포 패키지 준비  
4. 모니터링 및 평가 메트릭 구성  

## 엣지 AI 개발을 위한 모범 사례  

### 모델 선택  
- **크기 제약**: 대상 디바이스의 메모리 제한에 맞는 모델 선택  
- **추론 속도**: 실시간 애플리케이션을 위해 빠른 추론 속도를 가진 모델 우선  
- **정확도 절충**: 모델 정확도와 리소스 제약 간의 균형 유지  
- **포맷 호환성**: 엣지 배포를 위해 ONNX 또는 하드웨어 최적화 포맷 선호  

### 최적화 기술  
- **양자화**: INT8 또는 INT4 양자화를 사용하여 모델 크기 축소 및 속도 향상  
- **프루닝**: 불필요한 모델 매개변수를 제거하여 계산 요구 사항 감소  
- **지식 증류**: 성능을 유지하면서 더 작은 모델 생성  
- **하드웨어 가속**: 사용 가능한 경우 NPU, GPU 또는 특수 가속기 활용  

### 개발 워크플로우  
- **반복적 테스트**: 개발 중 엣지와 유사한 조건에서 자주 테스트  
- **성능 모니터링**: 리소스 사용량과 추론 속도를 지속적으로 모니터링  
- **버전 관리**: 모델 버전 및 최적화 설정 추적  
- **문서화**: 모든 최적화 결정 및 성능 절충 사항 문서화  

### 배포 고려사항  
- **리소스 모니터링**: 프로덕션에서 메모리, CPU, 전력 사용량 모니터링  
- **대체 전략**: 모델 실패 시 대체 메커니즘 구현  
- **업데이트 메커니즘**: 모델 업데이트 및 버전 관리 계획  
- **보안**: 엣지 AI 애플리케이션에 적합한 보안 조치 구현  

## 엣지 AI 프레임워크와의 통합  

### ONNX Runtime  
- **크로스 플랫폼 배포**: 다양한 엣지 플랫폼에서 ONNX 모델 배포  
- **하드웨어 최적화**: ONNX Runtime의 하드웨어별 최적화 활용  
- **모바일 지원**: 스마트폰 및 태블릿 애플리케이션을 위한 ONNX Runtime Mobile 사용  
- **IoT 통합**: ONNX Runtime의 경량 배포판을 사용하여 IoT 디바이스에 배포  

### Windows ML  
- **Windows 디바이스**: Windows 기반 엣지 디바이스 및 PC에 최적화  
- **NPU 가속**: Windows 디바이스의 Neural Processing Units 활용  
- **DirectML**: Windows 플랫폼에서 GPU 가속을 위해 DirectML 사용  
- **UWP 통합**: Universal Windows Platform 애플리케이션과 통합  

### TensorFlow Lite  
- **모바일 최적화**: 모바일 및 임베디드 디바이스에서 TensorFlow Lite 모델 배포  
- **하드웨어 대리자**: 가속화를 위한 특수 하드웨어 대리자 사용  
- **마이크로컨트롤러**: TensorFlow Lite Micro를 사용하여 마이크로컨트롤러에 배포  
- **크로스 플랫폼 지원**: Android, iOS 및 임베디드 Linux 시스템에서 배포  

### Azure IoT Edge  
- **클라우드-엣지 하이브리드**: 클라우드 학습과 엣지 추론 결합  
- **모듈 배포**: IoT Edge 모듈로 AI 모델 배포  
- **디바이스 관리**: 엣지 디바이스 및 모델 업데이트 원격 관리  
- **원격 측정**: 엣지 배포에서 성능 데이터 및 모델 메트릭 수집  

## 고급 엣지 AI 시나리오  

### 다중 모델 배포  
- **모델 앙상블**: 정확도 향상 또는 중복성을 위해 여러 모델 배포  
- **A/B 테스트**: 엣지 디바이스에서 다양한 모델을 동시에 테스트  
- **동적 선택**: 현재 디바이스 상태에 따라 모델 선택  
- **리소스 공유**: 여러 배포된 모델 간 리소스 사용 최적화  

### 연합 학습  
- **분산 학습**: 여러 엣지 디바이스에서 모델 학습  
- **프라이버시 보호**: 학습 데이터를 로컬에 유지하면서 모델 개선 사항 공유  
- **협력 학습**: 디바이스가 집단 경험에서 학습 가능  
- **엣지-클라우드 조정**: 엣지 디바이스와 클라우드 인프라 간 학습 조정  

### 실시간 처리  
- **스트림 처리**: 엣지 디바이스에서 연속 데이터 스트림 처리  
- **저지연 추론**: 최소 추론 지연을 위해 최적화  
- **배치 처리**: 엣지 디바이스에서 데이터 배치를 효율적으로 처리  
- **적응형 처리**: 현재 디바이스 기능에 따라 처리 조정  

## 엣지 AI 개발 문제 해결  

### 일반적인 문제  
- **메모리 제약**: 대상 디바이스 메모리에 비해 모델이 너무 큼  
- **추론 속도**: 실시간 요구 사항에 비해 모델 추론이 너무 느림  
- **정확도 저하**: 최적화로 인해 모델 정확도가 허용할 수 없을 정도로 감소  
- **하드웨어 호환성**: 모델이 대상 하드웨어와 호환되지 않음  

### 디버깅 전략  
- **성능 프로파일링**: AI Toolkit의 추적 기능을 사용하여 병목 현상 식별  
- **리소스 모니터링**: 개발 중 메모리 및 CPU 사용량 모니터링  
- **점진적 테스트**: 최적화를 점진적으로 테스트하여 문제를 격리  
- **하드웨어 시뮬레이션**: 개발 도구를 사용하여 대상 하드웨어 시뮬레이션  

### 최적화 솔루션  
- **추가 양자화**: 더 공격적인 양자화 기술 적용  
- **모델 아키텍처**: 엣지에 최적화된 다른 모델 아키텍처 고려  
- **전처리 최적화**: 엣지 제약 조건에 맞게 데이터 전처리 최적화  
- **추론 최적화**: 하드웨어별 추론 최적화 사용  

## 리소스 및 다음 단계  

### 공식 문서  
- [AI Toolkit 개발자 문서](https://aka.ms/AIToolkit/doc)  
- [설치 및 설정 가이드](https://code.visualstudio.com/docs/intelligentapps/overview#_install-and-setup)  
- [VS Code Intelligent Apps 문서](https://code.visualstudio.com/docs/intelligentapps)  
- [Model Context Protocol (MCP) 문서](https://modelcontextprotocol.io/)  

### 커뮤니티 및 지원  
- [AI Toolkit GitHub 저장소](https://github.com/microsoft/vscode-ai-toolkit)  
- [GitHub 문제 및 기능 요청](https://aka.ms/AIToolkit/feedback)  
- [Azure AI Foundry Discord 커뮤니티](https://aka.ms/azureaifoundry/discord)  
- [VS Code 확장 마켓플레이스](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)  

### 기술 리소스  
- [ONNX Runtime 문서](https://onnxruntime.ai/)  
- [Ollama 문서](https://ollama.ai/)  
- [Windows ML 문서](https://docs.microsoft.com/en-us/windows/ai/)  
- [Azure AI Foundry 문서](https://learn.microsoft.com/en-us/azure/ai-foundry/)  

### 학습 경로  
- [엣지 AI 기본 과정](../Module01/README.md)  
- [소형 언어 모델 가이드](../Module02/README.md)  
- [엣지 배포 전략](../Module03/README.md)  
- [Windows 엣지 AI 개발](./windowdeveloper.md)  

### 추가 리소스  
- **저장소 통계**: 1.8k+ stars, 150+ forks, 18+ 기여자  
- **라이선스**: MIT 라이선스  
- **보안**: Microsoft 보안 정책 적용  
- **원격 측정**: VS Code 원격 측정 설정 준수  

## 결론  

Visual Studio Code용 AI Toolkit은 현대 AI 개발을 위한 종합 플랫폼으로, 특히 엣지 AI 애플리케이션에 유용한 에이전트 개발 기능을 제공합니다. Anthropic, OpenAI, GitHub, Google과 같은 제공업체를 지원하는 광범위한 모델 카탈로그와 ONNX 및 Ollama를 통한 로컬 실행을 결합하여 다양한 엣지 배포 시나리오에 필요한 유연성을 제공합니다.

이 툴킷의 강점은 통합된 접근 방식에 있습니다. Playground에서 모델 탐색 및 실험부터 Prompt Builder를 사용한 정교한 에이전트 개발, 포괄적인 평가 기능, MCP 도구 통합까지 모든 과정을 지원합니다. 엣지 AI 개발자에게는 엣지 배포 전에 AI 에이전트를 빠르게 프로토타입하고 테스트하며, 리소스 제한 환경에 맞게 신속히 반복하고 최적화할 수 있는 기능을 제공합니다.

엣지 AI 개발을 위한 주요 장점:  
- **빠른 실험**: 엣지 배포 전에 모델과 에이전트를 빠르게 테스트  
- **다중 제공업체 유연성**: 다양한 소스에서 모델을 액세스하여 최적의 엣지 솔루션 찾기  
- **로컬 개발**: 오프라인 및 프라이버시 보호 개발을 위해 ONNX 및 Ollama 사용  
- **프로덕션 준비**: 프로덕션 준비 코드를 생성하고 MCP를 통해 외부 도구와 통합  
- **포괄적 평가**: 내장 및 사용자 정의 메트릭을 사용하여 엣지 AI 성능 검증  

AI가 엣지 배포 시나리오로 계속 이동함에 따라, VS Code용 AI Toolkit은 리소스 제한 환경에서 지능형 애플리케이션을 구축, 테스트 및 최적화하기 위한 개발 환경과 워크플로우를 제공합니다. IoT 솔루션, 모바일 AI 애플리케이션 또는 임베디드 인텔리전스 시스템을 개발하든, 이 툴킷의 포괄적인 기능 세트와 통합된 워크플로우는 엣지 AI 개발 라이프사이클 전체를 지원합니다.

1.8k+ GitHub stars와 활발한 커뮤니티를 통해 지속적인 개발이 이루어지고 있는 AI Toolkit은 엣지 배포 시나리오를 구축하는 현대 AI 개발자의 요구를 충족하기 위해 계속 진화하고 있습니다.

[Next Foundry Local](./foundrylocal.md)  

---

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있으나, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전을 신뢰할 수 있는 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.