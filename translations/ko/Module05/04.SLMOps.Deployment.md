<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-07-22T04:14:05+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "ko"
}
-->
# ì„¹ì…˜ 4: ë°°í¬ - í”„ë¡œë•ì…˜ ì¤€ë¹„ ëª¨ë¸ êµ¬í˜„

## ê°œìš”

ì´ ì¢…í•© íŠœí† ë¦¬ì–¼ì€ Foundry Localì„ ì‚¬ìš©í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •ëœ ì–‘ìí™” ëª¨ë¸ì„ ë°°í¬í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ì•ˆë‚´í•©ë‹ˆë‹¤. ëª¨ë¸ ë³€í™˜, ì–‘ìí™” ìµœì í™”, ë°°í¬ ì„¤ì •ì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ ë‹¤ë£° ê²ƒì…ë‹ˆë‹¤.

## ì‚¬ì „ ìš”êµ¬ì‚¬í•­

ì‹œì‘í•˜ê¸° ì „ì— ë‹¤ìŒì„ ì¤€ë¹„í•˜ì„¸ìš”:

- âœ… ë°°í¬ ì¤€ë¹„ê°€ ëœ ë¯¸ì„¸ ì¡°ì •ëœ ONNX ëª¨ë¸
- âœ… Windows ë˜ëŠ” Mac ì»´í“¨í„°
- âœ… Python 3.10 ì´ìƒ
- âœ… ìµœì†Œ 8GBì˜ ì‚¬ìš© ê°€ëŠ¥í•œ RAM
- âœ… ì‹œìŠ¤í…œì— ì„¤ì¹˜ëœ Foundry Local

## 1ë¶€: í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜

í„°ë¯¸ë„(Windowsì—ì„œëŠ” ëª…ë ¹ í”„ë¡¬í”„íŠ¸, Macì—ì„œëŠ” í„°ë¯¸ë„)ì„ ì—´ê³  ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì„¸ìš”:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

âš ï¸ **ì¤‘ìš” ì°¸ê³ ì‚¬í•­**: CMake 3.31 ë²„ì „ ì´ìƒë„ í•„ìš”í•˜ë©°, [cmake.org](https://cmake.org/download/)ì—ì„œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 2ë¶€: ëª¨ë¸ ë³€í™˜ ë° ì–‘ìí™”

### ì˜¬ë°”ë¥¸ í˜•ì‹ ì„ íƒ

ë¯¸ì„¸ ì¡°ì •ëœ ì†Œí˜• ì–¸ì–´ ëª¨ë¸ì˜ ê²½ìš° **ONNX í˜•ì‹**ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- ğŸš€ ë” ë‚˜ì€ ì„±ëŠ¥ ìµœì í™”
- ğŸ”§ í•˜ë“œì›¨ì–´ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” ë°°í¬
- ğŸ­ í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ ê¸°ëŠ¥
- ğŸ“± í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ì„±

### ë°©ë²• 1: ë‹¨ì¼ ëª…ë ¹ì–´ ë³€í™˜ (ê¶Œì¥)

ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ì§ì ‘ ë³€í™˜í•˜ì„¸ìš”:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**ë§¤ê°œë³€ìˆ˜ ì„¤ëª…:**
- `--model_name_or_path`: ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì˜ ê²½ë¡œ
- `--device cpu`: ìµœì í™”ë¥¼ ìœ„í•´ CPU ì‚¬ìš©
- `--precision int4`: INT4 ì–‘ìí™” ì‚¬ìš© (ì•½ 75% í¬ê¸° ê°ì†Œ)
- `--output_path`: ë³€í™˜ëœ ëª¨ë¸ì˜ ì¶œë ¥ ê²½ë¡œ

### ë°©ë²• 2: êµ¬ì„± íŒŒì¼ ì ‘ê·¼ë²• (ê³ ê¸‰ ì‚¬ìš©ììš©)

`finetuned_conversion_config.json`ì´ë¼ëŠ” êµ¬ì„± íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

ê·¸ëŸ° ë‹¤ìŒ ì‹¤í–‰í•˜ì„¸ìš”:

```bash
olive run --config ./finetuned_conversion_config.json
```

### ì–‘ìí™” ì˜µì…˜ ë¹„êµ

| ì •ë°€ë„   | íŒŒì¼ í¬ê¸°        | ì¶”ë¡  ì†ë„      | ëª¨ë¸ í’ˆì§ˆ      | ê¶Œì¥ ì‚¬ìš© ì‚¬ë¡€       |
|-----------|------------------|----------------|----------------|---------------------|
| FP16      | ê¸°ì¤€ Ã— 0.5       | ë¹ ë¦„           | ìµœê³            | ê³ ê¸‰ í•˜ë“œì›¨ì–´        |
| INT8      | ê¸°ì¤€ Ã— 0.25      | ë§¤ìš° ë¹ ë¦„      | ì¢‹ìŒ           | ê· í˜• ì¡íŒ ì„ íƒ       |
| INT4      | ê¸°ì¤€ Ã— 0.125     | ê°€ì¥ ë¹ ë¦„      | ìˆ˜ìš© ê°€ëŠ¥      | ìì›ì´ ì œí•œëœ í™˜ê²½   |

ğŸ’¡ **ê¶Œì¥ì‚¬í•­**: ì²« ë°°í¬ ì‹œ INT4 ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. í’ˆì§ˆì´ ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šë‹¤ë©´ INT8 ë˜ëŠ” FP16ì„ ì‹œë„í•´ë³´ì„¸ìš”.

## 3ë¶€: Foundry Local ë°°í¬ ì„¤ì •

### ëª¨ë¸ êµ¬ì„± ìƒì„±

Foundry Local ëª¨ë¸ ë””ë ‰í„°ë¦¬ë¡œ ì´ë™í•˜ì„¸ìš”:

```bash
foundry cache cd ./models/
```

ëª¨ë¸ ë””ë ‰í„°ë¦¬ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ì„¸ìš”:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

ëª¨ë¸ ë””ë ‰í„°ë¦¬ì— `inference_model.json` êµ¬ì„± íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### ëª¨ë¸ë³„ í…œí”Œë¦¿ êµ¬ì„±

#### Qwen ì‹œë¦¬ì¦ˆ ëª¨ë¸ì˜ ê²½ìš°:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## 4ë¶€: ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë° ìµœì í™”

### ëª¨ë¸ ì„¤ì¹˜ í™•ì¸

Foundry Localì´ ëª¨ë¸ì„ ì¸ì‹í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:

```bash
foundry cache ls
```

`your-finetuned-model-int4`ê°€ ëª©ë¡ì— í‘œì‹œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

### ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘

```bash
foundry model run your-finetuned-model-int4
```

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

í…ŒìŠ¤íŠ¸ ì¤‘ ë‹¤ìŒ ì£¼ìš” ì§€í‘œë¥¼ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”:

1. **ì‘ë‹µ ì‹œê°„**: ì‘ë‹µë‹¹ í‰ê·  ì‹œê°„ ì¸¡ì •
2. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: RAM ì†Œë¹„ëŸ‰ ëª¨ë‹ˆí„°ë§
3. **CPU í™œìš©ë„**: í”„ë¡œì„¸ì„œ ë¶€í•˜ í™•ì¸
4. **ì¶œë ¥ í’ˆì§ˆ**: ì‘ë‹µì˜ ê´€ë ¨ì„±ê³¼ ì¼ê´€ì„± í‰ê°€

### í’ˆì§ˆ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- âœ… ëª¨ë¸ì´ ë¯¸ì„¸ ì¡°ì •ëœ ë„ë©”ì¸ ì¿¼ë¦¬ì— ì ì ˆíˆ ì‘ë‹µí•¨
- âœ… ì‘ë‹µ í˜•ì‹ì´ ì˜ˆìƒ ì¶œë ¥ êµ¬ì¡°ì™€ ì¼ì¹˜í•¨
- âœ… ì¥ì‹œê°„ ì‚¬ìš© ì¤‘ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ê°€ ì—†ìŒ
- âœ… ë‹¤ì–‘í•œ ì…ë ¥ ê¸¸ì´ì— ê±¸ì³ ì¼ê´€ëœ ì„±ëŠ¥ ì œê³µ
- âœ… ì—£ì§€ ì¼€ì´ìŠ¤ ë° ì˜ëª»ëœ ì…ë ¥ì„ ì ì ˆíˆ ì²˜ë¦¬í•¨

## ìš”ì•½

ì¶•í•˜í•©ë‹ˆë‹¤! ë‹¤ìŒì„ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤:

- âœ… ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ í˜•ì‹ ë³€í™˜
- âœ… ëª¨ë¸ ì–‘ìí™” ìµœì í™”
- âœ… Foundry Local ë°°í¬ ì„¤ì •
- âœ… ì„±ëŠ¥ íŠœë‹ ë° ë¬¸ì œ í•´ê²°

**ë©´ì±… ì¡°í•­**:  
ì´ ë¬¸ì„œëŠ” AI ë²ˆì—­ ì„œë¹„ìŠ¤ [Co-op Translator](https://github.com/Azure/co-op-translator)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤. ì •í™•ì„±ì„ ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ê³  ìˆì§€ë§Œ, ìë™ ë²ˆì—­ì—ëŠ” ì˜¤ë¥˜ë‚˜ ë¶€ì •í™•ì„±ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›ë³¸ ë¬¸ì„œì˜ ì›ì–´ ë²„ì „ì„ ê¶Œìœ„ ìˆëŠ” ì¶œì²˜ë¡œ ê°„ì£¼í•´ì•¼ í•©ë‹ˆë‹¤. ì¤‘ìš”í•œ ì •ë³´ì˜ ê²½ìš°, ì „ë¬¸ì ì¸ ì¸ê°„ ë²ˆì—­ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ì´ ë²ˆì—­ ì‚¬ìš©ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ì˜¤í•´ë‚˜ ì˜ëª»ëœ í•´ì„ì— ëŒ€í•´ ì±…ì„ì„ ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.