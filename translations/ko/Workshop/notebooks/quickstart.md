<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddaad917d0c16fc3d498a6b4eabc8088",
  "translation_date": "2025-10-08T19:36:15+00:00",
  "source_file": "Workshop/notebooks/quickstart.md",
  "language_code": "ko"
}
-->
# ì›Œí¬ìˆ ë…¸íŠ¸ë¶ - ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## ëª©ì°¨

- [í•„ìˆ˜ ì¡°ê±´](../../../../Workshop/notebooks)
- [ì´ˆê¸° ì„¤ì •](../../../../Workshop/notebooks)
- [ì„¸ì…˜ 04: ëª¨ë¸ ë¹„êµ](../../../../Workshop/notebooks)
- [ì„¸ì…˜ 05: ë©€í‹° ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°](../../../../Workshop/notebooks)
- [ì„¸ì…˜ 06: ì˜ë„ ê¸°ë°˜ ëª¨ë¸ ë¼ìš°íŒ…](../../../../Workshop/notebooks)
- [í™˜ê²½ ë³€ìˆ˜](../../../../Workshop/notebooks)
- [ê³µí†µ ëª…ë ¹ì–´](../../../../Workshop/notebooks)

---

## í•„ìˆ˜ ì¡°ê±´

### 1. Foundry Local ì„¤ì¹˜

**Windows:**
```bash
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**ì„¤ì¹˜ í™•ì¸:**
```bash
foundry --version
```

### 2. Python ì¢…ì†ì„± ì„¤ì¹˜

```bash
cd Workshop
pip install -r requirements.txt
```

ê°œë³„ì ìœ¼ë¡œ ì„¤ì¹˜í•˜ë ¤ë©´:
```bash
pip install foundry-local-sdk openai numpy requests
```

---

## ì´ˆê¸° ì„¤ì •

### Foundry Local ì„œë¹„ìŠ¤ ì‹œì‘

**ëª¨ë“  ë…¸íŠ¸ë¶ ì‹¤í–‰ ì „ì— í•„ìš”:**

```bash
# Start the service
foundry service start

# Verify it's running
foundry service status
```

ì˜ˆìƒ ì¶œë ¥:
```
âœ… Service started successfully
Endpoint: http://localhost:59959
```

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ

ë…¸íŠ¸ë¶ì€ ê¸°ë³¸ì ìœ¼ë¡œ ë‹¤ìŒ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

```bash
# Download models (first time only - may take several minutes)
foundry model download phi-4-mini
foundry model download qwen2.5-3b
foundry model download phi-3.5-mini
foundry model download qwen2.5-0.5b

# Load models into memory
foundry model run phi-4-mini
foundry model run qwen2.5-3b
foundry model run phi-3.5-mini
```

### ì„¤ì • í™•ì¸

```bash
# List loaded models
foundry model ls

# Check service health
curl http://localhost:59959/v1/models
```

---

## ì„¸ì…˜ 04: ëª¨ë¸ ë¹„êµ

### ëª©ì 
ì†Œí˜• ì–¸ì–´ ëª¨ë¸(SLM)ê³¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.

### ë¹ ë¥¸ ì„¤ì •

```bash
# Start service (if not already running)
foundry service start

# Load required models
foundry model run phi-4-mini
foundry model run qwen2.5-3b
```

### ë…¸íŠ¸ë¶ ì‹¤í–‰

1. VS Code ë˜ëŠ” Jupyterì—ì„œ `session04_model_compare.ipynb` **ì—´ê¸°**
2. **ì»¤ë„ ì¬ì‹œì‘** (ì»¤ë„ â†’ ì»¤ë„ ì¬ì‹œì‘)
3. ëª¨ë“  ì…€ì„ **ìˆœì„œëŒ€ë¡œ ì‹¤í–‰**

### ì£¼ìš” ì„¤ì •

**ê¸°ë³¸ ëª¨ë¸:**
- **SLM:** `phi-4-mini` (~4GB RAM, ë¹ ë¦„)
- **LLM:** `qwen2.5-3b` (~3GB RAM, ë©”ëª¨ë¦¬ ìµœì í™”)

**í™˜ê²½ ë³€ìˆ˜ (ì„ íƒ ì‚¬í•­):**
```python
import os
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'
```

### ì˜ˆìƒ ì¶œë ¥

```
================================================================================
COMPARISON SUMMARY
================================================================================
Alias                Latency(s)      Tokens     Route               
--------------------------------------------------------------------------------
phi-4-mini           1.234           150        chat.completions    
qwen2.5-3b           2.456           180        chat.completions    
================================================================================

ğŸ’¡ SLM is 1.99x faster than LLM for this prompt
```

### ì‚¬ìš©ì ì •ì˜

**ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©:**
```python
os.environ['SLM_ALIAS'] = 'phi-3.5-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-1.5b'
```

**ì‚¬ìš©ì ì§€ì • í”„ë¡¬í”„íŠ¸:**
```python
os.environ['COMPARE_PROMPT'] = 'Explain quantum computing in simple terms'
```

### ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ì…€ 12ì— ì˜¬ë°”ë¥¸ ëª¨ë¸ í‘œì‹œ (phi-4-mini, qwen2.5-3b)
- [ ] ì…€ 12ì— ì˜¬ë°”ë¥¸ ì—”ë“œí¬ì¸íŠ¸ í‘œì‹œ (í¬íŠ¸ 59959)
- [ ] ì…€ 16 ì§„ë‹¨ í†µê³¼ (âœ… ì„œë¹„ìŠ¤ ì‹¤í–‰ ì¤‘)
- [ ] ì…€ 20 ì‚¬ì „ ì ê²€ í†µê³¼ (ë‘ ëª¨ë¸ ëª¨ë‘ í™•ì¸ë¨)
- [ ] ì…€ 22 ë¹„êµ ì™„ë£Œ ë° ì§€ì—° ì‹œê°„ ê°’ í‘œì‹œ
- [ ] ì…€ 24 ê²€ì¦ ê²°ê³¼ ğŸ‰ ëª¨ë“  ì²´í¬ í†µê³¼!

### ì†Œìš” ì‹œê°„
- **ì²« ì‹¤í–‰:** 5-10ë¶„ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í¬í•¨)
- **ì¶”ê°€ ì‹¤í–‰:** 1-2ë¶„

---

## ì„¸ì…˜ 05: ë©€í‹° ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

### ëª©ì 
Foundry Local SDKë¥¼ ì‚¬ìš©í•˜ì—¬ ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—…ì„ ì‹œì—° - ì—ì´ì „íŠ¸ê°€ í˜‘ë ¥í•˜ì—¬ ì •ì œëœ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

### ë¹ ë¥¸ ì„¤ì •

```bash
# Start service
foundry service start

# Load models
foundry model run phi-4-mini  # Primary model
foundry model run qwen2.5-7b  # Optional: higher quality editor
```

### ë…¸íŠ¸ë¶ ì‹¤í–‰

1. `session05_agents_orchestrator.ipynb` **ì—´ê¸°**
2. **ì»¤ë„ ì¬ì‹œì‘**
3. ëª¨ë“  ì…€ì„ **ìˆœì„œëŒ€ë¡œ ì‹¤í–‰**

### ì£¼ìš” ì„¤ì •

**ê¸°ë³¸ ì„¤ì • (ë‘ ì—ì´ì „íŠ¸ì— ë™ì¼í•œ ëª¨ë¸ ì‚¬ìš©):**
```python
PRIMARY_ALIAS = 'phi-4-mini'
EDITOR_ALIAS = 'phi-4-mini'  # Uses same model
```

**ê³ ê¸‰ ì„¤ì • (ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©):**
```python
import os
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'     # Fast for research
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'      # High quality for editing
```

### ì•„í‚¤í…ì²˜

```
User Question
    â†“
Researcher Agent (phi-4-mini)
  â†’ Gathers bullet points
    â†“
Editor Agent (phi-4-mini or qwen2.5-7b)
  â†’ Refines into executive summary
    â†“
Final Output
```

### ì˜ˆìƒ ì¶œë ¥

```
================================================================================
[Pipeline] Question: Explain why edge AI matters for compliance.
================================================================================

[Stage 1: Research]
Output: â€¢ Edge AI processes data locally, reducing transmission...

[Stage 2: Editorial Refinement]
Output: Executive Summary: Edge AI enhances compliance by keeping data...

[FINAL OUTPUT]
Executive Summary: Edge AI enhances compliance by keeping sensitive data 
on-premises and reduces latency through local processing.

[METADATA]
Models used: {'researcher': 'phi-4-mini', 'editor': 'phi-4-mini'}
```

### í™•ì¥

**ì—ì´ì „íŠ¸ ì¶”ê°€:**
```python
critic = Agent(
    name='Critic',
    system='Review content for accuracy',
    client=client,
    model_id=model_id
)
```

**ë°°ì¹˜ í…ŒìŠ¤íŠ¸:**
```python
test_questions = [
    "What are benefits of local AI?",
    "How does RAG improve accuracy?",
]

for q in test_questions:
    result = pipeline(q, verbose=False)
    print(result['final'])
```

### ì†Œìš” ì‹œê°„
- **ì²« ì‹¤í–‰:** 3-5ë¶„
- **ì¶”ê°€ ì‹¤í–‰:** ì§ˆë¬¸ë‹¹ 1-2ë¶„

---

## ì„¸ì…˜ 06: ì˜ë„ ê¸°ë°˜ ëª¨ë¸ ë¼ìš°íŒ…

### ëª©ì 
ê°ì§€ëœ ì˜ë„ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë¬¸í™”ëœ ëª¨ë¸ë¡œ ì§€ëŠ¥ì ìœ¼ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.

### ë¹ ë¥¸ ì„¤ì •

```bash
# Start service
foundry service start

# Load all routing models (CPU variants recommended)
foundry model run phi-4-mini-cpu
foundry model run qwen2.5-0.5b-cpu
foundry model run phi-3.5-mini-cpu
```

**ì°¸ê³ :** ì„¸ì…˜ 06ì€ ìµœëŒ€ í˜¸í™˜ì„±ì„ ìœ„í•´ ê¸°ë³¸ì ìœ¼ë¡œ CPU ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

### ë…¸íŠ¸ë¶ ì‹¤í–‰

1. `session06_models_router.ipynb` **ì—´ê¸°**
2. **ì»¤ë„ ì¬ì‹œì‘**
3. ëª¨ë“  ì…€ì„ **ìˆœì„œëŒ€ë¡œ ì‹¤í–‰**

### ì£¼ìš” ì„¤ì •

**ê¸°ë³¸ ì¹´íƒˆë¡œê·¸ (CPU ëª¨ë¸):**
```python
CATALOG = {
    'phi-4-mini-cpu': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b-cpu': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini-cpu': {'capabilities':['code','refactor'],'priority':3},
}
```

**ëŒ€ì•ˆ (GPU ëª¨ë¸):**
```python
# Uncomment GPU catalog in Cell #6 if you have sufficient VRAM (8GB+)
CATALOG = {
    'phi-4-mini': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini': {'capabilities':['code','refactor'],'priority':3},
}
```

### ì˜ë„ ê°ì§€

ë¼ìš°í„°ëŠ” ì •ê·œ í‘œí˜„ì‹ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ì˜ë„ë¥¼ ê°ì§€í•©ë‹ˆë‹¤:

| ì˜ë„ | íŒ¨í„´ ì˜ˆì‹œ | ë¼ìš°íŒ… ëŒ€ìƒ |
|------|-----------|-------------|
| `code` | "refactor", "implement function" | phi-3.5-mini-cpu |
| `classification` | "categorize", "classify this" | qwen2.5-0.5b-cpu |
| `summarize` | "summarize", "tl;dr" | phi-4-mini-cpu |
| `general` | ê¸°íƒ€ ëª¨ë“  ê²ƒ | phi-4-mini-cpu |

### ì˜ˆìƒ ì¶œë ¥

```
âœ“ Using CPU-optimized models (default configuration)
  Models: phi-4-mini-cpu, qwen2.5-0.5b-cpu, phi-3.5-mini-cpu

Routing prompts to specialized models...
============================================================

Prompt: Refactor this Python function for readability
  Intent: code           | Model: phi-3.5-mini-cpu
  Output: Here's a refactored version...
  Tokens: 156

Prompt: Categorize this email as urgent or normal
  Intent: classification | Model: qwen2.5-0.5b-cpu
  Output: Category: Normal
  Tokens: 45

âœ“ Success! All prompts routed correctly.
```

### ì‚¬ìš©ì ì •ì˜

**ì‚¬ìš©ì ì§€ì • ì˜ë„ ì¶”ê°€:**
```python
import re

# Add to RULES
RULES.append((re.compile('translate|ç¿»è¯‘', re.I), 'translation'))

# Add capability to catalog
CATALOG['phi-4-mini-cpu']['capabilities'].append('translation')
```

**í† í° ì¶”ì  í™œì„±í™”:**
```python
import os
os.environ['SHOW_USAGE'] = '1'
```

### GPU ëª¨ë¸ë¡œ ì „í™˜

8GB ì´ìƒì˜ VRAMì´ ìˆëŠ” ê²½ìš°:

1. **ì…€ #6**ì—ì„œ CPU ì¹´íƒˆë¡œê·¸ ì£¼ì„ ì²˜ë¦¬
2. GPU ì¹´íƒˆë¡œê·¸ ì£¼ì„ í•´ì œ
3. GPU ëª¨ë¸ ë¡œë“œ:
   ```bash
   foundry model run phi-4-mini
   foundry model run qwen2.5-0.5b
   foundry model run phi-3.5-mini
   ```
4. ì»¤ë„ ì¬ì‹œì‘ ë° ë…¸íŠ¸ë¶ ì¬ì‹¤í–‰

### ì†Œìš” ì‹œê°„
- **ì²« ì‹¤í–‰:** 5-10ë¶„ (ëª¨ë¸ ë¡œë“œ í¬í•¨)
- **ì¶”ê°€ ì‹¤í–‰:** í…ŒìŠ¤íŠ¸ë‹¹ 30-60ì´ˆ

---

## í™˜ê²½ ë³€ìˆ˜

### ê¸€ë¡œë²Œ ì„¤ì •

Jupyter/VS Code ì‹œì‘ ì „ì— ì„¤ì •:

**Windows (ëª…ë ¹ í”„ë¡¬í”„íŠ¸):**
```cmd
set FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
set SHOW_USAGE=1
set RETRY_ON_FAIL=1
```

**Windows (PowerShell):**
```powershell
$env:FOUNDRY_LOCAL_ENDPOINT="http://localhost:59959/v1"
$env:SHOW_USAGE="1"
$env:RETRY_ON_FAIL="1"
```

**macOS/Linux:**
```bash
export FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
export SHOW_USAGE=1
export RETRY_ON_FAIL=1
```

### ë…¸íŠ¸ë¶ ë‚´ ì„¤ì •

ëª¨ë“  ë…¸íŠ¸ë¶ ì‹œì‘ ì‹œ ì„¤ì •:

```python
import os

# Foundry Local configuration
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'

# Model selection
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'

# Agent models
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'

# Debugging
os.environ['SHOW_USAGE'] = '1'       # Show token usage
os.environ['RETRY_ON_FAIL'] = '1'    # Enable retries
os.environ['RETRY_BACKOFF'] = '2.0'  # Retry delay
```

---

## ê³µí†µ ëª…ë ¹ì–´

### ì„œë¹„ìŠ¤ ê´€ë¦¬

```bash
# Start service
foundry service start

# Check status
foundry service status

# Stop service
foundry service stop

# View logs
foundry service logs
```

### ëª¨ë¸ ê´€ë¦¬

```bash
# List all available models in catalog
foundry model catalog

# List loaded models
foundry model ls

# Download a model
foundry model download phi-4-mini

# Load a model
foundry model run phi-4-mini

# Unload a model
foundry model unload phi-4-mini

# Remove a model
foundry model remove phi-4-mini

# Get model info
foundry model info phi-4-mini
```

### ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸

```bash
# Check service health
curl http://localhost:59959/health

# List available models via API
curl http://localhost:59959/v1/models

# Test model completion
curl http://localhost:59959/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role":"user","content":"Hello"}],
    "max_tokens": 50
  }'
```

### ì§„ë‹¨ ëª…ë ¹ì–´

```bash
# Check everything
foundry --version
foundry service status
foundry model ls
foundry device info

# GPU status (NVIDIA)
nvidia-smi

# NPU status (Qualcomm)
foundry device info
```

---

## ëª¨ë²” ì‚¬ë¡€

### ëª¨ë“  ë…¸íŠ¸ë¶ ì‹œì‘ ì „ì—

1. **ì„œë¹„ìŠ¤ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸:**
   ```bash
   foundry service status
   ```

2. **ëª¨ë¸ ë¡œë“œ ì—¬ë¶€ í™•ì¸:**
   ```bash
   foundry model ls
   ```

3. **ë…¸íŠ¸ë¶ ì»¤ë„ ì¬ì‹œì‘** (ì¬ì‹¤í–‰ ì‹œ)

4. **ëª¨ë“  ì¶œë ¥ ì§€ìš°ê¸°** (ê¹¨ë—í•œ ì‹¤í–‰ì„ ìœ„í•´)

### ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

1. **ê¸°ë³¸ì ìœ¼ë¡œ CPU ëª¨ë¸ ì‚¬ìš©** (í˜¸í™˜ì„±)
2. **GPU ëª¨ë¸ë¡œ ì „í™˜** (8GB ì´ìƒì˜ VRAMì´ ìˆëŠ” ê²½ìš°)
3. **ë‹¤ë¥¸ GPU ì• í”Œë¦¬ì¼€ì´ì…˜ ë‹«ê¸°** (ì‹¤í–‰ ì „)
4. **ì„œë¹„ìŠ¤ ìœ ì§€ ì‹¤í–‰** (ë…¸íŠ¸ë¶ ì„¸ì…˜ ê°„)
5. **ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§** (ì‘ì—… ê´€ë¦¬ì / nvidia-smi)

### ë¬¸ì œ í•´ê²°

1. **ì½”ë“œ ë””ë²„ê¹… ì „ì— í•­ìƒ ì„œë¹„ìŠ¤ í™•ì¸**
2. **êµ¬ì„±ì´ ì˜¤ë˜ëœ ê²½ìš° ì»¤ë„ ì¬ì‹œì‘**
3. **ë³€ê²½ í›„ ì§„ë‹¨ ì…€ ì¬ì‹¤í–‰**
4. **ëª¨ë¸ ì´ë¦„ í™•ì¸** (ë¡œë“œëœ ëª¨ë¸ê³¼ ì¼ì¹˜)
5. **ì—”ë“œí¬ì¸íŠ¸ í¬íŠ¸ í™•ì¸** (ì„œë¹„ìŠ¤ ìƒíƒœì™€ ì¼ì¹˜)

---

## ë¹ ë¥¸ ì°¸ì¡°: ëª¨ë¸ ë³„ì¹­

### ì¼ë°˜ ëª¨ë¸

| ë³„ì¹­ | í¬ê¸° | ìµœì  ìš©ë„ | RAM/VRAM | ë³€í˜• |
|------|------|-----------|----------|------|
| `phi-4-mini` | ~4B | ì¼ë°˜ ì±„íŒ…, ìš”ì•½ | 4-6GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `phi-3.5-mini` | ~3.5B | ì½”ë“œ ìƒì„±, ë¦¬íŒ©í† ë§ | 3-5GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `qwen2.5-3b` | ~3B | ì¼ë°˜ ì‘ì—…, íš¨ìœ¨ì  | 3-4GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-1.5b` | ~1.5B | ë¹ ë¥´ê³  ì €ìì› | 2-3GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-0.5b` | ~0.5B | ë¶„ë¥˜, ìµœì†Œ ìì› | 1-2GB | `-cpu`, `-cuda-gpu` |

### ë³€í˜• ì´ë¦„

- **ê¸°ë³¸ ì´ë¦„** (ì˜ˆ: `phi-4-mini`): í•˜ë“œì›¨ì–´ì— ìµœì í™”ëœ ë³€í˜• ìë™ ì„ íƒ
- **`-cpu`**: CPU ìµœì í™”, ëª¨ë“  í™˜ê²½ì—ì„œ ì‘ë™
- **`-cuda-gpu`**: NVIDIA GPU ìµœì í™”, 8GB ì´ìƒì˜ VRAM í•„ìš”
- **`-npu`**: Qualcomm NPU ìµœì í™”, NPU ë“œë¼ì´ë²„ í•„ìš”

**ì¶”ì²œ:** ê¸°ë³¸ ì´ë¦„(ì ‘ë¯¸ì‚¬ ì—†ì´)ì„ ì‚¬ìš©í•˜ê³  Foundry Localì´ ìµœì ì˜ ë³€í˜•ì„ ìë™ ì„ íƒí•˜ë„ë¡ ì„¤ì •.

---

## ì„±ê³µ ì§€í‘œ

ì¤€ë¹„ ì™„ë£Œ ìƒíƒœ:

âœ… `foundry service status`ê°€ "running" í‘œì‹œ
âœ… `foundry model ls`ê°€ í•„ìš”í•œ ëª¨ë¸ í‘œì‹œ
âœ… ì„œë¹„ìŠ¤ê°€ ì˜¬ë°”ë¥¸ ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥
âœ… ìƒíƒœ í™•ì¸ì´ 200 OK ë°˜í™˜
âœ… ë…¸íŠ¸ë¶ ì§„ë‹¨ ì…€ì´ í†µê³¼
âœ… ì¶œë ¥ì— ì—°ê²° ì˜¤ë¥˜ ì—†ìŒ

---

## ë„ì›€ ë°›ê¸°

### ë¬¸ì„œ
- **ë©”ì¸ ì €ì¥ì†Œ**: https://github.com/microsoft/Foundry-Local
- **Python SDK**: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python
- **CLI ì°¸ì¡°**: https://github.com/microsoft/Foundry-Local/blob/main/docs/reference/reference-cli.md
- **ë¬¸ì œ í•´ê²°**: ì´ ë””ë ‰í† ë¦¬ì˜ `troubleshooting.md` ì°¸ì¡°

### GitHub ì´ìŠˆ
- https://github.com/microsoft/Foundry-Local/issues
- https://github.com/microsoft/edgeai-for-beginners/issues

---

**ìµœì¢… ì—…ë°ì´íŠ¸:** 2025ë…„ 10ì›” 8ì¼  
**ë²„ì „:** ì›Œí¬ìˆ ë…¸íŠ¸ë¶ 2.0

---

**ë©´ì±… ì¡°í•­**:  
ì´ ë¬¸ì„œëŠ” AI ë²ˆì—­ ì„œë¹„ìŠ¤ [Co-op Translator](https://github.com/Azure/co-op-translator)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤. ì •í™•ì„±ì„ ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ê³  ìˆìœ¼ë‚˜, ìë™ ë²ˆì—­ì—ëŠ” ì˜¤ë¥˜ë‚˜ ë¶€ì •í™•ì„±ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›ë³¸ ë¬¸ì„œì˜ ì›ì–´ ë²„ì „ì´ ê¶Œìœ„ ìˆëŠ” ì¶œì²˜ë¡œ ê°„ì£¼ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì¤‘ìš”í•œ ì •ë³´ì˜ ê²½ìš°, ì „ë¬¸ì ì¸ ì¸ê°„ ë²ˆì—­ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ì´ ë²ˆì—­ ì‚¬ìš©ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ì˜¤í•´ë‚˜ ì˜ëª»ëœ í•´ì„ì— ëŒ€í•´ ë‹¹ì‚¬ëŠ” ì±…ì„ì„ ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.