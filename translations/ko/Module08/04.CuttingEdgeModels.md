<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-09-30T23:36:58+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "ko"
}
-->
# ì„¸ì…˜ 4: Chainlitì„ í™œìš©í•œ í”„ë¡œë•ì…˜ ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•

## ê°œìš”

ì´ë²ˆ ì„¸ì…˜ì—ì„œëŠ” Chainlitê³¼ Microsoft Foundry Localì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤. AI ëŒ€í™”ë¥¼ ìœ„í•œ í˜„ëŒ€ì ì¸ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ ë§Œë“¤ê³ , ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ êµ¬í˜„í•˜ë©°, ì˜¤ë¥˜ ì²˜ë¦¬ì™€ ì‚¬ìš©ì ê²½í—˜ ë””ìì¸ì„ í†µí•´ ê²¬ê³ í•œ ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë°°í¬í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

**êµ¬ì¶•í•  ë‚´ìš©:**
- **Chainlit ì±„íŒ… ì•±**: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì§€ì›í•˜ëŠ” í˜„ëŒ€ì ì¸ ì›¹ UI
- **WebGPU ë°ëª¨**: ë¸Œë¼ìš°ì € ê¸°ë°˜ ì¶”ë¡ ìœ¼ë¡œ ê°œì¸ì •ë³´ ë³´í˜¸ ì¤‘ì‹¬ ì• í”Œë¦¬ì¼€ì´ì…˜  
- **Open WebUI í†µí•©**: Foundry Localì„ í™œìš©í•œ ì „ë¬¸ì ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
- **í”„ë¡œë•ì…˜ íŒ¨í„´**: ì˜¤ë¥˜ ì²˜ë¦¬, ëª¨ë‹ˆí„°ë§ ë° ë°°í¬ ì „ëµ

## í•™ìŠµ ëª©í‘œ

- Chainlitì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•
- ì‚¬ìš©ì ê²½í—˜ì„ í–¥ìƒì‹œí‚¤ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ êµ¬í˜„
- Foundry Local SDK í†µí•© íŒ¨í„´ ìˆ™ë‹¬
- ì ì ˆí•œ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì ì§„ì  ì„±ëŠ¥ ì €í•˜ ì ìš©
- ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ ë° êµ¬ì„±
- ëŒ€í™”í˜• AIë¥¼ ìœ„í•œ í˜„ëŒ€ì ì¸ ì›¹ UI íŒ¨í„´ ì´í•´

## ì‚¬ì „ ì¤€ë¹„ ì‚¬í•­

- **Foundry Local**: ì„¤ì¹˜ ë° ì‹¤í–‰ ì™„ë£Œ ([ì„¤ì¹˜ ê°€ì´ë“œ](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: 3.10 ì´ìƒ ë° ê°€ìƒ í™˜ê²½ ê¸°ëŠ¥
- **ëª¨ë¸**: ìµœì†Œ í•˜ë‚˜ì˜ ëª¨ë¸ ë¡œë“œ (`foundry model run phi-4-mini`)
- **ë¸Œë¼ìš°ì €**: WebGPU ì§€ì› ìµœì‹  ì›¹ ë¸Œë¼ìš°ì € (Chrome/Edge)
- **Docker**: Open WebUI í†µí•©ìš© (ì„ íƒ ì‚¬í•­)

## Part 1: í˜„ëŒ€ì ì¸ ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜ ì´í•´í•˜ê¸°

### ì•„í‚¤í…ì²˜ ê°œìš”

```
User Browser â†â†’ Chainlit UI â†â†’ Python Backend â†â†’ Foundry Local â†â†’ AI Model
      â†“              â†“              â†“              â†“            â†“
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### ì£¼ìš” ê¸°ìˆ 

**Foundry Local SDK íŒ¨í„´:**
- `FoundryLocalManager(alias)`: ìë™ ì„œë¹„ìŠ¤ ê´€ë¦¬
- `manager.endpoint` ë° `manager.api_key`: ì—°ê²° ì„¸ë¶€ ì •ë³´
- `manager.get_model_info(alias).id`: ëª¨ë¸ ì‹ë³„

**Chainlit í”„ë ˆì„ì›Œí¬:**
- `@cl.on_chat_start`: ì±„íŒ… ì„¸ì…˜ ì´ˆê¸°í™”
- `@cl.on_message`: ì‚¬ìš©ì ë©”ì‹œì§€ ì²˜ë¦¬  
- `cl.Message().stream_token()`: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
- ìë™ UI ìƒì„± ë° WebSocket ê´€ë¦¬

## Part 2: ë¡œì»¬ vs í´ë¼ìš°ë“œ ê²°ì • ë§¤íŠ¸ë¦­ìŠ¤

### ì„±ëŠ¥ íŠ¹ì„±

| í•­ëª© | ë¡œì»¬ (Foundry) | í´ë¼ìš°ë“œ (Azure OpenAI) |
|------|----------------|-------------------------|
| **ì§€ì—° ì‹œê°„** | ğŸš€ 50-200ms (ë„¤íŠ¸ì›Œí¬ ì—†ìŒ) | â±ï¸ 200-2000ms (ë„¤íŠ¸ì›Œí¬ ì˜ì¡´) |
| **ê°œì¸ì •ë³´ ë³´í˜¸** | ğŸ”’ ë°ì´í„°ê°€ ì¥ì¹˜ë¥¼ ë²—ì–´ë‚˜ì§€ ì•ŠìŒ | âš ï¸ ë°ì´í„°ê°€ í´ë¼ìš°ë“œë¡œ ì „ì†¡ë¨ |
| **ë¹„ìš©** | ğŸ’° í•˜ë“œì›¨ì–´ ì´í›„ ë¬´ë£Œ | ğŸ’¸ í† í°ë‹¹ ë¹„ìš© ë°œìƒ |
| **ì˜¤í”„ë¼ì¸** | âœ… ì¸í„°ë„· ì—†ì´ ì‘ë™ | âŒ ì¸í„°ë„· í•„ìš” |
| **ëª¨ë¸ í¬ê¸°** | âš ï¸ í•˜ë“œì›¨ì–´ ì œí•œ | âœ… ê°€ì¥ í° ëª¨ë¸ ì ‘ê·¼ ê°€ëŠ¥ |
| **í™•ì¥ì„±** | âš ï¸ í•˜ë“œì›¨ì–´ ì˜ì¡´ | âœ… ë¬´ì œí•œ í™•ì¥ ê°€ëŠ¥ |

### í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ íŒ¨í„´

**ë¡œì»¬ ìš°ì„  ë° í´ë°±:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**ì‘ì—… ê¸°ë°˜ ë¼ìš°íŒ…:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Part 3: ìƒ˜í”Œ 04 - Chainlit ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜

### ë¹ ë¥¸ ì‹œì‘

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

ì• í”Œë¦¬ì¼€ì´ì…˜ì€ `http://localhost:8080`ì—ì„œ ìë™ìœ¼ë¡œ ì—´ë¦¬ë©° í˜„ëŒ€ì ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### í•µì‹¬ êµ¬í˜„

ìƒ˜í”Œ 04 ì• í”Œë¦¬ì¼€ì´ì…˜ì€ í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ íŒ¨í„´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:

**ìë™ ì„œë¹„ìŠ¤ ê²€ìƒ‰:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… í•¸ë“¤ëŸ¬:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### êµ¬ì„± ì˜µì…˜

**í™˜ê²½ ë³€ìˆ˜:**

| ë³€ìˆ˜ | ì„¤ëª… | ê¸°ë³¸ê°’ | ì˜ˆì‹œ |
|------|------|--------|------|
| `MODEL` | ì‚¬ìš©í•  ëª¨ë¸ ë³„ì¹­ | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Foundry Local ì—”ë“œí¬ì¸íŠ¸ | ìë™ ê°ì§€ | `http://localhost:51211` |
| `API_KEY` | API í‚¤ (ë¡œì»¬ì—ì„œëŠ” ì„ íƒ ì‚¬í•­) | `""` | `your-api-key` |

**ê³ ê¸‰ ì‚¬ìš©ë²•:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Part 4: Jupyter ë…¸íŠ¸ë¶ ìƒì„± ë° ì‚¬ìš©

### ë…¸íŠ¸ë¶ ì§€ì› ê°œìš”

ìƒ˜í”Œ 04ì—ëŠ” í¬ê´„ì ì¸ Jupyter ë…¸íŠ¸ë¶(`chainlit_app.ipynb`)ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

- **ğŸ“š êµìœ¡ ì½˜í…ì¸ **: ë‹¨ê³„ë³„ í•™ìŠµ ìë£Œ
- **ğŸ”¬ ëŒ€í™”í˜• íƒìƒ‰**: ì½”ë“œ ì…€ ì‹¤í–‰ ë° ì‹¤í—˜
- **ğŸ“Š ì‹œê°ì  ë°ëª¨**: ì°¨íŠ¸, ë‹¤ì´ì–´ê·¸ë¨ ë° ì¶œë ¥ ì‹œê°í™”
- **ğŸ› ï¸ ê°œë°œ ë„êµ¬**: í…ŒìŠ¤íŠ¸ ë° ë””ë²„ê¹… ê¸°ëŠ¥

### ë‚˜ë§Œì˜ ë…¸íŠ¸ë¶ ìƒì„±í•˜ê¸°

#### Step 1: Jupyter í™˜ê²½ ì„¤ì •

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### Step 2: ìƒˆ ë…¸íŠ¸ë¶ ìƒì„±

**VS Code ì‚¬ìš©:**
1. Module08 ë””ë ‰í† ë¦¬ì—ì„œ VS Code ì—´ê¸°
2. `.ipynb` í™•ì¥ìë¡œ ìƒˆ íŒŒì¼ ìƒì„±
3. "Foundry Local" ì»¤ë„ ì„ íƒ
4. ì½˜í…ì¸ ë¥¼ ì¶”ê°€í•˜ë©° ì…€ ì‘ì„± ì‹œì‘

**Jupyter Lab ì‚¬ìš©:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### ë…¸íŠ¸ë¶ êµ¬ì¡° ëª¨ë²” ì‚¬ë¡€

#### ì…€ êµ¬ì„±

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("âœ… Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### ëŒ€í™”í˜• ì˜ˆì œ ë° ì—°ìŠµ

#### ì—°ìŠµ 1: í´ë¼ì´ì–¸íŠ¸ êµ¬ì„± í…ŒìŠ¤íŠ¸

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\nğŸ§ª Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'âœ… Success' if result['status'] == 'ok' else 'âŒ Failed'}")
```

#### ì—°ìŠµ 2: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("ğŸŒŠ Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\nâœ… Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Part 5: WebGPU ë¸Œë¼ìš°ì € ì¶”ë¡  ë°ëª¨

### ê°œìš”

WebGPUëŠ” AI ëª¨ë¸ì„ ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ìµœëŒ€ ê°œì¸ì •ë³´ ë³´í˜¸ì™€ ì„¤ì¹˜ ì—†ëŠ” ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ ìƒ˜í”Œì€ ONNX Runtime Webê³¼ WebGPU ì‹¤í–‰ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

### Step 1: WebGPU ì§€ì› í™•ì¸

**ë¸Œë¼ìš°ì € ìš”êµ¬ ì‚¬í•­:**
- WebGPUê°€ í™œì„±í™”ëœ Chrome/Edge 113+
- í™•ì¸: `chrome://gpu` â†’ "WebGPU" ìƒíƒœ í™•ì¸
- í”„ë¡œê·¸ë˜ë° í™•ì¸: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### Step 2: WebGPU ë°ëª¨ ìƒì„±

ë””ë ‰í† ë¦¬ ìƒì„±: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>ğŸš€ WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = 'âŒ WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'ğŸ” WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('âœ… ONNX Runtime session created with WebGPU');
        log(`ğŸ“Š Input names: ${session.inputNames.join(', ')}`);
        log(`ğŸ“Š Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = 'âœ… WebGPU inference complete!';
        log(`ğŸ¯ Predicted class: ${maxIdx}`);
        log(`ğŸ“ˆ Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `âŒ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### Step 3: ë°ëª¨ ì‹¤í–‰

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Part 6: Open WebUI í†µí•©

### ê°œìš”

Open WebUIëŠ” Foundry Localì˜ OpenAI í˜¸í™˜ APIì— ì—°ê²°ë˜ëŠ” ì „ë¬¸ì ì¸ ChatGPT ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### Step 1: ì‚¬ì „ ì¤€ë¹„

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### Step 2: Docker ì„¤ì • (ê¶Œì¥)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**ì°¸ê³ :** `host.docker.internal`ì€ Windowsì—ì„œ Docker ì»¨í…Œì´ë„ˆê°€ í˜¸ìŠ¤íŠ¸ ë¨¸ì‹ ì— ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

### Step 3: êµ¬ì„±

1. **ë¸Œë¼ìš°ì € ì—´ê¸°:** `http://localhost:3000`ìœ¼ë¡œ ì´ë™
2. **ì´ˆê¸° ì„¤ì •:** ê´€ë¦¬ì ê³„ì • ìƒì„±
3. **ëª¨ë¸ êµ¬ì„±:**
   - ì„¤ì • â†’ ëª¨ë¸ â†’ OpenAI API  
   - ê¸°ë³¸ URL: `http://host.docker.internal:51211/v1`
   - API í‚¤: `foundry-local-key` (ì•„ë¬´ ê°’ì´ë‚˜ ì‚¬ìš© ê°€ëŠ¥)
4. **ì—°ê²° í…ŒìŠ¤íŠ¸:** ëª¨ë¸ì´ ë“œë¡­ë‹¤ìš´ì— í‘œì‹œë˜ì–´ì•¼ í•¨

### ë¬¸ì œ í•´ê²°

**ì¼ë°˜ì ì¸ ë¬¸ì œ:**

1. **ì—°ê²° ê±°ë¶€:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **ëª¨ë¸ì´ í‘œì‹œë˜ì§€ ì•ŠìŒ:**
   - ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸: `foundry model list`
   - API ì‘ë‹µ í™•ì¸: `curl http://localhost:51211/v1/models`
   - Open WebUI ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘

## Part 7: í”„ë¡œë•ì…˜ ë°°í¬ ê³ ë ¤ ì‚¬í•­

### í™˜ê²½ êµ¬ì„±

**ê°œë°œ ì„¤ì •:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**í”„ë¡œë•ì…˜ ë°°í¬:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### ì¼ë°˜ì ì¸ í¬íŠ¸ ë¬¸ì œ ë° í•´ê²°ì±…

**í¬íŠ¸ 51211 ì¶©ëŒ ë°©ì§€:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

**ìƒíƒœ í™•ì¸ êµ¬í˜„:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## ìš”ì•½

ì„¸ì…˜ 4ì—ì„œëŠ” ëŒ€í™”í˜• AIë¥¼ ìœ„í•œ í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ Chainlit ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•ì„ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. í•™ìŠµí•œ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- âœ… **Chainlit í”„ë ˆì„ì›Œí¬**: ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ í˜„ëŒ€ì ì¸ UI ë° ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
- âœ… **Foundry Local í†µí•©**: SDK ì‚¬ìš© ë° êµ¬ì„± íŒ¨í„´  
- âœ… **WebGPU ì¶”ë¡ **: ìµœëŒ€ ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ìœ„í•œ ë¸Œë¼ìš°ì € ê¸°ë°˜ AI
- âœ… **Open WebUI ì„¤ì •**: ì „ë¬¸ì ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ë°°í¬
- âœ… **í”„ë¡œë•ì…˜ íŒ¨í„´**: ì˜¤ë¥˜ ì²˜ë¦¬, ëª¨ë‹ˆí„°ë§ ë° í™•ì¥

ìƒ˜í”Œ 04 ì• í”Œë¦¬ì¼€ì´ì…˜ì€ Microsoft Foundry Localì„ í†µí•´ ë¡œì»¬ AI ëª¨ë¸ì„ í™œìš©í•˜ë©´ì„œ ìš°ìˆ˜í•œ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³µí•˜ëŠ” ê²¬ê³ í•œ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ êµ¬ì¶•ì„ ìœ„í•œ ëª¨ë²” ì‚¬ë¡€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ì°¸ê³  ìë£Œ

- **[ìƒ˜í”Œ 04: Chainlit ì• í”Œë¦¬ì¼€ì´ì…˜](samples/04/README.md)**: ë¬¸ì„œê°€ í¬í•¨ëœ ì™„ì „í•œ ì• í”Œë¦¬ì¼€ì´ì…˜
- **[Chainlit êµìœ¡ìš© ë…¸íŠ¸ë¶](samples/04/chainlit_app.ipynb)**: ëŒ€í™”í˜• í•™ìŠµ ìë£Œ
- **[Foundry Local ë¬¸ì„œ](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: í”Œë«í¼ ì „ì²´ ë¬¸ì„œ
- **[Chainlit ë¬¸ì„œ](https://docs.chainlit.io/)**: ê³µì‹ í”„ë ˆì„ì›Œí¬ ë¬¸ì„œ
- **[Open WebUI í†µí•© ê°€ì´ë“œ](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: ê³µì‹ íŠœí† ë¦¬ì–¼

---

**ë©´ì±… ì¡°í•­**:  
ì´ ë¬¸ì„œëŠ” AI ë²ˆì—­ ì„œë¹„ìŠ¤ [Co-op Translator](https://github.com/Azure/co-op-translator)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤. ì •í™•ì„±ì„ ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ê³  ìˆìœ¼ë‚˜, ìë™ ë²ˆì—­ì—ëŠ” ì˜¤ë¥˜ë‚˜ ë¶€ì •í™•ì„±ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›ë³¸ ë¬¸ì„œì˜ ì›ì–´ ë²„ì „ì´ ê¶Œìœ„ ìˆëŠ” ì¶œì²˜ë¡œ ê°„ì£¼ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì¤‘ìš”í•œ ì •ë³´ì˜ ê²½ìš°, ì „ë¬¸ì ì¸ ì¸ê°„ ë²ˆì—­ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ì´ ë²ˆì—­ ì‚¬ìš©ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ì˜¤í•´ë‚˜ ì˜ëª»ëœ í•´ì„ì— ëŒ€í•´ ë‹¹ì‚¬ëŠ” ì±…ì„ì„ ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.