{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af028554",
   "metadata": {},
   "source": [
    "# рдЗрдВрдЯреЗрдВрдЯ-рдЖрдзрд╛рд░рд┐рдд рдореЙрдбреЗрд▓ рд░рд╛рдЙрдЯрд░ рдлрд╛рдЙрдВрдбреНрд░реА рд▓реЛрдХрд▓ SDK рд╕рд╣\n",
    "\n",
    "**CPU-рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭреНрдб рдорд▓реНрдЯреА-рдореЙрдбреЗрд▓ рд░рд╛рдЙрдЯрд┐рдВрдЧ рд╕рд┐рд╕реНрдЯрдо**\n",
    "\n",
    "рдпрд╛ рдиреЛрдЯрдмреБрдХрдордзреНрдпреЗ рдПрдХ рдмреБрджреНрдзрд┐рдорд╛рди рд░рд╛рдЙрдЯрд┐рдВрдЧ рд╕рд┐рд╕реНрдЯрдо рджрд╛рдЦрд╡рд▓реА рдЖрд╣реЗ рдЬреА рд╡рд╛рдкрд░рдХрд░реНрддреНрдпрд╛рдЪреНрдпрд╛ рд╣реЗрддреВрдиреБрд╕рд╛рд░ рд╕рд░реНрд╡реЛрддреНрддрдо рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓ рдЖрдкреЛрдЖрдк рдирд┐рд╡рдбрддреЗ. рдЕрд╢рд╛ рдПрдЬ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдкрд░рд┐рд╕реНрдерд┐рддреАрд╕рд╛рдареА рдпреЛрдЧреНрдп рдЖрд╣реЗ рдЬрд┐рдереЗ рддреБрдореНрд╣реА рдЕрдиреЗрдХ рд╡рд┐рд╢реЗрд╖ рдореЙрдбреЗрд▓реНрд╕ рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрдиреЗ рд╡рд╛рдкрд░реВ рдЗрдЪреНрдЫрд┐рддрд╛.\n",
    "\n",
    "## ЁЯОп рддреБрдореНрд╣реА рдХрд╛рдп рд╢рд┐рдХрд╛рд▓\n",
    "\n",
    "- **рд╣реЗрддреВ рдУрд│рдЦрдгреЗ**: рдкреНрд░реЙрдореНрдкреНрдЯреНрд╕рдЪреЗ рдЖрдкреЛрдЖрдк рд╡рд░реНрдЧреАрдХрд░рдг рдХрд░рд╛ (рдХреЛрдб, рд╕рд╛рд░рд╛рдВрд╢, рд╡рд░реНрдЧреАрдХрд░рдг, рд╕рд╛рдорд╛рдиреНрдп)\n",
    "- **рд╕реНрдорд╛рд░реНрдЯ рдореЙрдбреЗрд▓ рдирд┐рд╡рдб**: рдкреНрд░рддреНрдпреЗрдХ рдХрд╛рд░реНрдпрд╛рд╕рд╛рдареА рд╕рд░реНрд╡рд╛рдд рд╕рдХреНрд╖рдо рдореЙрдбреЗрд▓рдХрдбреЗ рд░реВрдЯ рдХрд░рд╛\n",
    "- **CPU рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди**: рдХреЛрдгрддреНрдпрд╛рд╣реА рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░рд╡рд░ рдХрд╛рд░реНрдп рдХрд░рдгрд╛рд░реА рдореЗрдорд░реА-рдХрд╛рд░реНрдпрдХреНрд╖рдо рдореЙрдбреЗрд▓реНрд╕\n",
    "- **рдорд▓реНрдЯреА-рдореЙрдбреЗрд▓ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди**: `--retain true` рд╕рд╣ рдЕрдиреЗрдХ рдореЙрдбреЗрд▓реНрд╕ рд▓реЛрдб рдареЗрд╡рд╛\n",
    "- **рдЙрддреНрдкрд╛рджрди рдкреЕрдЯрд░реНрдиреНрд╕**: рд░реАрдЯреНрд░рд╛рдп рд▓реЙрдЬрд┐рдХ, рдПрд░рд░ рд╣рдБрдбрд▓рд┐рдВрдЧ, рдЖрдгрд┐ рдЯреЛрдХрди рдЯреНрд░реЕрдХрд┐рдВрдЧ\n",
    "\n",
    "## ЁЯУЛ рдкрд░рд┐рд╕реНрдерд┐рддреАрдЪрд╛ рдЖрдврд╛рд╡рд╛\n",
    "\n",
    "рдпрд╛ рдкреЕрдЯрд░реНрдирдордзреНрдпреЗ рджрд╛рдЦрд╡рд▓реЗ рдЖрд╣реЗ:\n",
    "\n",
    "1. **рд╣реЗрддреВ рдУрд│рдЦрдгреЗ**: рдкреНрд░рддреНрдпреЗрдХ рд╡рд╛рдкрд░рдХрд░реНрддреНрдпрд╛рдЪреНрдпрд╛ рдкреНрд░реЙрдореНрдкреНрдЯрдЪреЗ рд╡рд░реНрдЧреАрдХрд░рдг рдХрд░рд╛ (рдХреЛрдб, рд╕рд╛рд░рд╛рдВрд╢, рд╡рд░реНрдЧреАрдХрд░рдг, рдХрд┐рдВрд╡рд╛ рд╕рд╛рдорд╛рдиреНрдп)\n",
    "2. **рдореЙрдбреЗрд▓ рдирд┐рд╡рдб**: рдХреНрд╖рдорддрд╛рдВрд╡рд░ рдЖрдзрд╛рд░рд┐рдд рд╕рд░реНрд╡рд╛рдд рдпреЛрдЧреНрдп рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓ рдЖрдкреЛрдЖрдк рдирд┐рд╡рдбрд╛\n",
    "3. **рд╕реНрдерд╛рдирд┐рдХ рдЕрдВрдорд▓рдмрдЬрд╛рд╡рдгреА**: рдлрд╛рдЙрдВрдбреНрд░реА рд▓реЛрдХрд▓ рд╕реЗрд╡реЗрджреНрд╡рд╛рд░реЗ рд╕реНрдерд╛рдирд┐рдХ рдореЙрдбреЗрд▓реНрд╕рдХрдбреЗ рд░реВрдЯ рдХрд░рд╛\n",
    "4. **рдПрдХрд╕рдВрдз рдЗрдВрдЯрд░рдлреЗрд╕**: рдЕрдиреЗрдХ рд╡рд┐рд╢реЗрд╖ рдореЙрдбреЗрд▓реНрд╕рдХрдбреЗ рд░реВрдЯ рдХрд░рдгрд╛рд░реЗ рдПрдХрдЪ рдЪреЕрдЯ рдкреНрд░рд╡реЗрд╢ рдмрд┐рдВрджреВ\n",
    "\n",
    "**рдпреЛрдЧреНрдп рдЖрд╣реЗ**: рдЕрдиреЗрдХ рд╡рд┐рд╢реЗрд╖ рдореЙрдбреЗрд▓реНрд╕рд╕рд╣ рдПрдЬ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯреНрд╕рд╕рд╛рдареА рдЬрд┐рдереЗ рддреБрдореНрд╣рд╛рд▓рд╛ рдореЕрдиреНрдпреБрдЕрд▓ рдореЙрдбреЗрд▓ рдирд┐рд╡рдбреАрд╢рд┐рд╡рд╛рдп рдмреБрджреНрдзрд┐рдорд╛рди рд╡рд┐рдирдВрддреА рд░рд╛рдЙрдЯрд┐рдВрдЧ рд╣рд╡реЗ рдЖрд╣реЗ.\n",
    "\n",
    "## ЁЯФз рдкреВрд░реНрд╡рддрдпрд╛рд░реА\n",
    "\n",
    "- **Foundry Local** рд╕реНрдерд╛рдкрд┐рдд рдЖрдгрд┐ рд╕реЗрд╡рд╛ рдЪрд╛рд▓реВ\n",
    "- **Python 3.8+** pip рд╕рд╣\n",
    "- **8GB+ RAM** (рдЕрдиреЗрдХ рдореЙрдбреЗрд▓реНрд╕рд╕рд╛рдареА 16GB+ рд╢рд┐рдлрд╛рд░рд╕ рдХреЗрд▓реА рдЬрд╛рддреЗ)\n",
    "- **workshop_utils** рдореЙрдбреНрдпреВрд▓ (../samples/ рдордзреНрдпреЗ)\n",
    "\n",
    "## ЁЯЪА рдЬрд▓рдж рд╕реБрд░реБрд╡рд╛рдд\n",
    "\n",
    "рдиреЛрдЯрдмреБрдХ:\n",
    "1. рддреБрдордЪреНрдпрд╛ рд╕рд┐рд╕реНрдЯрдо рдореЗрдорд░реАрдЪрд╛ рд╢реЛрдз рдШреЗрдИрд▓\n",
    "2. рдпреЛрдЧреНрдп CPU рдореЙрдбреЗрд▓реНрд╕рдЪреА рд╢рд┐рдлрд╛рд░рд╕ рдХрд░реЗрд▓\n",
    "3. `--retain true` рд╕рд╣ рдореЙрдбреЗрд▓реНрд╕ рдЖрдкреЛрдЖрдк рд▓реЛрдб рдХрд░реЗрд▓\n",
    "4. рд╕рд░реНрд╡ рдореЙрдбреЗрд▓реНрд╕ рддрдпрд╛рд░ рдЕрд╕рд▓реНрдпрд╛рдЪреА рдкрдбрддрд╛рд│рдгреА рдХрд░реЗрд▓\n",
    "5. рдЪрд╛рдЪрдгреА рдкреНрд░реЙрдореНрдкреНрдЯреНрд╕ рд╡рд┐рд╢реЗрд╖ рдореЙрдбреЗрд▓реНрд╕рдХрдбреЗ рд░реВрдЯ рдХрд░реЗрд▓\n",
    "\n",
    "**рдЕрдВрджрд╛рдЬреЗ рд╕реЗрдЯрдЕрдк рд╡реЗрд│**: 5-7 рдорд┐рдирд┐рдЯреЗ (рдореЙрдбреЗрд▓ рд▓реЛрдбрд┐рдВрдЧрд╕рд╣)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa55f0",
   "metadata": {},
   "source": [
    "## ЁЯУж рдЪрд░рдг 1: рдЕрд╡рд▓рдВрдмрд┐рддреНрд╡реЗ рд╕реНрдерд╛рдкрд┐рдд рдХрд░рд╛\n",
    "\n",
    "рдЕрдзрд┐рдХреГрдд Foundry Local SDK рдЖрдгрд┐ рдЖрд╡рд╢реНрдпрдХ рд▓рд╛рдпрдмреНрд░рд░реА рд╕реНрдерд╛рдкрд┐рдд рдХрд░рд╛:\n",
    "\n",
    "- **foundry-local-sdk**: рд╕реНрдерд╛рдирд┐рдХ рдореЙрдбреЗрд▓ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрдирд╛рд╕рд╛рдареА рдЕрдзрд┐рдХреГрдд Python SDK  \n",
    "- **openai**: рдЪреЕрдЯ рдкреВрд░реНрдгрддреЗрд╕рд╛рдареА OpenAI-рд╕реБрд╕рдВрдЧрдд API  \n",
    "- **psutil**: рдкреНрд░рдгрд╛рд▓реА рдореЗрдорд░реА рд╢реЛрдз рдЖрдгрд┐ рдирд┐рд░реАрдХреНрд╖рдгрд╛рд╕рд╛рдареА  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2929c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "!pip install -q foundry-local-sdk openai psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990799e7",
   "metadata": {},
   "source": [
    "## ЁЯТ╗ рдЪрд░рдг 2: рд╕рд┐рд╕реНрдЯрдо рдореЗрдорд░реА рд╢реЛрдзрд╛\n",
    "\n",
    "рдЙрдкрд▓рдмреНрдз рд╕рд┐рд╕реНрдЯрдо рдореЗрдорд░реА рд╢реЛрдзрд╛ рдЬреЗрдгреЗрдХрд░реВрди рдХреЛрдгрддреЗ CPU рдореЙрдбреЗрд▓реНрд╕ рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрдиреЗ рдЪрд╛рд▓рд╡рддрд╛ рдпреЗрддреАрд▓ рд╣реЗ рдард░рд╡рддрд╛ рдпреЗрдИрд▓. рдпрд╛рдореБрд│реЗ рддреБрдордЪреНрдпрд╛ рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░рд╕рд╛рдареА рд╕рд░реНрд╡реЛрддреНрддрдо рдореЙрдбреЗрд▓ рдирд┐рд╡рдб рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рд╣реЛрддреЗ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ff58f1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯЦея╕П  System Memory Information\n",
      "======================================================================\n",
      "Total Memory:     63.30 GB\n",
      "Available Memory: 16.19 GB\n",
      "\n",
      "тЬЕ High Memory System (32GB+)\n",
      "   Can run 3-4 models simultaneously\n",
      "\n",
      "ЁЯУЛ Recommended Model Aliases for Your System:\n",
      "   тАв phi-4-mini\n",
      "   тАв phi-3.5-mini\n",
      "   тАв qwen2.5-0.5b\n",
      "   тАв qwen2.5-coder-0.5b\n",
      "\n",
      "ЁЯТб About Model Aliases:\n",
      "   тЬУ Use base alias (e.g., phi-4-mini, not phi-4-mini-cpu)\n",
      "   тЬУ Foundry Local automatically selects CPU variant for your hardware\n",
      "   тЬУ No GPU required - optimized for CPU inference\n",
      "   тЬУ Predictable memory usage and consistent performance\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get system memory information\n",
    "total_memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "\n",
    "print('ЁЯЦея╕П  System Memory Information')\n",
    "print('=' * 70)\n",
    "print(f'Total Memory:     {total_memory_gb:.2f} GB')\n",
    "print(f'Available Memory: {available_memory_gb:.2f} GB')\n",
    "print()\n",
    "\n",
    "# Recommend models based on available memory\n",
    "# Using model aliases - Foundry Local will automatically select CPU variant\n",
    "model_aliases = []\n",
    "\n",
    "if total_memory_gb >= 32:\n",
    "    model_aliases = ['phi-4-mini', 'phi-3.5-mini', 'qwen2.5-0.5b', 'qwen2.5-coder-0.5b']\n",
    "    print('тЬЕ High Memory System (32GB+)')\n",
    "    print('   Can run 3-4 models simultaneously')\n",
    "elif total_memory_gb >= 16:\n",
    "    model_aliases = ['phi-4-mini', 'qwen2.5-0.5b', 'phi-3.5-mini']\n",
    "    print('тЬЕ Medium Memory System (16-32GB)')\n",
    "    print('   Can run 2-3 models simultaneously')\n",
    "elif total_memory_gb >= 8:\n",
    "    model_aliases = ['qwen2.5-0.5b', 'phi-3.5-mini']\n",
    "    print('тЪая╕П  Lower Memory System (8-16GB)')\n",
    "    print('   Recommended: 2 smaller models')\n",
    "else:\n",
    "    model_aliases = ['qwen2.5-0.5b']\n",
    "    print('тЪая╕П  Limited Memory System (<8GB)')\n",
    "    print('   Recommended: Use only smallest model')\n",
    "\n",
    "print()\n",
    "print('ЁЯУЛ Recommended Model Aliases for Your System:')\n",
    "for model in model_aliases:\n",
    "    print(f'   тАв {model}')\n",
    "\n",
    "print()\n",
    "print('ЁЯТб About Model Aliases:')\n",
    "print('   тЬУ Use base alias (e.g., phi-4-mini, not phi-4-mini-cpu)')\n",
    "print('   тЬУ Foundry Local automatically selects CPU variant for your hardware')\n",
    "print('   тЬУ No GPU required - optimized for CPU inference')\n",
    "print('   тЬУ Predictable memory usage and consistent performance')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69590b94",
   "metadata": {},
   "source": [
    "## ЁЯдЦ рдЪрд░рдг 3: рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рдореЙрдбреЗрд▓ рд▓реЛрдбрд┐рдВрдЧ\n",
    "\n",
    "рдпрд╛ рд╕реЗрд▓рджреНрд╡рд╛рд░реЗ рд╕реНрд╡рдпрдВрдЪрд▓рд┐рддрдкрдгреЗ:\n",
    "1. Foundry Local рд╕реЗрд╡рд╛ рд╕реБрд░реВ рдХреЗрд▓реА рдЬрд╛рддреЗ (рдЬрд░ рддреА рдЪрд╛рд▓реВ рдирд╕реЗрд▓)\n",
    "2. `--retain true` рд╡рд╛рдкрд░реВрди рд╢рд┐рдлрд╛рд░рд╕ рдХреЗрд▓реЗрд▓реЗ рдореЙрдбреЗрд▓реНрд╕ рд▓реЛрдб рдХреЗрд▓реЗ рдЬрд╛рддрд╛рдд (рдПрдХрд╛рдЪ рд╡реЗрд│реА рдЕрдиреЗрдХ рдореЙрдбреЗрд▓реНрд╕ рдореЗрдорд░реАрдд рдареЗрд╡рд▓реА рдЬрд╛рддрд╛рдд)\n",
    "3. SDK рд╡рд╛рдкрд░реВрди рд╕рд░реНрд╡ рдореЙрдбреЗрд▓реНрд╕ рддрдпрд╛рд░ рдЕрд╕рд▓реНрдпрд╛рдЪреА рдЦрд╛рддреНрд░реА рдХреЗрд▓реА рдЬрд╛рддреЗ\n",
    "\n",
    "тП▒я╕П **рдЕрдкреЗрдХреНрд╖рд┐рдд рд╡реЗрд│**: рд╕рд░реНрд╡ рдореЙрдбреЗрд▓реНрд╕рд╕рд╛рдареА 3-5 рдорд┐рдирд┐рдЯреЗ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "543fd976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯЪА Automatic Model Loading with SDK Verification\n",
      "======================================================================\n",
      "ЁЯУЛ Loading 3 models: ['phi-4-mini', 'phi-3.5-mini', 'qwen2.5-0.5b']\n",
      "ЁЯТб Using model aliases - Foundry will load CPU variants automatically\n",
      "\n",
      "ЁЯУб Step 1: Checking Foundry Local service...\n",
      "   тЬЕ Service is already running\n",
      "\n",
      "ЁЯдЦ Step 2: Loading models with retention...\n",
      "   [1/3] Starting phi-4-mini...\n",
      "       тЬЕ phi-4-mini loading in background\n",
      "   [2/3] Starting phi-3.5-mini...\n",
      "       тЬЕ phi-3.5-mini loading in background\n",
      "   [3/3] Starting qwen2.5-0.5b...\n",
      "       тЬЕ qwen2.5-0.5b loading in background\n",
      "\n",
      "тЬЕ Step 3: Verifying models (this may take 2-3 minutes)...\n",
      "======================================================================\n",
      "\n",
      "   Attempt 1/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 2/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 3/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 4/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 5/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 6/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 7/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 8/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 9/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 10/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 11/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 12/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 13/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 14/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 15/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 16/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 17/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 18/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 19/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 20/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 21/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 22/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 23/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 24/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 25/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 26/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 27/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 28/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 29/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 30/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "======================================================================\n",
      "ЁЯУж Final Status: 0/3 models ready\n",
      "   тЭМ phi-4-mini - NOT READY\n",
      "   тЭМ phi-3.5-mini - NOT READY\n",
      "   тЭМ qwen2.5-0.5b - NOT READY\n",
      "\n",
      "тЪая╕П  Some models not ready. Check: foundry model ls\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add samples directory for workshop_utils (Foundry SDK pattern)\n",
    "sys.path.append(os.path.join('..', 'samples'))\n",
    "\n",
    "print('ЁЯЪА Automatic Model Loading with SDK Verification')\n",
    "print('=' * 70)\n",
    "\n",
    "# Use top 3 recommended models (aliases)\n",
    "# Foundry will automatically load CPU variants\n",
    "REQUIRED_MODELS = model_aliases[:3]\n",
    "print(f'ЁЯУЛ Loading {len(REQUIRED_MODELS)} models: {REQUIRED_MODELS}')\n",
    "print('ЁЯТб Using model aliases - Foundry will load CPU variants automatically')\n",
    "print()\n",
    "\n",
    "# Step 1: Ensure Foundry Local service is running\n",
    "print('ЁЯУб Step 1: Checking Foundry Local service...')\n",
    "try:\n",
    "    result = subprocess.run(['foundry', 'service', 'status'], \n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print('   тЬЕ Service is already running')\n",
    "    else:\n",
    "        print('   тЪЩя╕П  Starting Foundry Local service...')\n",
    "        subprocess.run(['foundry', 'service', 'start'], \n",
    "                      capture_output=True, text=True, timeout=30)\n",
    "        time.sleep(5)\n",
    "        print('   тЬЕ Service started')\n",
    "except Exception as e:\n",
    "    print(f'   тЪая╕П  Could not verify service: {e}')\n",
    "    print('   ЁЯТб Try manually: foundry service start')\n",
    "\n",
    "# Step 2: Load each model with --retain true\n",
    "print(f'\\nЁЯдЦ Step 2: Loading models with retention...')\n",
    "for i, model in enumerate(REQUIRED_MODELS, 1):\n",
    "    print(f'   [{i}/{len(REQUIRED_MODELS)}] Starting {model}...')\n",
    "    try:\n",
    "        subprocess.Popen(['foundry', 'model', 'run', model, '--retain', 'true'],\n",
    "                        stdout=subprocess.DEVNULL,\n",
    "                        stderr=subprocess.DEVNULL)\n",
    "        print(f'       тЬЕ {model} loading in background')\n",
    "    except Exception as e:\n",
    "        print(f'       тЭМ Error starting {model}: {e}')\n",
    "\n",
    "# Step 3: Verify models are ready\n",
    "print(f'\\nтЬЕ Step 3: Verifying models (this may take 2-3 minutes)...')\n",
    "print('=' * 70)\n",
    "\n",
    "try:\n",
    "    from workshop_utils import get_client\n",
    "    \n",
    "    ready_models = []\n",
    "    max_attempts = 30\n",
    "    attempt = 0\n",
    "    \n",
    "    while len(ready_models) < len(REQUIRED_MODELS) and attempt < max_attempts:\n",
    "        attempt += 1\n",
    "        print(f'\\n   Attempt {attempt}/{max_attempts}...')\n",
    "        \n",
    "        for model in REQUIRED_MODELS:\n",
    "            if model in ready_models:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                manager, client, model_id = get_client(model, None)\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "                    max_tokens=5,\n",
    "                    temperature=0\n",
    "                )\n",
    "                \n",
    "                if response and response.choices:\n",
    "                    ready_models.append(model)\n",
    "                    print(f'   тЬЕ {model} is READY')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_msg = str(e).lower()\n",
    "                if 'connection' in error_msg or 'timeout' in error_msg:\n",
    "                    print(f'   тП│ {model} still loading...')\n",
    "                else:\n",
    "                    print(f'   тЪая╕П  {model} error: {str(e)[:60]}...')\n",
    "        \n",
    "        if len(ready_models) == len(REQUIRED_MODELS):\n",
    "            break\n",
    "            \n",
    "        if len(ready_models) < len(REQUIRED_MODELS):\n",
    "            time.sleep(10)\n",
    "    \n",
    "    # Final status\n",
    "    print('\\n' + '=' * 70)\n",
    "    print(f'ЁЯУж Final Status: {len(ready_models)}/{len(REQUIRED_MODELS)} models ready')\n",
    "    \n",
    "    for model in REQUIRED_MODELS:\n",
    "        if model in ready_models:\n",
    "            print(f'   тЬЕ {model} - READY (retained in memory)')\n",
    "        else:\n",
    "            print(f'   тЭМ {model} - NOT READY')\n",
    "    \n",
    "    if len(ready_models) == len(REQUIRED_MODELS):\n",
    "        print('\\nЁЯОЙ All models loaded and verified!')\n",
    "        print('   тЬЕ Ready for intent-based routing')\n",
    "    else:\n",
    "        print(f'\\nтЪая╕П  Some models not ready. Check: foundry model ls')\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f'\\nтЭМ Cannot import workshop_utils: {e}')\n",
    "    print('   ЁЯТб Ensure workshop_utils.py is in ../samples/')\n",
    "except Exception as e:\n",
    "    print(f'\\nтЭМ Verification error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682909b",
   "metadata": {},
   "source": [
    "## ЁЯОп рдЪрд░рдг 4: рд╣реЗрддреВ рд╢реЛрдз рдЖрдгрд┐ рдореЙрдбреЗрд▓ рдХреЕрдЯрд▓реЙрдЧ рдХреЙрдиреНрдлрд┐рдЧрд░ рдХрд░рд╛\n",
    "\n",
    "рд░рд╛рдЙрдЯрд┐рдВрдЧ рд╕рд┐рд╕реНрдЯрдо рд╕реЗрдЯрдЕрдк рдХрд░рд╛:\n",
    "- **рд╣реЗрддреВ рдирд┐рдпрдо**: рдкреНрд░реЙрдореНрдкреНрдЯ рд╡рд░реНрдЧреАрдХрд░рдгрд╛рд╕рд╛рдареА Regex рдкреЕрдЯрд░реНрдиреНрд╕\n",
    "- **рдореЙрдбреЗрд▓ рдХреЕрдЯрд▓реЙрдЧ**: рдореЙрдбреЗрд▓ рдХреНрд╖рдорддрд╛рдВрдирд╛ рд╣реЗрддреВ рд╢реНрд░реЗрдгреАрдВрд╢реА рдЬреБрд│рд╡рддреЗ\n",
    "- **рдкреНрд░рд╛рдзрд╛рдиреНрдп рдкреНрд░рдгрд╛рд▓реА**: рдЬреЗрд╡реНрд╣рд╛ рдЕрдиреЗрдХ рдореЙрдбреЗрд▓реНрд╕ рдЬреБрд│рддрд╛рдд рддреЗрд╡реНрд╣рд╛ рдореЙрдбреЗрд▓ рдирд┐рд╡рдб рдирд┐рд╢реНрдЪрд┐рдд рдХрд░рддреЗ\n",
    "\n",
    "**CPU рдореЙрдбреЗрд▓рдЪреЗ рдлрд╛рдпрджреЗ**:\n",
    "- тЬЕ GPU рдЖрд╡рд╢реНрдпрдХ рдирд╛рд╣реА\n",
    "- тЬЕ рд╕рд╛рддрддреНрдпрдкреВрд░реНрдг рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛\n",
    "- тЬЕ рдХрдореА рдКрд░реНрдЬрд╛ рд╡рд╛рдкрд░\n",
    "- тЬЕ рдЕрдВрджрд╛рдЬреЗ рдореЗрдорд░реА рд╡рд╛рдкрд░\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3620a4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯУЛ Active Model Catalog (Hardware-Optimized Aliases)\n",
      "======================================================================\n",
      "ЁЯТб Using model aliases - Foundry automatically selects CPU variants\n",
      "\n",
      "   тАв phi-4-mini\n",
      "     Capabilities: general, summarize, reasoning\n",
      "     Priority: 3\n",
      "\n",
      "   тАв qwen2.5-0.5b\n",
      "     Capabilities: classification, fast, general\n",
      "     Priority: 1\n",
      "\n",
      "   тАв phi-3.5-mini\n",
      "     Capabilities: code, refactor, technical\n",
      "     Priority: 2\n",
      "\n",
      "   тАв qwen2.5-coder-0.5b\n",
      "     Capabilities: code, programming, debug\n",
      "     Priority: 1\n",
      "\n",
      "тЬЕ Intent detection and model selection configured\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ЁЯТб Using model aliases - Foundry automatically selects CPU variants\n",
      "\n",
      "   тАв phi-4-mini\n",
      "     Capabilities: general, summarize, reasoning\n",
      "     Priority: 3\n",
      "\n",
      "   тАв qwen2.5-0.5b\n",
      "     Capabilities: classification, fast, general\n",
      "     Priority: 1\n",
      "\n",
      "   тАв phi-3.5-mini\n",
      "     Capabilities: code, refactor, technical\n",
      "     Priority: 2\n",
      "\n",
      "   тАв qwen2.5-coder-0.5b\n",
      "     Capabilities: code, programming, debug\n",
      "     Priority: 1\n",
      "\n",
      "тЬЕ Intent detection and model selection configured\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Model capability catalog (maps model aliases to capabilities)\n",
    "# Use base aliases - Foundry Local will automatically select CPU variants\n",
    "CATALOG = {\n",
    "    'phi-4-mini': {\n",
    "        'capabilities': ['general', 'summarize', 'reasoning'],\n",
    "        'priority': 3\n",
    "    },\n",
    "    'qwen2.5-0.5b': {\n",
    "        'capabilities': ['classification', 'fast', 'general'],\n",
    "        'priority': 1\n",
    "    },\n",
    "    'phi-3.5-mini': {\n",
    "        'capabilities': ['code', 'refactor', 'technical'],\n",
    "        'priority': 2\n",
    "    },\n",
    "    'qwen2.5-coder-0.5b': {\n",
    "        'capabilities': ['code', 'programming', 'debug'],\n",
    "        'priority': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Filter to only include models recommended for this system\n",
    "CATALOG = {k: v for k, v in CATALOG.items() if k in model_aliases}\n",
    "\n",
    "print('ЁЯУЛ Active Model Catalog (Hardware-Optimized Aliases)')\n",
    "print('=' * 70)\n",
    "print('ЁЯТб Using model aliases - Foundry automatically selects CPU variants')\n",
    "print()\n",
    "for model, info in CATALOG.items():\n",
    "    caps = ', '.join(info['capabilities'])\n",
    "    print(f'   тАв {model}')\n",
    "    print(f'     Capabilities: {caps}')\n",
    "    print(f'     Priority: {info[\"priority\"]}')\n",
    "    print()\n",
    "\n",
    "# Intent detection rules (regex pattern -> intent label)\n",
    "INTENT_RULES = [\n",
    "    (re.compile(r'code|refactor|function|debug|program', re.I), 'code'),\n",
    "    (re.compile(r'summar|abstract|tl;?dr|brief', re.I), 'summarize'),\n",
    "    (re.compile(r'classif|categor|label|sentiment', re.I), 'classification'),\n",
    "    (re.compile(r'explain|teach|describe', re.I), 'general'),\n",
    "]\n",
    "\n",
    "def detect_intent(prompt: str) -> str:\n",
    "    \"\"\"Detect intent from prompt using regex patterns.\n",
    "    \n",
    "    Args:\n",
    "        prompt: User input text\n",
    "        \n",
    "    Returns:\n",
    "        Intent label: 'code', 'summarize', 'classification', or 'general'\n",
    "    \"\"\"\n",
    "    for pattern, intent in INTENT_RULES:\n",
    "        if pattern.search(prompt):\n",
    "            return intent\n",
    "    return 'general'\n",
    "\n",
    "def pick_model(intent: str) -> str:\n",
    "    \"\"\"Select best model for intent based on capabilities and priority.\n",
    "    \n",
    "    Args:\n",
    "        intent: Detected intent category\n",
    "        \n",
    "    Returns:\n",
    "        Model alias string, or first available model if no match\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        (alias, info['priority']) \n",
    "        for alias, info in CATALOG.items() \n",
    "        if intent in info['capabilities']\n",
    "    ]\n",
    "    \n",
    "    if candidates:\n",
    "        # Sort by priority (higher = better)\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return candidates[0][0]\n",
    "    \n",
    "    # Fallback to first available model\n",
    "    return list(CATALOG.keys())[0] if CATALOG else None\n",
    "\n",
    "print('тЬЕ Intent detection and model selection configured')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb6d09",
   "metadata": {},
   "source": [
    "## ЁЯзк рдЪрд░рдг 5: рд╣реЗрддреВ рд╢реЛрдзрдгреНрдпрд╛рдЪреА рдЪрд╛рдЪрдгреА рдХрд░рд╛\n",
    "\n",
    "рдкрд╣рд╛ рдХреА рд╣реЗрддреВ рд╢реЛрдзрдгреНрдпрд╛рдЪреА рдкреНрд░рдгрд╛рд▓реА рд╡рд┐рд╡рд┐рдз рдкреНрд░рдХрд╛рд░рдЪреНрдпрд╛ рдкреНрд░реЙрдореНрдкреНрдЯреНрд╕рдЪреЗ рдпреЛрдЧреНрдп рд╡рд░реНрдЧреАрдХрд░рдг рдХрд░рддреЗ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0fd85468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯзк Testing Intent Detection\n",
      "======================================================================\n",
      "\n",
      "Prompt: Refactor this Python function for better readabili...\n",
      "   Intent: code            тЖТ Model: phi-3.5-mini\n",
      "\n",
      "Prompt: Summarize the key points of this article...\n",
      "   Intent: summarize       тЖТ Model: phi-4-mini\n",
      "\n",
      "Prompt: Classify this customer feedback as positive or neg...\n",
      "   Intent: classification  тЖТ Model: qwen2.5-0.5b\n",
      "\n",
      "Prompt: Explain how edge AI differs from cloud AI...\n",
      "   Intent: general         тЖТ Model: phi-4-mini\n",
      "\n",
      "Prompt: Write a function to calculate fibonacci numbers...\n",
      "   Intent: code            тЖТ Model: phi-3.5-mini\n",
      "\n",
      "Prompt: Give me a brief overview of small language models...\n",
      "   Intent: summarize       тЖТ Model: phi-4-mini\n",
      "\n",
      "======================================================================\n",
      "тЬЕ Intent detection working correctly\n"
     ]
    }
   ],
   "source": [
    "# Test intent detection with sample prompts\n",
    "test_prompts = [\n",
    "    'Refactor this Python function for better readability',\n",
    "    'Summarize the key points of this article',\n",
    "    'Classify this customer feedback as positive or negative',\n",
    "    'Explain how edge AI differs from cloud AI',\n",
    "    'Write a function to calculate fibonacci numbers',\n",
    "    'Give me a brief overview of small language models'\n",
    "]\n",
    "\n",
    "print('ЁЯзк Testing Intent Detection')\n",
    "print('=' * 70)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    intent = detect_intent(prompt)\n",
    "    model = pick_model(intent)\n",
    "    print(f'\\nPrompt: {prompt[:50]}...')\n",
    "    print(f'   Intent: {intent:15s} тЖТ Model: {model}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('тЬЕ Intent detection working correctly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6a08b",
   "metadata": {},
   "source": [
    "## ЁЯЪА рдЪрд░рдг 6: рд░реВрдЯрд┐рдВрдЧ рдлрдВрдХреНрд╢рди рд▓рд╛рдЧреВ рдХрд░рд╛\n",
    "\n",
    "рдореБрдЦреНрдп рд░реВрдЯрд┐рдВрдЧ рдлрдВрдХреНрд╢рди рддрдпрд╛рд░ рдХрд░рд╛ рдЬреЛ:\n",
    "1. рдкреНрд░реЙрдореНрдкреНрдЯрдордзреВрди рд╣реЗрддреВ рдУрд│рдЦрддреЛ\n",
    "2. рд╕рд░реНрд╡реЛрддреНрддрдо рдореЙрдбреЗрд▓ рдирд┐рд╡рдбрддреЛ\n",
    "3. Foundry Local SDK рдЪреНрдпрд╛ рдорд╛рдзреНрдпрдорд╛рддреВрди рд╡рд┐рдирдВрддреА рдкреВрд░реНрдг рдХрд░рддреЛ\n",
    "4. рдЯреЛрдХрди рд╡рд╛рдкрд░ рдЖрдгрд┐ рддреНрд░реБрдЯреАрдВрдЪреЗ рдЯреНрд░реЕрдХрд┐рдВрдЧ рдХрд░рддреЛ\n",
    "\n",
    "**workshop_utils рдкреЕрдЯрд░реНрди рд╡рд╛рдкрд░рддреЛ**:\n",
    "- рдПрдХреНрд╕рдкреЛрдиреЗрдВрд╢рд┐рдпрд▓ рдмреЕрдХрдСрдлрд╕рд╣ рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рдкреБрдирд░реНрдкреНрд░рдпрддреНрди\n",
    "- OpenAI-рд╕реБрд╕рдВрдЧрдд API\n",
    "- рдЯреЛрдХрди рдЯреНрд░реЕрдХрд┐рдВрдЧ рдЖрдгрд┐ рддреНрд░реБрдЯреА рд╣рд╛рддрд╛рд│рдгреА\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "24cc251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Routing function ready\n",
      "   Using Foundry Local SDK via workshop_utils\n",
      "   Token tracking: Enabled\n",
      "   Retry logic: Automatic with exponential backoff\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from workshop_utils import chat_once\n",
    "\n",
    "# Fix RETRY_BACKOFF environment variable if it has comments\n",
    "if 'RETRY_BACKOFF' in os.environ:\n",
    "    retry_val = os.environ['RETRY_BACKOFF'].strip().split()[0]\n",
    "    try:\n",
    "        float(retry_val)\n",
    "        os.environ['RETRY_BACKOFF'] = retry_val\n",
    "    except ValueError:\n",
    "        os.environ['RETRY_BACKOFF'] = '1.0'\n",
    "\n",
    "def route(prompt: str, max_tokens: int = 200, temperature: float = 0.7):\n",
    "    \"\"\"Route prompt to appropriate model based on intent.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Detect intent using regex patterns\n",
    "    2. Select best model by capability + priority\n",
    "    3. Execute via Foundry Local SDK\n",
    "    \n",
    "    Args:\n",
    "        prompt: User input text\n",
    "        max_tokens: Maximum tokens in response\n",
    "        temperature: Sampling temperature (0-1)\n",
    "        \n",
    "    Returns:\n",
    "        Dict with: intent, model, output, tokens, usage, error\n",
    "    \"\"\"\n",
    "    intent = detect_intent(prompt)\n",
    "    model_alias = pick_model(intent)\n",
    "    \n",
    "    if not model_alias:\n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'model': None,\n",
    "            'output': '',\n",
    "            'tokens': None,\n",
    "            'usage': {},\n",
    "            'error': 'No suitable model found'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Call Foundry Local via workshop_utils\n",
    "        text, usage = chat_once(\n",
    "            model_alias,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        # Extract token information\n",
    "        usage_info = {}\n",
    "        if usage:\n",
    "            usage_info['prompt_tokens'] = getattr(usage, 'prompt_tokens', None)\n",
    "            usage_info['completion_tokens'] = getattr(usage, 'completion_tokens', None)\n",
    "            usage_info['total_tokens'] = getattr(usage, 'total_tokens', None)\n",
    "        \n",
    "        # Estimate if not provided\n",
    "        if not usage_info.get('total_tokens'):\n",
    "            est_prompt = len(prompt) // 4\n",
    "            est_completion = len(text or '') // 4\n",
    "            usage_info['estimated_tokens'] = est_prompt + est_completion\n",
    "        \n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'model': model_alias,\n",
    "            'output': (text or '').strip(),\n",
    "            'tokens': usage_info.get('total_tokens') or usage_info.get('estimated_tokens'),\n",
    "            'usage': usage_info,\n",
    "            'error': None\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'model': model_alias,\n",
    "            'output': '',\n",
    "            'tokens': None,\n",
    "            'usage': {},\n",
    "            'error': f'{type(e).__name__}: {str(e)}'\n",
    "        }\n",
    "\n",
    "print('тЬЕ Routing function ready')\n",
    "print('   Using Foundry Local SDK via workshop_utils')\n",
    "print('   Token tracking: Enabled')\n",
    "print('   Retry logic: Automatic with exponential backoff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5c915",
   "metadata": {},
   "source": [
    "## ЁЯОп рдкрд╛рдпрд░реА 7: рд░реВрдЯрд┐рдВрдЧ рдЪрд╛рдЪрдгреНрдпрд╛ рдЪрд╛рд▓рд╡рд╛\n",
    "\n",
    "рд╡рд┐рд╡рд┐рдз рдкреНрд░реЙрдореНрдкреНрдЯрд╕рд╣ рд╕рдВрдкреВрд░реНрдг рд░реВрдЯрд┐рдВрдЧ рдкреНрд░рдгрд╛рд▓реАрдЪреА рдЪрд╛рдЪрдгреА рдШреНрдпрд╛ рдЖрдгрд┐ рджрд╛рдЦрд╡рд╛:\n",
    "- рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рд╣реЗрддреВ рдУрд│рдЦ\n",
    "- рдмреБрджреНрдзрд┐рдорд╛рди рдореЙрдбреЗрд▓ рдирд┐рд╡рдб\n",
    "- рдЕрдиреЗрдХ рдореЙрдбреЗрд▓реНрд╕рд╕рд╣ рд░реВрдЯрд┐рдВрдЧ рдЖрдгрд┐ рдореЙрдбреЗрд▓реНрд╕ рдЯрд┐рдХрд╡реВрди рдареЗрд╡рдгреЗ\n",
    "- рдЯреЛрдХрди рдЯреНрд░реЕрдХрд┐рдВрдЧ рдЖрдгрд┐ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдореЗрдЯреНрд░рд┐рдХреНрд╕\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c46ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯОп Running Intent-Based Routing Tests\n",
      "================================================================================\n",
      "\n",
      "[1/6] Testing prompt...\n",
      "Prompt: Refactor this Python function to make it more efficient and readable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Expected Intent: code\n",
      "   Detected Intent: code тЬЕ\n",
      "   Selected Model:  phi-3.5-mini\n",
      "   тЬЕ Response: To refactor a Python function for efficiency and readability, I would need to see the specific funct...\n",
      "   ЁЯУК Tokens: ~158 (estimated)\n",
      "\n",
      "[2/6] Testing prompt...\n",
      "Prompt: Summarize the key benefits of using small language models at the edge\n",
      "   Expected Intent: summarize\n",
      "   Detected Intent: summarize тЬЕ\n",
      "   Selected Model:  phi-4-mini\n",
      "   тЭМ Error: APIConnectionError: Connection error.\n",
      "\n",
      "[3/6] Testing prompt...\n",
      "Prompt: Classify this user feedback: The app is slow but the UI looks great\n",
      "   Expected Intent: classification\n",
      "   Detected Intent: classification тЬЕ\n",
      "   Selected Model:  qwen2.5-0.5b\n",
      "   тЭМ Error: APIConnectionError: Connection error.\n",
      "\n",
      "[4/6] Testing prompt...\n",
      "Prompt: Explain the difference between local and cloud inference\n",
      "   Expected Intent: general\n",
      "   Detected Intent: general тЬЕ\n",
      "   Selected Model:  phi-4-mini\n",
      "   тЭМ Error: APIConnectionError: Connection error.\n",
      "\n",
      "[5/6] Testing prompt...\n",
      "Prompt: Write a Python function to calculate the Fibonacci sequence\n"
     ]
    }
   ],
   "source": [
    "# Test prompts covering all intent categories\n",
    "test_cases = [\n",
    "    {\n",
    "        'prompt': 'Refactor this Python function to make it more efficient and readable',\n",
    "        'expected_intent': 'code'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Summarize the key benefits of using small language models at the edge',\n",
    "        'expected_intent': 'summarize'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Classify this user feedback: The app is slow but the UI looks great',\n",
    "        'expected_intent': 'classification'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Explain the difference between local and cloud inference',\n",
    "        'expected_intent': 'general'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Write a Python function to calculate the Fibonacci sequence',\n",
    "        'expected_intent': 'code'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Give me a brief overview of the Phi model family',\n",
    "        'expected_intent': 'summarize'\n",
    "    }\n",
    "]\n",
    "\n",
    "print('ЁЯОп Running Intent-Based Routing Tests')\n",
    "print('=' * 80)\n",
    "\n",
    "results = []\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f'\\n[{i}/{len(test_cases)}] Testing prompt...')\n",
    "    print(f'Prompt: {test[\"prompt\"]}')\n",
    "    \n",
    "    result = route(test['prompt'], max_tokens=150)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f'   Expected Intent: {test[\"expected_intent\"]}')\n",
    "    print(f'   Detected Intent: {result[\"intent\"]} {\"тЬЕ\" if result[\"intent\"] == test[\"expected_intent\"] else \"тЪая╕П\"}')\n",
    "    print(f'   Selected Model:  {result[\"model\"]}')\n",
    "    \n",
    "    if result['error']:\n",
    "        print(f'   тЭМ Error: {result[\"error\"]}')\n",
    "    else:\n",
    "        output_preview = result['output'][:100] + '...' if len(result['output']) > 100 else result['output']\n",
    "        print(f'   тЬЕ Response: {output_preview}')\n",
    "        \n",
    "        tokens = result.get('tokens', 0)\n",
    "        if tokens:\n",
    "            usage = result.get('usage', {})\n",
    "            if 'estimated_tokens' in usage:\n",
    "                print(f'   ЁЯУК Tokens: ~{tokens} (estimated)')\n",
    "            else:\n",
    "                print(f'   ЁЯУК Tokens: {tokens}')\n",
    "\n",
    "# Summary statistics\n",
    "print('\\n' + '=' * 80)\n",
    "print('ЁЯУК ROUTING SUMMARY')\n",
    "print('=' * 80)\n",
    "\n",
    "success_count = sum(1 for r in results if not r['error'])\n",
    "total_tokens = sum(r.get('tokens', 0) or 0 for r in results if not r['error'])\n",
    "intent_accuracy = sum(1 for i, r in enumerate(results) if r['intent'] == test_cases[i]['expected_intent'])\n",
    "\n",
    "print(f'Total Prompts:        {len(results)}')\n",
    "print(f'тЬЕ Successful:         {success_count}/{len(results)}')\n",
    "print(f'тЭМ Failed:             {len(results) - success_count}')\n",
    "print(f'ЁЯОп Intent Accuracy:    {intent_accuracy}/{len(results)} ({intent_accuracy/len(results)*100:.1f}%)')\n",
    "print(f'ЁЯУК Total Tokens Used:  {total_tokens}')\n",
    "\n",
    "# Model usage distribution\n",
    "print('\\nЁЯУЛ Model Usage Distribution:')\n",
    "model_counts = {}\n",
    "for r in results:\n",
    "    if r['model']:\n",
    "        model_counts[r['model']] = model_counts.get(r['model'], 0) + 1\n",
    "\n",
    "for model, count in sorted(model_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / len(results)) * 100\n",
    "    print(f'   тАв {model}: {count} requests ({percentage:.1f}%)')\n",
    "\n",
    "if success_count == len(results):\n",
    "    print('\\nЁЯОЙ All routing tests passed successfully!')\n",
    "else:\n",
    "    print(f'\\nтЪая╕П  {len(results) - success_count} test(s) failed')\n",
    "    print('   Check Foundry Local service: foundry service status')\n",
    "    print('   Verify models loaded: foundry model ls')\n",
    "\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764811e",
   "metadata": {},
   "source": [
    "## ЁЯФз рдЪрд░рдг 8: рдкрд░рд╕реНрдкрд░ рдЪрд╛рдЪрдгреА\n",
    "\n",
    "рддреБрдордЪреЗ рд╕реНрд╡рддрдГрдЪреЗ рдкреНрд░реЙрдореНрдкреНрдЯ рд╡рд╛рдкрд░реВрди рд░рд╛рдЙрдЯрд┐рдВрдЧ рдкреНрд░рдгрд╛рд▓реА рдХрд╢реА рдХрд╛рд░реНрдп рдХрд░рддреЗ рддреЗ рдкрд╛рд╣рд╛!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fdd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯОп Interactive Routing Test\n",
      "================================================================================\n",
      "Your prompt: Explain how model quantization reduces memory usage\n",
      "\n",
      "Detected Intent: general\n",
      "Selected Model:  phi-4-mini\n",
      "\n",
      "тЬЕ Response:\n",
      "--------------------------------------------------------------------------------\n",
      "Model quantization is a technique used to reduce the memory footprint of a machine learning model, particularly deep learning models. It works by converting the high-precision weights of a neural network, typically represented as 32-bit floating-point numbers, into lower-precision representations, such as 8-bit integers or even binary values.\n",
      "\n",
      "\n",
      "The primary reason for quantization is to decrease the amount of memory required to store the model's parameters. Since floating-point numbers take up more space than integers, by quantizing the weights, we can significantly reduce the model's size. This reduction in size not only saves memory but also can lead to faster computation during inference, as integer operations are generally faster than floating-point operations on many hardware platforms.\n",
      "\n",
      "\n",
      "However, quantization can introduce some loss of accuracy because the lower precision representation may not capture the full range of values that the floating-point representation can. To mitigate this, techniques such as quantization-aware training can be used, where the model is trained with quantization in mind,\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ЁЯУК Tokens used: 292\n",
      "\n",
      "ЁЯТб Try different prompts to test routing behavior!\n",
      "Detected Intent: general\n",
      "Selected Model:  phi-4-mini\n",
      "\n",
      "тЬЕ Response:\n",
      "--------------------------------------------------------------------------------\n",
      "Model quantization is a technique used to reduce the memory footprint of a machine learning model, particularly deep learning models. It works by converting the high-precision weights of a neural network, typically represented as 32-bit floating-point numbers, into lower-precision representations, such as 8-bit integers or even binary values.\n",
      "\n",
      "\n",
      "The primary reason for quantization is to decrease the amount of memory required to store the model's parameters. Since floating-point numbers take up more space than integers, by quantizing the weights, we can significantly reduce the model's size. This reduction in size not only saves memory but also can lead to faster computation during inference, as integer operations are generally faster than floating-point operations on many hardware platforms.\n",
      "\n",
      "\n",
      "However, quantization can introduce some loss of accuracy because the lower precision representation may not capture the full range of values that the floating-point representation can. To mitigate this, techniques such as quantization-aware training can be used, where the model is trained with quantization in mind,\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ЁЯУК Tokens used: 292\n",
      "\n",
      "ЁЯТб Try different prompts to test routing behavior!\n"
     ]
    }
   ],
   "source": [
    "# Interactive testing - modify the prompt and run this cell\n",
    "custom_prompt = \"Explain how model quantization reduces memory usage\"\n",
    "\n",
    "print('ЁЯОп Interactive Routing Test')\n",
    "print('=' * 80)\n",
    "print(f'Your prompt: {custom_prompt}')\n",
    "print()\n",
    "\n",
    "result = route(custom_prompt, max_tokens=200)\n",
    "\n",
    "print(f'Detected Intent: {result[\"intent\"]}')\n",
    "print(f'Selected Model:  {result[\"model\"]}')\n",
    "print()\n",
    "\n",
    "if result['error']:\n",
    "    print(f'тЭМ Error: {result[\"error\"]}')\n",
    "else:\n",
    "    print('тЬЕ Response:')\n",
    "    print('-' * 80)\n",
    "    print(result['output'])\n",
    "    print('-' * 80)\n",
    "    \n",
    "    if result['tokens']:\n",
    "        print(f'\\nЁЯУК Tokens used: {result[\"tokens\"]}')\n",
    "\n",
    "print('\\nЁЯТб Try different prompts to test routing behavior!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c17226c",
   "metadata": {},
   "source": [
    "## ЁЯУК рдЪрд░рдг 9: рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рд╡рд┐рд╢реНрд▓реЗрд╖рдг\n",
    "\n",
    "рд░реВрдЯрд┐рдВрдЧ рдкреНрд░рдгрд╛рд▓реАрдЪреА рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдЖрдгрд┐ рдореЙрдбреЗрд▓рдЪрд╛ рдЙрдкрдпреЛрдЧ рд╡рд┐рд╢реНрд▓реЗрд╖рд┐рдд рдХрд░рд╛.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЪб Performance Benchmark\n",
      "================================================================================\n",
      "\n",
      "Prompt: Write a hello world function...\n",
      "   Model: phi-3.5-mini\n",
      "   Time: 3.31s\n",
      "   Tokens: 60\n",
      "\n",
      "Prompt: Write a hello world function...\n",
      "   Model: phi-3.5-mini\n",
      "   Time: 3.31s\n",
      "   Tokens: 60\n",
      "\n",
      "Prompt: Summarize: AI at the edge is powerful...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 84\n",
      "\n",
      "Prompt: Summarize: AI at the edge is powerful...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 84\n",
      "\n",
      "Prompt: Classify: Good product...\n",
      "   Model: qwen2.5-0.5b\n",
      "   Time: 7.21s\n",
      "   Tokens: 69\n",
      "\n",
      "Prompt: Classify: Good product...\n",
      "   Model: qwen2.5-0.5b\n",
      "   Time: 7.21s\n",
      "   Tokens: 69\n",
      "\n",
      "Prompt: Explain edge computing...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 72\n",
      "\n",
      "================================================================================\n",
      "ЁЯУК Performance Statistics:\n",
      "   Average response time: 27.46s\n",
      "   Fastest response:      3.31s\n",
      "   Slowest response:      49.67s\n",
      "\n",
      "ЁЯТб Note: First request may be slower due to model initialization\n",
      "================================================================================\n",
      "\n",
      "Prompt: Explain edge computing...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 72\n",
      "\n",
      "================================================================================\n",
      "ЁЯУК Performance Statistics:\n",
      "   Average response time: 27.46s\n",
      "   Fastest response:      3.31s\n",
      "   Slowest response:      49.67s\n",
      "\n",
      "ЁЯТб Note: First request may be slower due to model initialization\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Performance benchmark\n",
    "benchmark_prompts = [\n",
    "    'Write a hello world function',\n",
    "    'Summarize: AI at the edge is powerful',\n",
    "    'Classify: Good product',\n",
    "    'Explain edge computing'\n",
    "]\n",
    "\n",
    "print('тЪб Performance Benchmark')\n",
    "print('=' * 80)\n",
    "\n",
    "timings = []\n",
    "for prompt in benchmark_prompts:\n",
    "    start = time.time()\n",
    "    result = route(prompt, max_tokens=50)\n",
    "    duration = time.time() - start\n",
    "    timings.append(duration)\n",
    "    \n",
    "    print(f'\\nPrompt: {prompt[:40]}...')\n",
    "    print(f'   Model: {result[\"model\"]}')\n",
    "    print(f'   Time: {duration:.2f}s')\n",
    "    if result.get('tokens'):\n",
    "        print(f'   Tokens: {result[\"tokens\"]}')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('ЁЯУК Performance Statistics:')\n",
    "print(f'   Average response time: {sum(timings)/len(timings):.2f}s')\n",
    "print(f'   Fastest response:      {min(timings):.2f}s')\n",
    "print(f'   Slowest response:      {max(timings):.2f}s')\n",
    "print('\\nЁЯТб Note: First request may be slower due to model initialization')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db64ff",
   "metadata": {},
   "source": [
    "## ЁЯОУ рдореБрдЦреНрдп рдореБрджреНрджреЗ рдЖрдгрд┐ рдкреБрдвреАрд▓ рдкрд╛рд╡рд▓реЗ\n",
    "\n",
    "### тЬЕ рддреБрдореНрд╣реА рдХрд╛рдп рд╢рд┐рдХрд▓рд╛рдд\n",
    "\n",
    "1. **рдЗрдВрдЯреЗрдВрдЯ-рдЖрдзрд╛рд░рд┐рдд рд░реВрдЯрд┐рдВрдЧ**: рдкреНрд░реЙрдореНрдкреНрдЯреНрд╕ рдЖрдкреЛрдЖрдк рд╡рд░реНрдЧреАрдХреГрдд рдХрд░рд╛ рдЖрдгрд┐ рд╡рд┐рд╢реЗрд╖ рдореЙрдбреЗрд▓реНрд╕рдХрдбреЗ рд░реВрдЯ рдХрд░рд╛  \n",
    "2. **рдореЗрдорд░реА-рдЕрд╡реЗрдпрд░ рдирд┐рд╡рдб**: рдЙрдкрд▓рдмреНрдз рд╕рд┐рд╕реНрдЯрдо RAM рдЪреНрдпрд╛ рдЖрдзрд╛рд░реЗ CPU рдореЙрдбреЗрд▓реНрд╕ рдирд┐рд╡рдбрд╛  \n",
    "3. **рдорд▓реНрдЯреА-рдореЙрдбреЗрд▓ рд░рд┐рдЯреЗрдиреНрд╢рди**: `--retain true` рд╡рд╛рдкрд░реВрди рдЕрдиреЗрдХ рдореЙрдбреЗрд▓реНрд╕ рд▓реЛрдб рдареЗрд╡рд╛  \n",
    "4. **рдкреНрд░реЙрдбрдХреНрд╢рди рдкреЕрдЯрд░реНрдиреНрд╕**: рд░рд┐рдЯреНрд░рд╛рдп рд▓реЙрдЬрд┐рдХ, рдПрд░рд░ рд╣рдБрдбрд▓рд┐рдВрдЧ, рдЖрдгрд┐ рдЯреЛрдХрди рдЯреНрд░реЕрдХрд┐рдВрдЧ  \n",
    "5. **CPU рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди**: GPU рд╢рд┐рд╡рд╛рдп рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрдиреЗ рдбрд┐рдкреНрд▓реЙрдп рдХрд░рд╛  \n",
    "\n",
    "### ЁЯЪА рдкреНрд░рдпреЛрдЧ рдХрд▓реНрдкрдирд╛\n",
    "\n",
    "1. **рдХрд╕реНрдЯрдо рдЗрдВрдЯреЗрдВрдЯреНрд╕ рдЬреЛрдбрд╛**:  \n",
    "   ```python\n",
    "   INTENT_RULES.append(\n",
    "       (re.compile(r'translate|convert', re.I), 'translation')\n",
    "   )\n",
    "   ```\n",
    "  \n",
    "2. **рдЕрддрд┐рд░рд┐рдХреНрдд рдореЙрдбреЗрд▓реНрд╕ рд▓реЛрдб рдХрд░рд╛**:  \n",
    "   ```bash\n",
    "   foundry model run llama-3.2-1b-cpu --retain true\n",
    "   ```\n",
    "  \n",
    "3. **рдореЙрдбреЗрд▓ рдирд┐рд╡рдб рдЯреНрдпреВрди рдХрд░рд╛**:  \n",
    "   - CATALOG рдордзреНрдпреЗ рдкреНрд░рд╛рдзрд╛рдиреНрдп рдореВрд▓реНрдпреЗ рд╕рдорд╛рдпреЛрдЬрд┐рдд рдХрд░рд╛  \n",
    "   - рдЕрдзрд┐рдХ рдХреНрд╖рдорддрд╛ рдЯреЕрдЧреНрд╕ рдЬреЛрдбрд╛  \n",
    "   - рдлреЙрд▓рдмреЕрдХ рд╕реНрдЯреНрд░реЕрдЯреЗрдЬреА рдЕрдВрдорд▓рд╛рдд рдЖрдгрд╛  \n",
    "\n",
    "4. **рдХрд╛рдордЧрд┐рд░реАрдЪреЗ рдирд┐рд░реАрдХреНрд╖рдг рдХрд░рд╛**:  \n",
    "   ```python\n",
    "   import psutil\n",
    "   print(f\"Memory: {psutil.virtual_memory().percent}%\")\n",
    "   ```\n",
    "  \n",
    "\n",
    "### ЁЯУЪ рдЕрддрд┐рд░рд┐рдХреНрдд рд╕рдВрд╕рд╛рдзрдиреЗ\n",
    "\n",
    "- **Foundry Local SDK**: https://github.com/microsoft/Foundry-Local  \n",
    "- **рд╡рд░реНрдХрд╢реЙрдк рдирдореБрдиреЗ**: ../samples/  \n",
    "- **рдПрдЬ AI рдХреЛрд░реНрд╕**: ../../Module08/  \n",
    "\n",
    "### ЁЯТб рд╕рд░реНрд╡реЛрддреНрддрдо рдкрджреНрдзрддреА\n",
    "\n",
    "тЬЕ рдХреНрд░реЙрд╕-рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо рд╕реБрд╕рдВрдЧрддрддреЗрд╕рд╛рдареА CPU рдореЙрдбреЗрд▓реНрд╕ рд╡рд╛рдкрд░рд╛  \n",
    "тЬЕ рдЕрдиреЗрдХ рдореЙрдбреЗрд▓реНрд╕ рд▓реЛрдб рдХрд░рдгреНрдпрд╛рдкреВрд░реНрд╡реА рд╕рд┐рд╕реНрдЯрдо рдореЗрдорд░реА рдиреЗрд╣рдореА рддрдкрд╛рд╕рд╛  \n",
    "тЬЕ рд░реВрдЯрд┐рдВрдЧ рдкрд░рд┐рд╕реНрдерд┐рддреАрд╕рд╛рдареА `--retain true` рд╡рд╛рдкрд░рд╛  \n",
    "тЬЕ рдпреЛрдЧреНрдп рдПрд░рд░ рд╣рдБрдбрд▓рд┐рдВрдЧ рдЖрдгрд┐ рд░рд┐рдЯреНрд░рд╛рдп рдЕрдВрдорд▓рд╛рдд рдЖрдгрд╛  \n",
    "тЬЕ рдЦрд░реНрдЪ/рдХрд╛рдордЧрд┐рд░реА рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рдирд╕рд╛рдареА рдЯреЛрдХрди рд╡рд╛рдкрд░ рдЯреНрд░реЕрдХ рдХрд░рд╛  \n",
    "\n",
    "---\n",
    "\n",
    "**ЁЯОЙ рдЕрднрд┐рдирдВрджрди!** рддреБрдореНрд╣реА Foundry Local SDK рд╡рд╛рдкрд░реВрди CPU-рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭреНрдб рдореЙрдбреЗрд▓реНрд╕рд╕рд╣ рдЙрддреНрдкрд╛рджрди-рддрдпрд╛рд░ рдЗрдВрдЯреЗрдВрдЯ-рдЖрдзрд╛рд░рд┐рдд рдореЙрдбреЗрд▓ рд░рд╛рдЙрдЯрд░ рддрдпрд╛рд░ рдХреЗрд▓рд╛ рдЖрд╣реЗ!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**рдЕрд╕реНрд╡реАрдХрд░рдг**:  \nрд╣рд╛ рджрд╕реНрддрдРрд╡рдЬ AI рднрд╛рд╖рд╛рдВрддрд░ рд╕реЗрд╡рд╛ [Co-op Translator](https://github.com/Azure/co-op-translator) рдЪрд╛ рд╡рд╛рдкрд░ рдХрд░реВрди рднрд╛рд╖рд╛рдВрддрд░рд┐рдд рдХрд░рдгреНрдпрд╛рдд рдЖрд▓рд╛ рдЖрд╣реЗ. рдЖрдореНрд╣реА рдЕрдЪреВрдХрддреЗрд╕рд╛рдареА рдкреНрд░рдпрддреНрдирд╢реАрд▓ рдЕрд╕рд▓реЛ рддрд░реА рдХреГрдкрдпрд╛ рд▓рдХреНрд╖рд╛рдд рдареЗрд╡рд╛ рдХреА рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рднрд╛рд╖рд╛рдВрддрд░реЗ рддреНрд░реБрдЯреА рдХрд┐рдВрд╡рд╛ рдЕрдЪреВрдХрддреЗрдЪреНрдпрд╛ рдЕрднрд╛рд╡рд╛рдиреЗ рдпреБрдХреНрдд рдЕрд╕реВ рд╢рдХрддрд╛рдд. рдореВрд│ рднрд╛рд╖реЗрддреАрд▓ рджрд╕реНрддрдРрд╡рдЬ рд╣рд╛ рдЕрдзрд┐рдХреГрдд рд╕реНрд░реЛрдд рдорд╛рдирд▓рд╛ рдЬрд╛рд╡рд╛. рдорд╣рддреНрддреНрд╡рд╛рдЪреНрдпрд╛ рдорд╛рд╣рд┐рддреАрд╕рд╛рдареА рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рдорд╛рдирд╡реА рднрд╛рд╖рд╛рдВрддрд░рд╛рдЪреА рд╢рд┐рдлрд╛рд░рд╕ рдХреЗрд▓реА рдЬрд╛рддреЗ. рдпрд╛ рднрд╛рд╖рд╛рдВрддрд░рд╛рдЪрд╛ рд╡рд╛рдкрд░ рдХрд░реВрди рдЙрджреНрднрд╡рд▓реЗрд▓реНрдпрд╛ рдХреЛрдгрддреНрдпрд╛рд╣реА рдЧреИрд░рд╕рдордЬ рдХрд┐рдВрд╡рд╛ рдЪреБрдХреАрдЪреНрдпрд╛ рдЕрд░реНрдерд╛рд╕рд╛рдареА рдЖрдореНрд╣реА рдЬрдмрд╛рдмрджрд╛рд░ рд░рд╛рд╣рдгрд╛рд░ рдирд╛рд╣реА.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "149e1ff0f023ecf1f4221663a7928ff5",
   "translation_date": "2025-10-09T09:57:37+00:00",
   "source_file": "Workshop/notebooks/session06_models_router.ipynb",
   "language_code": "mr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}