<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-17T19:44:48+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "mr"
}
-->
# рд╡рд┐рднрд╛рдЧ 2: Qwen рдХреБрдЯреБрдВрдмрд╛рдЪреА рдореВрд▓рднреВрдд рдорд╛рд╣рд┐рддреА

Qwen рдореЙрдбреЗрд▓ рдХреБрдЯреБрдВрдм Alibaba Cloud рдЪреНрдпрд╛ рдореЛрдареНрдпрд╛ рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓реНрд╕ рдЖрдгрд┐ рдорд▓реНрдЯреАрдореЛрдбрд▓ AI рд╕рд╛рдареА рд╡реНрдпрд╛рдкрдХ рджреГрд╖реНрдЯрд┐рдХреЛрдирд╛рдЪреЗ рдкреНрд░рддрд┐рдирд┐рдзрд┐рддреНрд╡ рдХрд░рддреЗ, рд╣реЗ рджрд░реНрд╢рд╡рд┐рддреЗ рдХреА рдУрдкрди-рд╕реЛрд░реНрд╕ рдореЙрдбреЗрд▓реНрд╕ рдЙрд▓реНрд▓реЗрдЦрдиреАрдп рдХрд╛рдордЧрд┐рд░реА рд╕рд╛рдзреНрдп рдХрд░реВ рд╢рдХрддрд╛рдд рдЖрдгрд┐ рд╡рд┐рд╡рд┐рдз рддреИрдирд╛рддреА рдкрд░рд┐рд╕реНрдерд┐рддреАрдВрдордзреНрдпреЗ рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдп рдЕрд╕реВ рд╢рдХрддрд╛рдд. Qwen рдХреБрдЯреБрдВрдм рд╢рдХреНрддрд┐рд╢рд╛рд▓реА AI рдХреНрд╖рдорддрд╛ рд╕рдХреНрд╖рдо рдХрд╕реЗ рдХрд░рддреЗ рд╣реЗ рд╕рдордЬреВрди рдШреЗрдгреЗ рдорд╣рддреНрддреНрд╡рд╛рдЪреЗ рдЖрд╣реЗ, рд▓рд╡рдЪрд┐рдХ рддреИрдирд╛рддреА рдкрд░реНрдпрд╛рдпрд╛рдВрд╕рд╣, рд╡рд┐рд╡рд┐рдз рдХрд╛рд░реНрдпрд╛рдВрдордзреНрдпреЗ рд╕реНрдкрд░реНрдзрд╛рддреНрдордХ рдХрд╛рдордЧрд┐рд░реА рд░рд╛рдЦрддрд╛рдирд╛.

## рд╡рд┐рдХрд╕рдХрд╛рдВрд╕рд╛рдареА рд╕рдВрд╕рд╛рдзрдиреЗ

### Hugging Face рдореЙрдбреЗрд▓ рд░рд┐рдкреЙрдЭрд┐рдЯрд░реА
рдирд┐рд╡рдбрд▓реЗрд▓реЗ Qwen рдХреБрдЯреБрдВрдм рдореЙрдбреЗрд▓реНрд╕ [Hugging Face](https://huggingface.co/models?search=qwen) рджреНрд╡рд╛рд░реЗ рдЙрдкрд▓рдмреНрдз рдЖрд╣реЗрдд, рдЬреЗ рдпрд╛ рдореЙрдбреЗрд▓реНрд╕рдЪреНрдпрд╛ рдХрд╛рд╣реА рдкреНрд░рдХрд╛рд░рд╛рдВрдордзреНрдпреЗ рдкреНрд░рд╡реЗрд╢ рдкреНрд░рджрд╛рди рдХрд░рддрд╛рдд. рддреБрдореНрд╣реА рдЙрдкрд▓рдмреНрдз рдкреНрд░рдХрд╛рд░ рдПрдХреНрд╕рдкреНрд▓реЛрд░ рдХрд░реВ рд╢рдХрддрд╛, рддреБрдордЪреНрдпрд╛ рд╡рд┐рд╢рд┐рд╖реНрдЯ рд╡рд╛рдкрд░ рдкреНрд░рдХрд░рдгрд╛рдВрд╕рд╛рдареА рддреНрдпрд╛рдВрдирд╛ рдлрд╛рдЗрди-рдЯреНрдпреВрди рдХрд░реВ рд╢рдХрддрд╛ рдЖрдгрд┐ рд╡рд┐рд╡рд┐рдз рдлреНрд░реЗрдорд╡рд░реНрдХрджреНрд╡рд╛рд░реЗ рддреИрдирд╛рдд рдХрд░реВ рд╢рдХрддрд╛.

### рд╕реНрдерд╛рдирд┐рдХ рд╡рд┐рдХрд╛рд╕ рд╕рд╛рдзрдиреЗ
рд╕реНрдерд╛рдирд┐рдХ рд╡рд┐рдХрд╛рд╕ рдЖрдгрд┐ рдЪрд╛рдЪрдгреАрд╕рд╛рдареА, рддреБрдореНрд╣реА [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) рд╡рд╛рдкрд░реВ рд╢рдХрддрд╛ рдЬреЗрдгреЗрдХрд░реВрди рддреБрдордЪреНрдпрд╛ рд╡рд┐рдХрд╛рд╕ рдорд╢реАрдирд╡рд░ рдЙрдкрд▓рдмреНрдз Qwen рдореЙрдбреЗрд▓реНрд╕ рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХреЗрд▓реЗрд▓реНрдпрд╛ рдХрд╛рдордЧрд┐рд░реАрд╕рд╣ рдЪрд╛рд▓рд╡рддрд╛ рдпреЗрддреАрд▓.

### рджрд╕реНрддрдРрд╡рдЬ рд╕рдВрд╕рд╛рдзрдиреЗ
- [Qwen рдореЙрдбреЗрд▓ рджрд╕реНрддрдРрд╡рдЬ](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Qwen рдореЙрдбреЗрд▓реНрд╕ рдПрдЬ рддреИрдирд╛рддреАрд╕рд╛рдареА рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХрд░рдгреЗ](https://github.com/microsoft/olive)

## рдкрд░рд┐рдЪрдп

рдпрд╛ рдЯреНрдпреВрдЯреЛрд░рд┐рдпрд▓рдордзреНрдпреЗ, рдЖрдкрдг Alibaba рдЪреНрдпрд╛ Qwen рдореЙрдбреЗрд▓ рдХреБрдЯреБрдВрдм рдЖрдгрд┐ рддреНрдпрд╛рдЪреНрдпрд╛ рдореВрд▓рднреВрдд рд╕рдВрдХрд▓реНрдкрдирд╛рдВрдЪрд╛ рдЕрднреНрдпрд╛рд╕ рдХрд░реВ. рдЖрдкрдг Qwen рдХреБрдЯреБрдВрдмрд╛рдЪрд╛ рд╡рд┐рдХрд╛рд╕, Qwen рдореЙрдбреЗрд▓реНрд╕ рдкреНрд░рднрд╛рд╡реА рдмрдирд╡рдгрд╛рд▒реНрдпрд╛ рдирд╛рд╡рд┐рдиреНрдпрдкреВрд░реНрдг рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдкрджреНрдзрддреА, рдХреБрдЯреБрдВрдмрд╛рддреАрд▓ рдкреНрд░рдореБрдЦ рдкреНрд░рдХрд╛рд░ рдЖрдгрд┐ рд╡рд┐рд╡рд┐рдз рдкрд░рд┐рд╕реНрдерд┐рддреАрдВрдордзреНрдпреЗ рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рдЕрдиреБрдкреНрд░рдпреЛрдЧ рдпрд╛рд╡рд░ рдЪрд░реНрдЪрд╛ рдХрд░реВ.

## рд╢рд┐рдХрдгреНрдпрд╛рдЪреА рдЙрджреНрджрд┐рд╖реНрдЯреЗ

рдпрд╛ рдЯреНрдпреВрдЯреЛрд░рд┐рдпрд▓рдЪреНрдпрд╛ рд╢реЗрд╡рдЯреА, рддреБрдореНрд╣реА рдЦрд╛рд▓реАрд▓ рдЧреЛрд╖реНрдЯреА рд╕рдХреНрд╖рдордкрдгреЗ рдХрд░реВ рд╢рдХрд╛рд▓:

- Alibaba рдЪреНрдпрд╛ Qwen рдореЙрдбреЗрд▓ рдХреБрдЯреБрдВрдмрд╛рдЪреА рдбрд┐рдЭрд╛рдЗрди рддрддреНрддреНрд╡рдЬреНрдЮрд╛рди рдЖрдгрд┐ рд╡рд┐рдХрд╛рд╕ рд╕рдордЬреВрди рдШреЗрдгреЗ
- рд╡рд┐рд╡рд┐рдз рдкреЕрд░рд╛рдореАрдЯрд░ рдЖрдХрд╛рд░рд╛рдВрдордзреНрдпреЗ рдЙрдЪреНрдЪ рдХрд╛рдордЧрд┐рд░реА рд╕рд╛рдзреНрдп рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА Qwen рдореЙрдбреЗрд▓реНрд╕ рд╕рдХреНрд╖рдо рдХрд░рдгрд╛рд▒реНрдпрд╛ рдкреНрд░рдореБрдЦ рдирд╛рд╡рд┐рдиреНрдпрдкреВрд░реНрдг рдЧреЛрд╖реНрдЯреА рдУрд│рдЦрдгреЗ
- рд╡рд┐рд╡рд┐рдз Qwen рдореЙрдбреЗрд▓ рдкреНрд░рдХрд╛рд░рд╛рдВрдЪреЗ рдлрд╛рдпрджреЗ рдЖрдгрд┐ рдорд░реНрдпрд╛рджрд╛ рдУрд│рдЦрдгреЗ
- рд╡рд╛рд╕реНрддрд╡рд┐рдХ рдкрд░рд┐рд╕реНрдерд┐рддреАрд╕рд╛рдареА рдпреЛрдЧреНрдп рдкреНрд░рдХрд╛рд░ рдирд┐рд╡рдбрдгреНрдпрд╛рд╕рд╛рдареА Qwen рдореЙрдбреЗрд▓реНрд╕рдЪреЗ рдЬреНрдЮрд╛рди рд▓рд╛рдЧреВ рдХрд░рдгреЗ

## рдЖрдзреБрдирд┐рдХ AI рдореЙрдбреЗрд▓ рд▓рдБрдбрд╕реНрдХреЗрдк рд╕рдордЬреВрди рдШреЗрдгреЗ

AI рд▓рдБрдбрд╕реНрдХреЗрдкрдордзреНрдпреЗ рд▓рдХреНрд╖рдгреАрдп рдмрджрд▓ рдЭрд╛рд▓реЗ рдЖрд╣реЗрдд, рдЬреНрдпрд╛рдордзреНрдпреЗ рд╡рд┐рд╡рд┐рдз рд╕рдВрд╕реНрдерд╛рдВрдиреА рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓ рд╡рд┐рдХрд╛рд╕рд╛рд╕рд╛рдареА рд╡рд┐рд╡рд┐рдз рджреГрд╖реНрдЯрд┐рдХреЛрди рд╕реНрд╡реАрдХрд╛рд░рд▓реЗ рдЖрд╣реЗрдд. рдХрд╛рд╣реА рдмрдВрдж-рд╕реНрд░реЛрдд рдореЙрдбреЗрд▓реНрд╕рд╡рд░ рд▓рдХреНрд╖ рдХреЗрдВрджреНрд░рд┐рдд рдХрд░рддрд╛рдд, рддрд░ рдХрд╛рд╣реА рдУрдкрди-рд╕реЛрд░реНрд╕ рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдпрддрд╛ рдЖрдгрд┐ рдкрд╛рд░рджрд░реНрд╢рдХрддреЗрд╡рд░ рдЬреЛрд░ рджреЗрддрд╛рдд. рдкрд╛рд░рдВрдкрд░рд┐рдХ рджреГрд╖реНрдЯрд┐рдХреЛрдирд╛рдордзреНрдпреЗ рдкреНрд░рдЪрдВрдб рдорд╛рд▓рдХреАрдЪреЗ рдореЙрдбреЗрд▓реНрд╕ рд╕рдорд╛рд╡рд┐рд╖реНрдЯ рдЕрд╕рддрд╛рдд рдЬреЗ рдлрдХреНрдд API рджреНрд╡рд╛рд░реЗ рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдп рдЕрд╕рддрд╛рдд рдХрд┐рдВрд╡рд╛ рдУрдкрди-рд╕реЛрд░реНрд╕ рдореЙрдбреЗрд▓реНрд╕ рдЬреЗ рдХреНрд╖рдорддрд╛рдВрдордзреНрдпреЗ рдорд╛рдЧреЗ рд░рд╛рд╣реВ рд╢рдХрддрд╛рдд.

рд╣реА рдкрджреНрдзрдд рд╢рдХреНрддрд┐рд╢рд╛рд▓реА AI рдХреНрд╖рдорддрд╛ рд╢реЛрдзрдгрд╛рд▒реНрдпрд╛ рд╕рдВрд╕реНрдерд╛рдВрд╕рд╛рдареА рдЖрд╡реНрд╣рд╛рдиреЗ рдирд┐рд░реНрдорд╛рдг рдХрд░рддреЗ, рддреНрдпрд╛рдВрдЪрд╛ рдбреЗрдЯрд╛, рдЦрд░реНрдЪ рдЖрдгрд┐ рддреИрдирд╛рддреАрдЪреА рд▓рд╡рдЪрд┐рдХрддрд╛ рдирд┐рдпрдВрддреНрд░рд┐рдд рдХрд░рддрд╛рдирд╛. рдкрд╛рд░рдВрдкрд░рд┐рдХ рджреГрд╖реНрдЯрд┐рдХреЛрди рдкреНрд░рдЧрдд рдХрд╛рдордЧрд┐рд░реА рдЖрдгрд┐ рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рддреИрдирд╛рддреА рд╡рд┐рдЪрд╛рд░рд╛рдВрдордзреНрдпреЗ рдирд┐рд╡рдб рдХрд░рдгреНрдпрд╛рдЪреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рдирд┐рд░реНрдорд╛рдг рдХрд░рддреЛ.

## рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдп AI рдЙрддреНрдХреГрд╖реНрдЯрддреЗрдЪреЗ рдЖрд╡реНрд╣рд╛рди

рдЙрдЪреНрдЪ-рдЧреБрдгрд╡рддреНрддреЗрдЪреНрдпрд╛, рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдп AI рдЪреА рдЧрд░рдЬ рд╡рд┐рд╡рд┐рдз рдкрд░рд┐рд╕реНрдерд┐рддреАрдВрдордзреНрдпреЗ рдЕрдзрд┐рдХрд╛рдзрд┐рдХ рдорд╣рддреНрддреНрд╡рд╛рдЪреА рд╣реЛрдд рдЖрд╣реЗ. рд▓рд╡рдЪрд┐рдХ рддреИрдирд╛рддреА рдкрд░реНрдпрд╛рдпрд╛рдВрдЪреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рдЕрд╕рд▓реЗрд▓реНрдпрд╛ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрдЪрд╛ рд╡рд┐рдЪрд╛рд░ рдХрд░рд╛, рдЬрд┐рдереЗ API рдЦрд░реНрдЪ рдорд╣рддреНрддреНрд╡рдкреВрд░реНрдг рд╣реЛрдК рд╢рдХрддреЛ, рдЬрд╛рдЧрддрд┐рдХ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдмрд╣реБрднрд╛рд╖рд┐рдХ рдХреНрд╖рдорддрд╛ рдХрд┐рдВрд╡рд╛ рдХреЛрдбрд┐рдВрдЧ рдЖрдгрд┐ рдЧрдгрд┐рдд рдпрд╛рд╕рд╛рд░рдЦреНрдпрд╛ рдХреНрд╖реЗрддреНрд░рд╛рдВрдордзреНрдпреЗ рд╡рд┐рд╢реЗрд╖ рдбреЛрдореЗрди рддрдЬреНрдЮрддрд╛.

### рдкреНрд░рдореБрдЦ рддреИрдирд╛рддреА рдЖрд╡рд╢реНрдпрдХрддрд╛

рдЖрдзреБрдирд┐рдХ AI рддреИрдирд╛рддреАрдВрдирд╛ рдЕрдиреЗрдХ рдореВрд▓рднреВрдд рдЖрд╡рд╢реНрдпрдХрддрд╛ рдЖрд╣реЗрдд рдЬреНрдпрд╛ рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рдЙрдкрдпреЛрдЧрд┐рддреЗрд▓рд╛ рдорд░реНрдпрд╛рджрд┐рдд рдХрд░рддрд╛рдд:

- **рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдпрддрд╛**: рдкрд╛рд░рджрд░реНрд╢рдХрддрд╛ рдЖрдгрд┐ рд╕рд╛рдиреБрдХреВрд▓рдирд╛рд╕рд╛рдареА рдУрдкрди-рд╕реЛрд░реНрд╕ рдЙрдкрд▓рдмреНрдзрддрд╛
- **рдЦрд░реНрдЪ рдкреНрд░рднрд╛рд╡реАрддрд╛**: рд╡рд┐рд╡рд┐рдз рдмрдЬреЗрдЯрд╕рд╛рдареА рд╡рд╛рдЬрд╡реА рд╕рдВрдЧрдгрдХреАрдп рдЖрд╡рд╢реНрдпрдХрддрд╛
- **рд▓рд╡рдЪрд┐рдХрддрд╛**: рд╡рд┐рд╡рд┐рдз рддреИрдирд╛рддреА рдкрд░рд┐рд╕реНрдерд┐рддреАрд╕рд╛рдареА рдПрдХрд╛рдзрд┐рдХ рдореЙрдбреЗрд▓ рдЖрдХрд╛рд░
- **рдЬрд╛рдЧрддрд┐рдХ рдкреЛрд╣реЛрдЪ**: рдордЬрдмреВрдд рдмрд╣реБрднрд╛рд╖рд┐рдХ рдЖрдгрд┐ рд╕рд╛рдВрд╕реНрдХреГрддрд┐рдХ рдХреНрд╖рдорддрд╛
- **рд╡рд┐рд╢реЗрд╖реАрдХрд░рдг**: рд╡рд┐рд╢рд┐рд╖реНрдЯ рд╡рд╛рдкрд░ рдкреНрд░рдХрд░рдгрд╛рдВрд╕рд╛рдареА рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рдкреНрд░рдХрд╛рд░

## Qwen рдореЙрдбреЗрд▓ рддрддреНрддреНрд╡рдЬреНрдЮрд╛рди

Qwen рдореЙрдбреЗрд▓ рдХреБрдЯреБрдВрдм AI рдореЙрдбреЗрд▓ рд╡рд┐рдХрд╛рд╕рд╛рд╕рд╛рдареА рдПрдХ рд╡реНрдпрд╛рдкрдХ рджреГрд╖реНрдЯрд┐рдХреЛрдирд╛рдЪреЗ рдкреНрд░рддрд┐рдирд┐рдзрд┐рддреНрд╡ рдХрд░рддреЗ, рдУрдкрди-рд╕реЛрд░реНрд╕ рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдпрддрд╛, рдмрд╣реБрднрд╛рд╖рд┐рдХ рдХреНрд╖рдорддрд╛ рдЖрдгрд┐ рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рддреИрдирд╛рддреАрд▓рд╛ рдкреНрд░рд╛рдзрд╛рдиреНрдп рджреЗрдд рдЕрд╕рддрд╛рдирд╛ рд╕реНрдкрд░реНрдзрд╛рддреНрдордХ рдХрд╛рдордЧрд┐рд░реА рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ рд░рд╛рдЦрддреЗ. Qwen рдореЙрдбреЗрд▓реНрд╕ рд╡рд┐рд╡рд┐рдз рдореЙрдбреЗрд▓ рдЖрдХрд╛рд░, рдЙрдЪреНрдЪ-рдЧреБрдгрд╡рддреНрддреЗрдЪреНрдпрд╛ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдкрджреНрдзрддреА рдЖрдгрд┐ рд╡рд┐рд╡рд┐рдз рдбреЛрдореЗрдирд╕рд╛рдареА рд╡рд┐рд╢реЗрд╖ рдкреНрд░рдХрд╛рд░рд╛рдВрджреНрд╡рд╛рд░реЗ рд╣реЗ рд╕рд╛рдзреНрдп рдХрд░рддрд╛рдд.

Qwen рдХреБрдЯреБрдВрдм рдХрд╛рдордЧрд┐рд░реА-рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рд╕реНрдкреЗрдХреНрдЯреНрд░рдордордзреНрдпреЗ рдкрд░реНрдпрд╛рдп рдкреНрд░рджрд╛рди рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рд╡рд┐рд╡рд┐рдз рджреГрд╖реНрдЯрд┐рдХреЛрдирд╛рдВрдЪрд╛ рд╕рдорд╛рд╡реЗрд╢ рдХрд░рддреЗ, рдореЛрдмрд╛рдЗрд▓ рдбрд┐рд╡реНрд╣рд╛рдЗрд╕рдкрд╛рд╕реВрди рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рд╕рд░реНрд╡реНрд╣рд░рдкрд░реНрдпрдВрдд рддреИрдирд╛рддреА рд╕рдХреНрд╖рдо рдХрд░рддреЗ рдЖрдгрд┐ рдЕрд░реНрдердкреВрд░реНрдг AI рдХреНрд╖рдорддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддреЗ. рдЙрдЪреНрдЪ-рдЧреБрдгрд╡рддреНрддреЗрдЪреНрдпрд╛ AI рдЪрд╛ рдкреНрд░рд╡реЗрд╢ рд▓реЛрдХрд╢рд╛рд╣реАрдХрд░рдг рдХрд░рдгреЗ рдЖрдгрд┐ рддреИрдирд╛рддреА рдирд┐рд╡рдбреАрдВрдордзреНрдпреЗ рд▓рд╡рдЪрд┐рдХрддрд╛ рдкреНрд░рджрд╛рди рдХрд░рдгреЗ рд╣реЗ рдЙрджреНрджрд┐рд╖реНрдЯ рдЖрд╣реЗ.

### Qwen рдбрд┐рдЭрд╛рдЗрдирдЪреЗ рдореБрдЦреНрдп рддрддреНрддреНрд╡реЗ

Qwen рдореЙрдбреЗрд▓реНрд╕ рдЗрддрд░ рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓ рдХреБрдЯреБрдВрдмрд╛рдВрдкрд╛рд╕реВрди рд╡реЗрдЧрд│реЗ рдХрд░рдгрд╛рд▒реНрдпрд╛ рдЕрдиреЗрдХ рдореВрд▓рднреВрдд рддрддреНрддреНрд╡рд╛рдВрд╡рд░ рдЖрдзрд╛рд░рд┐рдд рдЖрд╣реЗрдд:

- **рдУрдкрди рд╕реЛрд░реНрд╕ рдкреНрд░рдердо**: рд╕рдВрд╢реЛрдзрди рдЖрдгрд┐ рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рд╡рд╛рдкрд░рд╛рд╕рд╛рдареА рд╕рдВрдкреВрд░реНрдг рдкрд╛рд░рджрд░реНрд╢рдХрддрд╛ рдЖрдгрд┐ рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдпрддрд╛
- **рд╡реНрдпрд╛рдкрдХ рдкреНрд░рд╢рд┐рдХреНрд╖рдг**: рдПрдХрд╛рдзрд┐рдХ рднрд╛рд╖рд╛ рдЖрдгрд┐ рдбреЛрдореЗрди рдХрд╡реНрд╣рд░ рдХрд░рдгрд╛рд▒реНрдпрд╛ рдкреНрд░рдЪрдВрдб, рд╡рд┐рд╡рд┐рдз рдбреЗрдЯрд╛рд╕реЗрдЯрд╡рд░ рдкреНрд░рд╢рд┐рдХреНрд╖рдг
- **рд╕реНрдХреЗрд▓реЗрдмрд▓ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░**: рд╡рд┐рд╡рд┐рдз рд╕рдВрдЧрдгрдХреАрдп рдЖрд╡рд╢реНрдпрдХрддрд╛ рдЬреБрд│рд╡рдгреНрдпрд╛рд╕рд╛рдареА рдПрдХрд╛рдзрд┐рдХ рдореЙрдбреЗрд▓ рдЖрдХрд╛рд░
- **рд╡рд┐рд╢реЗрд╖ рдЙрддреНрдХреГрд╖реНрдЯрддрд╛**: рд╡рд┐рд╢рд┐рд╖реНрдЯ рдХрд╛рд░реНрдпрд╛рдВрд╕рд╛рдареА рдЕрдиреБрдХреВрд▓рд┐рдд рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рдкреНрд░рдХрд╛рд░

## Qwen рдХреБрдЯреБрдВрдм рд╕рдХреНрд╖рдо рдХрд░рдгрд╛рд▒реНрдпрд╛ рдкреНрд░рдореБрдЦ рддрдВрддреНрд░рдЬреНрдЮрд╛рди

### рдкреНрд░рдЪрдВрдб рдкреНрд░рдорд╛рдгрд╛рдд рдкреНрд░рд╢рд┐рдХреНрд╖рдг

Qwen рдХреБрдЯреБрдВрдмрд╛рдЪреА рдПрдХ рдкрд░рд┐рднрд╛рд╖рд┐рдд рдмрд╛рдЬреВ рдореНрд╣рдгрдЬреЗ рдореЙрдбреЗрд▓ рд╡рд┐рдХрд╛рд╕рд╛рдд рдЧреБрдВрддрд╡рд▓реЗрд▓реА рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдбреЗрдЯрд╛ рдЖрдгрд┐ рд╕рдВрдЧрдгрдХреАрдп рд╕рдВрд╕рд╛рдзрдирд╛рдВрдЪреА рдкреНрд░рдЪрдВрдб рдкреНрд░рдорд╛рдг. Qwen рдореЙрдбреЗрд▓реНрд╕ рдХрд╛рд│рдЬреАрдкреВрд░реНрд╡рдХ рдХреНрдпреБрд░реЗрдЯ рдХреЗрд▓реЗрд▓реНрдпрд╛, рдмрд╣реБрднрд╛рд╖рд┐рдХ рдбреЗрдЯрд╛рд╕реЗрдЯреНрд╕рдЪрд╛ рд▓рд╛рдн рдШреЗрддрд╛рдд рдЬреНрдпрд╛рдордзреНрдпреЗ рдЯреНрд░рд┐рд▓рд┐рдпрдиреНрд╕ рдСрдл рдЯреЛрдХрдиреНрд╕ рдЕрд╕рддрд╛рдд, рдЬреЗ рд╡реНрдпрд╛рдкрдХ рдЬрд╛рдЧрддрд┐рдХ рдЬреНрдЮрд╛рди рдЖрдгрд┐ рддрд░реНрдХ рдХреНрд╖рдорддрд╛ рдкреНрд░рджрд╛рди рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдбрд┐рдЭрд╛рдЗрди рдХреЗрд▓реЗрд▓реЗ рдЖрд╣реЗрдд.

рд╣реА рдкрджреНрдзрдд рдЙрдЪреНрдЪ-рдЧреБрдгрд╡рддреНрддреЗрдЪреНрдпрд╛ рд╡реЗрдм рд╕рд╛рдордЧреНрд░реА, рд╢реИрдХреНрд╖рдгрд┐рдХ рд╕рд╛рд╣рд┐рддреНрдп, рдХреЛрдб рд░рд┐рдкреЙрдЭрд┐рдЯрд░реА рдЖрдгрд┐ рдмрд╣реБрднрд╛рд╖рд┐рдХ рд╕рдВрд╕рд╛рдзрдирд╛рдВрдЪреЗ рд╕рдВрдпреЛрдЬрди рдХрд░реВрди рдХрд╛рд░реНрдп рдХрд░рддреЗ. рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдкрджреНрдзрддреА рд╡рд┐рд╡рд┐рдз рдбреЛрдореЗрди рдЖрдгрд┐ рднрд╛рд╖рд╛рдВрдордзреНрдпреЗ рдЬреНрдЮрд╛рдирд╛рдЪреА рд░реБрдВрджреА рдЖрдгрд┐ рд╕рдордЬрдгреНрдпрд╛рдЪреА рдЦреЛрд▓реА рдпрд╛рд╡рд░ рднрд░ рджреЗрддреЗ.

### рдкреНрд░рдЧрдд рддрд░реНрдХ рдЖрдгрд┐ рд╡рд┐рдЪрд╛рд░

рдЕрд▓реАрдХрдбреАрд▓ Qwen рдореЙрдбреЗрд▓реНрд╕рдордзреНрдпреЗ рдкрд░рд┐рд╖реНрдХреГрдд рддрд░реНрдХ рдХреНрд╖рдорддрд╛ рд╕рдорд╛рд╡рд┐рд╖реНрдЯ рдЖрд╣реЗрдд рдЬреНрдпрд╛ рдЬрдЯрд┐рд▓ рдорд▓реНрдЯреА-рд╕реНрдЯреЗрдк рд╕рдорд╕реНрдпрд╛ рд╕реЛрдбрд╡рдгреНрдпрд╛рд╕ рд╕рдХреНрд╖рдо рдХрд░рддрд╛рдд:

**Thinking Mode (Qwen3)**: рдореЙрдбреЗрд▓реНрд╕ рдЕрдВрддрд┐рдо рдЙрддреНрддрд░ рджреЗрдгреНрдпрд╛рдкреВрд░реНрд╡реА рддрдкрд╢реАрд▓рд╡рд╛рд░ рдЪрд░рдг-рджрд░-рдЪрд░рдг рддрд░реНрдХрд╛рдд рдЧреБрдВрддреВ рд╢рдХрддрд╛рдд, рдорд╛рдирд╡реА рд╕рдорд╕реНрдпрд╛ рд╕реЛрдбрд╡рдгреНрдпрд╛рдЪреНрдпрд╛ рджреГрд╖реНрдЯрд┐рдХреЛрдирд╛рд╕рд╛рд░рдЦреЗ.

**Dual-Mode Operation**: рд╕реЛрдкреНрдпрд╛ рдХреНрд╡реЗрд░реАрд╕рд╛рдареА рдЬрд▓рдж рдкреНрд░рддрд┐рд╕рд╛рдж рдореЛрдб рдЖрдгрд┐ рдЬрдЯрд┐рд▓ рд╕рдорд╕реНрдпрд╛рдВрд╕рд╛рдареА рд╕рдЦреЛрд▓ рд╡рд┐рдЪрд╛рд░ рдореЛрдб рдпрд╛рдордзреНрдпреЗ рд╕реНрд╡рд┐рдЪ рдХрд░рдгреНрдпрд╛рдЪреА рдХреНрд╖рдорддрд╛.

**Chain-of-Thought Integration**: рддрд░реНрдХ рдЪрд░рдгрд╛рдВрдЪреА рдиреИрд╕рд░реНрдЧрд┐рдХ рд╕рдорд╛рд╡реЗрд╢рд┐рддрд╛ рдЬреА рдЬрдЯрд┐рд▓ рдХрд╛рд░реНрдпрд╛рдВрдордзреНрдпреЗ рдкрд╛рд░рджрд░реНрд╢рдХрддрд╛ рдЖрдгрд┐ рдЕрдЪреВрдХрддрд╛ рд╕реБрдзрд╛рд░рддреЗ.

### рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░рд▓ рдирд╛рд╡рд┐рдиреНрдпрдкреВрд░реНрдгрддрд╛

Qwen рдХреБрдЯреБрдВрдм рдХрд╛рдордЧрд┐рд░реА рдЖрдгрд┐ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рджреЛрдиреНрд╣реАрд╕рд╛рдареА рдбрд┐рдЭрд╛рдЗрди рдХреЗрд▓реЗрд▓реНрдпрд╛ рдЕрдиреЗрдХ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░рд▓ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди рд╕рдорд╛рд╡рд┐рд╖реНрдЯ рдХрд░рддреЗ:

**рд╕реНрдХреЗрд▓реЗрдмрд▓ рдбрд┐рдЭрд╛рдЗрди**: рдореЙрдбреЗрд▓ рдЖрдХрд╛рд░рд╛рдВрдордзреНрдпреЗ рд╕реБрд╕рдВрдЧрдд рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░ рдЬреЗ рд╕реЛрдкреЗ рд╕реНрдХреЗрд▓рд┐рдВрдЧ рдЖрдгрд┐ рддреБрд▓рдирд╛ рд╕рдХреНрд╖рдо рдХрд░рддреЗ.

**рдорд▓реНрдЯреАрдореЛрдбрд▓ рдЗрдВрдЯрд┐рдЧреНрд░реЗрд╢рди**: рдПрдХрддреНрд░рд┐рдд рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░рдордзреНрдпреЗ рдордЬрдХреВрд░, рд╡реНрд╣рд┐рдЬрди рдЖрдгрд┐ рдСрдбрд┐рдУ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рдХреНрд╖рдорддрд╛ рд╕рд╣рдЬрдкрдгреЗ рд╕рдорд╛рдХрд▓рд┐рдд рдХрд░рдгреЗ.

**рддреИрдирд╛рддреА рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди**: рд╡рд┐рд╡рд┐рдз рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рдирд╕рд╛рдареА рдПрдХрд╛рдзрд┐рдХ рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭреЗрд╢рди рдкрд░реНрдпрд╛рдп рдЖрдгрд┐ рддреИрдирд╛рддреА рд╕реНрд╡рд░реВрдк.

## рдореЙрдбреЗрд▓ рдЖрдХрд╛рд░ рдЖрдгрд┐ рддреИрдирд╛рддреА рдкрд░реНрдпрд╛рдп

рдЖрдзреБрдирд┐рдХ рддреИрдирд╛рддреА рд╡рд╛рддрд╛рд╡рд░рдг рд╡рд┐рд╡рд┐рдз рд╕рдВрдЧрдгрдХреАрдп рдЖрд╡рд╢реНрдпрдХрддрд╛ рдУрд▓рд╛рдВрдбреВрди Qwen рдореЙрдбреЗрд▓реНрд╕рдЪреНрдпрд╛ рд▓рд╡рдЪрд┐рдХрддреЗрдЪрд╛ рд▓рд╛рдн рдШреЗрддрд╛рдд:

### рд▓рд╣рд╛рди рдореЙрдбреЗрд▓реНрд╕ (0.5B-3B)

Qwen рдХрд╛рд░реНрдпрдХреНрд╖рдо рд▓рд╣рд╛рди рдореЙрдбреЗрд▓реНрд╕ рдкреНрд░рджрд╛рди рдХрд░рддреЗ рдЬреЗ рдПрдЬ рддреИрдирд╛рддреА, рдореЛрдмрд╛рдЗрд▓ рдЕрдиреБрдкреНрд░рдпреЛрдЧ рдЖрдгрд┐ рд╕рдВрд╕рд╛рдзрди-рдЖрдзрд╛рд░рд┐рдд рд╡рд╛рддрд╛рд╡рд░рдгрд╛рд╕рд╛рдареА рдпреЛрдЧреНрдп рдЖрд╣реЗрдд, рдкреНрд░рднрд╛рд╡реА рдХреНрд╖рдорддрд╛ рд░рд╛рдЦреВрди.

### рдордзреНрдпрдо рдореЙрдбреЗрд▓реНрд╕ (7B-32B)

рдордзреНрдпрдо рд╢реНрд░реЗрдгреАрддреАрд▓ рдореЙрдбреЗрд▓реНрд╕ рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рд╡рд╛рдвреАрд╡ рдХреНрд╖рдорддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддрд╛рдд, рдХрд╛рдордЧрд┐рд░реА рдЖрдгрд┐ рд╕рдВрдЧрдгрдХреАрдп рдЖрд╡рд╢реНрдпрдХрддрд╛ рдпрд╛рдордзреНрдпреЗ рдЙрддреНрдХреГрд╖реНрдЯ рд╕рдВрддреБрд▓рди рдкреНрд░рджрд╛рди рдХрд░рддрд╛рдд.

### рдореЛрдареЗ рдореЙрдбреЗрд▓реНрд╕ (72B+)

рдкреВрд░реНрдг-рд╕реНрдХреЗрд▓ рдореЙрдбреЗрд▓реНрд╕ рдЕрддреНрдпрд╛рдзреБрдирд┐рдХ рдХрд╛рдордЧрд┐рд░реА рдкреНрд░рджрд╛рди рдХрд░рддрд╛рдд, рдорд╛рдЧрдгреА рдЕрд╕рд▓реЗрд▓реНрдпрд╛ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА, рд╕рдВрд╢реЛрдзрдирд╛рд╕рд╛рдареА рдЖрдгрд┐ рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рддреИрдирд╛рддреАрд╕рд╛рдареА рдЬрд╛рд╕реНрддреАрдд рдЬрд╛рд╕реНрдд рдХреНрд╖рдорддрд╛ рдЖрд╡рд╢реНрдпрдХ рдЖрд╣реЗ.

## Qwen рдореЙрдбреЗрд▓ рдХреБрдЯреБрдВрдмрд╛рдЪреЗ рдлрд╛рдпрджреЗ

### рдУрдкрди рд╕реЛрд░реНрд╕ рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдпрддрд╛

Qwen рдореЙрдбреЗрд▓реНрд╕ рд╕рдВрдкреВрд░реНрдг рдкрд╛рд░рджрд░реНрд╢рдХрддрд╛ рдЖрдгрд┐ рд╕рд╛рдиреБрдХреВрд▓рди рдХреНрд╖рдорддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддрд╛рдд, рд╕рдВрд╕реНрдерд╛рдВрдирд╛ рд╡рд┐рд╢рд┐рд╖реНрдЯ рдЧрд░рдЬрд╛рдВрд╕рд╛рдареА рдореЙрдбреЗрд▓реНрд╕ рд╕рдордЬреВрди рдШреЗрдгреНрдпрд╛рд╕, рдмрджрд▓рдгреНрдпрд╛рд╕ рдЖрдгрд┐ рдЕрдиреБрдХреВрд▓ рдХрд░рдгреНрдпрд╛рд╕ рд╕рдХреНрд╖рдо рдХрд░рддрд╛рдд, рд╡рд┐рдХреНрд░реЗрддрд╛ рд▓реЙрдХ-рдЗрди рдЯрд╛рд│реВрди.

### рддреИрдирд╛рддреА рд▓рд╡рдЪрд┐рдХрддрд╛

рдореЙрдбреЗрд▓ рдЖрдХрд╛рд░рд╛рдВрдЪреА рд╢реНрд░реЗрдгреА рд╡рд┐рд╡рд┐рдз рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рдирдордзреНрдпреЗ рддреИрдирд╛рддреА рд╕рдХреНрд╖рдо рдХрд░рддреЗ, рдореЛрдмрд╛рдЗрд▓ рдбрд┐рд╡реНрд╣рд╛рдЗрд╕рдкрд╛рд╕реВрди рдЙрдЪреНрдЪ-рд╕реНрддрд░реАрдп рд╕рд░реНрд╡реНрд╣рд░рдкрд░реНрдпрдВрдд, рд╕рдВрд╕реНрдерд╛рдВрдирд╛ рддреНрдпрд╛рдВрдЪреНрдпрд╛ AI рдкрд╛рдпрд╛рднреВрдд рд╕реБрд╡рд┐рдзрд╛рдВрдЪреНрдпрд╛ рдирд┐рд╡рдбреАрдВрдордзреНрдпреЗ рд▓рд╡рдЪрд┐рдХрддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддреЗ.

### рдмрд╣реБрднрд╛рд╖рд┐рдХ рдЙрддреНрдХреГрд╖реНрдЯрддрд╛

Qwen рдореЙрдбреЗрд▓реНрд╕ рдмрд╣реБрднрд╛рд╖рд┐рдХ рд╕рдордЬ рдЖрдгрд┐ рдирд┐рд░реНрдорд┐рддреАрдордзреНрдпреЗ рдЙрддреНрдХреГрд╖реНрдЯ рдЖрд╣реЗрдд, рдЗрдВрдЧреНрд░рдЬреА рдЖрдгрд┐ рдЪрд┐рдиреА рднрд╛рд╖рд╛рдВрдордзреНрдпреЗ рд╡рд┐рд╢реЗрд╖ рддрд╛рдХрджреАрд╕рд╣ рдбрдЭрдирднрд░ рднрд╛рд╖рд╛рдВрдирд╛ рд╕рдорд░реНрдерди рджреЗрддрд╛рдд, рдЬреНрдпрд╛рдореБрд│реЗ рддреЗ рдЬрд╛рдЧрддрд┐рдХ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдпреЛрдЧреНрдп рдмрдирддрд╛рдд.

### рд╕реНрдкрд░реНрдзрд╛рддреНрдордХ рдХрд╛рдордЧрд┐рд░реА

Qwen рдореЙрдбреЗрд▓реНрд╕ рд╕рддрдд рдмреЗрдВрдЪрдорд╛рд░реНрдХрд╡рд░ рд╕реНрдкрд░реНрдзрд╛рддреНрдордХ рдкрд░рд┐рдгрд╛рдо рд╕рд╛рдзреНрдп рдХрд░рддрд╛рдд, рдУрдкрди-рд╕реЛрд░реНрд╕ рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдпрддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддрд╛рдирд╛, рд╣реЗ рджрд░реНрд╢рд╡рд┐рддреЗ рдХреА рдУрдкрди рдореЙрдбреЗрд▓реНрд╕ рдорд╛рд▓рдХреАрдЪреНрдпрд╛ рдкрд░реНрдпрд╛рдпрд╛рдВрд╢реА рдЬреБрд│реВ рд╢рдХрддрд╛рдд.

### рд╡рд┐рд╢реЗрд╖ рдХреНрд╖рдорддрд╛

Qwen-Coder рдЖрдгрд┐ Qwen-Math рд╕рд╛рд░рдЦреЗ рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рдкреНрд░рдХрд╛рд░ рд╕рд╛рдорд╛рдиреНрдп рднрд╛рд╖рд╛ рд╕рдордЬрдгреНрдпрд╛рдЪреНрдпрд╛ рдХреНрд╖рдорддреЗрд╕рд╣ рд╡рд┐рд╢реЗрд╖ рддрдЬреНрдЮрддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддрд╛рдд.

## рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рдЙрджрд╛рд╣рд░рдгреЗ рдЖрдгрд┐ рд╡рд╛рдкрд░ рдкреНрд░рдХрд░рдгреЗ

рддрд╛рдВрддреНрд░рд┐рдХ рддрдкрд╢реАрд▓рд╛рдВрдордзреНрдпреЗ рдЬрд╛рдгреНрдпрд╛рдкреВрд░реНрд╡реА, Qwen рдореЙрдбреЗрд▓реНрд╕ рдХрд╛рдп рд╕рд╛рдзреНрдп рдХрд░реВ рд╢рдХрддрд╛рдд рдпрд╛рдЪреЗ рдХрд╛рд╣реА рдареЛрд╕ рдЙрджрд╛рд╣рд░рдгреЗ рдПрдХреНрд╕рдкреНрд▓реЛрд░ рдХрд░реВрдпрд╛:

### рдЧрдгрд┐рддреАрдп рддрд░реНрдХ рдЙрджрд╛рд╣рд░рдг

Qwen-Math рдЪрд░рдг-рджрд░-рдЪрд░рдг рдЧрдгрд┐рддреАрдп рд╕рдорд╕реНрдпрд╛ рд╕реЛрдбрд╡рдгреНрдпрд╛рдд рдЙрддреНрдХреГрд╖реНрдЯ рдЖрд╣реЗ. рдЙрджрд╛рд╣рд░рдгрд╛рд░реНрде, рдЬрдЯрд┐рд▓ рдХреЕрд▓реНрдХреНрдпреБрд▓рд╕ рд╕рдорд╕реНрдпрд╛ рд╕реЛрдбрд╡рдгреНрдпрд╛рд╕ рд╕рд╛рдВрдЧрд┐рддрд▓реЗ рдЕрд╕рддрд╛:

```
User: Find the derivative of f(x) = x┬│ + 2x┬▓ - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x┬│ + 2x┬▓ - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xтБ┐) = n┬╖xтБ┐тБ╗┬╣

Breaking down each term:
- d/dx(x┬│) = 3x┬▓
- d/dx(2x┬▓) = 2 ┬╖ 2x┬╣ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x┬▓ + 4x - 5
```

### рдмрд╣реБрднрд╛рд╖рд┐рдХ рд╕рдорд░реНрдерди рдЙрджрд╛рд╣рд░рдг

Qwen рдореЙрдбреЗрд▓реНрд╕ рд╡рд┐рд╡рд┐рдз рднрд╛рд╖рд╛рдВрдордзреНрдпреЗ рдордЬрдмреВрдд рдмрд╣реБрднрд╛рд╖рд┐рдХ рдХреНрд╖рдорддрд╛ рджрд░реНрд╢рд╡рддрд╛рдд:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**ф╕нцЦЗ:**
ф║║х╖ецЩ║шГ╜я╝ИAIя╝ЙцШпцМЗцЬ║хЩия╝ИчЙ╣хИлцШпшобчоЧцЬ║ч│╗ч╗Яя╝Йхп╣ф║║ч▒╗цЩ║шГ╜ш┐ЗчиЛчЪДцибцЛЯуАВш┐Щф║Ыш┐ЗчиЛхМЕцЛмхнжф╣ауАБцОичРЖуАБшзгхЖ│щЧощвШуАБцДЯчЯехТМшпншиАчРЖшзгуАВф║║х╖ецЩ║шГ╜ф╜┐цЬ║хЩишГ╜хдЯцЙзшбМщАЪх╕╕щЬАшжБф║║ч▒╗шодчЯешГ╜хКЫчЪДф╗╗хКбуАВ
```

### рдорд▓реНрдЯреАрдореЛрдбрд▓ рдХреНрд╖рдорддрд╛ рдЙрджрд╛рд╣рд░рдг

Qwen-VL рдордЬрдХреВрд░ рдЖрдгрд┐ рдкреНрд░рддрд┐рдорд╛ рдПрдХрд╛рдЪ рд╡реЗрд│реА рдкреНрд░рдХреНрд░рд┐рдпрд╛ рдХрд░реВ рд╢рдХрддреЗ:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### рдХреЛрдб рдЬрдирд░реЗрд╢рди рдЙрджрд╛рд╣рд░рдг

Qwen-Coder рдЕрдиреЗрдХ рдкреНрд░реЛрдЧреНрд░рд╛рдорд┐рдВрдЧ рднрд╛рд╖рд╛рдВрдордзреНрдпреЗ рдХреЛрдб рддрдпрд╛рд░ рдХрд░рдгреНрдпрд╛рдд рдЖрдгрд┐ рд╕реНрдкрд╖реНрдЯ рдХрд░рдгреНрдпрд╛рдд рдЙрддреНрдХреГрд╖реНрдЯ рдЖрд╣реЗ:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### рдПрдЬ рддреИрдирд╛рддреА рдЙрджрд╛рд╣рд░рдг

Qwen рдореЙрдбреЗрд▓реНрд╕ рд╡рд┐рд╡рд┐рдз рдПрдЬ рдбрд┐рд╡реНрд╣рд╛рдЗрд╕рд╡рд░ рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХреЗрд▓реЗрд▓реНрдпрд╛ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рдирд╕рд╣ рддреИрдирд╛рдд рдХреЗрд▓реЗ рдЬрд╛рдК рд╢рдХрддрд╛рдд:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Qwen рдХреБрдЯреБрдВрдмрд╛рдЪрд╛ рд╡рд┐рдХрд╛рд╕

### Qwen 1.0 рдЖрдгрд┐ 1.5: рдлрд╛рдЙрдВрдбреЗрд╢рди рдореЙрдбреЗрд▓реНрд╕

рдкреНрд░рд╛рд░рдВрднрд┐рдХ Qwen рдореЙрдбреЗрд▓реНрд╕рдиреЗ рд╡реНрдпрд╛рдкрдХ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдЖрдгрд┐ рдУрдкрди-рд╕реЛрд░реНрд╕ рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдпрддреЗрдЪреНрдпрд╛ рдореВрд▓рднреВрдд рддрддреНрддреНрд╡рд╛рдВрдЪреА рд╕реНрдерд╛рдкрдирд╛ рдХреЗрд▓реА:

- **Qwen-7B (7B рдкреЕрд░рд╛рдореАрдЯрд░реНрд╕)**: рдЪреАрдиреА рдЖрдгрд┐ рдЗрдВрдЧреНрд░рдЬреА рднрд╛рд╖рд╛ рд╕рдордЬрдгреНрдпрд╛рд╡рд░ рд▓рдХреНрд╖ рдХреЗрдВрджреНрд░рд┐рдд рдХрд░рдгрд╛рд░реЗ рдкреНрд░рд╛рд░рдВрднрд┐рдХ рдкреНрд░рдХрд╛рд╢рди
- **Qwen-14B (14B рдкреЕрд░рд╛рдореАрдЯрд░реНрд╕)**: рд╕реБрдзрд╛рд░рд┐рдд рддрд░реНрдХ рдЖрдгрд┐ рдЬреНрдЮрд╛рдирд╛рд╕рд╣ рд╡рд╛рдвреАрд╡ рдХреНрд╖рдорддрд╛
- **Qwen-72B (72B рдкреЕрд░рд╛рдореАрдЯрд░реНрд╕)**: рдЕрддреНрдпрд╛рдзреБрдирд┐рдХ рдХрд╛рдордЧрд┐рд░реА рдкреНрд░рджрд╛рди рдХрд░рдгрд╛рд░реЗ рдореЛрдареНрдпрд╛ рдкреНрд░рдорд╛рдгрд╛рдЪреЗ рдореЙрдбреЗрд▓
- **Qwen1.5 рдорд╛рд▓рд┐рдХрд╛**: рд╕реБрдзрд╛рд░рд┐рдд рд▓рд╛рдВрдм-рдХреЙрдиреНрдЯреЗрдХреНрд╕реНрдЯ рд╣рд╛рддрд╛рд│рдгреАрд╕рд╣ рдПрдХрд╛рдзрд┐рдХ рдЖрдХрд╛рд░рд╛рдВрдордзреНрдпреЗ (0.5B рддреЗ 110B) рд╡рд┐рд╕реНрддрд╛рд░рд┐рдд

### Qwen2 рдХреБрдЯреБрдВрдм: рдорд▓реНрдЯреАрдореЛрдбрд▓ рд╡рд┐рд╕реНрддрд╛рд░

Qwen2 рдорд╛рд▓рд┐рдХреЗрдиреЗ рднрд╛рд╖рд╛ рдЖрдгрд┐ рдорд▓реНрдЯреАрдореЛрдбрд▓ рдХреНрд╖рдорддрд╛ рджреЛрдиреНрд╣реАрдордзреНрдпреЗ рдорд╣рддреНрддреНрд╡рдкреВрд░реНрдг рдкреНрд░рдЧрддреА рдХреЗрд▓реА:

- **Qwen2-0.5B рддреЗ 72B**: рд╡рд┐рд╡рд┐рдз рддреИрдирд╛рддреА рдЧрд░рдЬрд╛рдВрд╕рд╛рдареА рд╡реНрдпрд╛рдкрдХ рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓реНрд╕рдЪреА рд╢реНрд░реЗрдгреА
- **Qwen2-57B-A14B (MoE)**: рдХрд╛рд░реНрдпрдХреНрд╖рдо рдкреЕрд░рд╛рдореАрдЯрд░ рд╡рд╛рдкрд░рд╛рд╕рд╛рдареА рдорд┐рд╢реНрд░рдг-рдСрдл-рдПрдХреНрд╕рдкрд░реНрдЯреНрд╕ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░
- **Qwen2-VL**: рдкреНрд░рддрд┐рдорд╛ рд╕рдордЬрдгреНрдпрд╛рд╕рд╛рдареА рдкреНрд░рдЧрдд рд╡реНрд╣рд┐рдЬрди-рднрд╛рд╖рд╛ рдХреНрд╖рдорддрд╛
- **Qwen2-Audio**: рдСрдбрд┐рдУ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рдЖрдгрд┐ рд╕рдордЬрдгреНрдпрд╛рдЪреА рдХреНрд╖рдорддрд╛
- **Qwen2-Math**: рд╡рд┐рд╢реЗрд╖ рдЧрдгрд┐рддреАрдп рддрд░реНрдХ рдЖрдгрд┐ рд╕рдорд╕реНрдпрд╛ рд╕реЛрдбрд╡рдгреЗ

### Qwen2.5 рдХреБрдЯреБрдВрдм: рд╕реБрдзрд╛рд░рд┐рдд рдХрд╛рдордЧрд┐рд░реА

Qwen2.5 рдорд╛рд▓рд┐рдХреЗрдиреЗ рд╕рд░реНрд╡ рдкрд░рд┐рдорд╛рдгрд╛рдВрдордзреНрдпреЗ рдорд╣рддреНрддреНрд╡рдкреВрд░реНрдг рд╕реБрдзрд╛рд░рдгрд╛ рдЖрдгрд▓реНрдпрд╛:

- **рд╡рд┐рд╕реНрддрд╛рд░рд┐рдд рдкреНрд░рд╢рд┐рдХреНрд╖рдг**: рд╕реБрдзрд╛рд░рд┐рдд рдХреНрд╖рдорддрд╛ рдкреНрд░рджрд╛рди рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА 18 рдЯреНрд░рд┐рд▓рд┐рдпрди рдЯреЛрдХрдиреНрд╕ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдбреЗрдЯрд╛
- **рд╡рд┐рд╕реНрддрд╛рд░рд┐рдд рд╕рдВрджрд░реНрдн**: 128K рдЯреЛрдХрдиреНрд╕ рд╕рдВрджрд░реНрдн рд▓рд╛рдВрдмреА, Turbo рдкреНрд░рдХрд╛рд░ 1M рдЯреЛрдХрдиреНрд╕рд▓рд╛ рд╕рдорд░реНрдерди рджреЗрддреЛ
- **рд╕реБрдзрд╛рд░рд┐рдд рд╡рд┐рд╢реЗрд╖реАрдХрд░рдг**: рд╕реБрдзрд╛рд░рд┐рдд Qwen2.5-Coder рдЖрдгрд┐ Qwen2.5-Math рдкреНрд░рдХрд╛рд░
- **рдЙрддреНрддрдо рдмрд╣реБрднрд╛рд╖рд┐рдХ рд╕рдорд░реНрдерди**: 27+ рднрд╛рд╖рд╛рдВрдордзреНрдпреЗ рд╕реБрдзрд╛рд░рд┐рдд рдХрд╛рдордЧрд┐рд░реА

### Qwen3 рдХреБрдЯреБрдВрдм: рдкреНрд░рдЧрдд рддрд░реНрдХ

рдирд╡реАрдирддрдо рдкрд┐рдвреА рддрд░реНрдХ рдЖрдгрд┐ рд╡рд┐рдЪрд╛рд░ рдХреНрд╖рдорддрд╛рдВрдЪреНрдпрд╛ рд╕реАрдорд╛рдВрдирд╛ рдкреБрдвреЗ рдврдХрд▓рддреЗ:

- **Qwen3-235B-A22B**: 235B рдПрдХреВрдг рдкреЕрд░рд╛рдореАрдЯрд░реНрд╕рд╕рд╣ рдлреНрд▓реЕрдЧрд╢рд┐рдк рдорд┐рд╢реНрд░рдг-рдСрдл-рдПрдХреНрд╕рдкрд░реНрдЯреНрд╕ рдореЙрдбреЗрд▓
- **Qwen3-30B-A3B**: рд╕рдХреНрд░рд┐рдп рдкреЕрд░рд╛рдореАрдЯрд░ рдкреНрд░рддрд┐ рдордЬрдмреВрдд рдХрд╛рдордЧрд┐рд░реАрд╕рд╣ рдХрд╛рд░реНрдпрдХреНрд╖рдо MoE рдореЙрдбреЗрд▓
- **рдШрди рдореЙрдбреЗрд▓реНрд╕**: рд╡рд┐рд╡рд┐рдз рддреИрдирд╛рддреА рдкрд░рд┐рд╕реНрдерд┐рддреАрд╕рд╛рдареА Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B
- **Thinking Mode**: рдЬрд▓рдж рдкреНрд░рддрд┐рд╕рд╛рдж рдЖрдгрд┐ рд╕рдЦреЛрд▓ рд╡рд┐рдЪрд╛рд░рд╛рдВрдирд╛ рд╕рдорд░реНрдерди рджреЗрдгрд╛рд░рд╛ рд╣рд╛рдпрдмреНрд░рд┐рдб рддрд░реНрдХ рджреГрд╖реНрдЯрд┐рдХреЛрди
- **рдмрд╣реБрднрд╛рд╖рд┐рдХ рдЙрддреНрдХреГрд╖реНрдЯрддрд╛**: 119 рднрд╛рд╖рд╛ рдЖрдгрд┐ рдмреЛрд▓реАрдВрдирд╛ рд╕рдорд░реНрдерди
- **рд╕реБрдзрд╛рд░рд┐рдд рдкреНрд░рд╢рд┐рдХреНрд╖рдг**: 36 рдЯреНрд░рд┐рд▓рд┐рдпрди рдЯреЛрдХрдиреНрд╕ рд╡рд┐рд╡рд┐рдз, рдЙрдЪреНрдЪ-рдЧреБрдгрд╡рддреНрддреЗрдЪреНрдпрд╛ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдбреЗрдЯрд╛

## Qwen рдореЙрдбреЗрд▓реНрд╕рдЪреЗ рдЕрдиреБрдкреНрд░рдпреЛрдЧ

### рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рдЕрдиреБрдкреНрд░рдпреЛрдЧ

рд╕рдВрд╕реНрдерд╛ рджрд╕реНрддрдРрд╡рдЬ рд╡рд┐рд╢реНрд▓реЗрд╖рдг, рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛ рдСрдЯреЛрдореЗрд╢рди, рдХреЛрдб рдЬрдирд░реЗрд╢рди рд╕рд╣рд╛рдпреНрдп рдЖрдгрд┐ рд╡реНрдпрд╡рд╕рд╛рдп рдмреБрджреНрдзрд┐рдорддреНрддрд╛ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА Qwen рдореЙрдбреЗрд▓реНрд╕ рд╡рд╛рдкрд░рддрд╛рдд. рдУрдкрди
Qwen рдореЙрдбреЗрд▓реНрд╕рд╕рд╣ Hugging Face Transformers рд▓рд╛рдпрдмреНрд░рд░реА рд╡рд╛рдкрд░рдгреНрдпрд╛рд╕ рд╕реБрд░реБрд╡рд╛рдд рдХрд╢реА рдХрд░рд╛рд╡реА:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Qwen2.5 рдореЙрдбреЗрд▓реНрд╕рдЪрд╛ рд╡рд╛рдкрд░

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### рд╡рд┐рд╢реЗрд╖ рдореЙрдбреЗрд▓ рд╡рд╛рдкрд░

**Qwen-Coder рд╕рд╣ рдХреЛрдб рдЬрдирд░реЗрд╢рди:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**рдЧрдгрд┐рддреАрдп рд╕рдорд╕реНрдпрд╛ рд╕реЛрдбрд╡рдгреЗ:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = x┬│ + 2x┬▓ - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**рд╡реНрд╣рд┐рдЬрди-рд▓рдБрдЧреНрд╡реЗрдЬ рдЯрд╛рд╕реНрдХреНрд╕:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### рдерд┐рдВрдХрд┐рдВрдЧ рдореЛрдб (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### ЁЯУ▒ рдореЛрдмрд╛рдЗрд▓ рдЖрдгрд┐ рдПрдЬ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### API рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдЙрджрд╛рд╣рд░рдг

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдмреЗрдВрдЪрдорд╛рд░реНрдХреНрд╕ рдЖрдгрд┐ рдЙрдкрд▓рдмреНрдзреА

Qwen рдореЙрдбреЗрд▓ рдХреБрдЯреБрдВрдмрд╛рдиреЗ рд╡рд┐рд╡рд┐рдз рдмреЗрдВрдЪрдорд╛рд░реНрдХреНрд╕рдордзреНрдпреЗ рдЙрд▓реНрд▓реЗрдЦрдиреАрдп рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдкреНрд░рд╛рдкреНрдд рдХреЗрд▓реА рдЖрд╣реЗ рдЖрдгрд┐ рдУрдкрди-рд╕реЛрд░реНрд╕ рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдпрддрд╛ рдХрд╛рдпрдо рдареЗрд╡рд▓реА рдЖрд╣реЗ:

### рдореБрдЦреНрдп рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ

**рддрд░реНрдХрд╢рдХреНрддреАрддреАрд▓ рдЙрддреНрдХреГрд╖реНрдЯрддрд╛:**
- Qwen3-235B-A22B рдиреЗ рдХреЛрдбрд┐рдВрдЧ, рдЧрдгрд┐рдд, рд╕рд╛рдорд╛рдиреНрдп рдХреНрд╖рдорддрд╛ рдпрд╛рдордзреНрдпреЗ DeepSeek-R1, o1, o3-mini, Grok-3, рдЖрдгрд┐ Gemini-2.5-Pro рд╕рд╛рд░рдЦреНрдпрд╛ рдЯреЙрдк-рдЯрд┐рдпрд░ рдореЙрдбреЗрд▓реНрд╕рд╢реА рд╕реНрдкрд░реНрдзрд╛рддреНрдордХ рдирд┐рдХрд╛рд▓ рдкреНрд░рд╛рдкреНрдд рдХреЗрд▓реЗ рдЖрд╣реЗрдд.
- Qwen3-30B-A3B рдиреЗ QwQ-32B рдкреЗрдХреНрд╖рд╛ 10 рдкрдЯ рд╕рдХреНрд░рд┐рдп рдкреЕрд░рд╛рдореАрдЯрд░реНрд╕рд╕рд╣ рдЪрд╛рдВрдЧрд▓реА рдХрд╛рдордЧрд┐рд░реА рдХреЗрд▓реА рдЖрд╣реЗ.
- Qwen3-4B рдиреЗ Qwen2.5-72B-Instruct рдЪреНрдпрд╛ рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрд╢реА рд╕реНрдкрд░реНрдзрд╛ рдХреЗрд▓реА рдЖрд╣реЗ.

**рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдЙрдкрд▓рдмреНрдзреА:**
- Qwen3-MoE рдмреЗрд╕ рдореЙрдбреЗрд▓реНрд╕рдиреЗ Qwen2.5 рдбреЗрдВрд╕ рдмреЗрд╕ рдореЙрдбреЗрд▓реНрд╕рд╕рд╛рд░рдЦреАрдЪ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдкреНрд░рд╛рдкреНрдд рдХреЗрд▓реА рдЖрд╣реЗ, рдкрд░рдВрддреБ рдлрдХреНрдд 10% рд╕рдХреНрд░рд┐рдп рдкреЕрд░рд╛рдореАрдЯрд░реНрд╕ рд╡рд╛рдкрд░реВрди.
- рдбреЗрдВрд╕ рдореЙрдбреЗрд▓реНрд╕рдЪреНрдпрд╛ рддреБрд▓рдиреЗрдд рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдЖрдгрд┐ рдЗрдирдлрд░рдиреНрд╕рдордзреНрдпреЗ рдорд╣рддреНрддреНрд╡рдкреВрд░реНрдг рдЦрд░реНрдЪ рдмрдЪрдд.

**рдмрд╣реБрднрд╛рд╖рд┐рдХ рдХреНрд╖рдорддрд╛:**
- Qwen3 рдореЙрдбреЗрд▓реНрд╕ 119 рднрд╛рд╖рд╛ рдЖрдгрд┐ рдмреЛрд▓реАрдВрдирд╛ рд╕рдорд░реНрдерди рджреЗрддрд╛рдд.
- рд╡рд┐рд╡рд┐рдз рднрд╛рд╖рд┐рдХ рдЖрдгрд┐ рд╕рд╛рдВрд╕реНрдХреГрддрд┐рдХ рд╕рдВрджрд░реНрднрд╛рдВрдордзреНрдпреЗ рдордЬрдмреВрдд рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛.

**рдкреНрд░рд╢рд┐рдХреНрд╖рдгрд╛рдЪрд╛ рд╕реНрдХреЗрд▓:**
- Qwen3 рдиреЗ Qwen2.5 рдЪреНрдпрд╛ 18 рдЯреНрд░рд┐рд▓рд┐рдпрди рдЯреЛрдХрдиреНрд╕рдЪреНрдпрд╛ рддреБрд▓рдиреЗрдд 36 рдЯреНрд░рд┐рд▓рд┐рдпрди рдЯреЛрдХрдиреНрд╕рд╕рд╣ 119 рднрд╛рд╖рд╛ рдЖрдгрд┐ рдмреЛрд▓реАрдВрдирд╛ рдХрд╡реНрд╣рд░ рдХрд░рдд рдЬрд╡рд│рдкрд╛рд╕ рджреБрдкреНрдкрдЯ рдкреНрд░рдорд╛рдгрд╛рдд рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдШреЗрддрд▓реЗ рдЖрд╣реЗ.

### рдореЙрдбреЗрд▓ рддреБрд▓рдирд╛ рдореЕрдЯреНрд░рд┐рдХреНрд╕

| рдореЙрдбреЗрд▓ рдорд╛рд▓рд┐рдХрд╛ | рдкреЕрд░рд╛рдореАрдЯрд░реНрд╕ рд╢реНрд░реЗрдгреА | рд╕рдВрджрд░реНрдн рд▓рд╛рдВрдмреА | рдореБрдЦреНрдп рддрд╛рдХрджреА | рд╕рд░реНрд╡реЛрддреНрддрдо рдЙрдкрдпреЛрдЧ рдкреНрд░рдХрд░рдгреЗ |
|--------------|------------------|----------------|---------------|----------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | рд╕рдВрддреБрд▓рд┐рдд рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛, рдмрд╣реБрднрд╛рд╖рд┐рдХ | рд╕рд╛рдорд╛рдиреНрдп рдЕрдиреБрдкреНрд░рдпреЛрдЧ, рдЙрддреНрдкрд╛рджрди рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | рдХреЛрдб рдЬрдирд░реЗрд╢рди, рдкреНрд░реЛрдЧреНрд░рд╛рдорд┐рдВрдЧ | рд╕реЙрдлреНрдЯрд╡реЗрдЕрд░ рд╡рд┐рдХрд╛рд╕, рдХреЛрдбрд┐рдВрдЧ рд╕рд╣рд╛рдпреНрдп |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | рдЧрдгрд┐рддреАрдп рддрд░реНрдХрд╢рдХреНрддреА | рд╢реИрдХреНрд╖рдгрд┐рдХ рдкреНрд▓реЕрдЯрдлреЙрд░реНрдореНрд╕, STEM рдЕрдиреБрдкреНрд░рдпреЛрдЧ |
| **Qwen2.5-VL** | рд╡рд┐рд╡рд┐рдз | рдмрджрд▓рдгрд╛рд░реЗ | рд╡реНрд╣рд┐рдЬрди-рд▓рдБрдЧреНрд╡реЗрдЬ рд╕рдордЬ | рдорд▓реНрдЯреАрдореЛрдбрд▓ рдЕрдиреБрдкреНрд░рдпреЛрдЧ, рдкреНрд░рддрд┐рдорд╛ рд╡рд┐рд╢реНрд▓реЗрд╖рдг |
| **Qwen3** | 0.6B-235B | рдмрджрд▓рдгрд╛рд░реЗ | рдкреНрд░рдЧрдд рддрд░реНрдХрд╢рдХреНрддреА, рдерд┐рдВрдХрд┐рдВрдЧ рдореЛрдб | рдЬрдЯрд┐рд▓ рддрд░реНрдХрд╢рдХреНрддреА, рд╕рдВрд╢реЛрдзрди рдЕрдиреБрдкреНрд░рдпреЛрдЧ |
| **Qwen3 MoE** | 30B-235B рдПрдХреВрдг | рдмрджрд▓рдгрд╛рд░реЗ | рдХрд╛рд░реНрдпрдХреНрд╖рдо рдореЛрдареНрдпрд╛ рдкреНрд░рдорд╛рдгрд╛рд╡рд░ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ | рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рдЕрдиреБрдкреНрд░рдпреЛрдЧ, рдЙрдЪреНрдЪ-рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрдЪреА рдЧрд░рдЬ |

## рдореЙрдбреЗрд▓ рдирд┐рд╡рдб рдорд╛рд░реНрдЧрджрд░реНрд╢рдХ

### рдореВрд▓рднреВрдд рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА
- **Qwen2.5-0.5B/1.5B**: рдореЛрдмрд╛рдЗрд▓ рдЕреЕрдкреНрд╕, рдПрдЬ рдбрд┐рд╡реНрд╣рд╛рдЗрд╕реЗрд╕, рд░рд┐рдЕрд▓-рдЯрд╛рдЗрдо рдЕрдиреБрдкреНрд░рдпреЛрдЧ
- **Qwen2.5-3B/7B**: рд╕рд╛рдорд╛рдиреНрдп рдЪреЕрдЯрдмреЙрдЯреНрд╕, рд╕рд╛рдордЧреНрд░реА рдирд┐рд░реНрдорд┐рддреА, рдкреНрд░рд╢реНрдиреЛрддреНрддрд░ рдкреНрд░рдгрд╛рд▓реА

### рдЧрдгрд┐рддреАрдп рдЖрдгрд┐ рддрд░реНрдХрд╢рдХреНрддреА рдХрд╛рд░реНрдпрд╛рдВрд╕рд╛рдареА
- **Qwen2.5-Math**: рдЧрдгрд┐рддреАрдп рд╕рдорд╕реНрдпрд╛ рд╕реЛрдбрд╡рдгреЗ рдЖрдгрд┐ STEM рд╢рд┐рдХреНрд╖рдг
- **Qwen3 рд╕рд╣ рдерд┐рдВрдХрд┐рдВрдЧ рдореЛрдб**: рдЪрд░рдг-рджрд░-рдЪрд░рдг рд╡рд┐рд╢реНрд▓реЗрд╖рдг рдЖрд╡рд╢реНрдпрдХ рдЕрд╕рд▓реЗрд▓реА рдЬрдЯрд┐рд▓ рддрд░реНрдХрд╢рдХреНрддреА

### рдкреНрд░реЛрдЧреНрд░рд╛рдорд┐рдВрдЧ рдЖрдгрд┐ рд╡рд┐рдХрд╛рд╕рд╛рд╕рд╛рдареА
- **Qwen2.5-Coder**: рдХреЛрдб рдЬрдирд░реЗрд╢рди, рдбреАрдмрдЧрд┐рдВрдЧ, рдкреНрд░реЛрдЧреНрд░рд╛рдорд┐рдВрдЧ рд╕рд╣рд╛рдпреНрдп
- **Qwen3**: рддрд░реНрдХрд╢рдХреНрддреА рдХреНрд╖рдорддрд╛рдВрд╕рд╣ рдкреНрд░рдЧрдд рдкреНрд░реЛрдЧреНрд░рд╛рдорд┐рдВрдЧ рдХрд╛рд░реНрдпреЗ

### рдорд▓реНрдЯреАрдореЛрдбрд▓ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА
- **Qwen2.5-VL**: рдкреНрд░рддрд┐рдорд╛ рд╕рдордЬ, рд╡реНрд╣рд┐рдЬреНрдпреБрдЕрд▓ рдкреНрд░рд╢реНрдиреЛрддреНрддрд░
- **Qwen-Audio**: рдСрдбрд┐рдУ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рдЖрдгрд┐ рднрд╛рд╖рдг рд╕рдордЬ

### рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯрд╕рд╛рдареА
- **Qwen2.5-32B/72B**: рдЙрдЪреНрдЪ-рдХрд╛рд░реНрдпрдХреНрд╖рдо рднрд╛рд╖рд╛ рд╕рдордЬ
- **Qwen3-235B-A22B**: рдорд╛рдЧрдгреА рдЕрд╕рд▓реЗрд▓реНрдпрд╛ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдХрдорд╛рд▓ рдХреНрд╖рдорддрд╛

## рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдкреНрд▓реЕрдЯрдлреЙрд░реНрдореНрд╕ рдЖрдгрд┐ рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдпрддрд╛
### рдХреНрд▓рд╛рдЙрдб рдкреНрд▓реЕрдЯрдлреЙрд░реНрдореНрд╕
- **Hugging Face Hub**: рд╕рдореБрджрд╛рдп рд╕рдорд░реНрдердирд╛рд╕рд╣ рд╡реНрдпрд╛рдкрдХ рдореЙрдбреЗрд▓ рд░рд┐рдкреЙрдЭрд┐рдЯрд░реА
- **ModelScope**: Alibaba рдЪрд╛ рдореЙрдбреЗрд▓ рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо рдЖрдгрд┐ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди рдЯреВрд▓реНрд╕
- **рд╡рд┐рд╡рд┐рдз рдХреНрд▓рд╛рдЙрдб рдкреНрд░рджрд╛рддреЗ**: рдорд╛рдирдХ ML рдкреНрд▓реЕрдЯрдлреЙрд░реНрдореНрд╕рджреНрд╡рд╛рд░реЗ рд╕рдорд░реНрдерди

### рд╕реНрдерд╛рдирд┐рдХ рд╡рд┐рдХрд╛рд╕ рдлреНрд░реЗрдорд╡рд░реНрдХреНрд╕
- **Transformers**: рд╕реЛрдкреА рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯрд╕рд╛рдареА Hugging Face рдЪреЗ рдорд╛рдирдХ рдПрдХрддреНрд░реАрдХрд░рдг
- **vLLM**: рдЙрддреНрдкрд╛рджрди рд╡рд╛рддрд╛рд╡рд░рдгрд╛рд╕рд╛рдареА рдЙрдЪреНрдЪ-рдХрд╛рд░реНрдпрдХреНрд╖рдо рд╕рд░реНрд╡реНрд╣рд┐рдВрдЧ
- **Ollama**: рд╕реНрдерд╛рдирд┐рдХ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдЖрдгрд┐ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди рд╕реБрд▓рдн
- **ONNX Runtime**: рд╡рд┐рд╡рд┐рдз рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░рд╕рд╛рдареА рдХреНрд░реЙрд╕-рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди
- **llama.cpp**: рд╡рд┐рд╡рд┐рдз рдкреНрд▓реЕрдЯрдлреЙрд░реНрдореНрд╕рд╕рд╛рдареА рдХрд╛рд░реНрдпрдХреНрд╖рдо C++ рдЕрдВрдорд▓рдмрдЬрд╛рд╡рдгреА

### рд╢рд┐рдХреНрд╖рдг рд╕рдВрд╕рд╛рдзрдиреЗ
- **Qwen рджрд╕реНрддрдРрд╡рдЬ**: рдЕрдзрд┐рдХреГрдд рджрд╕реНрддрдРрд╡рдЬ рдЖрдгрд┐ рдореЙрдбреЗрд▓ рдХрд╛рд░реНрдбреНрд╕
- **Hugging Face Model Hub**: рдЗрдВрдЯрд░реЕрдХреНрдЯрд┐рд╡реНрд╣ рдбреЗрдореЛ рдЖрдгрд┐ рд╕рдореБрджрд╛рдп рдЙрджрд╛рд╣рд░рдгреЗ
- **рд╕рдВрд╢реЛрдзрди рдкреЗрдкрд░реНрд╕**: рд╕рдЦреЛрд▓ рд╕рдордЬрдгреНрдпрд╛рд╕рд╛рдареА arxiv рд╡рд░реАрд▓ рддрд╛рдВрддреНрд░рд┐рдХ рдкреЗрдкрд░реНрд╕
- **рд╕рдореБрджрд╛рдп рдордВрдЪ**: рд╕рдХреНрд░рд┐рдп рд╕рдореБрджрд╛рдп рд╕рдорд░реНрдерди рдЖрдгрд┐ рдЪрд░реНрдЪрд╛

### Qwen рдореЙрдбреЗрд▓реНрд╕рд╕рд╣ рд╕реБрд░реБрд╡рд╛рдд

#### рд╡рд┐рдХрд╛рд╕ рдкреНрд▓реЕрдЯрдлреЙрд░реНрдореНрд╕
1. **Hugging Face Transformers**: рдорд╛рдирдХ Python рдПрдХрддреНрд░реАрдХрд░рдгрд╛рд╕рд╣ рд╕реБрд░реБрд╡рд╛рдд рдХрд░рд╛
2. **ModelScope**: Alibaba рдЪреЗ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭрдб рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдЯреВрд▓реНрд╕ рдПрдХреНрд╕рдкреНрд▓реЛрд░ рдХрд░рд╛
3. **рд╕реНрдерд╛рдирд┐рдХ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ**: Ollama рдХрд┐рдВрд╡рд╛ рдереЗрдЯ Transformers рд╡рд╛рдкрд░реВрди рд╕реНрдерд╛рдирд┐рдХ рдЪрд╛рдЪрдгреА рдХрд░рд╛

#### рд╢рд┐рдХреНрд╖рдг рдорд╛рд░реНрдЧ
1. **рдХреЛрд░ рд╕рдВрдХрд▓реНрдкрдирд╛ рд╕рдордЬреВрди рдШреНрдпрд╛**: Qwen рдХреБрдЯреБрдВрдмрд╛рдЪреА рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░ рдЖрдгрд┐ рдХреНрд╖рдорддрд╛ рдЕрднреНрдпрд╛рд╕рд╛
2. **рд╡рд┐рд╡рд┐рдз рдкреНрд░рдХрд╛рд░рд╛рдВрд╕рд╣ рдкреНрд░рдпреЛрдЧ рдХрд░рд╛**: рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрддреАрд▓ рд╡реНрдпрд╛рдкрд╛рд░ рд╕рдордЬрдгреНрдпрд╛рд╕рд╛рдареА рд╡реЗрдЧрд╡реЗрдЧрд│реНрдпрд╛ рдореЙрдбреЗрд▓ рдЖрдХрд╛рд░рд╛рдВрдЪрд╛ рдкреНрд░рдпрддреНрди рдХрд░рд╛
3. **рдЕрдВрдорд▓рдмрдЬрд╛рд╡рдгреАрдЪрд╛ рд╕рд░рд╛рд╡ рдХрд░рд╛**: рд╡рд┐рдХрд╛рд╕ рд╡рд╛рддрд╛рд╡рд░рдгрд╛рдд рдореЙрдбреЗрд▓реНрд╕ рдбрд┐рдкреНрд▓реЙрдп рдХрд░рд╛
4. **рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХрд░рд╛**: рдЙрддреНрдкрд╛рджрди рдЙрдкрдпреЛрдЧ рдкреНрд░рдХрд░рдгрд╛рдВрд╕рд╛рдареА рдлрд╛рдЗрди-рдЯреНрдпреВрди рдХрд░рд╛

#### рд╕рд░реНрд╡реЛрддреНрддрдо рдкрджреНрдзрддреА
- **рд▓рд╣рд╛рди рд╕реБрд░реБрд╡рд╛рдд рдХрд░рд╛**: рдкреНрд░рд╛рд░рдВрднрд┐рдХ рд╡рд┐рдХрд╛рд╕рд╛рд╕рд╛рдареА рд▓рд╣рд╛рди рдореЙрдбреЗрд▓реНрд╕ (1.5B-7B) рд╡рд╛рдкрд░рд╛
- **рдЪреЕрдЯ рдЯреЗрдореНрдкрд▓реЗрдЯреНрд╕ рд╡рд╛рдкрд░рд╛**: рд╕рд░реНрд╡реЛрддреНрддрдо рдкрд░рд┐рдгрд╛рдорд╛рдВрд╕рд╛рдареА рдпреЛрдЧреНрдп рд╕реНрд╡рд░реВрдкрди рд▓рд╛рдЧреВ рдХрд░рд╛
- **рд╕рдВрд╕рд╛рдзрдиреЗ рдореЙрдирд┐рдЯрд░ рдХрд░рд╛**: рдореЗрдорд░реА рд╡рд╛рдкрд░ рдЖрдгрд┐ рдЗрдирдлрд░рдиреНрд╕ рдЧрддреА рдЯреНрд░реЕрдХ рдХрд░рд╛
- **рд╡рд┐рд╢реЗрд╖реАрдХрд░рдг рд╡рд┐рдЪрд╛рд░рд╛рдд рдШреНрдпрд╛**: рдпреЛрдЧреНрдп рддреЗрд╡реНрд╣рд╛ рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рдкреНрд░рдХрд╛рд░ рдирд┐рд╡рдбрд╛

## рдкреНрд░рдЧрдд рд╡рд╛рдкрд░ рдкреЕрдЯрд░реНрдиреНрд╕

### рдлрд╛рдЗрди-рдЯреНрдпреВрдирд┐рдВрдЧ рдЙрджрд╛рд╣рд░рдгреЗ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### рд╡рд┐рд╢реЗрд╖ рдкреНрд░реЙрдореНрдкреНрдЯ рдЗрдВрдЬрд┐рдирд┐рдЕрд░рд┐рдВрдЧ

**рдЬрдЯрд┐рд▓ рддрд░реНрдХрд╢рдХреНрддреА рдХрд╛рд░реНрдпрд╛рдВрд╕рд╛рдареА:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**рд╕рдВрджрд░реНрднрд╛рд╕рд╣ рдХреЛрдб рдЬрдирд░реЗрд╢рдирд╕рд╛рдареА:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### рдмрд╣реБрднрд╛рд╖рд┐рдХ рдЕрдиреБрдкреНрд░рдпреЛрдЧ

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ф╕нцЦЗ)",
        "es": "Spanish (Espa├▒ol)",
        "fr": "French (Fran├зais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (цЧецЬмшкЮ)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ЁЯФз рдЙрддреНрдкрд╛рджрди рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдкреЕрдЯрд░реНрдиреНрд╕

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди рдзреЛрд░рдгреЗ

### рдореЗрдорд░реА рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### рдЗрдирдлрд░рдиреНрд╕ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## рд╕рд░реНрд╡реЛрддреНрддрдо рдкрджреНрдзрддреА рдЖрдгрд┐ рдорд╛рд░реНрдЧрджрд░реНрд╢рдХ рддрддреНрддреНрд╡реЗ

### рд╕реБрд░рдХреНрд╖рд╛ рдЖрдгрд┐ рдЧреЛрдкрдиреАрдпрддрд╛

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### рдореЙрдирд┐рдЯрд░рд┐рдВрдЧ рдЖрдгрд┐ рдореВрд▓реНрдпрд╛рдВрдХрди

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## рдирд┐рд╖реНрдХрд░реНрд╖

Qwen рдореЙрдбреЗрд▓ рдХреБрдЯреБрдВрдм рд╡рд┐рд╡рд┐рдз рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрдордзреНрдпреЗ рд╕реНрдкрд░реНрдзрд╛рддреНрдордХ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рд░рд╛рдЦрдд AI рддрдВрддреНрд░рдЬреНрдЮрд╛рди рд▓реЛрдХрд╢рд╛рд╣реАрдХрд░рдг рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рд╡реНрдпрд╛рдкрдХ рджреГрд╖реНрдЯрд┐рдХреЛрди рд╕рд╛рджрд░ рдХрд░рддреЗ. рдУрдкрди-рд╕реЛрд░реНрд╕ рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдпрддрд╛, рдмрд╣реБрднрд╛рд╖рд┐рдХ рдХреНрд╖рдорддрд╛, рдЖрдгрд┐ рд▓рд╡рдЪрд┐рдХ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдкрд░реНрдпрд╛рдпрд╛рдВрджреНрд╡рд╛рд░реЗ, Qwen рд╕рдВрд╕реНрдерд╛рдВрдирд╛ рдЖрдгрд┐ рд╡рд┐рдХрд╕рдХрд╛рдВрдирд╛ рддреНрдпрд╛рдВрдЪреНрдпрд╛ рд╕рдВрд╕рд╛рдзрдирд╛рдВрдиреБрд╕рд╛рд░ рдХрд┐рдВрд╡рд╛ рд╡рд┐рд╢рд┐рд╖реНрдЯ рдЖрд╡рд╢реНрдпрдХрддрд╛ рдкреВрд░реНрдг рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рд╢рдХреНрддрд┐рд╢рд╛рд▓реА AI рдХреНрд╖рдорддрд╛ рд╡рд╛рдкрд░рдгреНрдпрд╛рд╕ рд╕рдХреНрд╖рдо рдХрд░рддреЗ.

### рдореБрдЦреНрдп рдореБрджреНрджреЗ

**рдУрдкрди рд╕реЛрд░реНрд╕ рдЙрддреНрдХреГрд╖реНрдЯрддрд╛**: Qwen рд╕рд┐рджреНрдз рдХрд░рддреЗ рдХреА рдУрдкрди-рд╕реЛрд░реНрд╕ рдореЙрдбреЗрд▓реНрд╕ рдорд╛рд▓рдХреАрдЪреНрдпрд╛ рдкрд░реНрдпрд╛рдпрд╛рдВрд╢реА рд╕реНрдкрд░реНрдзрд╛рддреНрдордХ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдкреНрд░рд╛рдкреНрдд рдХрд░реВ рд╢рдХрддрд╛рдд, рддрд╕реЗрдЪ рдкрд╛рд░рджрд░реНрд╢рдХрддрд╛, рд╕рд╛рдиреБрдХреВрд▓рди, рдЖрдгрд┐ рдирд┐рдпрдВрддреНрд░рдг рдкреНрд░рджрд╛рди рдХрд░рддрд╛рдд.

**рд╕реНрдХреЗрд▓реЗрдмрд▓ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░**: 0.5B рддреЗ 235B рдкреЕрд░рд╛рдореАрдЯрд░реНрд╕рдЪреА рд╢реНрд░реЗрдгреА рдореЛрдмрд╛рдЗрд▓ рдбрд┐рд╡реНрд╣рд╛рдЗрд╕реЗрд╕рдкрд╛рд╕реВрди рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рдХреНрд▓рд╕реНрдЯрд░реНрд╕рдкрд░реНрдпрдВрдд рд╕рдВрдкреВрд░реНрдг рд╕рдВрдЧрдгрдХреАрдп рд╡рд╛рддрд╛рд╡рд░рдгрд╛рдд рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рд╕рдХреНрд╖рдо рдХрд░рддреЗ.

**рд╡рд┐рд╢реЗрд╖ рдХреНрд╖рдорддрд╛**: Qwen-Coder, Qwen-Math, рдЖрдгрд┐ Qwen-VL рд╕рд╛рд░рдЦреНрдпрд╛ рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рдкреНрд░рдХрд╛рд░ рд╕рд╛рдорд╛рдиреНрдп рднрд╛рд╖рд╛ рд╕рдордЬ рд░рд╛рдЦрдд рд╡рд┐рд╢реЗрд╖ рдХреМрд╢рд▓реНрдп рдкреНрд░рджрд╛рди рдХрд░рддрд╛рдд.

**рдЬрд╛рдЧрддрд┐рдХ рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдпрддрд╛**: 119+ рднрд╛рд╖рд╛рдВрдордзреНрдпреЗ рдордЬрдмреВрдд рдмрд╣реБрднрд╛рд╖рд┐рдХ рд╕рдорд░реНрдерди Qwen рд▓рд╛ рдЖрдВрддрд░рд░рд╛рд╖реНрдЯреНрд░реАрдп рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдЖрдгрд┐ рд╡рд┐рд╡рд┐рдз рд╡рд╛рдкрд░рдХрд░реНрддреНрдпрд╛рдВрд╕рд╛рдареА рдпреЛрдЧреНрдп рдмрдирд╡рддреЗ.

**рд╕рддрдд рдирд╛рд╡реАрдиреНрдпрдкреВрд░реНрдгрддрд╛**: Qwen 1.0 рдкрд╛рд╕реВрди Qwen3 рдкрд░реНрдпрдВрддрдЪреНрдпрд╛ рдЙрддреНрдХреНрд░рд╛рдВрддреАрдд рдХреНрд╖рдорддрд╛, рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛, рдЖрдгрд┐ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдкрд░реНрдпрд╛рдпрд╛рдВрдордзреНрдпреЗ рд╕рд╛рддрддреНрдпрдкреВрд░реНрдг рд╕реБрдзрд╛рд░рдгрд╛ рджрд┐рд╕реВрди рдпреЗрддреЗ.

### рднрд╡рд┐рд╖реНрдпрд╛рддреАрд▓ рджреГрд╖реНрдЯрд┐рдХреЛрди

Qwen рдХреБрдЯреБрдВрдм рд╡рд┐рдХрд╕рд┐рдд рд╣реЛрдд рд░рд╛рд╣рд┐рд▓реНрдпрд╛рдореБрд│реЗ, рдЖрдкрдг рдЕрдкреЗрдХреНрд╖рд╛ рдХрд░реВ рд╢рдХрддреЛ:

- **рд╡рд░реНрдзрд┐рдд рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛**: рдЪрд╛рдВрдЧрд▓реНрдпрд╛ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛-рдкреНрд░рддрд┐-рдкреЕрд░рд╛рдореАрдЯрд░ рдЧреБрдгреЛрддреНрддрд░рд╛рдВрд╕рд╛рдареА рд╕рддрдд рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди
- **рд╡рд┐рд╕реНрддрд╛рд░рд┐рдд рдорд▓реНрдЯреАрдореЛрдбрд▓ рдХреНрд╖рдорддрд╛**: рдЕрдзрд┐рдХ рдкреНрд░рдЧрдд рд╡реНрд╣рд┐рдЬрди, рдСрдбрд┐рдУ, рдЖрдгрд┐ рдЯреЗрдХреНрд╕реНрдЯ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рд╕рдорд╛рдХрд▓рд┐рдд рдХрд░рдгреЗ
- **рддрд░реНрдХрд╢рдХреНрддреА рд╕реБрдзрд╛рд░рдгрд╛**: рдкреНрд░рдЧрдд рд╡рд┐рдЪрд╛рд░ рдпрдВрддреНрд░рдгрд╛ рдЖрдгрд┐ рдорд▓реНрдЯреА-рд╕реНрдЯреЗрдк рд╕рдорд╕реНрдпрд╛ рд╕реЛрдбрд╡рдгреНрдпрд╛рдЪреНрдпрд╛ рдХреНрд╖рдорддрд╛
- **рдЪрд╛рдВрдЧрд▓реЗ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдЯреВрд▓реНрд╕**: рд╡рд┐рд╡рд┐рдз рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдкрд░рд┐рд╕реНрдерд┐рддреАрд╕рд╛рдареА рд╡рд░реНрдзрд┐рдд рдлреНрд░реЗрдорд╡рд░реНрдХреНрд╕ рдЖрдгрд┐ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди рдЯреВрд▓реНрд╕
- **рд╕рдореБрджрд╛рдп рд╡рд╛рдв**: рдЯреВрд▓реНрд╕, рдЕрдиреБрдкреНрд░рдпреЛрдЧ, рдЖрдгрд┐ рд╕рдореБрджрд╛рдп рдпреЛрдЧрджрд╛рдирд╛рдВрдЪреЗ рд╡рд┐рд╕реНрддрд╛рд░рд┐рдд рдЗрдХреЛрд╕рд┐рд╕реНрдЯрдо

### рдкреБрдвреАрд▓ рдкрд╛рдпрд▒реНрдпрд╛

рдЪреЕрдЯрдмреЙрдЯ рддрдпрд╛рд░ рдХрд░рдгреЗ, рд╢реИрдХреНрд╖рдгрд┐рдХ рдЯреВрд▓реНрд╕ рд╡рд┐рдХрд╕рд┐рдд рдХрд░рдгреЗ, рдХреЛрдбрд┐рдВрдЧ рд╕рд╣рд╛рдпреНрдпрдХ рддрдпрд╛рд░ рдХрд░рдгреЗ, рдХрд┐рдВрд╡рд╛ рдмрд╣реБрднрд╛рд╖рд┐рдХ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╡рд░ рдХрд╛рдо рдХрд░рдгреЗ рдЕрд╕реЛ, Qwen рдХреБрдЯреБрдВрдм рдордЬрдмреВрдд рд╕рдореБрджрд╛рдп рд╕рдорд░реНрдерди рдЖрдгрд┐ рд╡реНрдпрд╛рдкрдХ рджрд╕реНрддрдРрд╡рдЬреАрдХрд░рдгрд╛рд╕рд╣ рд╕реНрдХреЗрд▓реЗрдмрд▓ рд╕реЛрд▓реНрдпреВрд╢рдиреНрд╕ рдкреНрд░рджрд╛рди рдХрд░рддреЗ.

рдирд╡реАрдирддрдо рдЕрджреНрдпрддрдиреЗ, рдореЙрдбреЗрд▓ рд░рд┐рд▓реАрдЬ, рдЖрдгрд┐ рддрдкрд╢реАрд▓рд╡рд╛рд░ рддрд╛рдВрддреНрд░рд┐рдХ рджрд╕реНрддрдРрд╡рдЬреАрдХрд░рдгрд╛рд╕рд╛рдареА, Hugging Face рд╡рд░реАрд▓ рдЕрдзрд┐рдХреГрдд Qwen рд░рд┐рдкреЙрдЭрд┐рдЯрд░реАрд▓рд╛ рднреЗрдЯ рджреНрдпрд╛ рдЖрдгрд┐ рд╕рдХреНрд░рд┐рдп рд╕рдореБрджрд╛рдп рдЪрд░реНрдЪрд╛ рдЖрдгрд┐ рдЙрджрд╛рд╣рд░рдгреЗ рдПрдХреНрд╕рдкреНрд▓реЛрд░ рдХрд░рд╛.

AI рд╡рд┐рдХрд╛рд╕рд╛рдЪреЗ рднрд╡рд┐рд╖реНрдп рд╕рд░реНрд╡ рдХреНрд╖реЗрддреНрд░рд╛рдВрдордзреНрдпреЗ рдЖрдгрд┐ рд╕реНрдХреЗрд▓реНрд╕рдордзреНрдпреЗ рдирд╛рд╡реАрдиреНрдпрдкреВрд░реНрдгрддреЗрд╕рд╛рдареА рд╕рдХреНрд╖рдо рдкрд╛рд░рджрд░реНрд╢рдХ рдЖрдгрд┐ рд╢рдХреНрддрд┐рд╢рд╛рд▓реА рдЯреВрд▓реНрд╕рдордзреНрдпреЗ рдЖрд╣реЗ. Qwen рдХреБрдЯреБрдВрдм рдпрд╛ рджреГрд╖реНрдЯрд┐рдХреЛрдирд╛рдЪреЗ рдЙрджрд╛рд╣рд░рдг рджреЗрддреЗ, рд╕рдВрд╕реНрдерд╛рдВрдирд╛ рдЖрдгрд┐ рд╡рд┐рдХрд╕рдХрд╛рдВрдирд╛ AI-рд╕рдХреНрд╖рдо рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрдЪреА рдкреБрдвреАрд▓ рдкрд┐рдвреА рддрдпрд╛рд░ рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдкрд╛рдпрд╛ рдкреНрд░рджрд╛рди рдХрд░рддреЗ.

## рдЕрддрд┐рд░рд┐рдХреНрдд рд╕рдВрд╕рд╛рдзрдиреЗ

- **рдЕрдзрд┐рдХреГрдд рджрд╕реНрддрдРрд╡рдЬ**: [Qwen рджрд╕реНрддрдРрд╡рдЬ](https://qwen.readthedocs.io/)
- **рдореЙрдбреЗрд▓ рд╣рдм**: [Hugging Face Qwen рд╕рдВрдЧреНрд░рд╣](https://huggingface.co/collections/Qwen/)
- **рддрд╛рдВрддреНрд░рд┐рдХ рдкреЗрдкрд░реНрд╕**: [Qwen рд╕рдВрд╢реЛрдзрди рдкреНрд░рдХрд╛рд╢рдиреЗ](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **рд╕рдореБрджрд╛рдп**: [GitHub рдЪрд░реНрдЪрд╛ рдЖрдгрд┐ рд╕рдорд╕реНрдпрд╛](https://github.com/QwenLM/)
- **ModelScope рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## рд╢рд┐рдХреНрд╖рдг рдкрд░рд┐рдгрд╛рдо

рдпрд╛ рдореЙрдбреНрдпреВрд▓ рдкреВрд░реНрдг рдХреЗрд▓реНрдпрд╛рдирдВрддрд░, рддреБрдореНрд╣реА рд╣реЗ рдХрд░реВ рд╢рдХрд╛рд▓:

1. Qwen рдореЙрдбреЗрд▓ рдХреБрдЯреБрдВрдмрд╛рдЪреНрдпрд╛ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░рд▓ рдлрд╛рдпрджреЗ рдЖрдгрд┐ рддреНрдпрд╛рдЪрд╛ рдУрдкрди-рд╕реЛрд░реНрд╕ рджреГрд╖реНрдЯрд┐рдХреЛрди рд╕реНрдкрд╖реНрдЯ рдХрд░рд╛
2. рд╡рд┐рд╢рд┐рд╖реНрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧ рдЖрд╡рд╢реНрдпрдХрддрд╛ рдЖрдгрд┐ рд╕рдВрд╕рд╛рдзрди рдорд░реНрдпрд╛рджрд╛рдВрдиреБрд╕рд╛рд░ рдпреЛрдЧреНрдп Qwen рдкреНрд░рдХрд╛рд░ рдирд┐рд╡рдбрд╛
3. рд╡рд┐рд╡рд┐рдз рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдкрд░рд┐рд╕реНрдерд┐рддреАрдВрдордзреНрдпреЗ Qwen рдореЙрдбреЗрд▓реНрд╕ рдЕрдВрдорд▓рд╛рдд рдЖрдгрд╛ рдЖрдгрд┐ рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХреЗрд▓реЗрд▓реНрдпрд╛ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рдиреНрд╕рд╕рд╣
4. Qwen рдореЙрдбреЗрд▓ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рд╕реБрдзрд╛рд░рдгреНрдпрд╛рд╕рд╛рдареА рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭреЗрд╢рди рдЖрдгрд┐ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди рддрдВрддреНрд░ рд▓рд╛рдЧреВ рдХрд░рд╛
5. Qwen рдХреБрдЯреБрдВрдмрд╛рддреАрд▓ рдореЙрдбреЗрд▓ рдЖрдХрд╛рд░, рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛, рдЖрдгрд┐ рдХреНрд╖рдорддрд╛ рдпрд╛рдордзреАрд▓ рд╡реНрдпрд╛рдкрд╛рд░рд╛рдВрдЪреЗ рдореВрд▓реНрдпрд╛рдВрдХрди рдХрд░рд╛

## рдкреБрдвреЗ рдХрд╛рдп

- [03: Gemma рдХреБрдЯреБрдВрдм рдореВрд▓рднреВрдд рдЧреЛрд╖реНрдЯреА](03.GemmaFamily.md)

---

**рдЕрд╕реНрд╡реАрдХрд░рдг**:  
рд╣рд╛ рджрд╕реНрддрдРрд╡рдЬ AI рднрд╛рд╖рд╛рдВрддрд░ рд╕реЗрд╡рд╛ [Co-op Translator](https://github.com/Azure/co-op-translator) рд╡рд╛рдкрд░реВрди рднрд╛рд╖рд╛рдВрддрд░рд┐рдд рдХрд░рдгреНрдпрд╛рдд рдЖрд▓рд╛ рдЖрд╣реЗ. рдЖрдореНрд╣реА рдЕрдЪреВрдХрддреЗрд╕рд╛рдареА рдкреНрд░рдпрддреНрдирд╢реАрд▓ рдЕрд╕рд▓реЛ рддрд░реА, рдХреГрдкрдпрд╛ рд▓рдХреНрд╖рд╛рдд рдареЗрд╡рд╛ рдХреА рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рднрд╛рд╖рд╛рдВрддрд░реЗ рддреНрд░реБрдЯреА рдХрд┐рдВрд╡рд╛ рдЕрдЪреВрдХрддреЗрдЪреНрдпрд╛ рдЕрднрд╛рд╡рд╛рдиреЗ рдпреБрдХреНрдд рдЕрд╕реВ рд╢рдХрддрд╛рдд. рдореВрд│ рднрд╛рд╖реЗрддреАрд▓ рджрд╕реНрддрдРрд╡рдЬ рд╣рд╛ рдЕрдзрд┐рдХреГрдд рд╕реНрд░реЛрдд рдорд╛рдирд▓рд╛ рдЬрд╛рд╡рд╛. рдорд╣рддреНрддреНрд╡рд╛рдЪреНрдпрд╛ рдорд╛рд╣рд┐рддреАрд╕рд╛рдареА, рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рдорд╛рдирд╡реА рднрд╛рд╖рд╛рдВрддрд░рд╛рдЪреА рд╢рд┐рдлрд╛рд░рд╕ рдХреЗрд▓реА рдЬрд╛рддреЗ. рдпрд╛ рднрд╛рд╖рд╛рдВрддрд░рд╛рдЪрд╛ рд╡рд╛рдкрд░ рдХрд░реВрди рдЙрджреНрднрд╡рд▓реЗрд▓реНрдпрд╛ рдХреЛрдгрддреНрдпрд╛рд╣реА рдЧреИрд░рд╕рдордЬ рдХрд┐рдВрд╡рд╛ рдЪреБрдХреАрдЪреНрдпрд╛ рдЕрд░реНрдерд╛рд╕рд╛рдареА рдЖрдореНрд╣реА рдЬрдмрд╛рдмрджрд╛рд░ рд░рд╛рд╣рдгрд╛рд░ рдирд╛рд╣реА.