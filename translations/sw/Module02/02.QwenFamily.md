<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-18T15:52:55+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "sw"
}
-->
# Sehemu ya 2: Misingi ya Familia ya Qwen

Familia ya modeli ya Qwen inawakilisha mbinu ya kina ya Alibaba Cloud kwa modeli kubwa za lugha na AI ya multimodal, ikionyesha kwamba modeli za wazi zinaweza kufanikisha utendaji wa ajabu huku zikiwa na upatikanaji katika hali mbalimbali za utekelezaji. Ni muhimu kuelewa jinsi familia ya Qwen inavyowezesha uwezo wa AI wenye nguvu kwa chaguo za utekelezaji zinazobadilika huku ikidumisha utendaji wa ushindani katika kazi mbalimbali.

## Rasilimali kwa Watengenezaji

### Hifadhi ya Modeli ya Hugging Face
Baadhi ya modeli za familia ya Qwen zinapatikana kupitia [Hugging Face](https://huggingface.co/models?search=qwen), zikitoa upatikanaji wa baadhi ya aina za modeli hizi. Unaweza kuchunguza aina zinazopatikana, kuziboresha kwa matumizi yako maalum, na kuzitekeleza kupitia mifumo mbalimbali.

### Zana za Maendeleo ya Kawaida
Kwa maendeleo ya ndani na majaribio, unaweza kutumia [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) kuendesha modeli za Qwen zinazopatikana kwenye mashine yako ya maendeleo kwa utendaji ulioboreshwa.

### Rasilimali za Nyaraka
- [Nyaraka za Modeli ya Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Kuboresha Modeli za Qwen kwa Utekelezaji wa Edge](https://github.com/microsoft/olive)

## Utangulizi

Katika mafunzo haya, tutachunguza familia ya modeli ya Qwen ya Alibaba na dhana zake za msingi. Tutajadili mabadiliko ya familia ya Qwen, mbinu za mafunzo za ubunifu zinazofanya modeli za Qwen kuwa bora, aina kuu katika familia, na matumizi ya vitendo katika hali tofauti.

## Malengo ya Kujifunza

Mwisho wa mafunzo haya, utaweza:

- Kuelewa falsafa ya muundo na mabadiliko ya familia ya modeli ya Qwen ya Alibaba
- Kutambua ubunifu muhimu unaowezesha modeli za Qwen kufanikisha utendaji wa juu katika ukubwa mbalimbali wa vigezo
- Kutambua faida na mapungufu ya aina tofauti za modeli za Qwen
- Kutumia maarifa ya modeli za Qwen kuchagua aina zinazofaa kwa hali halisi

## Kuelewa Mandhari ya Kisasa ya Modeli za AI

Mandhari ya AI imebadilika sana, huku mashirika tofauti yakifuata mbinu mbalimbali za maendeleo ya modeli za lugha. Wakati wengine wanazingatia modeli za wamiliki zisizo wazi, wengine wanasisitiza upatikanaji wa wazi na uwazi. Mbinu ya jadi inahusisha ama modeli kubwa za wamiliki zinazopatikana tu kupitia API au modeli za wazi ambazo zinaweza kuwa nyuma katika uwezo.

Paradigma hii inaleta changamoto kwa mashirika yanayotafuta uwezo wa AI wenye nguvu huku yakidumisha udhibiti wa data zao, gharama, na kubadilika kwa utekelezaji. Mbinu ya kawaida mara nyingi inahitaji kuchagua kati ya utendaji wa hali ya juu na kuzingatia utekelezaji wa vitendo.

## Changamoto ya Ubora wa AI Unaopatikana

Hitaji la AI ya ubora wa juu inayopatikana limekuwa muhimu zaidi katika hali mbalimbali. Fikiria matumizi yanayohitaji chaguo za utekelezaji zinazobadilika kwa mahitaji tofauti ya shirika, utekelezaji wa gharama nafuu ambapo gharama za API zinaweza kuwa kubwa, uwezo wa lugha nyingi kwa matumizi ya kimataifa, au utaalamu maalum wa kikoa katika maeneo kama vile usimbaji na hesabu.

### Mahitaji Muhimu ya Utekelezaji

Utekelezaji wa kisasa wa AI unakabiliwa na mahitaji kadhaa ya msingi yanayopunguza matumizi ya vitendo:

- **Upatikanaji**: Upatikanaji wa wazi kwa uwazi na ubinafsishaji
- **Ufanisi wa Gharama**: Mahitaji ya kompyuta yanayofaa kwa bajeti mbalimbali
- **Kubadilika**: Ukubwa tofauti wa modeli kwa hali tofauti za utekelezaji
- **Ufikiaji wa Kimataifa**: Uwezo wa lugha nyingi na tamaduni mbalimbali
- **Utaalamu**: Aina maalum za kikoa kwa matumizi maalum

## Falsafa ya Modeli ya Qwen

Familia ya modeli ya Qwen inawakilisha mbinu ya kina ya maendeleo ya modeli za AI, ikipa kipaumbele upatikanaji wa wazi, uwezo wa lugha nyingi, na utekelezaji wa vitendo huku ikidumisha sifa za utendaji wa ushindani. Modeli za Qwen zinafanikisha hili kupitia ukubwa tofauti wa modeli, mbinu za mafunzo za ubora wa juu, na aina maalum kwa kikoa tofauti.

Familia ya Qwen inajumuisha mbinu mbalimbali zilizoundwa kutoa chaguo katika wigo wa utendaji-ufanisi, kuwezesha utekelezaji kutoka kwa vifaa vya mkononi hadi seva za biashara huku ikitoa uwezo wa maana wa AI. Lengo ni kueneza upatikanaji wa AI ya ubora wa juu huku ikitoa kubadilika kwa chaguo za utekelezaji.

### Kanuni za Msingi za Muundo wa Qwen

Modeli za Qwen zimejengwa juu ya kanuni kadhaa za msingi zinazozitofautisha na familia nyingine za modeli za lugha:

- **Kipaumbele kwa Chanzo Wazi**: Uwazi kamili na upatikanaji kwa utafiti na matumizi ya kibiashara
- **Mafunzo ya Kina**: Mafunzo kwenye seti kubwa, tofauti za data zinazojumuisha lugha nyingi na kikoa
- **Usanifu Unaoweza Kupimika**: Ukubwa tofauti wa modeli ili kuendana na mahitaji tofauti ya kompyuta
- **Ubora Maalum**: Aina maalum za kikoa zilizoboreshwa kwa kazi maalum

## Teknolojia Muhimu Zinazowezesha Familia ya Qwen

### Mafunzo ya Kiwango Kikubwa

Moja ya vipengele vinavyotambulika vya familia ya Qwen ni kiwango kikubwa cha data ya mafunzo na rasilimali za kompyuta zilizowekezwa katika maendeleo ya modeli. Modeli za Qwen zinatumia seti za data za lugha nyingi zilizochaguliwa kwa uangalifu zinazojumuisha trilioni za tokeni, zilizoundwa kutoa maarifa ya kina ya dunia na uwezo wa kufikiri.

Mbinu hii inafanya kazi kwa kuchanganya maudhui ya wavuti ya ubora wa juu, fasihi ya kitaaluma, hifadhi za msimbo, na rasilimali za lugha nyingi. Mbinu ya mafunzo inasisitiza upana wa maarifa na kina cha uelewa katika kikoa na lugha mbalimbali.

### Uwezo wa Kufikiri na Kuamua

Modeli za Qwen za hivi karibuni zinajumuisha uwezo wa kufikiri wa hali ya juu unaowezesha utatuzi wa matatizo ya hatua nyingi:

**Hali ya Kufikiri (Qwen3)**: Modeli zinaweza kushiriki katika kufikiri kwa kina hatua kwa hatua kabla ya kutoa majibu ya mwisho, sawa na mbinu za utatuzi wa matatizo za binadamu.

**Uendeshaji wa Njia Mbili**: Uwezo wa kubadilika kati ya hali ya majibu ya haraka kwa maswali rahisi na hali ya kufikiri kwa kina kwa matatizo magumu.

**Muunganiko wa Mnyororo wa Mawazo**: Kujumuisha hatua za kufikiri kwa asili zinazoboresha uwazi na usahihi katika kazi ngumu.

### Ubunifu wa Kimuundo

Familia ya Qwen inajumuisha uboreshaji kadhaa wa usanifu ulioundwa kwa utendaji na ufanisi:

**Muundo Unaoweza Kupimika**: Usanifu thabiti katika ukubwa wa modeli unaowezesha upanuzi rahisi na kulinganisha.

**Muunganiko wa Multimodal**: Muunganiko wa maandishi, maono, na uwezo wa usindikaji wa sauti ndani ya usanifu wa umoja.

**Uboreshaji wa Utekelezaji**: Chaguo nyingi za upunguzaji na miundo ya utekelezaji kwa usanidi mbalimbali wa vifaa.

## Ukubwa wa Modeli na Chaguo za Utekelezaji

Mazingira ya kisasa ya utekelezaji yanapata faida kutoka kwa kubadilika kwa modeli za Qwen katika mahitaji tofauti ya kompyuta:

### Modeli Ndogo (0.5B-3B)

Qwen inatoa modeli ndogo zenye ufanisi zinazofaa kwa utekelezaji wa edge, programu za mkononi, na mazingira yenye rasilimali ndogo huku zikidumisha uwezo wa kushangaza.

### Modeli za Kati (7B-32B)

Modeli za kiwango cha kati zinatoa uwezo ulioboreshwa kwa matumizi ya kitaalamu, zikitoa usawa bora kati ya utendaji na mahitaji ya kompyuta.

### Modeli Kubwa (72B+)

Modeli za kiwango kikubwa zinatoa utendaji wa hali ya juu kwa matumizi yanayohitaji sana, utafiti, na utekelezaji wa biashara unaohitaji uwezo wa juu zaidi.

## Faida za Familia ya Modeli ya Qwen

### Upatikanaji wa Chanzo Wazi

Modeli za Qwen zinatoa uwazi kamili na uwezo wa ubinafsishaji, zikiruhusu mashirika kuelewa, kurekebisha, na kubadilisha modeli kwa mahitaji yao maalum bila kufungiwa na muuzaji.

### Kubadilika kwa Utekelezaji

Wigo wa ukubwa wa modeli unaruhusu utekelezaji katika usanidi mbalimbali wa vifaa, kutoka kwa vifaa vya mkononi hadi seva za hali ya juu, zikitoa mashirika kubadilika katika chaguo zao za miundombinu ya AI.

### Ubora wa Lugha Nyingi

Modeli za Qwen zinang'aa katika uelewa na uzalishaji wa lugha nyingi, zikisaidia lugha kadhaa na nguvu maalum katika Kiingereza na Kichina, zikizifanya kufaa kwa matumizi ya kimataifa.

### Utendaji wa Ushindani

Modeli za Qwen mara kwa mara zinafanikisha matokeo ya ushindani kwenye viwango vya kupima huku zikitoa upatikanaji wa chanzo wazi, zikionyesha kwamba modeli za wazi zinaweza kulingana na mbadala za wamiliki.

### Uwezo Maalum

Aina maalum za kikoa kama Qwen-Coder na Qwen-Math zinatoa utaalamu maalum huku zikidumisha uwezo wa jumla wa uelewa wa lugha.

## Mifano ya Vitendo na Matumizi

Kabla ya kuingia katika maelezo ya kiufundi, hebu tuchunguze mifano halisi ya kile modeli za Qwen zinaweza kufanikisha:

### Mfano wa Uamuzi wa Kihesabu

Qwen-Math inang'aa katika utatuzi wa matatizo ya kihesabu hatua kwa hatua. Kwa mfano, inapoulizwa kutatua tatizo gumu la hesabu:

```
User: Find the derivative of f(x) = x¬≥ + 2x¬≤ - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x¬≥ + 2x¬≤ - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(x‚Åø) = n¬∑x‚Åø‚Åª¬π

Breaking down each term:
- d/dx(x¬≥) = 3x¬≤
- d/dx(2x¬≤) = 2 ¬∑ 2x¬π = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x¬≤ + 4x - 5
```

### Mfano wa Usaidizi wa Lugha Nyingi

Modeli za Qwen zinaonyesha uwezo mkubwa wa lugha nyingi katika lugha mbalimbali:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**‰∏≠Êñá:**
‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâÊòØÊåáÊú∫Âô®ÔºàÁâπÂà´ÊòØËÆ°ÁÆóÊú∫Á≥ªÁªüÔºâÂØπ‰∫∫Á±ªÊô∫ËÉΩËøáÁ®ãÁöÑÊ®°Êãü„ÄÇËøô‰∫õËøáÁ®ãÂåÖÊã¨Â≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥ÈóÆÈ¢ò„ÄÅÊÑüÁü•ÂíåËØ≠Ë®ÄÁêÜËß£„ÄÇ‰∫∫Â∑•Êô∫ËÉΩ‰ΩøÊú∫Âô®ËÉΩÂ§üÊâßË°åÈÄöÂ∏∏ÈúÄË¶Å‰∫∫Á±ªËÆ§Áü•ËÉΩÂäõÁöÑ‰ªªÂä°„ÄÇ
```

### Mfano wa Uwezo wa Multimodal

Qwen-VL inaweza kusindika maandishi na picha kwa wakati mmoja:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Mfano wa Uzalishaji wa Msimbo

Qwen-Coder inang'aa katika kuzalisha na kuelezea msimbo katika lugha mbalimbali za programu:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Fanya utafutaji wa binary kwenye orodha iliyopangwa ili kupata thamani lengwa.
    
    Args:
        arr (list): Orodha iliyopangwa ya vipengele vinavyolinganishwa
        target: Thamani ya kutafuta
        
    Returns:
        int: Kielezo cha lengwa ikiwa imepatikana, -1 ikiwa haijapatikana
        
    Ugumu wa Wakati: O(log n)
    Ugumu wa Nafasi: O(1)
    """
    # Anzisha vidokezo vya kushoto na kulia
    left, right = 0, len(arr) - 1
    
    # Endelea kutafuta wakati nafasi ya utafutaji ni halali
    while left <= right:
        # Hesabu kielezo cha katikati ili kuepuka kufurika kwa nambari
        mid = left + (right - left) // 2
        
        # Angalia ikiwa tumepata lengwa
        if arr[mid] == target:
            return mid
        
        # Ikiwa lengwa ni ndogo, tafuta nusu ya kushoto
        elif arr[mid] > target:
            right = mid - 1
        
        # Ikiwa lengwa ni kubwa, tafuta nusu ya kulia
        else:
            left = mid + 1
    
    # Lengwa halijapatikana
    return -1

# Mfano wa matumizi:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Kielezo cha 7: {result}")  # Matokeo: Kielezo cha 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### Mfano wa Utekelezaji wa Edge

Modeli za Qwen zinaweza kutekelezwa kwenye vifaa mbalimbali vya edge kwa usanidi ulioboreshwa:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Mabadiliko ya Familia ya Qwen

### Qwen 1.0 na 1.5: Modeli za Msingi

Modeli za awali za Qwen zilianzisha kanuni za msingi za mafunzo ya kina na upatikanaji wa chanzo wazi:

- **Qwen-7B (vigezo 7B)**: Toleo la awali likilenga uelewa wa lugha ya Kichina na Kiingereza
- **Qwen-14B (vigezo 14B)**: Uwezo ulioboreshwa na uelewa wa maarifa na kufikiri
- **Qwen-72B (vigezo 72B)**: Modeli kubwa inayotoa utendaji wa hali ya juu
- **Mfululizo wa Qwen1.5**: Ulipanuliwa kwa ukubwa tofauti (0.5B hadi 110B) na uboreshaji wa kushughulikia muktadha mrefu

### Familia ya Qwen2: Upanuzi wa Multimodal

Mfululizo wa Qwen2 ulileta maendeleo makubwa katika uwezo wa lugha na multimodal:

- **Qwen2-0.5B hadi 72B**: Wigo kamili wa modeli za lugha kwa mahitaji tofauti ya utekelezaji
- **Qwen2-57B-A14B (MoE)**: Usanifu wa mchanganyiko wa wataalamu kwa matumizi bora ya vigezo
- **Qwen2-VL**: Uwezo wa hali ya juu wa maono-lugha kwa uelewa wa picha
- **Qwen2-Audio**: Uwezo wa usindikaji na uelewa wa sauti
- **Qwen2-Math**: Uwezo maalum wa kufikiri na utatuzi wa matatizo ya kihesabu

### Familia ya Qwen2.5: Utendaji Ulioboreshwa

Mfululizo wa Qwen2.5 ulileta maboresho makubwa katika vipengele vyote:

- **Mafunzo Yaliyopanuliwa**: Trilioni 18 za data ya mafunzo kwa uwezo ulioboreshwa
- **Muktadha Ulioongezwa**: Urefu wa muktadha wa hadi tokeni 128K, na toleo la Turbo likiunga mkono tokeni milioni 1
- **Utaalamu Ulioboreshwa**: Qwen2.5-Coder na Qwen2.5-Math zilizoboreshwa
- **Usaidizi Bora wa Lugha Nyingi**: Utendaji ulioboreshwa katika lugha 27+

### Familia ya Qwen3: Kufikiri kwa Hali ya Juu

Kizazi cha hivi karibuni kinavuka mipaka ya uwezo wa kufikiri na kuamua:

- **Qwen3-235B-A22B**: Modeli kuu ya mchanganyiko wa wataalamu yenye vigezo 235B
- **Qwen3-30B-A3B**: Modeli ya MoE yenye ufanisi na utendaji mzuri kwa kila kigezo kinachotumika
- **Modeli Zenye Msongamano**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B kwa hali tofauti za utekelezaji
- **Hali ya Kufikiri**: Mbinu mseto ya kufikiri inayounga mkono majibu ya haraka na kufikiri kwa kina
- **Ubora wa Lugha Nyingi**: Usaidizi wa lugha na lahaja 119
- **Mafunzo Yaliyoboreshwa**: Trilioni 36 za data ya mafunzo ya ubora wa juu na tofauti

## Matumizi ya Modeli za Qwen

### Matumizi ya Biashara

Mashirika hutumia modeli za Qwen kwa uchambuzi wa nyaraka, otomatiki ya huduma kwa wateja, usaidizi wa uzalishaji wa msimbo, na matumizi ya akili ya biashara. Asili ya chanzo wazi inaruhusu ubinafsishaji kwa mahitaji maalum ya biashara huku ikidumisha faragha na udhibiti wa data.

### Kompyuta ya Mkono na Edge

Programu za mkononi zinatumia modeli za Qwen kwa tafsiri ya wakati halisi, wasaidizi wenye akili, uzalishaji wa maudhui, na mapendekezo ya kibinafsi. Wigo wa ukubwa wa modeli unaruhusu utekelezaji kutoka kwa vifaa vya mkononi hadi seva za edge.

### Teknolojia ya Elimu

Majukwaa ya elimu hutumia modeli za Qwen kwa ufundishaji wa kibinafsi, uzalishaji wa maudhui otomatiki, usaidizi wa kujifunza lugha, na uzoefu wa elimu wa maingiliano. Modeli maalum kama Qwen-Math hutoa utaalamu wa kikoa maalum.

### Matum
Hapa kuna jinsi ya kuanza kutumia mifano ya Qwen kwa kutumia maktaba ya Hugging Face Transformers:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Kutumia Mifano ya Qwen2.5

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### Matumizi Maalum ya Mifano

**Uundaji wa Nambari kwa Qwen-Coder:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**Kutatua Masuala ya Hisabati:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = x¬≥ + 2x¬≤ - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**Majukumu ya Lugha na Maono:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### Hali ya Kufikiri (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### üì± Utekelezaji wa Simu na Vifaa vya Edge

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### Mfano wa Utekelezaji wa API

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## Viwango vya Utendaji na Mafanikio

Familia ya mifano ya Qwen imepata utendaji wa kipekee katika viwango mbalimbali huku ikidumisha upatikanaji wa chanzo huria:

### Mambo Muhimu ya Utendaji

**Ubora wa Kufikiri:**
- Qwen3-235B-A22B inapata matokeo ya ushindani katika tathmini za viwango vya uundaji wa nambari, hisabati, na uwezo wa jumla ikilinganishwa na mifano mingine ya kiwango cha juu kama DeepSeek-R1, o1, o3-mini, Grok-3, na Gemini-2.5-Pro
- Qwen3-30B-A3B inazidi QwQ-32B kwa mara 10 ya vigezo vilivyoamilishwa
- Qwen3-4B inaweza kushindana na utendaji wa Qwen2.5-72B-Instruct

**Mafanikio ya Ufanisi:**
- Mifano ya msingi ya Qwen3-MoE inapata utendaji sawa na mifano ya msingi ya Qwen2.5 yenye msongamano huku ikitumia 10% tu ya vigezo vilivyoamilishwa
- Akiba kubwa ya gharama katika mafunzo na utambuzi ikilinganishwa na mifano yenye msongamano

**Uwezo wa Lugha Nyingi:**
- Mifano ya Qwen3 inaunga mkono lugha na lahaja 119
- Utendaji mzuri katika muktadha wa lugha na tamaduni mbalimbali

**Kiwango cha Mafunzo:**
- Qwen3 inatumia karibu mara mbili ya kiasi hicho, na takriban trilioni 36 za tokeni zinazofunika lugha na lahaja 119 ikilinganishwa na trilioni 18 za Qwen2.5

### Jedwali la Ulinganisho wa Mifano

| Mfululizo wa Mifano | Kiwango cha Vigezo | Urefu wa Muktadha | Nguvu Muhimu | Matumizi Bora |
|---------------------|--------------------|------------------|-------------|---------------|
| **Qwen2.5**         | 0.5B-72B          | 32K-128K         | Utendaji wa usawa, lugha nyingi | Matumizi ya jumla, utekelezaji wa uzalishaji |
| **Qwen2.5-Coder**   | 1.5B-32B          | 128K             | Uundaji wa nambari, programu | Maendeleo ya programu, msaada wa uundaji wa nambari |
| **Qwen2.5-Math**    | 1.5B-72B          | 4K-128K          | Ufikiri wa hisabati | Majukwaa ya elimu, matumizi ya STEM |
| **Qwen2.5-VL**      | Mbalimbali         | Mbalimbali        | Uelewa wa lugha na maono | Matumizi ya multimodal, uchambuzi wa picha |
| **Qwen3**           | 0.6B-235B         | Mbalimbali        | Ufikiri wa hali ya juu, hali ya kufikiri | Ufikiri mgumu, matumizi ya utafiti |
| **Qwen3 MoE**       | 30B-235B jumla    | Mbalimbali        | Utendaji mzuri wa kiwango kikubwa | Matumizi ya biashara, mahitaji ya utendaji wa juu |

## Mwongozo wa Uchaguzi wa Mfano

### Kwa Matumizi ya Msingi
- **Qwen2.5-0.5B/1.5B**: Programu za simu, vifaa vya edge, matumizi ya wakati halisi
- **Qwen2.5-3B/7B**: Chatbots za jumla, uundaji wa maudhui, mifumo ya maswali na majibu

### Kwa Majukumu ya Hisabati na Ufikiri
- **Qwen2.5-Math**: Kutatua masuala ya hisabati na elimu ya STEM
- **Qwen3 na Hali ya Kufikiri**: Ufikiri mgumu unaohitaji uchambuzi wa hatua kwa hatua

### Kwa Uundaji wa Nambari na Maendeleo
- **Qwen2.5-Coder**: Uundaji wa nambari, urekebishaji wa hitilafu, msaada wa programu
- **Qwen3**: Majukumu ya programu ya hali ya juu yenye uwezo wa kufikiri

### Kwa Matumizi ya Multimodal
- **Qwen2.5-VL**: Uelewa wa picha, maswali ya kuona
- **Qwen-Audio**: Usindikaji wa sauti na uelewa wa hotuba

### Kwa Utekelezaji wa Biashara
- **Qwen2.5-32B/72B**: Uelewa wa lugha wa utendaji wa juu
- **Qwen3-235B-A22B**: Uwezo wa juu zaidi kwa matumizi yanayohitaji sana

## Majukwaa ya Utekelezaji na Upatikanaji
### Majukwaa ya Wingu
- **Hugging Face Hub**: Hifadhi kamili ya mifano na msaada wa jamii
- **ModelScope**: Jukwaa la mifano la Alibaba lenye zana za uboreshaji
- **Watoa Huduma wa Wingu Mbalimbali**: Msaada kupitia majukwaa ya kawaida ya ML

### Mfumo wa Maendeleo ya Kawaida
- **Transformers**: Ujumuishaji wa kawaida wa Hugging Face kwa utekelezaji rahisi
- **vLLM**: Huduma ya utendaji wa juu kwa mazingira ya uzalishaji
- **Ollama**: Utekelezaji wa ndani uliorahisishwa na usimamizi
- **ONNX Runtime**: Uboreshaji wa majukwaa mbalimbali kwa vifaa mbalimbali
- **llama.cpp**: Utekelezaji mzuri wa C++ kwa majukwaa tofauti

### Rasilimali za Kujifunza
- **Nyaraka za Qwen**: Nyaraka rasmi na kadi za mifano
- **Hugging Face Model Hub**: Maonyesho ya maingiliano na mifano ya jamii
- **Karatasi za Utafiti**: Karatasi za kiufundi kwenye arxiv kwa uelewa wa kina
- **Mabaraza ya Jamii**: Msaada wa jamii hai na mijadala

### Kuanza na Mifano ya Qwen

#### Majukwaa ya Maendeleo
1. **Hugging Face Transformers**: Anza na ujumuishaji wa kawaida wa Python
2. **ModelScope**: Chunguza zana za utekelezaji zilizoboreshwa za Alibaba
3. **Utekelezaji wa Ndani**: Tumia Ollama au transformers moja kwa moja kwa majaribio ya ndani

#### Njia ya Kujifunza
1. **Elewa Dhana za Msingi**: Soma usanifu wa familia ya Qwen na uwezo wake
2. **Jaribu Tofauti**: Jaribu saizi tofauti za mifano ili kuelewa faida na hasara za utendaji
3. **Fanya Mazoezi ya Utekelezaji**: Tekeleza mifano katika mazingira ya maendeleo
4. **Boresha Utekelezaji**: Rekebisha kwa matumizi ya uzalishaji

#### Mazoea Bora
- **Anza na Kidogo**: Anza na mifano midogo (1.5B-7B) kwa maendeleo ya awali
- **Tumia Violezo vya Gumzo**: Tumia muundo sahihi kwa matokeo bora
- **Fuatilia Rasilimali**: Fuata matumizi ya kumbukumbu na kasi ya utambuzi
- **Fikiria Utaalamu**: Chagua tofauti maalum za kikoa inapofaa

## Mifumo ya Matumizi ya Juu

### Mifano ya Urekebishaji

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### Uhandisi Maalum wa Miongozo

**Kwa Majukumu ya Ufikiri Mgumu:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**Kwa Uundaji wa Nambari na Muktadha:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### Matumizi ya Lugha Nyingi

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (‰∏≠Êñá)",
        "es": "Spanish (Espa√±ol)",
        "fr": "French (Fran√ßais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (Êó•Êú¨Ë™û)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### üîß Mifumo ya Utekelezaji wa Uzalishaji

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## Mikakati ya Uboreshaji wa Utendaji

### Uboreshaji wa Kumbukumbu

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### Uboreshaji wa Utambuzi

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## Mazoea Bora na Miongozo

### Usalama na Faragha

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### Ufuatiliaji na Tathmini

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## Hitimisho

Familia ya mifano ya Qwen inawakilisha mbinu kamili ya kueneza teknolojia ya AI huku ikidumisha utendaji wa ushindani katika matumizi mbalimbali. Kupitia dhamira yake ya upatikanaji wa chanzo huria, uwezo wa lugha nyingi, na chaguo za utekelezaji zinazobadilika, Qwen inawezesha mashirika na watengenezaji kutumia uwezo wa AI bila kujali rasilimali zao au mahitaji maalum.

### Mambo Muhimu

**Ubora wa Chanzo Huria**: Qwen inaonyesha kuwa mifano ya chanzo huria inaweza kufikia utendaji wa ushindani na mbadala za wamiliki huku ikitoa uwazi, ubinafsishaji, na udhibiti.

**Usanifu Unaoweza Kupimika**: Kiwango cha vigezo kutoka 0.5B hadi 235B kinawezesha utekelezaji katika mazingira yote ya kompyuta, kutoka vifaa vya simu hadi makundi ya biashara.

**Uwezo Maalum**: Tofauti maalum za kikoa kama Qwen-Coder, Qwen-Math, na Qwen-VL zinatoa utaalamu maalum huku zikidumisha uelewa wa lugha ya jumla.

**Upatikanaji wa Kimataifa**: Msaada mzuri wa lugha nyingi katika lugha 119+ hufanya Qwen kufaa kwa matumizi ya kimataifa na watumiaji mbalimbali.

**Ubunifu Endelevu**: Mageuzi kutoka Qwen 1.0 hadi Qwen3 yanaonyesha uboreshaji wa mara kwa mara katika uwezo, ufanisi, na chaguo za utekelezaji.

### Mtazamo wa Baadaye

Kadri familia ya Qwen inavyoendelea kubadilika, tunaweza kutarajia:

- **Ufanisi Ulioboreshwa**: Uboreshaji unaoendelea kwa uwiano bora wa utendaji kwa vigezo
- **Uwezo wa Multimodal Ulioongezwa**: Ujumuishaji wa usindikaji wa maono, sauti, na maandishi wa hali ya juu zaidi
- **Ufikiri Ulioboreshwa**: Mifumo ya kufikiri ya hali ya juu na uwezo wa kutatua matatizo kwa hatua nyingi
- **Zana Bora za Utekelezaji**: Mfumo ulioboreshwa na zana za uboreshaji kwa hali mbalimbali za utekelezaji
- **Ukuaji wa Jamii**: Mfumo uliopanuliwa wa zana, matumizi, na michango ya jamii

### Hatua Zifuatazo

Ikiwa unajenga chatbot, unakuza zana za elimu, unaunda wasaidizi wa uundaji wa nambari, au unafanya kazi kwenye matumizi ya lugha nyingi, familia ya Qwen inatoa suluhisho zinazoweza kupimika na msaada mzuri wa jamii na nyaraka kamili.

Kwa masasisho ya hivi karibuni, matoleo ya mifano, na nyaraka za kiufundi za kina, tembelea hifadhi rasmi za Qwen kwenye Hugging Face na uchunguze mijadala ya jamii hai na mifano.

Mustakabali wa maendeleo ya AI uko katika zana zinazopatikana, wazi, na zenye nguvu zinazowezesha ubunifu katika sekta zote na viwango vyote. Familia ya Qwen inaonyesha maono haya, ikitoa mashirika na watengenezaji msingi wa kujenga kizazi kijacho cha matumizi yanayotumia AI.

## Rasilimali za Ziada

- **Nyaraka Rasmi**: [Nyaraka za Qwen](https://qwen.readthedocs.io/)
- **Hifadhi ya Mifano**: [Mkusanyiko wa Qwen wa Hugging Face](https://huggingface.co/collections/Qwen/)
- **Karatasi za Kiufundi**: [Machapisho ya Utafiti wa Qwen](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **Jamii**: [Majadiliano na Masuala ya GitHub](https://github.com/QwenLM/)
- **Jukwaa la ModelScope**: [ModelScope ya Alibaba](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## Matokeo ya Kujifunza

Baada ya kukamilisha moduli hii, utaweza:

1. Kueleza faida za usanifu wa familia ya mifano ya Qwen na mbinu yake ya chanzo huria
2. Kuchagua tofauti sahihi ya Qwen kulingana na mahitaji maalum ya matumizi na vikwazo vya rasilimali
3. Kutekeleza mifano ya Qwen katika hali mbalimbali za utekelezaji na usanidi ulioboreshwa
4. Kutumia mbinu za upunguzaji na uboreshaji kuboresha utendaji wa mifano ya Qwen
5. Kutathmini faida na hasara kati ya saizi ya mifano, utendaji, na uwezo katika familia ya Qwen

## Nini Kinachofuata

- [03: Misingi ya Familia ya Gemma](03.GemmaFamily.md)

---

**Kanusho**:  
Hati hii imetafsiriwa kwa kutumia huduma ya tafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kuhakikisha usahihi, tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya awali inapaswa kuchukuliwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatutawajibika kwa kutoelewana au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.