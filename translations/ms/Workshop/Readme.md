<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "48d0fb38be925084a6ebd957d4b045e5",
  "translation_date": "2025-10-09T19:22:38+00:00",
  "source_file": "Workshop/Readme.md",
  "language_code": "ms"
}
-->
# EdgeAI untuk Pemula - Bengkel

> **Jalur Pembelajaran Praktikal untuk Membina Aplikasi Edge AI Sedia Pengeluaran**
>
> Kuasai pengendalian AI tempatan dengan Microsoft Foundry Local, dari penyelesaian chat pertama hingga orkestrasi multi-agen dalam 6 sesi progresif.

---

## üéØ Pengenalan

Selamat datang ke **Bengkel EdgeAI untuk Pemula** - panduan praktikal anda untuk membina aplikasi pintar yang berjalan sepenuhnya pada perkakasan tempatan. Bengkel ini mengubah konsep teori Edge AI menjadi kemahiran dunia nyata melalui latihan yang semakin mencabar menggunakan Microsoft Foundry Local dan Small Language Models (SLMs).

### Mengapa Bengkel Ini?

**Revolusi Edge AI Telah Bermula**

Organisasi di seluruh dunia sedang beralih daripada AI yang bergantung kepada awan kepada pengkomputeran edge atas tiga sebab utama:

1. **Privasi & Pematuhan** - Memproses data sensitif secara tempatan tanpa penghantaran ke awan (HIPAA, GDPR, peraturan kewangan)
2. **Prestasi** - Menghapuskan latensi rangkaian (50-500ms tempatan vs 500-2000ms perjalanan awan)
3. **Kawalan Kos** - Menghapuskan kos API per-token dan skala tanpa perbelanjaan awan

**Tetapi Edge AI Berbeza**

Menjalankan AI secara tempatan memerlukan kemahiran baru:
- Pemilihan dan pengoptimuman model untuk kekangan sumber
- Pengurusan perkhidmatan tempatan dan pecutan perkakasan
- Kejuruteraan prompt untuk model yang lebih kecil
- Corak pengeluaran untuk peranti edge

**Bengkel Ini Menyampaikan Kemahiran Tersebut**

Dalam 6 sesi fokus (~3 jam keseluruhan), anda akan maju dari "Hello World" hingga melancarkan sistem multi-agen sedia pengeluaran - semuanya berjalan secara tempatan pada mesin anda.

---

## üìö Objektif Pembelajaran

Dengan melengkapkan bengkel ini, anda akan dapat:

### Kompetensi Teras
1. **Melancarkan dan Mengurus Perkhidmatan AI Tempatan**
   - Pasang dan konfigurasi Microsoft Foundry Local
   - Pilih model yang sesuai untuk pelancaran edge
   - Urus kitaran hayat model (muat turun, muatkan, cache)
   - Pantau penggunaan sumber dan optimalkan prestasi

2. **Membina Aplikasi Berkuasa AI**
   - Laksanakan penyelesaian chat yang serasi dengan OpenAI secara tempatan
   - Reka bentuk prompt yang efektif untuk Small Language Models
   - Tangani respons streaming untuk UX yang lebih baik
   - Integrasi model tempatan ke dalam aplikasi sedia ada

3. **Mencipta Sistem RAG (Retrieval Augmented Generation)**
   - Bina carian semantik dengan embeddings
   - Asaskan respons LLM dalam pengetahuan khusus domain
   - Nilai kualiti RAG dengan metrik standard industri
   - Skala dari prototaip ke pengeluaran

4. **Optimalkan Prestasi Model**
   - Benchmark pelbagai model untuk kes penggunaan anda
   - Ukur latensi, throughput, dan masa token pertama
   - Pilih model yang optimum berdasarkan kompromi kelajuan/kualiti
   - Bandingkan kompromi SLM vs LLM dalam senario sebenar

5. **Orkestrasi Sistem Multi-Agen**
   - Reka bentuk agen khusus untuk tugas yang berbeza
   - Laksanakan memori agen dan pengurusan konteks
   - Koordinasi agen dalam aliran kerja yang kompleks
   - Lalukan permintaan dengan bijak merentasi pelbagai model

6. **Melancarkan Penyelesaian Sedia Pengeluaran**
   - Laksanakan pengendalian ralat dan logik ulang
   - Pantau penggunaan token dan sumber sistem
   - Bina seni bina yang boleh diskalakan dengan corak model-sebagai-alat
   - Rancang laluan migrasi dari edge ke hibrid (edge + awan)

---

## üéì Hasil Pembelajaran

### Apa yang Akan Anda Bina

Menjelang akhir bengkel ini, anda akan menghasilkan:

| Sesi | Hasil | Kemahiran Ditunjukkan |
|------|-------|-----------------------|
| **1** | Aplikasi chat dengan streaming | Persediaan perkhidmatan, penyelesaian asas, UX streaming |
| **2** | Sistem RAG dengan penilaian | Embeddings, carian semantik, metrik kualiti |
| **3** | Suite benchmark multi-model | Pengukuran prestasi, perbandingan model |
| **4** | Perbandingan SLM vs LLM | Analisis kompromi, strategi pengoptimuman |
| **5** | Orkestrator multi-agen | Reka bentuk agen, pengurusan memori, koordinasi |
| **6** | Sistem penghalaan pintar | Pengesanan niat, pemilihan model, skalabiliti |

### Matriks Kompetensi

| Tahap Kemahiran | Sesi 1-2 | Sesi 3-4 | Sesi 5-6 |
|-----------------|----------|----------|----------|
| **Pemula** | ‚úÖ Persediaan & asas | ‚ö†Ô∏è Mencabar | ‚ùå Terlalu maju |
| **Pertengahan** | ‚úÖ Ulasan pantas | ‚úÖ Pembelajaran teras | ‚ö†Ô∏è Matlamat tambahan |
| **Lanjutan** | ‚úÖ Mudah dilalui | ‚úÖ Penambahbaikan | ‚úÖ Corak pengeluaran |

### Kemahiran Sedia Kerjaya

**Selepas bengkel ini, anda akan bersedia untuk:**

‚úÖ **Membina Aplikasi Berorientasi Privasi**
- Aplikasi penjagaan kesihatan yang mengendalikan PHI/PII secara tempatan
- Perkhidmatan kewangan dengan keperluan pematuhan
- Sistem kerajaan dengan keperluan kedaulatan data

‚úÖ **Optimalkan untuk Persekitaran Edge**
- Peranti IoT dengan sumber terhad
- Aplikasi mudah alih yang mengutamakan offline
- Sistem masa nyata dengan latensi rendah

‚úÖ **Reka Bentuk Seni Bina Pintar**
- Sistem multi-agen untuk aliran kerja kompleks
- Pelancaran hibrid edge-awan
- Infrastruktur AI yang dioptimumkan kos

‚úÖ **Memimpin Inisiatif Edge AI**
- Nilai kebolehlaksanaan Edge AI untuk projek
- Pilih model dan rangka kerja yang sesuai
- Reka bentuk penyelesaian AI tempatan yang boleh diskalakan

---

## üó∫Ô∏è Struktur Bengkel

### Gambaran Keseluruhan Sesi (6 Sesi √ó 30 Minit = 3 Jam)

| Sesi | Topik | Fokus | Tempoh |
|------|-------|-------|--------|
| **1** | Memulakan dengan Foundry Local | Pasang, sahkan, penyelesaian pertama | 30 min |
| **2** | Membina Penyelesaian AI dengan RAG | Kejuruteraan prompt, embeddings, penilaian | 30 min |
| **3** | Model Sumber Terbuka | Penemuan model, benchmarking, pemilihan | 30 min |
| **4** | Model Terkini | SLM vs LLM, pengoptimuman, rangka kerja | 30 min |
| **5** | Agen Berkuasa AI | Reka bentuk agen, orkestrasi, memori | 30 min |
| **6** | Model sebagai Alat | Penghalaan, rantai, strategi skalabiliti | 30 min |

---

## üöÄ Permulaan Pantas

### Prasyarat

**Keperluan Sistem:**
- **OS**: Windows 10/11, macOS 11+, atau Linux (Ubuntu 20.04+)
- **RAM**: Minimum 8GB, disyorkan 16GB+
- **Storan**: Ruang kosong 10GB+ untuk model
- **CPU**: Pemproses moden dengan sokongan AVX2
- **GPU** (pilihan): Serasi CUDA atau Qualcomm NPU untuk pecutan

**Keperluan Perisian:**
- **Python 3.8+** ([Muat Turun](https://www.python.org/downloads/))
- **Microsoft Foundry Local** ([Panduan Pemasangan](../../../Workshop))
- **Git** ([Muat Turun](https://git-scm.com/downloads))
- **Visual Studio Code** (disyorkan) ([Muat Turun](https://code.visualstudio.com/))

### Persediaan dalam 3 Langkah

#### 1. Pasang Foundry Local

**Windows:**
```powershell
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**Sahkan Pemasangan:**
```bash
foundry --version
foundry service status
```

#### 2. Klon Repositori & Pasang Kebergantungan

```bash
# Clone repository
git clone https://github.com/microsoft/edgeai-for-beginners.git
cd edgeai-for-beginners/Workshop

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.\.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

#### 3. Jalankan Sampel Pertama Anda

```bash
# Start Foundry Local and load a model
foundry model run phi-4-mini

# Run the chat bootstrap sample
cd samples/session01
python chat_bootstrap.py "What is edge AI?"
```

**‚úÖ Berjaya!** Anda sepatutnya melihat respons streaming tentang edge AI.

---

## üì¶ Sumber Bengkel

### Sampel Python

Contoh praktikal progresif yang menunjukkan setiap konsep:

| Sesi | Sampel | Penerangan | Masa Jalankan |
|------|--------|------------|---------------|
| 1 | [`chat_bootstrap.py`](../../../Workshop/samples/session01/chat_bootstrap.py) | Chat asas & streaming | ~30s |
| 2 | [`rag_pipeline.py`](../../../Workshop/samples/session02/rag_pipeline.py) | RAG dengan embeddings | ~45s |
| 2 | [`rag_eval_ragas.py`](../../../Workshop/samples/session02/rag_eval_ragas.py) | Penilaian kualiti RAG | ~60s |
| 3 | [`benchmark_oss_models.py`](../../../Workshop/samples/session03/benchmark_oss_models.py) | Benchmarking multi-model | ~2-3m |
| 4 | [`model_compare.py`](../../../Workshop/samples/session04/model_compare.py) | Perbandingan SLM vs LLM | ~45s |
| 5 | [`agents_orchestrator.py`](../../../Workshop/samples/session05/agents_orchestrator.py) | Sistem multi-agen | ~60s |
| 6 | [`models_router.py`](../../../Workshop/samples/session06/models_router.py) | Penghalaan berdasarkan niat | ~45s |
| 6 | [`models_pipeline.py`](../../../Workshop/samples/session06/models_pipeline.py) | Rantaian multi-langkah | ~60s |

### Jupyter Notebooks

Eksplorasi interaktif dengan penjelasan dan visualisasi:

| Sesi | Notebook | Penerangan | Kesukaran |
|------|----------|------------|-----------|
| 1 | [`session01_chat_bootstrap.ipynb`](./notebooks/session01_chat_bootstrap.ipynb) | Asas chat & streaming | ‚≠ê Pemula |
| 2 | [`session02_rag_pipeline.ipynb`](./notebooks/session02_rag_pipeline.ipynb) | Bina sistem RAG | ‚≠ê‚≠ê Pertengahan |
| 2 | [`session02_rag_eval_ragas.ipynb`](./notebooks/session02_rag_eval_ragas.ipynb) | Nilai kualiti RAG | ‚≠ê‚≠ê Pertengahan |
| 3 | [`session03_benchmark_oss_models.ipynb`](./notebooks/session03_benchmark_oss_models.ipynb) | Benchmarking model | ‚≠ê‚≠ê Pertengahan |
| 4 | [`session04_model_compare.ipynb`](./notebooks/session04_model_compare.ipynb) | Perbandingan model | ‚≠ê‚≠ê Pertengahan |
| 5 | [`session05_agents_orchestrator.ipynb`](./notebooks/session05_agents_orchestrator.ipynb) | Orkestrasi agen | ‚≠ê‚≠ê‚≠ê Lanjutan |
| 6 | [`session06_models_router.ipynb`](./notebooks/session06_models_router.ipynb) | Penghalaan niat | ‚≠ê‚≠ê‚≠ê Lanjutan |
| 6 | [`session06_models_pipeline.ipynb`](./notebooks/session06_models_pipeline.ipynb) | Orkestrasi rantaian | ‚≠ê‚≠ê‚≠ê Lanjutan |

### Dokumentasi

Panduan dan rujukan yang komprehensif:

| Dokumen | Penerangan | Gunakan Apabila |
|---------|------------|----------------|
| [QUICK_START.md](./QUICK_START.md) | Panduan persediaan pantas | Bermula dari awal |
| [QUICK_REFERENCE.md](./QUICK_REFERENCE.md) | Lembaran cheat perintah & API | Perlukan jawapan pantas |
| [FOUNDRY_SDK_QUICKREF.md](./FOUNDRY_SDK_QUICKREF.md) | Corak & contoh SDK | Menulis kod |
| [ENV_CONFIGURATION.md](./ENV_CONFIGURATION.md) | Panduan pembolehubah persekitaran | Konfigurasi sampel |
| [SAMPLES_UPDATE_SUMMARY.md](./SAMPLES_UPDATE_SUMMARY.md) | Penambahbaikan sampel terkini | Memahami perubahan |
| [SDK_MIGRATION_NOTES.md](./SDK_MIGRATION_NOTES.md) | Panduan migrasi | Meningkatkan kod |
| [notebooks/TROUBLESHOOTING.md](./notebooks/TROUBLESHOOTING.md) | Isu biasa & penyelesaian | Menyelesaikan masalah |

---

## üéì Cadangan Jalur Pembelajaran

### Untuk Pemula (3-4 jam)
1. ‚úÖ Sesi 1: Memulakan (fokus pada persediaan dan chat asas)
2. ‚úÖ Sesi 2: Asas RAG (langkau penilaian pada awalnya)
3. ‚úÖ Sesi 3: Benchmarking mudah (2 model sahaja)
4. ‚è≠Ô∏è Langkau Sesi 4-6 buat masa ini
5. üîÑ Kembali ke Sesi 4-6 selepas membina aplikasi pertama

### Untuk Pembangun Pertengahan (3 jam)
1. ‚ö° Sesi 1: Pengesahan persediaan pantas
2. ‚úÖ Sesi 2: Lengkapkan pipeline RAG dengan penilaian
3. ‚úÖ Sesi 3: Suite benchmarking penuh
4. ‚úÖ Sesi 4: Pengoptimuman model
5. ‚úÖ Sesi 5-6: Fokus pada corak seni bina

### Untuk Pengamal Lanjutan (2-3 jam)
1. ‚ö° Sesi 1-3: Ulasan pantas dan pengesahan
2. ‚úÖ Sesi 4: Pengoptimuman mendalam
3. ‚úÖ Sesi 5: Seni bina multi-agen
4. ‚úÖ Sesi 6: Corak pengeluaran dan skalabiliti
5. üöÄ Lanjutkan: Bina logik penghalaan tersuai dan pelancaran hibrid

---

## Pek Sesi Bengkel (Makmal Fokus 30‚ÄëMinit)

Jika anda mengikuti format bengkel 6 sesi yang dipadatkan, gunakan panduan khusus ini (setiap satu memetakan dan melengkapi dokumen modul yang lebih luas di atas):

| Sesi Bengkel | Panduan | Fokus Teras |
|--------------|---------|-------------|
| 1 | [Session01-GettingStartedFoundryLocal](./Session01-GettingStartedFoundryLocal.md) | Pasang, sahkan, jalankan phi & GPT-OSS-20B, pecutan |
| 2 | [Session02-BuildAISolutionsRAG](./Session02-BuildAISolutionsRAG.md) | Kejuruteraan prompt, corak RAG, CSV & asas dokumen, migrasi |
| 3 | [Session03-OpenSourceModels](./Session03-OpenSourceModels.md) | Integrasi Hugging Face, benchmarking, pemilihan model |
| 4 | [Session04-CuttingEdgeModels](./Session04-CuttingEdgeModels.md) | SLM vs LLM, WebGPU, Chainlit RAG, pecutan ONNX |
| 5 | [Session05-AIPoweredAgents](./Session05-AIPoweredAgents.md) | Peranan agen, memori, alat, orkestrasi |
| 6 | [Session06-ModelsAsTools](./Session06-ModelsAsTools.md) | Penghalaan, rantai, laluan skalabiliti ke Azure |
Setiap fail sesi merangkumi: abstrak, objektif pembelajaran, aliran demo 30 minit, projek permulaan, senarai semak pengesahan, penyelesaian masalah, dan rujukan kepada Foundry Local Python SDK rasmi.

### Skrip Contoh

Pasang keperluan bengkel (Windows):

```powershell
cd Workshop
py -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt
```

macOS / Linux:

```bash
cd Workshop
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Jika menjalankan perkhidmatan Foundry Local pada mesin atau VM (Windows) yang berbeza daripada macOS, eksport titik akhir:

```bash
export FOUNDRY_LOCAL_ENDPOINT=http://<windows-host>:5273/v1
```

| Sesi | Skrip | Penerangan |
|------|-------|------------|
| 1 | `samples/session01/chat_bootstrap.py` | Perkhidmatan bootstrap & sembang streaming |
| 2 | `samples/session02/rag_pipeline.py` | RAG minimum (embedding dalam memori) |
|   | `samples/session02/rag_eval_ragas.py` | Penilaian RAG dengan metrik ragas |
| 3 | `samples/session03/benchmark_oss_models.py` | Penanda aras kependaman & throughput pelbagai model |
| 4 | `samples/session04/model_compare.py` | Perbandingan SLM vs LLM (kependaman & output sampel) |
| 5 | `samples/session05/agents_orchestrator.py` | Saluran penyelidikan dua ejen ‚Üí editorial |
| 6 | `samples/session06/models_router.py` | Demo penghalaan berdasarkan niat |
|   | `samples/session06/models_pipeline.py` | Rantaian rancangan/laksana/perbaiki berbilang langkah |

### Pembolehubah Persekitaran (Umum untuk Semua Contoh)

| Pembolehubah | Tujuan | Contoh |
|--------------|--------|--------|
| `FOUNDRY_LOCAL_ALIAS` | Alias model tunggal lalai untuk contoh asas | `phi-4-mini` |
| `SLM_ALIAS` / `LLM_ALIAS` | SLM eksplisit vs model lebih besar untuk perbandingan | `phi-4-mini` / `gpt-oss-20b` |
| `BENCH_MODELS` | Senarai alias untuk penanda aras | `qwen2.5-0.5b,gemma-2-2b,mistral-7b` |
| `BENCH_ROUNDS` | Ulangan penanda aras setiap model | `3` |
| `BENCH_PROMPT` | Prompt yang digunakan dalam penanda aras | `Explain retrieval augmented generation briefly.` |
| `EMBED_MODEL` | Model embedding sentence-transformers | `sentence-transformers/all-MiniLM-L6-v2` |
| `RAG_QUESTION` | Menimpa pertanyaan ujian untuk saluran RAG | `Why use RAG with local inference?` |
| `AGENT_QUESTION` | Menimpa pertanyaan saluran ejen | `Explain why edge AI matters for compliance.` |
| `AGENT_MODEL_PRIMARY` | Alias model untuk ejen penyelidikan | `phi-4-mini` |
| `AGENT_MODEL_EDITOR` | Alias model untuk ejen editor (boleh berbeza) | `gpt-oss-20b` |
| `SHOW_USAGE` | Apabila `1`, mencetak penggunaan token setiap penyelesaian | `1` |
| `RETRY_ON_FAIL` | Apabila `1`, cuba semula sekali pada ralat sembang sementara | `1` |
| `RETRY_BACKOFF` | Masa tunggu sebelum cuba semula (dalam saat) | `1.0` |

Jika pembolehubah tidak ditetapkan, skrip akan menggunakan tetapan lalai yang munasabah. Untuk demo model tunggal, biasanya hanya perlu `FOUNDRY_LOCAL_ALIAS`.

### Modul Utiliti

Semua contoh kini berkongsi pembantu `samples/workshop_utils.py` yang menyediakan:

* Penciptaan cache `FoundryLocalManager` + klien OpenAI
* Pembantu `chat_once()` dengan pilihan cuba semula + cetakan penggunaan
* Laporan penggunaan token yang mudah (aktifkan melalui `SHOW_USAGE=1`)

Ini mengurangkan pengulangan dan menonjolkan amalan terbaik untuk orkestrasi model tempatan yang cekap.

## Penambahbaikan Pilihan (Merentas Sesi)

| Tema | Penambahbaikan | Sesi | Env / Togol |
|------|----------------|------|-------------|
| Ketentuan | Suhu tetap + set prompt stabil | 1‚Äì6 | Tetapkan `temperature=0`, `top_p=1` |
| Keterlihatan Penggunaan Token | Pengajaran kos/kecekapan yang konsisten | 1‚Äì6 | `SHOW_USAGE=1` |
| Penstriman Token Pertama | Metrik kependaman yang dirasakan | 1,3,4,6 | `BENCH_STREAM=1` (penanda aras) |
| Ketahanan Cuba Semula | Menangani permulaan sejuk sementara | Semua | `RETRY_ON_FAIL=1` + `RETRY_BACKOFF` |
| Ejen Pelbagai Model | Pengkhususan peranan heterogen | 5 | `AGENT_MODEL_PRIMARY`, `AGENT_MODEL_EDITOR` |
| Penghalaan Adaptif | Niat + heuristik kos | 6 | Kembangkan penghala dengan logik eskalasi |
| Memori Vektor | Ingatan semantik jangka panjang | 2,5,6 | Integrasi indeks embedding FAISS/Chroma |
| Eksport Jejak | Pengauditan & penilaian | 2,5,6 | Tambah baris JSON setiap langkah |
| Rubrik Kualiti | Penjejakan kualitatif | 3‚Äì6 | Prompt penilaian sekunder |
| Ujian Asap | Pengesahan pra-bengkel pantas | Semua | `python Workshop/tests/smoke.py` |

### Permulaan Pantas Deterministik

```powershell
set FOUNDRY_LOCAL_ALIAS=phi-4-mini
set SHOW_USAGE=1
python Workshop\tests\smoke.py
```

Jangkakan kiraan token yang stabil merentas input yang sama berulang kali.

### Penilaian RAG (Sesi 2)

Gunakan `rag_eval_ragas.py` untuk mengira relevansi jawapan, kesetiaan, dan ketepatan konteks pada dataset sintetik kecil:

```powershell
python samples/session02/rag_eval_ragas.py
```

Kembangkan dengan membekalkan JSONL yang lebih besar bagi soalan, konteks, dan kebenaran asas, kemudian menukarnya kepada `Dataset` Hugging Face.

## Lampiran Ketepatan Perintah CLI

Bengkel ini sengaja menggunakan hanya perintah CLI Foundry Local yang didokumentasikan / stabil pada masa ini.

### Perintah Stabil yang Dirujuk

| Kategori | Perintah | Tujuan |
|----------|----------|--------|
| Teras | `foundry --version` | Paparkan versi yang dipasang |
| Teras | `foundry init` | Inisialisasi konfigurasi |
| Perkhidmatan | `foundry service start` | Mulakan perkhidmatan tempatan (jika tidak automatik) |
| Perkhidmatan | `foundry status` | Paparkan status perkhidmatan |
| Model | `foundry model list` | Senaraikan katalog / model yang tersedia |
| Model | `foundry model download <alias>` | Muat turun berat model ke dalam cache |
| Model | `foundry model run <alias>` | Lancarkan (muat) model secara tempatan; gabungkan dengan `--prompt` untuk satu kali |
| Model | `foundry model unload <alias>` / `foundry model stop <alias>` | Nyahmuat model dari memori (jika disokong) |
| Cache | `foundry cache list` | Senaraikan model yang di-cache (dimuat turun) |
| Sistem | `foundry system info` | Snapshot keupayaan perkakasan & pecutan |
| Sistem | `foundry system gpu-info` | Maklumat diagnostik GPU |
| Konfigurasi | `foundry config list` | Paparkan nilai konfigurasi semasa |
| Konfigurasi | `foundry config set <key> <value>` | Kemas kini konfigurasi |

### Corak Prompt Satu Kali

Sebagai ganti subperintah `model chat` yang tidak lagi digunakan, gunakan:

```powershell
foundry model run <alias> --prompt "Your question here"
```

Ini melaksanakan satu kitaran prompt/respons kemudian keluar.

### Corak yang Dihapus / Dielakkan

| Tidak Lagi Digunakan / Tidak Didokumentasikan | Penggantian / Panduan |
|----------------------------------------------|------------------------|
| `foundry model chat <model> "..."` | `foundry model run <model> --prompt "..."` |
| `foundry model list --running` | Gunakan `foundry model list` biasa + aktiviti terkini / log |
| `foundry model list --cached` | `foundry cache list` |
| `foundry model stats <model>` | Gunakan skrip penanda aras Python + alat OS (Task Manager / `nvidia-smi`) |
| `foundry model benchmark ...` | `samples/session03/benchmark_oss_models.py` |

### Penanda Aras & Telemetri

- Kependaman, p95, token/saat: `samples/session03/benchmark_oss_models.py`
- Kependaman token pertama (penstriman): tetapkan `BENCH_STREAM=1`
- Penggunaan sumber: pemantau OS (Task Manager, Activity Monitor, `nvidia-smi`) + `foundry system info`.

Apabila perintah telemetri CLI baharu stabil di hulu, ia boleh dimasukkan dengan suntingan minimum pada markdown sesi.

### Penjaga Lint Automatik

Lint automatik menghalang pengenalan semula corak CLI yang tidak lagi digunakan dalam blok kod berpagar fail markdown:

Skrip: `Workshop/scripts/lint_markdown_cli.py`

Corak yang tidak lagi digunakan disekat dalam pagar kod.

Penggantian yang disyorkan:
| Tidak Lagi Digunakan | Penggantian |
|----------------------|-------------|
| `foundry model chat <a> "..."` | `foundry model run <a> --prompt "..."` |
| `model list --running` | `model list` |
| `model list --cached` | `cache list` |
| `model stats` | Skrip penanda aras + alat sistem |
| `model benchmark` | `samples/session03/benchmark_oss_models.py` |
| `model list --available` | `model list` |

Jalankan secara tempatan:
```powershell
python Workshop\scripts\lint_markdown_cli.py --verbose
```

GitHub Action: `.github/workflows/markdown-cli-lint.yml` dijalankan pada setiap push & PR.

Hook pra-komit pilihan:
```bash
echo "python Workshop/scripts/lint_markdown_cli.py" > .git/hooks/pre-commit
chmod +x .git/hooks/pre-commit
```

## Jadual Migrasi CLI ‚Üí SDK Pantas

| Tugas | Perintah CLI Satu Baris | Setara SDK (Python) | Nota |
|-------|--------------------------|---------------------|------|
| Jalankan model sekali (prompt) | `foundry model run phi-4-mini --prompt "Hello"` | `manager=FoundryLocalManager("phi-4-mini"); client=OpenAI(base_url=manager.endpoint, api_key=manager.api_key or "not-needed"); client.chat.completions.create(model=manager.get_model_info("phi-4-mini").id, messages=[{"role":"user","content":"Hello"}])` | SDK secara automatik memulakan perkhidmatan & caching |
| Muat turun (cache) model | `foundry model download qwen2.5-0.5b` | `FoundryLocalManager("qwen2.5-0.5b")  # triggers download/load` | Pengurus memilih varian terbaik jika alias memetakan kepada beberapa binaan |
| Senaraikan katalog | `foundry model list` | `# use manager for each alias or maintain known list` | CLI mengagregatkan; SDK kini per alias |
| Senaraikan model yang di-cache | `foundry cache list` | `manager.list_cached_models()` | Selepas inisialisasi pengurus (mana-mana alias) |
| Aktifkan pecutan GPU | `foundry config set compute.onnx.enable_gpu true` | `# CLI action; SDK assumes config already applied` | Konfigurasi adalah kesan sampingan luaran |
| Dapatkan URL titik akhir | (implisit) | `manager.endpoint` | Digunakan untuk mencipta klien yang serasi dengan OpenAI |
| Panaskan model | `foundry model run <alias>` kemudian prompt pertama | `chat_once(alias, messages=[...])` (utiliti) | Utiliti menangani pemanasan kependaman sejuk awal |
| Ukur kependaman | `python benchmark_oss_models.py` | `import benchmark_oss_models` (atau skrip pengeksport baharu) | Lebih suka skrip untuk metrik yang konsisten |
| Hentikan / nyahmuat model | `foundry model unload <alias>` | (Tidak didedahkan ‚Äì mulakan semula perkhidmatan / proses) | Biasanya tidak diperlukan untuk aliran bengkel |
| Dapatkan penggunaan token | (lihat output) | `resp.usage.total_tokens` | Disediakan jika backend mengembalikan objek penggunaan |

## Eksport Markdown Penanda Aras

Gunakan skrip `Workshop/scripts/export_benchmark_markdown.py` untuk menjalankan penanda aras baharu (logik yang sama seperti `samples/session03/benchmark_oss_models.py`) dan menghasilkan jadual Markdown mesra GitHub serta JSON mentah.

### Contoh

```powershell
python Workshop\scripts\export_benchmark_markdown.py --models "qwen2.5-0.5b,gemma-2-2b,mistral-7b" --prompt "Explain retrieval augmented generation briefly." --rounds 3 --output benchmark_report.md
```

Fail yang dijana:
| Fail | Kandungan |
|------|-----------|
| `benchmark_report.md` | Jadual Markdown + petunjuk interpretasi |
| `benchmark_report.json` | Array metrik mentah (untuk perbezaan / penjejakan trend) |

Tetapkan `BENCH_STREAM=1` dalam persekitaran untuk memasukkan kependaman token pertama jika disokong.

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat yang kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.