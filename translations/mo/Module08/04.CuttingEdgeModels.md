<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "949045c54206448d55d987e8b23a82cf",
  "translation_date": "2025-09-24T14:27:18+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "mo"
}
-->
# ç¬¬å››ç¯€ï¼šä½¿ç”¨ Chainlit å»ºç«‹ç”Ÿç”¢ç´šèŠå¤©æ‡‰ç”¨ç¨‹å¼

## æ¦‚è¿°

æœ¬ç¯€èª²ç¨‹å°ˆæ³¨æ–¼ä½¿ç”¨ Chainlit å’Œ Microsoft Foundry Local å»ºç«‹ç”Ÿç”¢ç´šèŠå¤©æ‡‰ç”¨ç¨‹å¼ã€‚æ‚¨å°‡å­¸ç¿’å¦‚ä½•ç‚º AI å°è©±å‰µå»ºç¾ä»£åŒ–çš„ç¶²é ä»‹é¢ã€å¯¦ç¾ä¸²æµå›æ‡‰ï¼Œä»¥åŠéƒ¨ç½²å…·æœ‰å®Œå–„éŒ¯èª¤è™•ç†å’Œç”¨æˆ¶é«”é©—è¨­è¨ˆçš„ç©©å¥èŠå¤©æ‡‰ç”¨ç¨‹å¼ã€‚

**æ‚¨å°‡å»ºç«‹çš„å…§å®¹ï¼š**
- **Chainlit èŠå¤©æ‡‰ç”¨ç¨‹å¼**ï¼šå…·æœ‰ä¸²æµå›æ‡‰çš„ç¾ä»£åŒ–ç¶²é ä»‹é¢
- **WebGPU æ¼”ç¤º**ï¼šåŸºæ–¼ç€è¦½å™¨çš„æ¨ç†ï¼Œæä¾›éš±ç§å„ªå…ˆçš„æ‡‰ç”¨ç¨‹å¼
- **Open WebUI æ•´åˆ**ï¼šèˆ‡ Foundry Local é…åˆçš„å°ˆæ¥­èŠå¤©ä»‹é¢
- **ç”Ÿç”¢æ¨¡å¼**ï¼šéŒ¯èª¤è™•ç†ã€ç›£æ§å’Œéƒ¨ç½²ç­–ç•¥

## å­¸ç¿’ç›®æ¨™

- ä½¿ç”¨ Chainlit å»ºç«‹ç”Ÿç”¢ç´šèŠå¤©æ‡‰ç”¨ç¨‹å¼
- å¯¦ç¾ä¸²æµå›æ‡‰ä»¥æå‡ç”¨æˆ¶é«”é©—
- æŒæ¡ Foundry Local SDK çš„æ•´åˆæ¨¡å¼
- æ‡‰ç”¨æ­£ç¢ºçš„éŒ¯èª¤è™•ç†å’Œå„ªé›…é™ç´šç­–ç•¥
- éƒ¨ç½²ä¸¦é…ç½®é©ç”¨æ–¼ä¸åŒç’°å¢ƒçš„èŠå¤©æ‡‰ç”¨ç¨‹å¼
- ç†è§£ç¾ä»£åŒ–ç¶²é ä»‹é¢æ¨¡å¼ä»¥æ”¯æŒå°è©±å¼ AI

## å…ˆæ±ºæ¢ä»¶

- **Foundry Local**ï¼šå·²å®‰è£ä¸¦é‹è¡Œ ([å®‰è£æŒ‡å—](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**ï¼šç‰ˆæœ¬ 3.10 æˆ–æ›´é«˜ï¼Œå…·å‚™è™›æ“¬ç’°å¢ƒåŠŸèƒ½
- **æ¨¡å‹**ï¼šè‡³å°‘è¼‰å…¥ä¸€å€‹æ¨¡å‹ (`foundry model run phi-4-mini`)
- **ç€è¦½å™¨**ï¼šæ”¯æŒ WebGPU çš„ç¾ä»£åŒ–ç€è¦½å™¨ï¼ˆChrome/Edgeï¼‰
- **Docker**ï¼šç”¨æ–¼ Open WebUI æ•´åˆï¼ˆå¯é¸ï¼‰

## ç¬¬ä¸€éƒ¨åˆ†ï¼šç†è§£ç¾ä»£èŠå¤©æ‡‰ç”¨ç¨‹å¼

### æ¶æ§‹æ¦‚è¿°

```
User Browser â†â†’ Chainlit UI â†â†’ Python Backend â†â†’ Foundry Local â†â†’ AI Model
      â†“              â†“              â†“              â†“            â†“
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### æ ¸å¿ƒæŠ€è¡“

**Foundry Local SDK æ¨¡å¼ï¼š**
- `FoundryLocalManager(alias)`ï¼šè‡ªå‹•åŒ–æœå‹™ç®¡ç†
- `manager.endpoint` å’Œ `manager.api_key`ï¼šé€£æ¥è©³ç´°è³‡è¨Š
- `manager.get_model_info(alias).id`ï¼šæ¨¡å‹è­˜åˆ¥

**Chainlit æ¡†æ¶ï¼š**
- `@cl.on_chat_start`ï¼šåˆå§‹åŒ–èŠå¤©æœƒè©±
- `@cl.on_message`ï¼šè™•ç†ç”¨æˆ¶ç™¼é€çš„æ¶ˆæ¯
- `cl.Message().stream_token()`ï¼šå¯¦æ™‚ä¸²æµ
- è‡ªå‹•åŒ– UI ç”Ÿæˆå’Œ WebSocket ç®¡ç†

## ç¬¬äºŒéƒ¨åˆ†ï¼šæœ¬åœ°èˆ‡é›²ç«¯çš„æ±ºç­–çŸ©é™£

### æ€§èƒ½ç‰¹å¾µ

| æ–¹é¢ | æœ¬åœ° (Foundry) | é›²ç«¯ (Azure OpenAI) |
|------|----------------|---------------------|
| **å»¶é²** | ğŸš€ 50-200msï¼ˆç„¡ç¶²çµ¡ï¼‰ | â±ï¸ 200-2000msï¼ˆå–æ±ºæ–¼ç¶²çµ¡ï¼‰ |
| **éš±ç§** | ğŸ”’ è³‡æ–™ä¸é›¢é–‹è¨­å‚™ | âš ï¸ è³‡æ–™ç™¼é€è‡³é›²ç«¯ |
| **æˆæœ¬** | ğŸ’° ç¡¬é«”æˆæœ¬å¾Œå…è²» | ğŸ’¸ æŒ‰ token è¨ˆè²» |
| **é›¢ç·š** | âœ… ç„¡éœ€ç¶²çµ¡å³å¯é‹è¡Œ | âŒ éœ€è¦ç¶²çµ¡ |
| **æ¨¡å‹å¤§å°** | âš ï¸ å—ç¡¬é«”é™åˆ¶ | âœ… å¯ä½¿ç”¨æœ€å¤§æ¨¡å‹ |
| **æ“´å±•æ€§** | âš ï¸ ä¾è³´ç¡¬é«” | âœ… ç„¡é™æ“´å±• |

### æ··åˆç­–ç•¥æ¨¡å¼

**æœ¬åœ°å„ªå…ˆï¼Œé›²ç«¯å‚™æ´ï¼š**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**åŸºæ–¼ä»»å‹™çš„è·¯ç”±ï¼š**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šç¯„ä¾‹ 04 - Chainlit èŠå¤©æ‡‰ç”¨ç¨‹å¼

### å¿«é€Ÿé–‹å§‹

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

æ‡‰ç”¨ç¨‹å¼æœƒè‡ªå‹•åœ¨ `http://localhost:8080` é–‹å•Ÿï¼Œæä¾›ç¾ä»£åŒ–èŠå¤©ä»‹é¢ã€‚

### æ ¸å¿ƒå¯¦ç¾

ç¯„ä¾‹ 04 å±•ç¤ºäº†ç”Ÿç”¢ç´šæ¨¡å¼ï¼š

**è‡ªå‹•åŒ–æœå‹™ç™¼ç¾ï¼š**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**ä¸²æµèŠå¤©è™•ç†ï¼š**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### é…ç½®é¸é …

**ç’°å¢ƒè®Šæ•¸ï¼š**

| è®Šæ•¸ | æè¿° | é è¨­å€¼ | ç¯„ä¾‹ |
|------|------|--------|------|
| `MODEL` | ä½¿ç”¨çš„æ¨¡å‹åˆ¥å | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | Foundry Local ç«¯é» | è‡ªå‹•æª¢æ¸¬ | `http://localhost:51211` |
| `API_KEY` | API é‡‘é‘°ï¼ˆæœ¬åœ°å¯é¸ï¼‰ | `""` | `your-api-key` |

**é€²éšä½¿ç”¨ï¼š**
```cmd
# Use different model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## ç¬¬å››éƒ¨åˆ†ï¼šå‰µå»ºå’Œä½¿ç”¨ Jupyter ç­†è¨˜æœ¬

### ç­†è¨˜æœ¬æ”¯æŒæ¦‚è¿°

ç¯„ä¾‹ 04 åŒ…å«ä¸€å€‹å®Œæ•´çš„ Jupyter ç­†è¨˜æœ¬ (`chainlit_app.ipynb`)ï¼Œæä¾›ï¼š

- **ğŸ“š æ•™è‚²å…§å®¹**ï¼šé€æ­¥å­¸ç¿’ææ–™
- **ğŸ”¬ äº’å‹•æ¢ç´¢**ï¼šé‹è¡Œä¸¦è©¦é©—ä»£ç¢¼å–®å…ƒ
- **ğŸ“Š è¦–è¦ºæ¼”ç¤º**ï¼šåœ–è¡¨ã€åœ–è§£å’Œè¼¸å‡ºå¯è¦–åŒ–
- **ğŸ› ï¸ é–‹ç™¼å·¥å…·**ï¼šæ¸¬è©¦å’Œèª¿è©¦åŠŸèƒ½

### å‰µå»ºè‡ªå·±çš„ç­†è¨˜æœ¬

#### æ­¥é©Ÿ 1ï¼šè¨­ç½® Jupyter ç’°å¢ƒ

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### æ­¥é©Ÿ 2ï¼šå‰µå»ºæ–°ç­†è¨˜æœ¬

**ä½¿ç”¨ VS Codeï¼š**
1. åœ¨ Module08 ç›®éŒ„ä¸­æ‰“é–‹ VS Code
2. å‰µå»ºä¸€å€‹ `.ipynb` æ“´å±•åçš„æ–°æ–‡ä»¶
3. é¸æ“‡ "Foundry Local" æ ¸å¿ƒç’°å¢ƒ
4. é–‹å§‹æ·»åŠ ä»£ç¢¼å–®å…ƒ

**ä½¿ç”¨ Jupyter Labï¼š**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### ç­†è¨˜æœ¬çµæ§‹æœ€ä½³å¯¦è¸

#### å–®å…ƒçµ„ç¹”

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("âœ… Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### äº’å‹•ç¯„ä¾‹å’Œç·´ç¿’

#### ç·´ç¿’ 1ï¼šå®¢æˆ¶ç«¯é…ç½®æ¸¬è©¦

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b-instruct"},
]

for config in configurations:
    print(f"\nğŸ§ª Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'âœ… Success' if result['status'] == 'ok' else 'âŒ Failed'}")
```

#### ç·´ç¿’ 2ï¼šä¸²æµå›æ‡‰æ¨¡æ“¬

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("ğŸŒŠ Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\nâœ… Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## ç¬¬äº”éƒ¨åˆ†ï¼šWebGPU ç€è¦½å™¨æ¨ç†æ¼”ç¤º

### æ¦‚è¿°

WebGPU å…è¨±ç›´æ¥åœ¨ç€è¦½å™¨ä¸­é‹è¡Œ AI æ¨¡å‹ï¼Œæä¾›æœ€å¤§éš±ç§å’Œé›¶å®‰è£é«”é©—ã€‚æœ¬ç¯„ä¾‹å±•ç¤ºäº†ä½¿ç”¨ ONNX Runtime Web å’Œ WebGPU åŸ·è¡Œçš„æ‡‰ç”¨ã€‚

### æ­¥é©Ÿ 1ï¼šæª¢æŸ¥ WebGPU æ”¯æŒ

**ç€è¦½å™¨è¦æ±‚ï¼š**
- Chrome/Edge 113+ï¼Œå•Ÿç”¨ WebGPU
- æª¢æŸ¥ï¼š`chrome://gpu` â†’ ç¢ºèª "WebGPU" ç‹€æ…‹
- ç¨‹å¼æª¢æŸ¥ï¼š`if (!('gpu' in navigator)) { /* no WebGPU */ }`

### æ­¥é©Ÿ 2ï¼šå‰µå»º WebGPU æ¼”ç¤º

å‰µå»ºç›®éŒ„ï¼š`samples/04/webgpu-demo/`

**index.htmlï¼š**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>ğŸš€ WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.jsï¼š**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = 'âŒ WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'ğŸ” WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('âœ… ONNX Runtime session created with WebGPU');
        log(`ğŸ“Š Input names: ${session.inputNames.join(', ')}`);
        log(`ğŸ“Š Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = 'âœ… WebGPU inference complete!';
        log(`ğŸ¯ Predicted class: ${maxIdx}`);
        log(`ğŸ“ˆ Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `âŒ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### æ­¥é©Ÿ 3ï¼šé‹è¡Œæ¼”ç¤º

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## ç¬¬å…­éƒ¨åˆ†ï¼šOpen WebUI æ•´åˆ

### æ¦‚è¿°

Open WebUI æä¾›ä¸€å€‹å°ˆæ¥­çš„ ChatGPT å¼ä»‹é¢ï¼Œé€£æ¥åˆ° Foundry Local çš„ OpenAI å…¼å®¹ APIã€‚

### æ­¥é©Ÿ 1ï¼šå…ˆæ±ºæ¢ä»¶

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### æ­¥é©Ÿ 2ï¼šDocker è¨­ç½®ï¼ˆæ¨è–¦ï¼‰

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**æ³¨æ„ï¼š** `host.docker.internal` å…è¨± Docker å®¹å™¨åœ¨ Windows ä¸Šè¨ªå•ä¸»æ©Ÿã€‚

### æ­¥é©Ÿ 3ï¼šé…ç½®

1. **æ‰“é–‹ç€è¦½å™¨ï¼š**å°èˆªè‡³ `http://localhost:3000`
2. **åˆå§‹è¨­ç½®ï¼š**å‰µå»ºç®¡ç†å“¡å¸³æˆ¶
3. **æ¨¡å‹é…ç½®ï¼š**
   - è¨­ç½® â†’ æ¨¡å‹ â†’ OpenAI API  
   - åŸºæœ¬ URLï¼š`http://host.docker.internal:51211/v1`
   - API é‡‘é‘°ï¼š`foundry-local-key`ï¼ˆä»»æ„å€¼å‡å¯ï¼‰
4. **æ¸¬è©¦é€£æ¥ï¼š**æ¨¡å‹æ‡‰å‡ºç¾åœ¨ä¸‹æ‹‰é¸å–®ä¸­

### æ•…éšœæ’é™¤

**å¸¸è¦‹å•é¡Œï¼š**

1. **é€£æ¥è¢«æ‹’ï¼š**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **æ¨¡å‹æœªé¡¯ç¤ºï¼š**
   - é©—è­‰æ¨¡å‹æ˜¯å¦å·²è¼‰å…¥ï¼š`foundry model list`
   - æª¢æŸ¥ API å›æ‡‰ï¼š`curl http://localhost:51211/v1/models`
   - é‡å•Ÿ Open WebUI å®¹å™¨

## ç¬¬ä¸ƒéƒ¨åˆ†ï¼šç”Ÿç”¢éƒ¨ç½²è€ƒé‡

### ç’°å¢ƒé…ç½®

**é–‹ç™¼è¨­ç½®ï¼š**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**ç”Ÿç”¢éƒ¨ç½²ï¼š**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### å¸¸è¦‹ç«¯å£å•é¡ŒåŠè§£æ±ºæ–¹æ¡ˆ

**ç«¯å£ 51211 è¡çªé é˜²ï¼š**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### æ€§èƒ½ç›£æ§

**å¥åº·æª¢æŸ¥å¯¦ç¾ï¼š**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## ç¸½çµ

ç¬¬å››ç¯€èª²ç¨‹æ¶µè“‹äº†ä½¿ç”¨ Chainlit å»ºç«‹ç”Ÿç”¢ç´šå°è©±å¼ AI æ‡‰ç”¨ç¨‹å¼çš„å…§å®¹ã€‚æ‚¨å­¸ç¿’äº†ï¼š

- âœ… **Chainlit æ¡†æ¶**ï¼šæ”¯æŒèŠå¤©æ‡‰ç”¨ç¨‹å¼çš„ç¾ä»£åŒ– UI å’Œä¸²æµåŠŸèƒ½
- âœ… **Foundry Local æ•´åˆ**ï¼šSDK ä½¿ç”¨å’Œé…ç½®æ¨¡å¼
- âœ… **WebGPU æ¨ç†**ï¼šåŸºæ–¼ç€è¦½å™¨çš„ AIï¼Œæä¾›æœ€å¤§éš±ç§
- âœ… **Open WebUI è¨­ç½®**ï¼šå°ˆæ¥­èŠå¤©ä»‹é¢éƒ¨ç½²
- âœ… **ç”Ÿç”¢æ¨¡å¼**ï¼šéŒ¯èª¤è™•ç†ã€ç›£æ§å’Œæ“´å±•

ç¯„ä¾‹ 04 æ‡‰ç”¨ç¨‹å¼å±•ç¤ºäº†æœ€ä½³å¯¦è¸ï¼Œé€šé Microsoft Foundry Local åˆ©ç”¨æœ¬åœ° AI æ¨¡å‹ï¼Œæä¾›å“è¶Šçš„ç”¨æˆ¶é«”é©—ã€‚

## åƒè€ƒè³‡æ–™

- **[ç¯„ä¾‹ 04ï¼šChainlit æ‡‰ç”¨ç¨‹å¼](samples/04/README.md)**ï¼šå®Œæ•´æ‡‰ç”¨ç¨‹å¼åŠæ–‡æª”
- **[Chainlit æ•™è‚²ç­†è¨˜æœ¬](samples/04/chainlit_app.ipynb)**ï¼šäº’å‹•å­¸ç¿’ææ–™
- **[Foundry Local æ–‡æª”](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**ï¼šå®Œæ•´å¹³å°æ–‡æª”
- **[Chainlit æ–‡æª”](https://docs.chainlit.io/)**ï¼šå®˜æ–¹æ¡†æ¶æ–‡æª”
- **[Open WebUI æ•´åˆæŒ‡å—](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**ï¼šå®˜æ–¹æ•™ç¨‹

---

