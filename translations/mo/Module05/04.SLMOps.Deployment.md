<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-17T18:55:51+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "mo"
}
-->
# ç¬¬å››ç« ï¼šéƒ¨ç½² - ç”Ÿç”¢ç’°å¢ƒæ¨¡å‹å¯¦ç¾

## æ¦‚è¿°

æœ¬æ•™ç¨‹å°‡å…¨é¢æŒ‡å°æ‚¨ä½¿ç”¨ Foundry Local éƒ¨ç½²ç¶“éå¾®èª¿çš„é‡åŒ–æ¨¡å‹çš„å®Œæ•´éç¨‹ã€‚æˆ‘å€‘å°‡å¾æ¨¡å‹è½‰æ›ã€é‡åŒ–å„ªåŒ–åˆ°éƒ¨ç½²é…ç½®é€²è¡Œè©³ç´°è¬›è§£ã€‚

## å‰ç½®æ¢ä»¶

åœ¨é–‹å§‹ä¹‹å‰ï¼Œè«‹ç¢ºä¿æ‚¨å…·å‚™ä»¥ä¸‹æ¢ä»¶ï¼š

- âœ… ä¸€å€‹å·²å¾®èª¿çš„ ONNX æ¨¡å‹ï¼Œæº–å‚™éƒ¨ç½²
- âœ… Windows æˆ– Mac é›»è…¦
- âœ… Python 3.10 æˆ–æ›´é«˜ç‰ˆæœ¬
- âœ… è‡³å°‘ 8GB å¯ç”¨ RAM
- âœ… å·²åœ¨ç³»çµ±ä¸Šå®‰è£ Foundry Local

## ç¬¬ä¸€éƒ¨åˆ†ï¼šç’°å¢ƒè¨­ç½®

### å®‰è£æ‰€éœ€å·¥å…·

æ‰“é–‹æ‚¨çš„çµ‚ç«¯ï¼ˆWindows ä¸Šçš„å‘½ä»¤æç¤ºç¬¦ï¼ŒMac ä¸Šçš„çµ‚ç«¯ï¼‰ï¼Œä¸¦æŒ‰é †åºåŸ·è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

âš ï¸ **é‡è¦æç¤º**ï¼šæ‚¨é‚„éœ€è¦ CMake 3.31 æˆ–æ›´æ–°ç‰ˆæœ¬ï¼Œå¯å¾ [cmake.org](https://cmake.org/download/) ä¸‹è¼‰ã€‚

## ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹è½‰æ›èˆ‡é‡åŒ–

### é¸æ“‡åˆé©çš„æ ¼å¼

å°æ–¼å¾®èª¿çš„å°å‹èªè¨€æ¨¡å‹ï¼Œæˆ‘å€‘æ¨è–¦ä½¿ç”¨ **ONNX æ ¼å¼**ï¼Œå› ç‚ºå®ƒæä¾›ï¼š

- ğŸš€ æ›´ä½³çš„æ€§èƒ½å„ªåŒ–
- ğŸ”§ èˆ‡ç¡¬ä»¶ç„¡é—œçš„éƒ¨ç½²
- ğŸ­ ç”Ÿç”¢ç’°å¢ƒçš„èƒ½åŠ›
- ğŸ“± è·¨å¹³å°å…¼å®¹æ€§

### æ–¹æ³•ä¸€ï¼šä¸€éµè½‰æ›ï¼ˆæ¨è–¦ï¼‰

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç›´æ¥è½‰æ›æ‚¨çš„å¾®èª¿æ¨¡å‹ï¼š

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**åƒæ•¸èªªæ˜ï¼š**
- `--model_name_or_path`ï¼šå¾®èª¿æ¨¡å‹çš„è·¯å¾‘
- `--device cpu`ï¼šä½¿ç”¨ CPU é€²è¡Œå„ªåŒ–
- `--precision int4`ï¼šä½¿ç”¨ INT4 é‡åŒ–ï¼ˆå¤§ç´„æ¸›å°‘ 75% çš„å¤§å°ï¼‰
- `--output_path`ï¼šè½‰æ›å¾Œæ¨¡å‹çš„è¼¸å‡ºè·¯å¾‘

### æ–¹æ³•äºŒï¼šé…ç½®æ–‡ä»¶æ–¹å¼ï¼ˆé€²éšç”¨æˆ¶ï¼‰

å‰µå»ºåç‚º `finetuned_conversion_config.json` çš„é…ç½®æ–‡ä»¶ï¼š

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

ç„¶å¾ŒåŸ·è¡Œï¼š

```bash
olive run --config ./finetuned_conversion_config.json
```

### é‡åŒ–é¸é …æ¯”è¼ƒ

| ç²¾åº¦       | æ–‡ä»¶å¤§å°       | æ¨ç†é€Ÿåº¦       | æ¨¡å‹è³ªé‡       | æ¨è–¦ç”¨é€”           |
|------------|----------------|----------------|----------------|--------------------|
| FP16       | åŸºæº– Ã— 0.5    | å¿«é€Ÿ           | æœ€ä½³           | é«˜ç«¯ç¡¬ä»¶           |
| INT8       | åŸºæº– Ã— 0.25   | éå¸¸å¿«é€Ÿ       | è‰¯å¥½           | å¹³è¡¡é¸æ“‡           |
| INT4       | åŸºæº– Ã— 0.125  | æœ€å¿«           | å¯æ¥å—         | è³‡æºæœ‰é™           |

ğŸ’¡ **å»ºè­°**ï¼šé¦–æ¬¡éƒ¨ç½²æ™‚ï¼Œå»ºè­°ä½¿ç”¨ INT4 é‡åŒ–ã€‚å¦‚æœè³ªé‡ä¸æ»¿æ„ï¼Œå¯å˜—è©¦ INT8 æˆ– FP16ã€‚

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šFoundry Local éƒ¨ç½²é…ç½®

### å‰µå»ºæ¨¡å‹é…ç½®

å°èˆªåˆ° Foundry Local çš„æ¨¡å‹ç›®éŒ„ï¼š

```bash
foundry cache cd ./models/
```

å‰µå»ºæ‚¨çš„æ¨¡å‹ç›®éŒ„çµæ§‹ï¼š

```bash
mkdir -p ./models/custom/your-finetuned-model
```

åœ¨æ¨¡å‹ç›®éŒ„ä¸­å‰µå»º `inference_model.json` é…ç½®æ–‡ä»¶ï¼š

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### ç‰¹å®šæ¨¡å‹æ¨¡æ¿é…ç½®

#### å°æ–¼ Qwen ç³»åˆ—æ¨¡å‹ï¼š

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹æ¸¬è©¦èˆ‡å„ªåŒ–

### é©—è­‰æ¨¡å‹å®‰è£

æª¢æŸ¥ Foundry Local æ˜¯å¦èƒ½è­˜åˆ¥æ‚¨çš„æ¨¡å‹ï¼š

```bash
foundry cache ls
```

æ‚¨æ‡‰è©²èƒ½çœ‹åˆ° `your-finetuned-model-int4` å‡ºç¾åœ¨åˆ—è¡¨ä¸­ã€‚

### é–‹å§‹æ¨¡å‹æ¸¬è©¦

```bash
foundry model run your-finetuned-model-int4
```

### æ€§èƒ½åŸºæº–æ¸¬è©¦

åœ¨æ¸¬è©¦éç¨‹ä¸­ç›£æ§ä»¥ä¸‹é—œéµæŒ‡æ¨™ï¼š

1. **éŸ¿æ‡‰æ™‚é–“**ï¼šæ¸¬é‡æ¯æ¬¡éŸ¿æ‡‰çš„å¹³å‡æ™‚é–“
2. **å…§å­˜ä½¿ç”¨**ï¼šç›£æ§ RAM æ¶ˆè€—
3. **CPU ä½¿ç”¨ç‡**ï¼šæª¢æŸ¥è™•ç†å™¨è² è¼‰
4. **è¼¸å‡ºè³ªé‡**ï¼šè©•ä¼°éŸ¿æ‡‰çš„ç›¸é—œæ€§å’Œé€£è²«æ€§

### è³ªé‡é©—è­‰æ¸…å–®

- âœ… æ¨¡å‹èƒ½é©ç•¶éŸ¿æ‡‰å¾®èª¿é ˜åŸŸçš„æŸ¥è©¢
- âœ… éŸ¿æ‡‰æ ¼å¼ç¬¦åˆé æœŸçš„è¼¸å‡ºçµæ§‹
- âœ… é•·æ™‚é–“ä½¿ç”¨ç„¡å…§å­˜æ´©æ¼
- âœ… åœ¨ä¸åŒè¼¸å…¥é•·åº¦ä¸‹ä¿æŒä¸€è‡´æ€§èƒ½
- âœ… æ­£ç¢ºè™•ç†é‚Šç•Œæƒ…æ³å’Œç„¡æ•ˆè¼¸å…¥

## ç¸½çµ

æ­å–œæ‚¨ï¼æ‚¨å·²æˆåŠŸå®Œæˆï¼š

- âœ… å¾®èª¿æ¨¡å‹æ ¼å¼è½‰æ›
- âœ… æ¨¡å‹é‡åŒ–å„ªåŒ–
- âœ… Foundry Local éƒ¨ç½²é…ç½®
- âœ… æ€§èƒ½èª¿æ•´èˆ‡æ•…éšœæ’é™¤

---

**å…è²¬è²æ˜**ï¼š  
æœ¬æ–‡ä»¶å·²ä½¿ç”¨ AI ç¿»è­¯æœå‹™ [Co-op Translator](https://github.com/Azure/co-op-translator) é€²è¡Œç¿»è­¯ã€‚å„˜ç®¡æˆ‘å€‘åŠªåŠ›ç¢ºä¿ç¿»è­¯çš„æº–ç¢ºæ€§ï¼Œä½†è«‹æ³¨æ„ï¼Œè‡ªå‹•ç¿»è­¯å¯èƒ½åŒ…å«éŒ¯èª¤æˆ–ä¸æº–ç¢ºä¹‹è™•ã€‚åŸå§‹æ–‡ä»¶çš„æ¯èªç‰ˆæœ¬æ‡‰è¢«è¦–ç‚ºæ¬Šå¨ä¾†æºã€‚å°æ–¼é—œéµä¿¡æ¯ï¼Œå»ºè­°ä½¿ç”¨å°ˆæ¥­äººå·¥ç¿»è­¯ã€‚æˆ‘å€‘å°å› ä½¿ç”¨æ­¤ç¿»è­¯è€Œå¼•èµ·çš„ä»»ä½•èª¤è§£æˆ–éŒ¯èª¤è§£é‡‹ä¸æ‰¿æ“”è²¬ä»»ã€‚