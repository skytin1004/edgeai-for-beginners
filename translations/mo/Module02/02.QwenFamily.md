<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-01T20:30:09+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "mo"
}
-->
# ç¬¬ 2 ç¯€ï¼šQwen å®¶æ—åŸºç¤çŸ¥è­˜

Qwen æ¨¡å‹å®¶æ—ä»£è¡¨äº†é˜¿é‡Œé›²åœ¨å¤§å‹èªè¨€æ¨¡å‹å’Œå¤šæ¨¡æ…‹ AI é ˜åŸŸçš„å…¨é¢æ–¹æ³•ï¼Œå±•ç¤ºäº†é–‹æºæ¨¡å‹åœ¨å„ç¨®éƒ¨ç½²å ´æ™¯ä¸­æ—¢èƒ½ä¿æŒå“è¶Šæ€§èƒ½ï¼Œåˆèƒ½ä¿æŒå¯è¨ªå•æ€§çš„é‡è¦æ€§ã€‚äº†è§£ Qwen å®¶æ—å¦‚ä½•é€šééˆæ´»çš„éƒ¨ç½²é¸é …æä¾›å¼·å¤§çš„ AI èƒ½åŠ›ï¼ŒåŒæ™‚åœ¨å¤šæ¨£åŒ–ä»»å‹™ä¸­ä¿æŒç«¶çˆ­åŠ›ï¼Œæ˜¯éå¸¸é‡è¦çš„ã€‚

## é–‹ç™¼è€…è³‡æº

### Hugging Face æ¨¡å‹åº«
éƒ¨åˆ† Qwen å®¶æ—æ¨¡å‹å¯é€šé [Hugging Face](https://huggingface.co/models?search=qwen) ç²å¾—ï¼Œæä¾›é€™äº›æ¨¡å‹çš„ä¸€äº›è®Šé«”ã€‚æ‚¨å¯ä»¥æ¢ç´¢å¯ç”¨çš„è®Šé«”ï¼Œæ ¹æ“šæ‚¨çš„ç‰¹å®šéœ€æ±‚é€²è¡Œå¾®èª¿ï¼Œä¸¦é€šéå„ç¨®æ¡†æ¶é€²è¡Œéƒ¨ç½²ã€‚

### æœ¬åœ°é–‹ç™¼å·¥å…·
å°æ–¼æœ¬åœ°é–‹ç™¼å’Œæ¸¬è©¦ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) åœ¨æ‚¨çš„é–‹ç™¼æ©Ÿå™¨ä¸Šé‹è¡Œå¯ç”¨çš„ Qwen æ¨¡å‹ï¼Œä¸¦ç²å¾—å„ªåŒ–çš„æ€§èƒ½ã€‚

### æ–‡æª”è³‡æº
- [Qwen æ¨¡å‹æ–‡æª”](https://huggingface.co/docs/transformers/model_doc/qwen)
- [å„ªåŒ– Qwen æ¨¡å‹ä»¥é©æ‡‰é‚Šç·£éƒ¨ç½²](https://github.com/microsoft/olive)

## ç°¡ä»‹

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘å€‘å°‡æ¢ç´¢é˜¿é‡Œå·´å·´çš„ Qwen æ¨¡å‹å®¶æ—åŠå…¶åŸºæœ¬æ¦‚å¿µã€‚æˆ‘å€‘å°‡æ¶µè“‹ Qwen å®¶æ—çš„æ¼”é€²ã€ä½¿ Qwen æ¨¡å‹é«˜æ•ˆçš„å‰µæ–°è¨“ç·´æ–¹æ³•ã€å®¶æ—ä¸­çš„ä¸»è¦è®Šé«”ï¼Œä»¥åŠåœ¨ä¸åŒå ´æ™¯ä¸­çš„å¯¦éš›æ‡‰ç”¨ã€‚

## å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬æ•™ç¨‹å¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š

- ç†è§£é˜¿é‡Œå·´å·´ Qwen æ¨¡å‹å®¶æ—çš„è¨­è¨ˆç†å¿µå’Œæ¼”é€²éç¨‹
- è­˜åˆ¥ä½¿ Qwen æ¨¡å‹åœ¨å„ç¨®åƒæ•¸è¦æ¨¡ä¸­å¯¦ç¾é«˜æ€§èƒ½çš„é—œéµå‰µæ–°
- èªè­˜ä¸åŒ Qwen æ¨¡å‹è®Šé«”çš„å„ªå‹¢å’Œå±€é™æ€§
- æ‡‰ç”¨ Qwen æ¨¡å‹çš„çŸ¥è­˜ï¼Œé¸æ“‡é©åˆå¯¦éš›å ´æ™¯çš„è®Šé«”

## ç†è§£ç¾ä»£ AI æ¨¡å‹çš„æ ¼å±€

AI çš„æ ¼å±€å·²ç¶“ç™¼ç”Ÿäº†é¡¯è‘—çš„è®ŠåŒ–ï¼Œä¸åŒçš„çµ„ç¹”æ¡å–äº†å„ç¨®æ–¹æ³•ä¾†é–‹ç™¼èªè¨€æ¨¡å‹ã€‚ä¸€äº›å°ˆæ³¨æ–¼å°ˆæœ‰çš„é–‰æºæ¨¡å‹ï¼Œå¦ä¸€äº›å‰‡å¼·èª¿é–‹æºçš„å¯è¨ªå•æ€§å’Œé€æ˜æ€§ã€‚å‚³çµ±æ–¹æ³•é€šå¸¸æ¶‰åŠåƒ…é€šé API å¯è¨ªå•çš„å¤§å‹å°ˆæœ‰æ¨¡å‹ï¼Œæˆ–å¯èƒ½åœ¨èƒ½åŠ›ä¸Šè½å¾Œçš„é–‹æºæ¨¡å‹ã€‚

é€™ç¨®ç¯„å¼ç‚ºå°‹æ±‚å¼·å¤§ AI èƒ½åŠ›çš„çµ„ç¹”å¸¶ä¾†äº†æŒ‘æˆ°ï¼ŒåŒæ™‚éœ€è¦ä¿æŒå°å…¶æ•¸æ“šã€æˆæœ¬å’Œéƒ¨ç½²éˆæ´»æ€§çš„æ§åˆ¶ã€‚å‚³çµ±æ–¹æ³•é€šå¸¸éœ€è¦åœ¨å°–ç«¯æ€§èƒ½å’Œå¯¦éš›éƒ¨ç½²è€ƒé‡ä¹‹é–“åšå‡ºé¸æ“‡ã€‚

## å¯è¨ªå• AI å“è¶Šæ€§çš„æŒ‘æˆ°

é«˜è³ªé‡ã€å¯è¨ªå•çš„ AI åœ¨å„ç¨®å ´æ™¯ä¸­è®Šå¾—è¶Šä¾†è¶Šé‡è¦ã€‚è€ƒæ…®éœ€è¦éˆæ´»éƒ¨ç½²é¸é …ä»¥æ»¿è¶³ä¸åŒçµ„ç¹”éœ€æ±‚çš„æ‡‰ç”¨ã€API æˆæœ¬å¯èƒ½è®Šå¾—é¡¯è‘—çš„æˆæœ¬æ•ˆç›Šå¯¦ç¾ã€å…¨çƒæ‡‰ç”¨ä¸­çš„å¤šèªè¨€èƒ½åŠ›ï¼Œæˆ–åœ¨ç·¨ç¨‹å’Œæ•¸å­¸ç­‰é ˜åŸŸçš„å°ˆæ¥­çŸ¥è­˜ã€‚

### é—œéµéƒ¨ç½²éœ€æ±‚

ç¾ä»£ AI éƒ¨ç½²é¢è‡¨ä¸€äº›åŸºæœ¬éœ€æ±‚ï¼Œé™åˆ¶äº†å…¶å¯¦éš›é©ç”¨æ€§ï¼š

- **å¯è¨ªå•æ€§**ï¼šé–‹æºå¯ç”¨æ€§ä»¥å¯¦ç¾é€æ˜æ€§å’Œå®šåˆ¶åŒ–
- **æˆæœ¬æ•ˆç›Š**ï¼šåˆç†çš„è¨ˆç®—éœ€æ±‚ä»¥é©æ‡‰ä¸åŒé ç®—
- **éˆæ´»æ€§**ï¼šå¤šç¨®æ¨¡å‹è¦æ¨¡ä»¥é©æ‡‰ä¸åŒéƒ¨ç½²å ´æ™¯
- **å…¨çƒè¦†è“‹**ï¼šå¼·å¤§çš„å¤šèªè¨€å’Œè·¨æ–‡åŒ–èƒ½åŠ›
- **å°ˆæ¥­åŒ–**ï¼šé‡å°ç‰¹å®šç”¨ä¾‹çš„é ˜åŸŸå°ˆå±¬è®Šé«”

## Qwen æ¨¡å‹ç†å¿µ

Qwen æ¨¡å‹å®¶æ—ä»£è¡¨äº†ä¸€ç¨®å…¨é¢çš„ AI æ¨¡å‹é–‹ç™¼æ–¹æ³•ï¼Œå„ªå…ˆè€ƒæ…®é–‹æºå¯è¨ªå•æ€§ã€å¤šèªè¨€èƒ½åŠ›å’Œå¯¦éš›éƒ¨ç½²ï¼ŒåŒæ™‚ä¿æŒç«¶çˆ­æ€§èƒ½ç‰¹æ€§ã€‚Qwen æ¨¡å‹é€šéå¤šæ¨£åŒ–çš„æ¨¡å‹è¦æ¨¡ã€é«˜è³ªé‡çš„è¨“ç·´æ–¹æ³•ï¼Œä»¥åŠé‡å°ä¸åŒé ˜åŸŸçš„å°ˆå±¬è®Šé«”ä¾†å¯¦ç¾é€™ä¸€ç›®æ¨™ã€‚

Qwen å®¶æ—æ¶µè“‹äº†å„ç¨®æ–¹æ³•ï¼Œæ—¨åœ¨æä¾›æ€§èƒ½èˆ‡æ•ˆç‡å…‰è­œä¸Šçš„é¸æ“‡ï¼Œæ”¯æŒå¾ç§»å‹•è¨­å‚™åˆ°ä¼æ¥­æœå‹™å™¨çš„éƒ¨ç½²ï¼ŒåŒæ™‚æä¾›æœ‰æ„ç¾©çš„ AI èƒ½åŠ›ã€‚ç›®æ¨™æ˜¯æ°‘ä¸»åŒ–é«˜è³ªé‡ AI çš„è¨ªå•ï¼ŒåŒæ™‚æä¾›éƒ¨ç½²é¸æ“‡çš„éˆæ´»æ€§ã€‚

### Qwen çš„æ ¸å¿ƒè¨­è¨ˆåŸå‰‡

Qwen æ¨¡å‹åŸºæ–¼å¹¾å€‹å€åˆ†æ–¼å…¶ä»–èªè¨€æ¨¡å‹å®¶æ—çš„åŸºæœ¬åŸå‰‡ï¼š

- **é–‹æºå„ªå…ˆ**ï¼šå®Œå…¨é€æ˜å’Œå¯è¨ªå•æ€§ï¼Œç”¨æ–¼ç ”ç©¶å’Œå•†æ¥­ç”¨é€”
- **å…¨é¢è¨“ç·´**ï¼šåŸºæ–¼æ¶µè“‹å¤šç¨®èªè¨€å’Œé ˜åŸŸçš„å¤§è¦æ¨¡ã€å¤šæ¨£åŒ–æ•¸æ“šé›†é€²è¡Œè¨“ç·´
- **å¯æ“´å±•æ¶æ§‹**ï¼šå¤šç¨®æ¨¡å‹è¦æ¨¡ä»¥åŒ¹é…ä¸åŒçš„è¨ˆç®—éœ€æ±‚
- **å°ˆæ¥­åŒ–å“è¶Š**ï¼šé‡å°ç‰¹å®šä»»å‹™å„ªåŒ–çš„é ˜åŸŸå°ˆå±¬è®Šé«”

## æ”¯æŒ Qwen å®¶æ—çš„é—œéµæŠ€è¡“

### å¤§è¦æ¨¡è¨“ç·´

Qwen å®¶æ—çš„ä¸€å€‹é¡¯è‘—ç‰¹é»æ˜¯æ¨¡å‹é–‹ç™¼ä¸­æŠ•å…¥çš„å¤§è¦æ¨¡è¨“ç·´æ•¸æ“šå’Œè¨ˆç®—è³‡æºã€‚Qwen æ¨¡å‹åˆ©ç”¨ç²¾å¿ƒç­–åŠƒçš„å¤šèªè¨€æ•¸æ“šé›†ï¼Œæ¶µè“‹æ•¸è¬å„„å€‹ tokenï¼Œæ—¨åœ¨æä¾›å…¨é¢çš„ä¸–ç•ŒçŸ¥è­˜å’Œæ¨ç†èƒ½åŠ›ã€‚

é€™ç¨®æ–¹æ³•é€šéçµåˆé«˜è³ªé‡çš„ç¶²çµ¡å…§å®¹ã€å­¸è¡“æ–‡ç»ã€ä»£ç¢¼åº«å’Œå¤šèªè¨€è³‡æºä¾†å¯¦ç¾ã€‚è¨“ç·´æ–¹æ³•å¼·èª¿çŸ¥è­˜çš„å»£åº¦å’Œåœ¨å„ç¨®é ˜åŸŸå’Œèªè¨€ä¸­çš„æ·±åº¦ç†è§£ã€‚

### é«˜ç´šæ¨ç†å’Œæ€è€ƒ

æœ€æ–°çš„ Qwen æ¨¡å‹èå…¥äº†è¤‡é›œçš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤ é€²è¡Œå¤šæ­¥é©Ÿçš„å•é¡Œè§£æ±ºï¼š

**æ€è€ƒæ¨¡å¼ (Qwen3)**ï¼šæ¨¡å‹å¯ä»¥åœ¨æä¾›æœ€çµ‚ç­”æ¡ˆä¹‹å‰é€²è¡Œè©³ç´°çš„é€æ­¥æ¨ç†ï¼Œé¡ä¼¼æ–¼äººé¡çš„å•é¡Œè§£æ±ºæ–¹æ³•ã€‚

**é›™æ¨¡å¼æ“ä½œ**ï¼šèƒ½å¤ åœ¨ç°¡å–®æŸ¥è©¢çš„å¿«é€ŸéŸ¿æ‡‰æ¨¡å¼å’Œè¤‡é›œå•é¡Œçš„æ·±åº¦æ€è€ƒæ¨¡å¼ä¹‹é–“åˆ‡æ›ã€‚

**æ€ç¶­éˆæ•´åˆ**ï¼šè‡ªç„¶åœ°æ•´åˆæ¨ç†æ­¥é©Ÿï¼Œæé«˜è¤‡é›œä»»å‹™çš„é€æ˜åº¦å’Œæº–ç¢ºæ€§ã€‚

### æ¶æ§‹å‰µæ–°

Qwen å®¶æ—èå…¥äº†å¹¾é …æ¶æ§‹å„ªåŒ–ï¼Œæ—¨åœ¨æé«˜æ€§èƒ½å’Œæ•ˆç‡ï¼š

**å¯æ“´å±•è¨­è¨ˆ**ï¼šä¸€è‡´çš„æ¶æ§‹è·¨è¶Šä¸åŒæ¨¡å‹è¦æ¨¡ï¼Œä¾¿æ–¼æ“´å±•å’Œæ¯”è¼ƒã€‚

**å¤šæ¨¡æ…‹æ•´åˆ**ï¼šåœ¨çµ±ä¸€æ¶æ§‹ä¸­ç„¡ç¸«æ•´åˆæ–‡æœ¬ã€è¦–è¦ºå’ŒéŸ³é »è™•ç†èƒ½åŠ›ã€‚

**éƒ¨ç½²å„ªåŒ–**ï¼šå¤šç¨®é‡åŒ–é¸é …å’Œéƒ¨ç½²æ ¼å¼ä»¥é©æ‡‰ä¸åŒçš„ç¡¬ä»¶é…ç½®ã€‚

## æ¨¡å‹è¦æ¨¡å’Œéƒ¨ç½²é¸é …

ç¾ä»£éƒ¨ç½²ç’°å¢ƒå—ç›Šæ–¼ Qwen æ¨¡å‹åœ¨å„ç¨®è¨ˆç®—éœ€æ±‚ä¸Šçš„éˆæ´»æ€§ï¼š

### å°å‹æ¨¡å‹ (0.5B-3B)

Qwen æä¾›é«˜æ•ˆçš„å°å‹æ¨¡å‹ï¼Œé©åˆé‚Šç·£éƒ¨ç½²ã€ç§»å‹•æ‡‰ç”¨å’Œè³‡æºå—é™çš„ç’°å¢ƒï¼ŒåŒæ™‚ä¿æŒä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚

### ä¸­å‹æ¨¡å‹ (7B-32B)

ä¸­å‹æ¨¡å‹ç‚ºå°ˆæ¥­æ‡‰ç”¨æä¾›å¢å¼·çš„èƒ½åŠ›ï¼Œåœ¨æ€§èƒ½å’Œè¨ˆç®—éœ€æ±‚ä¹‹é–“æä¾›å‡ºè‰²çš„å¹³è¡¡ã€‚

### å¤§å‹æ¨¡å‹ (72B+)

å…¨è¦æ¨¡æ¨¡å‹ç‚ºéœ€è¦æœ€å¤§èƒ½åŠ›çš„é«˜è¦æ±‚æ‡‰ç”¨ã€ç ”ç©¶å’Œä¼æ¥­éƒ¨ç½²æä¾›æœ€å…ˆé€²çš„æ€§èƒ½ã€‚

## Qwen æ¨¡å‹å®¶æ—çš„å„ªå‹¢

### é–‹æºå¯è¨ªå•æ€§

Qwen æ¨¡å‹æä¾›å®Œå…¨çš„é€æ˜æ€§å’Œå®šåˆ¶èƒ½åŠ›ï¼Œä½¿çµ„ç¹”èƒ½å¤ ç†è§£ã€ä¿®æ”¹å’Œé©æ‡‰æ¨¡å‹ä»¥æ»¿è¶³å…¶ç‰¹å®šéœ€æ±‚ï¼Œé¿å…ä¾›æ‡‰å•†é–å®šã€‚

### éƒ¨ç½²éˆæ´»æ€§

å¤šç¨®æ¨¡å‹è¦æ¨¡ä½¿å¾—èƒ½å¤ åœ¨å¤šæ¨£åŒ–çš„ç¡¬ä»¶é…ç½®ä¸­é€²è¡Œéƒ¨ç½²ï¼Œå¾ç§»å‹•è¨­å‚™åˆ°é«˜ç«¯æœå‹™å™¨ï¼Œç‚ºçµ„ç¹”æä¾› AI åŸºç¤è¨­æ–½é¸æ“‡çš„éˆæ´»æ€§ã€‚

### å¤šèªè¨€å“è¶Š

Qwen æ¨¡å‹åœ¨å¤šèªè¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç¾å‡ºè‰²ï¼Œæ”¯æŒæ•¸åç¨®èªè¨€ï¼Œå°¤å…¶åœ¨è‹±èªå’Œä¸­æ–‡æ–¹é¢å…·æœ‰ç‰¹åˆ¥çš„å„ªå‹¢ï¼Œé©åˆå…¨çƒæ‡‰ç”¨ã€‚

### ç«¶çˆ­æ€§èƒ½

Qwen æ¨¡å‹åœ¨åŸºæº–æ¸¬è©¦ä¸­å§‹çµ‚å–å¾—ç«¶çˆ­æ€§çµæœï¼ŒåŒæ™‚æä¾›é–‹æºå¯è¨ªå•æ€§ï¼Œè­‰æ˜é–‹æºæ¨¡å‹å¯ä»¥åª²ç¾å°ˆæœ‰æ›¿ä»£æ–¹æ¡ˆã€‚

### å°ˆæ¥­åŒ–èƒ½åŠ›

åƒ Qwen-Coder å’Œ Qwen-Math é€™æ¨£çš„é ˜åŸŸå°ˆå±¬è®Šé«”æä¾›å°ˆæ¥­çŸ¥è­˜ï¼ŒåŒæ™‚ä¿æŒä¸€èˆ¬èªè¨€ç†è§£èƒ½åŠ›ã€‚

## å¯¦éš›ç¤ºä¾‹å’Œç”¨ä¾‹

åœ¨æ·±å…¥æŠ€è¡“ç´°ç¯€ä¹‹å‰ï¼Œè®“æˆ‘å€‘æ¢ç´¢ä¸€äº› Qwen æ¨¡å‹å¯ä»¥å®Œæˆçš„å…·é«”ç¤ºä¾‹ï¼š

### æ•¸å­¸æ¨ç†ç¤ºä¾‹

Qwen-Math æ“…é•·é€æ­¥è§£æ±ºæ•¸å­¸å•é¡Œã€‚ä¾‹å¦‚ï¼Œç•¶è¢«è¦æ±‚è§£æ±ºä¸€å€‹è¤‡é›œçš„å¾®ç©åˆ†å•é¡Œæ™‚ï¼š

```
User: Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = xÂ³ + 2xÂ² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹

Breaking down each term:
- d/dx(xÂ³) = 3xÂ²
- d/dx(2xÂ²) = 2 Â· 2xÂ¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3xÂ² + 4x - 5
```

### å¤šèªè¨€æ”¯æŒç¤ºä¾‹

Qwen æ¨¡å‹åœ¨å¤šç¨®èªè¨€ä¸­å±•ç¾å‡ºå¼·å¤§çš„å¤šèªè¨€èƒ½åŠ›ï¼š

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**ä¸­æ–‡:**
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡æœºå™¨ï¼ˆç‰¹åˆ«æ˜¯è®¡ç®—æœºç³»ç»Ÿï¼‰å¯¹äººç±»æ™ºèƒ½è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚è¿™äº›è¿‡ç¨‹åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€è§£å†³é—®é¢˜ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚äººå·¥æ™ºèƒ½ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»è®¤çŸ¥èƒ½åŠ›çš„ä»»åŠ¡ã€‚
```

### å¤šæ¨¡æ…‹èƒ½åŠ›ç¤ºä¾‹

Qwen-VL å¯ä»¥åŒæ™‚è™•ç†æ–‡æœ¬å’Œåœ–åƒï¼š

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### ä»£ç¢¼ç”Ÿæˆç¤ºä¾‹

Qwen-Coder æ“…é•·ç”Ÿæˆå’Œè§£é‡‹å¤šç¨®ç·¨ç¨‹èªè¨€çš„ä»£ç¢¼ï¼š

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

æ­¤å¯¦ç¾éµå¾ªæœ€ä½³å¯¦è¸ï¼Œå…·æœ‰æ¸…æ™°çš„è®Šé‡åç¨±ã€å…¨é¢çš„æ–‡æª”å’Œé«˜æ•ˆçš„é‚è¼¯ã€‚
```

### Edge Deployment Example

Qwen models can be deployed on various edge devices with optimized configurations:

```
# åœ¨ç§»å‹•è¨­å‚™ä¸Šé€²è¡Œé‡åŒ–éƒ¨ç½²çš„ç¤ºä¾‹
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# åŠ è¼‰é‡åŒ–æ¨¡å‹ä»¥é€²è¡Œç§»å‹•éƒ¨ç½²

```
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Qwen å®¶æ—çš„æ¼”é€²

### Qwen 1.0 å’Œ 1.5ï¼šåŸºç¤æ¨¡å‹

æ—©æœŸçš„ Qwen æ¨¡å‹å»ºç«‹äº†å…¨é¢è¨“ç·´å’Œé–‹æºå¯è¨ªå•æ€§çš„åŸºæœ¬åŸå‰‡ï¼š

- **Qwen-7B (7B åƒæ•¸)**ï¼šåˆå§‹ç‰ˆæœ¬ï¼Œå°ˆæ³¨æ–¼ä¸­æ–‡å’Œè‹±æ–‡èªè¨€ç†è§£
- **Qwen-14B (14B åƒæ•¸)**ï¼šå¢å¼·èƒ½åŠ›ï¼Œæ”¹é€²æ¨ç†å’ŒçŸ¥è­˜
- **Qwen-72B (72B åƒæ•¸)**ï¼šå¤§è¦æ¨¡æ¨¡å‹ï¼Œæä¾›æœ€å…ˆé€²çš„æ€§èƒ½
- **Qwen1.5 ç³»åˆ—**ï¼šæ“´å±•åˆ°å¤šç¨®è¦æ¨¡ (0.5B è‡³ 110B)ï¼Œæ”¹é€²äº†é•·ä¸Šä¸‹æ–‡è™•ç†èƒ½åŠ›

### Qwen2 å®¶æ—ï¼šå¤šæ¨¡æ…‹æ“´å±•

Qwen2 ç³»åˆ—åœ¨èªè¨€å’Œå¤šæ¨¡æ…‹èƒ½åŠ›æ–¹é¢å–å¾—äº†é‡å¤§é€²å±•ï¼š

- **Qwen2-0.5B è‡³ 72B**ï¼šå…¨é¢çš„èªè¨€æ¨¡å‹ç¯„åœï¼Œé©æ‡‰å„ç¨®éƒ¨ç½²éœ€æ±‚
- **Qwen2-57B-A14B (MoE)**ï¼šå°ˆå®¶æ··åˆæ¶æ§‹ï¼Œæé«˜åƒæ•¸ä½¿ç”¨æ•ˆç‡
- **Qwen2-VL**ï¼šå…ˆé€²çš„è¦–è¦º-èªè¨€èƒ½åŠ›ï¼Œç”¨æ–¼åœ–åƒç†è§£
- **Qwen2-Audio**ï¼šéŸ³é »è™•ç†å’Œç†è§£èƒ½åŠ›
- **Qwen2-Math**ï¼šå°ˆæ¥­çš„æ•¸å­¸æ¨ç†å’Œå•é¡Œè§£æ±º

### Qwen2.5 å®¶æ—ï¼šæ€§èƒ½æå‡

Qwen2.5 ç³»åˆ—åœ¨æ‰€æœ‰æ–¹é¢å¸¶ä¾†äº†é¡¯è‘—æ”¹é€²ï¼š

- **æ“´å±•è¨“ç·´**ï¼š18 è¬å„„ token çš„è¨“ç·´æ•¸æ“šï¼Œæå‡èƒ½åŠ›
- **æ“´å±•ä¸Šä¸‹æ–‡**ï¼šæ”¯æŒæœ€å¤š 128K token çš„ä¸Šä¸‹æ–‡é•·åº¦ï¼ŒTurbo è®Šé«”æ”¯æŒ 1M token
- **å¢å¼·å°ˆæ¥­åŒ–**ï¼šæ”¹é€²çš„ Qwen2.5-Coder å’Œ Qwen2.5-Math è®Šé«”
- **æ›´å¥½çš„å¤šèªè¨€æ”¯æŒ**ï¼šæå‡åœ¨ 27+ ç¨®èªè¨€ä¸­çš„æ€§èƒ½

### Qwen3 å®¶æ—ï¼šé«˜ç´šæ¨ç†

æœ€æ–°ä¸€ä»£æ¨å‹•äº†æ¨ç†å’Œæ€è€ƒèƒ½åŠ›çš„é‚Šç•Œï¼š

- **Qwen3-235B-A22B**ï¼šæ——è‰¦å°ˆå®¶æ··åˆæ¨¡å‹ï¼Œæ“æœ‰ 235B çš„ç¸½åƒæ•¸
- **Qwen3-30B-A3B**ï¼šé«˜æ•ˆçš„ MoE æ¨¡å‹ï¼Œå…·æœ‰æ¯å€‹æ´»èºåƒæ•¸çš„å¼·å¤§æ€§èƒ½
- **å¯†é›†æ¨¡å‹**ï¼šQwen3-32Bã€14Bã€8Bã€4Bã€1.7Bã€0.6Bï¼Œé©æ‡‰å„ç¨®éƒ¨ç½²å ´æ™¯
- **æ€è€ƒæ¨¡å¼**ï¼šæ”¯æŒå¿«é€ŸéŸ¿æ‡‰å’Œæ·±åº¦æ€è€ƒçš„æ··åˆæ¨ç†æ–¹æ³•
- **å¤šèªè¨€å“è¶Š**ï¼šæ”¯æŒ 119 ç¨®èªè¨€å’Œæ–¹è¨€
- **å¢å¼·è¨“ç·´**ï¼š36 è¬å„„ token çš„å¤šæ¨£åŒ–é«˜è³ªé‡è¨“ç·´æ•¸æ“š

## Qwen æ¨¡å‹çš„æ‡‰ç”¨

### ä¼æ¥­æ‡‰ç”¨

çµ„ç¹”ä½¿ç”¨ Qwen æ¨¡å‹é€²è¡Œæ–‡æª”åˆ†æã€å®¢æˆ¶æœå‹™è‡ªå‹•åŒ–ã€ä»£ç¢¼ç”Ÿæˆè¼”åŠ©å’Œå•†æ¥­æ™ºèƒ½æ‡‰ç”¨ã€‚é–‹æºç‰¹æ€§ä½¿å¾—èƒ½å¤ æ ¹æ“šç‰¹å®šæ¥­å‹™éœ€æ±‚é€²è¡Œå®šåˆ¶ï¼ŒåŒæ™‚ä¿æŒæ•¸æ“šéš±ç§å’Œæ§åˆ¶ã€‚

### ç§»å‹•å’Œé‚Šç·£è¨ˆç®—

ç§»å‹•æ‡‰ç”¨åˆ©ç”¨ Qwen æ¨¡å‹é€²è¡Œå¯¦æ™‚ç¿»è­¯ã€æ™ºèƒ½åŠ©æ‰‹ã€å…§å®¹ç”Ÿæˆå’Œå€‹æ€§åŒ–æ¨è–¦ã€‚å¤šç¨®æ¨¡å‹è¦æ¨¡ä½¿å¾—èƒ½å¤ å¾ç§»å‹•è¨­å‚™åˆ°é‚Šç·£æœå‹™å™¨é€²è¡Œéƒ¨ç½²ã€‚

### æ•™è‚²æŠ€è¡“

æ•™è‚²å¹³å°ä½¿ç”¨ Qwen æ¨¡å‹é€²è¡Œå€‹æ€§åŒ–è¼”å°ã€è‡ªå‹•å…§å®¹ç”Ÿæˆã€èªè¨€å­¸ç¿’è¼”åŠ©å’Œäº’å‹•æ•™è‚²é«”é©—ã€‚åƒ Qwen-Math é€™æ¨£çš„å°ˆå±¬æ¨¡å‹æä¾›é ˜åŸŸå°ˆæ¥­çŸ¥è­˜ã€‚

### å…¨çƒæ‡‰ç”¨

åœ‹éš›æ‡‰ç”¨å—ç›Šæ–¼ Qwen æ¨¡å‹çš„å¼·å¤§å¤šèªè¨€èƒ½åŠ›ï¼Œèƒ½å¤ åœ¨ä¸åŒèªè¨€å’Œæ–‡åŒ–èƒŒæ™¯ä¸­æä¾›ä¸€è‡´çš„ AI é«”é©—ã€‚

## æŒ‘æˆ°å’Œå±€é™æ€§

### è¨ˆç®—éœ€æ±‚

å„˜ç®¡ Qwen æä¾›äº†å„ç¨®è¦æ¨¡çš„æ¨¡å‹ï¼Œä½†è¼ƒå¤§çš„è®Šé«”ä»éœ€è¦å¤§é‡è¨ˆç®—è³‡æºæ‰èƒ½é”åˆ°æœ€ä½³æ€§èƒ½ï¼Œé€™å¯èƒ½é™åˆ¶æŸäº›çµ„ç¹”çš„éƒ¨ç½²é¸é …ã€‚

### å°ˆæ¥­é ˜åŸŸæ€§èƒ½

å„˜ç®¡ Qwen æ¨¡å‹åœ¨ä¸€èˆ¬é ˜åŸŸè¡¨ç¾è‰¯å¥½ï¼Œä½†é«˜åº¦å°ˆæ¥­åŒ–çš„æ‡‰ç”¨å¯èƒ½éœ€è¦é ˜åŸŸå°ˆå±¬çš„å¾®èª¿æˆ–å°ˆæ¥­æ¨¡å‹ã€‚

### æ¨¡å‹é¸æ“‡çš„è¤‡é›œæ€§

å¯ç”¨æ¨¡å‹å’Œè®Šé«”çš„å»£æ³›ç¯„åœå¯èƒ½ä½¿å¾—å°æ–¼æ–°æ‰‹ç”¨æˆ¶ä¾†èªªé¸æ“‡è®Šå¾—å…·æœ‰æŒ‘æˆ°æ€§ã€‚

### èªè¨€ä¸å¹³è¡¡

å„˜ç®¡æ”¯æŒå¤šç¨®èªè¨€ï¼Œä½†åœ¨ä¸åŒèªè¨€ä¸­çš„æ€§èƒ½å¯èƒ½æœ‰æ‰€ä¸åŒï¼Œè‹±èªå’Œä¸­æ–‡çš„èƒ½åŠ›æœ€å¼·ã€‚

## Qwen æ¨¡å‹å®¶æ—çš„æœªä¾†

Qwen æ¨¡å‹å®¶æ—ä»£è¡¨äº†å‘æ°‘ä¸»åŒ–é«˜è³ªé‡ AI çš„æŒçºŒæ¼”é€²ã€‚æœªä¾†çš„ç™¼å±•åŒ…æ‹¬å¢å¼·æ•ˆç‡å„ªåŒ–ã€æ“´å±•å¤šæ¨¡æ…‹èƒ½åŠ›ã€æ”¹é€²æ¨ç†æ©Ÿåˆ¶ï¼Œä»¥åŠåœ¨ä¸åŒéƒ¨ç½²å ´æ™¯ä¸­çš„æ›´å¥½æ•´åˆã€‚

éš¨è‘—æŠ€è¡“çš„æŒçºŒé€²æ­¥ï¼Œæˆ‘å€‘å¯ä»¥æœŸå¾… Qwen æ¨¡å‹è®Šå¾—è¶Šä¾†è¶Šå¼·å¤§ï¼ŒåŒæ™‚ä¿æŒå…¶é–‹æºå¯è¨ªå•æ€§ï¼Œæ”¯æŒ AI åœ¨å¤šæ¨£åŒ–å ´æ™¯å’Œç”¨ä¾‹ä¸­çš„éƒ¨ç½²ã€‚

Qwen å®¶æ—å±•ç¤ºäº† AI é–‹ç™¼çš„æœªä¾†å¯ä»¥åŒæ™‚æ“æŠ±å°–ç«¯æ€§èƒ½å’Œé–‹æ”¾å¯è¨ªå•æ€§ï¼Œç‚ºçµ„ç¹”æä¾›å¼·å¤§çš„å·¥å…·ï¼ŒåŒæ™‚ä¿æŒé€æ˜æ€§å’Œæ§åˆ¶ã€‚

## é–‹ç™¼å’Œé›†æˆç¤ºä¾‹

### ä½¿ç”¨ Transformers å¿«é€Ÿå…¥é–€

ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨ Hugging Face Transformers åº«å¿«é€Ÿé–‹å§‹ä½¿ç”¨ Qwen æ¨¡å‹ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### ä½¿ç”¨ Qwen2.5 æ¨¡å‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### å°ˆå±¬æ¨¡å‹ä½¿ç”¨

**ä½¿ç”¨ Qwen-Coder é€²è¡Œä»£ç¢¼ç”Ÿæˆï¼š**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**æ•¸å­¸å•é¡Œè§£æ±ºï¼š**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**è¦–è¦º-èªè¨€ä»»å‹™ï¼š**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### æ€è€ƒæ¨¡å¼ (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### ğŸ“± ç§»å‹•å’Œé‚Šç·£éƒ¨ç½²

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### API éƒ¨ç½²ç¤ºä¾‹

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## æ€§èƒ½åŸºæº–å’Œæˆå°±

Qwen æ¨¡å‹å®¶æ—åœ¨å„ç¨®åŸºæº–æ¸¬è©¦ä¸­å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ™‚ä¿æŒé–‹æºå¯è¨ªå•æ€§ï¼š

### ä¸»è¦æ€§èƒ½äº®é»

**æ¨ç†å“è¶Šï¼š**
- Qwen3-235B-A22B åœ¨ç·¨ç¢¼ã€æ•¸å­¸åŠä¸€èˆ¬èƒ½åŠ›çš„åŸºæº–è©•ä¼°ä¸­ï¼Œèˆ‡å…¶ä»–é ‚å°–æ¨¡å‹å¦‚ DeepSeek-R1ã€o1ã€o3-miniã€Grok-3 å’Œ Gemini-2.5-Pro ç›¸æ¯”ï¼Œè¡¨ç¾å…·æœ‰ç«¶çˆ­åŠ›ã€‚
- Qwen3-30B-A3B è¶…è¶Šäº†æ“æœ‰åå€æ¿€æ´»åƒæ•¸çš„ QwQ-32Bã€‚
- Qwen3-4B çš„æ€§èƒ½å¯åª²ç¾ Qwen2.5-72B-Instructã€‚

**æ•ˆç‡æˆå°±ï¼š**
- Qwen3-MoE åŸºç¤æ¨¡å‹åƒ…ä½¿ç”¨ 10% çš„æ¿€æ´»åƒæ•¸ï¼Œæ€§èƒ½å»èˆ‡ Qwen2.5 å¯†é›†åŸºç¤æ¨¡å‹ç›¸ç•¶ã€‚
- èˆ‡å¯†é›†æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨è¨“ç·´å’Œæ¨ç†æ–¹é¢é¡¯è‘—ç¯€çœæˆæœ¬ã€‚

**å¤šèªè¨€èƒ½åŠ›ï¼š**
- Qwen3 æ¨¡å‹æ”¯æŒ 119 ç¨®èªè¨€å’Œæ–¹è¨€ã€‚
- åœ¨å¤šæ¨£åŒ–çš„èªè¨€å’Œæ–‡åŒ–èƒŒæ™¯ä¸­è¡¨ç¾å‡ºè‰²ã€‚

**è¨“ç·´è¦æ¨¡ï¼š**
- Qwen3 ä½¿ç”¨äº†ç´„ 36 å…†çš„è¨“ç·´èªæ–™ï¼Œæ¶µè“‹ 119 ç¨®èªè¨€å’Œæ–¹è¨€ï¼Œå¹¾ä¹æ˜¯ Qwen2.5 çš„ 18 å…†èªæ–™çš„å…©å€ã€‚

### æ¨¡å‹æ¯”è¼ƒçŸ©é™£

| æ¨¡å‹ç³»åˆ— | åƒæ•¸ç¯„åœ | ä¸Šä¸‹æ–‡é•·åº¦ | ä¸»è¦å„ªå‹¢ | æœ€ä½³ä½¿ç”¨å ´æ™¯ |
|----------|----------|------------|----------|--------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | å¹³è¡¡æ€§èƒ½ï¼Œå¤šèªè¨€æ”¯æŒ | ä¸€èˆ¬æ‡‰ç”¨ï¼Œç”Ÿç”¢éƒ¨ç½² |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | ä»£ç¢¼ç”Ÿæˆï¼Œç·¨ç¨‹ | è»Ÿä»¶é–‹ç™¼ï¼Œç·¨ç¨‹è¼”åŠ© |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | æ•¸å­¸æ¨ç† | æ•™è‚²å¹³å°ï¼ŒSTEM æ‡‰ç”¨ |
| **Qwen2.5-VL** | å¤šç¨® | å¯è®Š | è¦–è¦º-èªè¨€ç†è§£ | å¤šæ¨¡æ…‹æ‡‰ç”¨ï¼Œåœ–åƒåˆ†æ |
| **Qwen3** | 0.6B-235B | å¯è®Š | é«˜ç´šæ¨ç†ï¼Œæ€ç¶­æ¨¡å¼ | è¤‡é›œæ¨ç†ï¼Œç ”ç©¶æ‡‰ç”¨ |
| **Qwen3 MoE** | 30B-235B ç¸½è¨ˆ | å¯è®Š | é«˜æ•ˆçš„å¤§è¦æ¨¡æ€§èƒ½ | ä¼æ¥­æ‡‰ç”¨ï¼Œé«˜æ€§èƒ½éœ€æ±‚ |

## æ¨¡å‹é¸æ“‡æŒ‡å—

### åŸºæœ¬æ‡‰ç”¨
- **Qwen2.5-0.5B/1.5B**ï¼šç§»å‹•æ‡‰ç”¨ï¼Œé‚Šç·£è¨­å‚™ï¼Œå¯¦æ™‚æ‡‰ç”¨
- **Qwen2.5-3B/7B**ï¼šä¸€èˆ¬èŠå¤©æ©Ÿå™¨äººï¼Œå…§å®¹ç”Ÿæˆï¼Œå•ç­”ç³»çµ±

### æ•¸å­¸å’Œæ¨ç†ä»»å‹™
- **Qwen2.5-Math**ï¼šæ•¸å­¸å•é¡Œè§£æ±ºå’Œ STEM æ•™è‚²
- **Qwen3 çš„æ€ç¶­æ¨¡å¼**ï¼šéœ€è¦é€æ­¥åˆ†æçš„è¤‡é›œæ¨ç†

### ç·¨ç¨‹å’Œé–‹ç™¼
- **Qwen2.5-Coder**ï¼šä»£ç¢¼ç”Ÿæˆï¼Œèª¿è©¦ï¼Œç·¨ç¨‹è¼”åŠ©
- **Qwen3**ï¼šå…·æœ‰æ¨ç†èƒ½åŠ›çš„é«˜ç´šç·¨ç¨‹ä»»å‹™

### å¤šæ¨¡æ…‹æ‡‰ç”¨
- **Qwen2.5-VL**ï¼šåœ–åƒç†è§£ï¼Œè¦–è¦ºå•ç­”
- **Qwen-Audio**ï¼šéŸ³é »è™•ç†å’ŒèªéŸ³ç†è§£

### ä¼æ¥­éƒ¨ç½²
- **Qwen2.5-32B/72B**ï¼šé«˜æ€§èƒ½èªè¨€ç†è§£
- **Qwen3-235B-A22B**ï¼šæ»¿è¶³é«˜éœ€æ±‚æ‡‰ç”¨çš„æœ€å¤§èƒ½åŠ›

## éƒ¨ç½²å¹³å°èˆ‡å¯ç”¨æ€§
### é›²ç«¯å¹³å°
- **Hugging Face Hub**ï¼šå…¨é¢çš„æ¨¡å‹åº«å’Œç¤¾ç¾¤æ”¯æŒ
- **ModelScope**ï¼šé˜¿é‡Œå·´å·´çš„æ¨¡å‹å¹³å°ï¼Œæä¾›å„ªåŒ–å·¥å…·
- **å¤šç¨®é›²ç«¯ä¾›æ‡‰å•†**ï¼šé€šéæ¨™æº–æ©Ÿå™¨å­¸ç¿’å¹³å°æ”¯æŒ

### æœ¬åœ°é–‹ç™¼æ¡†æ¶
- **Transformers**ï¼šæ¨™æº– Hugging Face é›†æˆï¼Œä¾¿æ–¼éƒ¨ç½²
- **vLLM**ï¼šé«˜æ€§èƒ½æœå‹™ï¼Œé©ç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒ
- **Ollama**ï¼šç°¡åŒ–çš„æœ¬åœ°éƒ¨ç½²å’Œç®¡ç†
- **ONNX Runtime**ï¼šè·¨å¹³å°å„ªåŒ–ï¼Œæ”¯æŒå¤šç¨®ç¡¬ä»¶
- **llama.cpp**ï¼šé«˜æ•ˆçš„ C++ å¯¦ç¾ï¼Œé©ç”¨æ–¼å¤šç¨®å¹³å°

### å­¸ç¿’è³‡æº
- **Qwen æ–‡æª”**ï¼šå®˜æ–¹æ–‡æª”å’Œæ¨¡å‹å¡
- **Hugging Face æ¨¡å‹åº«**ï¼šäº’å‹•æ¼”ç¤ºå’Œç¤¾ç¾¤ç¯„ä¾‹
- **ç ”ç©¶è«–æ–‡**ï¼šarxiv ä¸Šçš„æŠ€è¡“è«–æ–‡ï¼Œæ·±å…¥äº†è§£
- **ç¤¾ç¾¤è«–å£‡**ï¼šæ´»èºçš„ç¤¾ç¾¤æ”¯æŒå’Œè¨è«–

### é–‹å§‹ä½¿ç”¨ Qwen æ¨¡å‹

#### é–‹ç™¼å¹³å°
1. **Hugging Face Transformers**ï¼šä½¿ç”¨æ¨™æº– Python é›†æˆé–‹å§‹
2. **ModelScope**ï¼šæ¢ç´¢é˜¿é‡Œå·´å·´çš„å„ªåŒ–éƒ¨ç½²å·¥å…·
3. **æœ¬åœ°éƒ¨ç½²**ï¼šä½¿ç”¨ Ollama æˆ–ç›´æ¥ Transformers é€²è¡Œæœ¬åœ°æ¸¬è©¦

#### å­¸ç¿’è·¯å¾‘
1. **ç†è§£æ ¸å¿ƒæ¦‚å¿µ**ï¼šå­¸ç¿’ Qwen ç³»åˆ—æ¶æ§‹å’Œèƒ½åŠ›
2. **å˜—è©¦ä¸åŒè®Šé«”**ï¼šæ¸¬è©¦ä¸åŒæ¨¡å‹å¤§å°ï¼Œäº†è§£æ€§èƒ½å–æ¨
3. **å¯¦è¸å¯¦æ–½**ï¼šåœ¨é–‹ç™¼ç’°å¢ƒä¸­éƒ¨ç½²æ¨¡å‹
4. **å„ªåŒ–éƒ¨ç½²**ï¼šé‡å°ç”Ÿç”¢ä½¿ç”¨æ¡ˆä¾‹é€²è¡Œå¾®èª¿

#### æœ€ä½³å¯¦è¸
- **å¾å°é–‹å§‹**ï¼šå¾è¼ƒå°çš„æ¨¡å‹ï¼ˆ1.5B-7Bï¼‰é–‹å§‹åˆæ­¥é–‹ç™¼
- **ä½¿ç”¨èŠå¤©æ¨¡æ¿**ï¼šæ‡‰ç”¨é©ç•¶æ ¼å¼ä»¥ç²å¾—æœ€ä½³çµæœ
- **ç›£æ§è³‡æº**ï¼šè·Ÿè¹¤å…§å­˜ä½¿ç”¨å’Œæ¨ç†é€Ÿåº¦
- **è€ƒæ…®å°ˆæ¥­åŒ–**ï¼šæ ¹æ“šéœ€è¦é¸æ“‡ç‰¹å®šé ˜åŸŸçš„è®Šé«”

## é«˜ç´šä½¿ç”¨æ¨¡å¼

### å¾®èª¿ç¯„ä¾‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### å°ˆæ¥­åŒ–æç¤ºå·¥ç¨‹

**é‡å°è¤‡é›œæ¨ç†ä»»å‹™ï¼š**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**é‡å°å¸¶ä¸Šä¸‹æ–‡çš„ä»£ç¢¼ç”Ÿæˆï¼š**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### å¤šèªè¨€æ‡‰ç”¨

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ä¸­æ–‡)",
        "es": "Spanish (EspaÃ±ol)",
        "fr": "French (FranÃ§ais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (æ—¥æœ¬èª)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ğŸ”§ ç”Ÿç”¢éƒ¨ç½²æ¨¡å¼

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## æ€§èƒ½å„ªåŒ–ç­–ç•¥

### è¨˜æ†¶é«”å„ªåŒ–

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### æ¨ç†å„ªåŒ–

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## æœ€ä½³å¯¦è¸èˆ‡æŒ‡å—

### å®‰å…¨èˆ‡éš±ç§

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### ç›£æ§èˆ‡è©•ä¼°

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## çµè«–

Qwen æ¨¡å‹ç³»åˆ—ä»£è¡¨äº†ä¸€ç¨®å…¨é¢çš„æ–¹å¼ï¼Œè‡´åŠ›æ–¼æ™®åŠ AI æŠ€è¡“ï¼ŒåŒæ™‚åœ¨å¤šæ¨£åŒ–æ‡‰ç”¨ä¸­ä¿æŒç«¶çˆ­æ€§èƒ½ã€‚é€šéå…¶å°é–‹æºå¯ç”¨æ€§ã€å¤šèªè¨€èƒ½åŠ›å’Œéˆæ´»éƒ¨ç½²é¸é …çš„æ‰¿è«¾ï¼ŒQwen ä½¿çµ„ç¹”å’Œé–‹ç™¼è€…èƒ½å¤ åˆ©ç”¨å¼·å¤§çš„ AI èƒ½åŠ›ï¼Œç„¡è«–å…¶è³‡æºæˆ–ç‰¹å®šéœ€æ±‚å¦‚ä½•ã€‚

### é—œéµè¦é»

**é–‹æºå“è¶Š**ï¼šQwen å±•ç¤ºäº†é–‹æºæ¨¡å‹å¯ä»¥åœ¨æ€§èƒ½ä¸Šèˆ‡å°ˆæœ‰æ›¿ä»£å“ç«¶çˆ­ï¼ŒåŒæ™‚æä¾›é€æ˜æ€§ã€å¯å®šåˆ¶æ€§å’Œæ§åˆ¶åŠ›ã€‚

**å¯æ“´å±•æ¶æ§‹**ï¼šå¾ 0.5B åˆ° 235B åƒæ•¸çš„ç¯„åœï¼Œæ”¯æŒå¾ç§»å‹•è¨­å‚™åˆ°ä¼æ¥­é›†ç¾¤çš„å…¨æ–¹ä½è¨ˆç®—ç’°å¢ƒéƒ¨ç½²ã€‚

**å°ˆæ¥­åŒ–èƒ½åŠ›**ï¼šåƒ Qwen-Coderã€Qwen-Math å’Œ Qwen-VL é€™æ¨£çš„ç‰¹å®šé ˜åŸŸè®Šé«”æä¾›å°ˆæ¥­çŸ¥è­˜ï¼ŒåŒæ™‚ä¿æŒä¸€èˆ¬èªè¨€ç†è§£èƒ½åŠ›ã€‚

**å…¨çƒå¯ç”¨æ€§**ï¼šå¼·å¤§çš„å¤šèªè¨€æ”¯æŒæ¶µè“‹ 119+ ç¨®èªè¨€ï¼Œä½¿ Qwen é©åˆåœ‹éš›æ‡‰ç”¨å’Œå¤šæ¨£åŒ–çš„ç”¨æˆ¶ç¾¤ã€‚

**æŒçºŒå‰µæ–°**ï¼šå¾ Qwen 1.0 åˆ° Qwen3 çš„æ¼”é€²ï¼Œé¡¯ç¤ºäº†èƒ½åŠ›ã€æ•ˆç‡å’Œéƒ¨ç½²é¸é …çš„æŒçºŒæ”¹é€²ã€‚

### æœªä¾†å±•æœ›

éš¨è‘— Qwen ç³»åˆ—çš„æŒçºŒç™¼å±•ï¼Œæˆ‘å€‘å¯ä»¥æœŸå¾…ï¼š

- **æ•ˆç‡æå‡**ï¼šæŒçºŒå„ªåŒ–ä»¥ç²å¾—æ›´å¥½çš„æ€§èƒ½åƒæ•¸æ¯”
- **å¤šæ¨¡æ…‹èƒ½åŠ›æ“´å±•**ï¼šæ•´åˆæ›´å…ˆé€²çš„è¦–è¦ºã€éŸ³é »å’Œæ–‡æœ¬è™•ç†
- **æ¨ç†èƒ½åŠ›æ”¹é€²**ï¼šé«˜ç´šæ€ç¶­æ©Ÿåˆ¶å’Œå¤šæ­¥å•é¡Œè§£æ±ºèƒ½åŠ›
- **æ›´å¥½çš„éƒ¨ç½²å·¥å…·**ï¼šé‡å°å¤šæ¨£åŒ–éƒ¨ç½²å ´æ™¯çš„å¢å¼·æ¡†æ¶å’Œå„ªåŒ–å·¥å…·
- **ç¤¾ç¾¤å¢é•·**ï¼šæ“´å±•å·¥å…·ã€æ‡‰ç”¨å’Œç¤¾ç¾¤è²¢ç»çš„ç”Ÿæ…‹ç³»çµ±

### ä¸‹ä¸€æ­¥

ç„¡è«–æ‚¨æ˜¯åœ¨æ§‹å»ºèŠå¤©æ©Ÿå™¨äººã€é–‹ç™¼æ•™è‚²å·¥å…·ã€å‰µå»ºç·¨ç¨‹åŠ©æ‰‹ï¼Œé‚„æ˜¯å¾äº‹å¤šèªè¨€æ‡‰ç”¨ï¼ŒQwen ç³»åˆ—éƒ½æä¾›äº†å…·æœ‰å¼·å¤§ç¤¾ç¾¤æ”¯æŒå’Œå…¨é¢æ–‡æª”çš„å¯æ“´å±•è§£æ±ºæ–¹æ¡ˆã€‚

æ¬²äº†è§£æœ€æ–°æ›´æ–°ã€æ¨¡å‹ç™¼å¸ƒå’Œè©³ç´°æŠ€è¡“æ–‡æª”ï¼Œè«‹è¨ªå• Hugging Face ä¸Šçš„å®˜æ–¹ Qwen è³‡æºåº«ï¼Œä¸¦æ¢ç´¢æ´»èºçš„ç¤¾ç¾¤è¨è«–å’Œç¯„ä¾‹ã€‚

AI é–‹ç™¼çš„æœªä¾†åœ¨æ–¼å¯è¨ªå•ã€é€æ˜ä¸”å¼·å¤§çš„å·¥å…·ï¼Œé€™äº›å·¥å…·èƒ½å¤ ä¿ƒé€²å„è¡Œæ¥­å’Œè¦æ¨¡çš„å‰µæ–°ã€‚Qwen ç³»åˆ—é«”ç¾äº†é€™ä¸€é¡˜æ™¯ï¼Œç‚ºçµ„ç¹”å’Œé–‹ç™¼è€…æä¾›äº†æ§‹å»ºä¸‹ä¸€ä»£ AI é©…å‹•æ‡‰ç”¨çš„åŸºç¤ã€‚

## é™„åŠ è³‡æº

- **å®˜æ–¹æ–‡æª”**ï¼š[Qwen æ–‡æª”](https://qwen.readthedocs.io/)
- **æ¨¡å‹åº«**ï¼š[Hugging Face Qwen é›†åˆ](https://huggingface.co/collections/Qwen/)
- **æŠ€è¡“è«–æ–‡**ï¼š[Qwen ç ”ç©¶å‡ºç‰ˆç‰©](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **ç¤¾ç¾¤**ï¼š[GitHub è¨è«–å’Œå•é¡Œ](https://github.com/QwenLM/)
- **ModelScope å¹³å°**ï¼š[é˜¿é‡Œå·´å·´ ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## å­¸ç¿’æˆæœ

å®Œæˆæ­¤æ¨¡çµ„å¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š

1. è§£é‡‹ Qwen æ¨¡å‹ç³»åˆ—çš„æ¶æ§‹å„ªå‹¢åŠå…¶é–‹æºæ–¹æ³•
2. æ ¹æ“šç‰¹å®šæ‡‰ç”¨éœ€æ±‚å’Œè³‡æºé™åˆ¶é¸æ“‡åˆé©çš„ Qwen è®Šé«”
3. åœ¨å„ç¨®éƒ¨ç½²å ´æ™¯ä¸­å¯¦æ–½ Qwen æ¨¡å‹ä¸¦é€²è¡Œå„ªåŒ–é…ç½®
4. æ‡‰ç”¨é‡åŒ–å’Œå„ªåŒ–æŠ€è¡“ä»¥æå‡ Qwen æ¨¡å‹æ€§èƒ½
5. è©•ä¼° Qwen ç³»åˆ—ä¸­æ¨¡å‹å¤§å°ã€æ€§èƒ½å’Œèƒ½åŠ›ä¹‹é–“çš„å–æ¨

## æ¥ä¸‹ä¾†çš„å…§å®¹

- [03: Gemma ç³»åˆ—åŸºç¤çŸ¥è­˜](03.GemmaFamily.md)

---

**å…è²¬è²æ˜**ï¼š  
æœ¬æ–‡ä»¶å·²ä½¿ç”¨ AI ç¿»è­¯æœå‹™ [Co-op Translator](https://github.com/Azure/co-op-translator) é€²è¡Œç¿»è­¯ã€‚é›–ç„¶æˆ‘å€‘è‡´åŠ›æ–¼æä¾›æº–ç¢ºçš„ç¿»è­¯ï¼Œä½†è«‹æ³¨æ„ï¼Œè‡ªå‹•ç¿»è­¯å¯èƒ½åŒ…å«éŒ¯èª¤æˆ–ä¸æº–ç¢ºä¹‹è™•ã€‚åŸå§‹æ–‡ä»¶çš„æ¯èªç‰ˆæœ¬æ‡‰è¢«è¦–ç‚ºæ¬Šå¨ä¾†æºã€‚å°æ–¼é—œéµè³‡è¨Šï¼Œå»ºè­°ä½¿ç”¨å°ˆæ¥­äººå·¥ç¿»è­¯ã€‚æˆ‘å€‘å°å› ä½¿ç”¨æ­¤ç¿»è­¯è€Œç”¢ç”Ÿçš„ä»»ä½•èª¤è§£æˆ–éŒ¯èª¤è§£é‡‹ä¸æ‰¿æ“”è²¬ä»»ã€‚