<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddaad917d0c16fc3d498a6b4eabc8088",
  "translation_date": "2025-10-09T08:10:32+00:00",
  "source_file": "Workshop/notebooks/quickstart.md",
  "language_code": "mo"
}
-->
# å·¥ä½œåŠç­†è¨˜æœ¬ - å¿«é€Ÿå…¥é–€æŒ‡å—

## ç›®éŒ„

- [å…ˆæ±ºæ¢ä»¶](../../../../Workshop/notebooks)
- [åˆå§‹è¨­ç½®](../../../../Workshop/notebooks)
- [èª²ç¨‹ 04ï¼šæ¨¡å‹æ¯”è¼ƒ](../../../../Workshop/notebooks)
- [èª²ç¨‹ 05ï¼šå¤šä»£ç†å”ä½œå™¨](../../../../Workshop/notebooks)
- [èª²ç¨‹ 06ï¼šåŸºæ–¼æ„åœ–çš„æ¨¡å‹è·¯ç”±](../../../../Workshop/notebooks)
- [ç’°å¢ƒè®Šæ•¸](../../../../Workshop/notebooks)
- [å¸¸ç”¨æŒ‡ä»¤](../../../../Workshop/notebooks)

---

## å…ˆæ±ºæ¢ä»¶

### 1. å®‰è£ Foundry Local

**Windows:**
```bash
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**é©—è­‰å®‰è£ï¼š**
```bash
foundry --version
```

### 2. å®‰è£ Python ä¾è³´é …

```bash
cd Workshop
pip install -r requirements.txt
```

æˆ–å–®ç¨å®‰è£ï¼š
```bash
pip install foundry-local-sdk openai numpy requests
```

---

## åˆå§‹è¨­ç½®

### å•Ÿå‹• Foundry Local æœå‹™

**åœ¨é‹è¡Œä»»ä½•ç­†è¨˜æœ¬ä¹‹å‰å¿…é ˆåŸ·è¡Œï¼š**

```bash
# Start the service
foundry service start

# Verify it's running
foundry service status
```

é æœŸè¼¸å‡ºï¼š
```
âœ… Service started successfully
Endpoint: http://localhost:59959
```

### ä¸‹è¼‰ä¸¦åŠ è¼‰æ¨¡å‹

ç­†è¨˜æœ¬é»˜èªä½¿ç”¨ä»¥ä¸‹æ¨¡å‹ï¼š

```bash
# Download models (first time only - may take several minutes)
foundry model download phi-4-mini
foundry model download qwen2.5-3b
foundry model download phi-3.5-mini
foundry model download qwen2.5-0.5b

# Load models into memory
foundry model run phi-4-mini
foundry model run qwen2.5-3b
foundry model run phi-3.5-mini
```

### é©—è­‰è¨­ç½®

```bash
# List loaded models
foundry model ls

# Check service health
curl http://localhost:59959/v1/models
```

---

## èª²ç¨‹ 04ï¼šæ¨¡å‹æ¯”è¼ƒ

### ç›®çš„
æ¯”è¼ƒå°å‹èªè¨€æ¨¡å‹ï¼ˆSLMï¼‰å’Œå¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹é–“çš„æ€§èƒ½ã€‚

### å¿«é€Ÿè¨­ç½®

```bash
# Start service (if not already running)
foundry service start

# Load required models
foundry model run phi-4-mini
foundry model run qwen2.5-3b
```

### é‹è¡Œç­†è¨˜æœ¬

1. **æ‰“é–‹** `session04_model_compare.ipynb`ï¼ˆåœ¨ VS Code æˆ– Jupyter ä¸­ï¼‰
2. **é‡å•Ÿå…§æ ¸**ï¼ˆKernel â†’ Restart Kernelï¼‰
3. **æŒ‰é †åºé‹è¡Œæ‰€æœ‰å–®å…ƒæ ¼**

### é—œéµé…ç½®

**é»˜èªæ¨¡å‹ï¼š**
- **SLM:** `phi-4-mini`ï¼ˆç´„ 4GB RAMï¼Œé€Ÿåº¦è¼ƒå¿«ï¼‰
- **LLM:** `qwen2.5-3b`ï¼ˆç´„ 3GB RAMï¼Œå…§å­˜å„ªåŒ–ï¼‰

**ç’°å¢ƒè®Šæ•¸ï¼ˆå¯é¸ï¼‰ï¼š**
```python
import os
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'
```

### é æœŸè¼¸å‡º

```
================================================================================
COMPARISON SUMMARY
================================================================================
Alias                Latency(s)      Tokens     Route               
--------------------------------------------------------------------------------
phi-4-mini           1.234           150        chat.completions    
qwen2.5-3b           2.456           180        chat.completions    
================================================================================

ğŸ’¡ SLM is 1.99x faster than LLM for this prompt
```

### è‡ªå®šç¾©

**ä½¿ç”¨ä¸åŒçš„æ¨¡å‹ï¼š**
```python
os.environ['SLM_ALIAS'] = 'phi-3.5-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-1.5b'
```

**è‡ªå®šç¾©æç¤ºï¼š**
```python
os.environ['COMPARE_PROMPT'] = 'Explain quantum computing in simple terms'
```

### é©—è­‰æ¸…å–®

- [ ] å–®å…ƒæ ¼ 12 é¡¯ç¤ºæ­£ç¢ºçš„æ¨¡å‹ï¼ˆphi-4-mini, qwen2.5-3bï¼‰
- [ ] å–®å…ƒæ ¼ 12 é¡¯ç¤ºæ­£ç¢ºçš„ç«¯é»ï¼ˆç«¯å£ 59959ï¼‰
- [ ] å–®å…ƒæ ¼ 16 è¨ºæ–·é€šéï¼ˆâœ… æœå‹™æ­£åœ¨é‹è¡Œï¼‰
- [ ] å–®å…ƒæ ¼ 20 é æª¢é€šéï¼ˆå…©å€‹æ¨¡å‹å‡æ­£å¸¸ï¼‰
- [ ] å–®å…ƒæ ¼ 22 æ¯”è¼ƒå®Œæˆä¸¦é¡¯ç¤ºå»¶é²å€¼
- [ ] å–®å…ƒæ ¼ 24 é©—è­‰é¡¯ç¤º ğŸ‰ ALL CHECKS PASSED!

### æ™‚é–“ä¼°ç®—
- **é¦–æ¬¡é‹è¡Œï¼š** 5-10 åˆ†é˜ï¼ˆåŒ…æ‹¬æ¨¡å‹ä¸‹è¼‰ï¼‰
- **å¾ŒçºŒé‹è¡Œï¼š** 1-2 åˆ†é˜

---

## èª²ç¨‹ 05ï¼šå¤šä»£ç†å”ä½œå™¨

### ç›®çš„
ä½¿ç”¨ Foundry Local SDK å±•ç¤ºå¤šä»£ç†å”ä½œï¼Œè®“ä»£ç†å…±åŒå®Œæˆç²¾ç…‰çš„è¼¸å‡ºã€‚

### å¿«é€Ÿè¨­ç½®

```bash
# Start service
foundry service start

# Load models
foundry model run phi-4-mini  # Primary model
foundry model run qwen2.5-7b  # Optional: higher quality editor
```

### é‹è¡Œç­†è¨˜æœ¬

1. **æ‰“é–‹** `session05_agents_orchestrator.ipynb`
2. **é‡å•Ÿå…§æ ¸**
3. **æŒ‰é †åºé‹è¡Œæ‰€æœ‰å–®å…ƒæ ¼**

### é—œéµé…ç½®

**é»˜èªè¨­ç½®ï¼ˆå…©å€‹ä»£ç†ä½¿ç”¨ç›¸åŒæ¨¡å‹ï¼‰ï¼š**
```python
PRIMARY_ALIAS = 'phi-4-mini'
EDITOR_ALIAS = 'phi-4-mini'  # Uses same model
```

**é«˜ç´šè¨­ç½®ï¼ˆä¸åŒæ¨¡å‹ï¼‰ï¼š**
```python
import os
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'     # Fast for research
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'      # High quality for editing
```

### æ¶æ§‹

```
User Question
    â†“
Researcher Agent (phi-4-mini)
  â†’ Gathers bullet points
    â†“
Editor Agent (phi-4-mini or qwen2.5-7b)
  â†’ Refines into executive summary
    â†“
Final Output
```

### é æœŸè¼¸å‡º

```
================================================================================
[Pipeline] Question: Explain why edge AI matters for compliance.
================================================================================

[Stage 1: Research]
Output: â€¢ Edge AI processes data locally, reducing transmission...

[Stage 2: Editorial Refinement]
Output: Executive Summary: Edge AI enhances compliance by keeping data...

[FINAL OUTPUT]
Executive Summary: Edge AI enhances compliance by keeping sensitive data 
on-premises and reduces latency through local processing.

[METADATA]
Models used: {'researcher': 'phi-4-mini', 'editor': 'phi-4-mini'}
```

### æ“´å±•

**æ·»åŠ æ›´å¤šä»£ç†ï¼š**
```python
critic = Agent(
    name='Critic',
    system='Review content for accuracy',
    client=client,
    model_id=model_id
)
```

**æ‰¹é‡æ¸¬è©¦ï¼š**
```python
test_questions = [
    "What are benefits of local AI?",
    "How does RAG improve accuracy?",
]

for q in test_questions:
    result = pipeline(q, verbose=False)
    print(result['final'])
```

### æ™‚é–“ä¼°ç®—
- **é¦–æ¬¡é‹è¡Œï¼š** 3-5 åˆ†é˜
- **å¾ŒçºŒé‹è¡Œï¼š** æ¯å€‹å•é¡Œ 1-2 åˆ†é˜

---

## èª²ç¨‹ 06ï¼šåŸºæ–¼æ„åœ–çš„æ¨¡å‹è·¯ç”±

### ç›®çš„
æ ¹æ“šæª¢æ¸¬åˆ°çš„æ„åœ–æ™ºèƒ½åœ°å°‡æç¤ºè·¯ç”±åˆ°å°ˆé–€çš„æ¨¡å‹ã€‚

### å¿«é€Ÿè¨­ç½®

```bash
# Start service
foundry service start

# Load all routing models (CPU variants recommended)
foundry model run phi-4-mini-cpu
foundry model run qwen2.5-0.5b-cpu
foundry model run phi-3.5-mini-cpu
```

**æ³¨æ„ï¼š** èª²ç¨‹ 06 é»˜èªä½¿ç”¨ CPU æ¨¡å‹ä»¥å¯¦ç¾æœ€å¤§å…¼å®¹æ€§ã€‚

### é‹è¡Œç­†è¨˜æœ¬

1. **æ‰“é–‹** `session06_models_router.ipynb`
2. **é‡å•Ÿå…§æ ¸**
3. **æŒ‰é †åºé‹è¡Œæ‰€æœ‰å–®å…ƒæ ¼**

### é—œéµé…ç½®

**é»˜èªç›®éŒ„ï¼ˆCPU æ¨¡å‹ï¼‰ï¼š**
```python
CATALOG = {
    'phi-4-mini-cpu': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b-cpu': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini-cpu': {'capabilities':['code','refactor'],'priority':3},
}
```

**æ›¿ä»£é¸é …ï¼ˆGPU æ¨¡å‹ï¼‰ï¼š**
```python
# Uncomment GPU catalog in Cell #6 if you have sufficient VRAM (8GB+)
CATALOG = {
    'phi-4-mini': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini': {'capabilities':['code','refactor'],'priority':3},
}
```

### æ„åœ–æª¢æ¸¬

è·¯ç”±å™¨ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æ¨¡å¼æª¢æ¸¬æ„åœ–ï¼š

| æ„åœ– | æ¨¡å¼ç¤ºä¾‹ | è·¯ç”±åˆ° |
|------|----------|--------|
| `code` | "refactor", "implement function" | phi-3.5-mini-cpu |
| `classification` | "categorize", "classify this" | qwen2.5-0.5b-cpu |
| `summarize` | "summarize", "tl;dr" | phi-4-mini-cpu |
| `general` | å…¶ä»–æ‰€æœ‰ | phi-4-mini-cpu |

### é æœŸè¼¸å‡º

```
âœ“ Using CPU-optimized models (default configuration)
  Models: phi-4-mini-cpu, qwen2.5-0.5b-cpu, phi-3.5-mini-cpu

Routing prompts to specialized models...
============================================================

Prompt: Refactor this Python function for readability
  Intent: code           | Model: phi-3.5-mini-cpu
  Output: Here's a refactored version...
  Tokens: 156

Prompt: Categorize this email as urgent or normal
  Intent: classification | Model: qwen2.5-0.5b-cpu
  Output: Category: Normal
  Tokens: 45

âœ“ Success! All prompts routed correctly.
```

### è‡ªå®šç¾©

**æ·»åŠ è‡ªå®šç¾©æ„åœ–ï¼š**
```python
import re

# Add to RULES
RULES.append((re.compile('translate|ç¿»è¯‘', re.I), 'translation'))

# Add capability to catalog
CATALOG['phi-4-mini-cpu']['capabilities'].append('translation')
```

**å•Ÿç”¨ä»¤ç‰Œè·Ÿè¹¤ï¼š**
```python
import os
os.environ['SHOW_USAGE'] = '1'
```

### åˆ‡æ›åˆ° GPU æ¨¡å‹

å¦‚æœæ‚¨æœ‰ 8GB+ VRAMï¼š

1. åœ¨ **å–®å…ƒæ ¼ #6** ä¸­ï¼Œè¨»é‡‹æ‰ CPU ç›®éŒ„
2. å–æ¶ˆè¨»é‡‹ GPU ç›®éŒ„
3. åŠ è¼‰ GPU æ¨¡å‹ï¼š
   ```bash
   foundry model run phi-4-mini
   foundry model run qwen2.5-0.5b
   foundry model run phi-3.5-mini
   ```
4. é‡å•Ÿå…§æ ¸ä¸¦é‡æ–°é‹è¡Œç­†è¨˜æœ¬

### æ™‚é–“ä¼°ç®—
- **é¦–æ¬¡é‹è¡Œï¼š** 5-10 åˆ†é˜ï¼ˆåŠ è¼‰æ¨¡å‹ï¼‰
- **å¾ŒçºŒé‹è¡Œï¼š** æ¯æ¬¡æ¸¬è©¦ 30-60 ç§’

---

## ç’°å¢ƒè®Šæ•¸

### å…¨å±€é…ç½®

åœ¨å•Ÿå‹• Jupyter/VS Code ä¹‹å‰è¨­ç½®ï¼š

**Windowsï¼ˆå‘½ä»¤æç¤ºç¬¦ï¼‰ï¼š**
```cmd
set FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
set SHOW_USAGE=1
set RETRY_ON_FAIL=1
```

**Windowsï¼ˆPowerShellï¼‰ï¼š**
```powershell
$env:FOUNDRY_LOCAL_ENDPOINT="http://localhost:59959/v1"
$env:SHOW_USAGE="1"
$env:RETRY_ON_FAIL="1"
```

**macOS/Linuxï¼š**
```bash
export FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
export SHOW_USAGE=1
export RETRY_ON_FAIL=1
```

### ç­†è¨˜æœ¬å…§é…ç½®

åœ¨ä»»ä½•ç­†è¨˜æœ¬çš„é–‹é ­è¨­ç½®ï¼š

```python
import os

# Foundry Local configuration
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'

# Model selection
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'

# Agent models
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'

# Debugging
os.environ['SHOW_USAGE'] = '1'       # Show token usage
os.environ['RETRY_ON_FAIL'] = '1'    # Enable retries
os.environ['RETRY_BACKOFF'] = '2.0'  # Retry delay
```

---

## å¸¸ç”¨æŒ‡ä»¤

### æœå‹™ç®¡ç†

```bash
# Start service
foundry service start

# Check status
foundry service status

# Stop service
foundry service stop

# View logs
foundry service logs
```

### æ¨¡å‹ç®¡ç†

```bash
# List all available models in catalog
foundry model catalog

# List loaded models
foundry model ls

# Download a model
foundry model download phi-4-mini

# Load a model
foundry model run phi-4-mini

# Unload a model
foundry model unload phi-4-mini

# Remove a model
foundry model remove phi-4-mini

# Get model info
foundry model info phi-4-mini
```

### æ¸¬è©¦ç«¯é»

```bash
# Check service health
curl http://localhost:59959/health

# List available models via API
curl http://localhost:59959/v1/models

# Test model completion
curl http://localhost:59959/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role":"user","content":"Hello"}],
    "max_tokens": 50
  }'
```

### è¨ºæ–·æŒ‡ä»¤

```bash
# Check everything
foundry --version
foundry service status
foundry model ls
foundry device info

# GPU status (NVIDIA)
nvidia-smi

# NPU status (Qualcomm)
foundry device info
```

---

## æœ€ä½³å¯¦è¸

### é–‹å§‹ä»»ä½•ç­†è¨˜æœ¬ä¹‹å‰

1. **æª¢æŸ¥æœå‹™æ˜¯å¦æ­£åœ¨é‹è¡Œï¼š**
   ```bash
   foundry service status
   ```

2. **é©—è­‰æ¨¡å‹æ˜¯å¦å·²åŠ è¼‰ï¼š**
   ```bash
   foundry model ls
   ```

3. **é‡å•Ÿç­†è¨˜æœ¬å…§æ ¸** å¦‚æœé‡æ–°é‹è¡Œ

4. **æ¸…é™¤æ‰€æœ‰è¼¸å‡º** ä»¥ç²å¾—ä¹¾æ·¨çš„é‹è¡Œ

### è³‡æºç®¡ç†

1. **é»˜èªä½¿ç”¨ CPU æ¨¡å‹** ä»¥ç¢ºä¿å…¼å®¹æ€§
2. **åƒ…åœ¨æ“æœ‰ 8GB+ VRAM æ™‚åˆ‡æ›åˆ° GPU æ¨¡å‹**
3. **åœ¨é‹è¡Œå‰é—œé–‰å…¶ä»– GPU æ‡‰ç”¨ç¨‹åº**
4. **åœ¨ç­†è¨˜æœ¬æœƒè©±ä¹‹é–“ä¿æŒæœå‹™é‹è¡Œ**
5. **ä½¿ç”¨ä»»å‹™ç®¡ç†å™¨ / nvidia-smi ç›£æ§è³‡æºä½¿ç”¨æƒ…æ³**

### ç–‘é›£æ’è§£

1. **åœ¨èª¿è©¦ä»£ç¢¼ä¹‹å‰ï¼Œå§‹çµ‚å…ˆæª¢æŸ¥æœå‹™**
2. **å¦‚æœçœ‹åˆ°éæ™‚çš„é…ç½®ï¼Œè«‹é‡å•Ÿå…§æ ¸**
3. **åœ¨ä»»ä½•æ›´æ”¹å¾Œé‡æ–°é‹è¡Œè¨ºæ–·å–®å…ƒæ ¼**
4. **æª¢æŸ¥æ¨¡å‹åç¨±æ˜¯å¦èˆ‡åŠ è¼‰çš„åŒ¹é…**
5. **é©—è­‰ç«¯é»ç«¯å£æ˜¯å¦èˆ‡æœå‹™ç‹€æ…‹åŒ¹é…**

---

## å¿«é€Ÿåƒè€ƒï¼šæ¨¡å‹åˆ¥å

### å¸¸è¦‹æ¨¡å‹

| åˆ¥å | å¤§å° | æœ€é©ç”¨æ–¼ | RAM/VRAM | è®Šé«” |
|------|------|----------|----------|------|
| `phi-4-mini` | ~4B | ä¸€èˆ¬èŠå¤©ã€æ‘˜è¦ | 4-6GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `phi-3.5-mini` | ~3.5B | ä»£ç¢¼ç”Ÿæˆã€é‡æ§‹ | 3-5GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `qwen2.5-3b` | ~3B | ä¸€èˆ¬ä»»å‹™ã€é«˜æ•ˆ | 3-4GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-1.5b` | ~1.5B | å¿«é€Ÿã€ä½è³‡æº | 2-3GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-0.5b` | ~0.5B | åˆ†é¡ã€è³‡æºéœ€æ±‚ä½ | 1-2GB | `-cpu`, `-cuda-gpu` |

### è®Šé«”å‘½å

- **åŸºç¤åç¨±**ï¼ˆä¾‹å¦‚ï¼Œ`phi-4-mini`ï¼‰ï¼šè‡ªå‹•é¸æ“‡æœ€é©åˆæ‚¨ç¡¬ä»¶çš„è®Šé«”
- **`-cpu`**ï¼šCPU å„ªåŒ–ï¼Œé©ç”¨æ–¼æ‰€æœ‰è¨­å‚™
- **`-cuda-gpu`**ï¼šNVIDIA GPU å„ªåŒ–ï¼Œéœ€è¦ 8GB+ VRAM
- **`-npu`**ï¼šQualcomm NPU å„ªåŒ–ï¼Œéœ€è¦ NPU é©…å‹•ç¨‹åº

**å»ºè­°ï¼š** ä½¿ç”¨åŸºç¤åç¨±ï¼ˆç„¡å¾Œç¶´ï¼‰ï¼Œè®“ Foundry Local è‡ªå‹•é¸æ“‡æœ€ä½³è®Šé«”ã€‚

---

## æˆåŠŸæŒ‡æ¨™

ç•¶æ‚¨çœ‹åˆ°ä»¥ä¸‹å…§å®¹æ™‚ï¼Œèªªæ˜å·²æº–å‚™å°±ç·’ï¼š

âœ… `foundry service status` é¡¯ç¤º "running"  
âœ… `foundry model ls` é¡¯ç¤ºæ‰€éœ€çš„æ¨¡å‹  
âœ… æœå‹™å¯åœ¨æ­£ç¢ºçš„ç«¯é»è¨ªå•  
âœ… å¥åº·æª¢æŸ¥è¿”å› 200 OK  
âœ… ç­†è¨˜æœ¬è¨ºæ–·å–®å…ƒæ ¼é€šé  
âœ… è¼¸å‡ºä¸­ç„¡é€£æ¥éŒ¯èª¤  

---

## ç²å–å¹«åŠ©

### æ–‡ä»¶
- **ä¸»å­˜å„²åº«ï¼š** https://github.com/microsoft/Foundry-Local  
- **Python SDKï¼š** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python  
- **CLI åƒè€ƒï¼š** https://github.com/microsoft/Foundry-Local/blob/main/docs/reference/reference-cli.md  
- **ç–‘é›£æ’è§£ï¼š** è«‹åƒé–±æ­¤ç›®éŒ„ä¸­çš„ `troubleshooting.md`  

### GitHub å•é¡Œ
- https://github.com/microsoft/Foundry-Local/issues  
- https://github.com/microsoft/edgeai-for-beginners/issues  

---

**æœ€å¾Œæ›´æ–°æ—¥æœŸï¼š** 2025 å¹´ 10 æœˆ 8 æ—¥  
**ç‰ˆæœ¬ï¼š** Workshop Notebooks 2.0

---

**å…è²¬è²æ˜**ï¼š  
æœ¬æ–‡ä»¶å·²ä½¿ç”¨ AI ç¿»è­¯æœå‹™ [Co-op Translator](https://github.com/Azure/co-op-translator) é€²è¡Œç¿»è­¯ã€‚å„˜ç®¡æˆ‘å€‘åŠªåŠ›ç¢ºä¿ç¿»è­¯çš„æº–ç¢ºæ€§ï¼Œä½†è«‹æ³¨æ„ï¼Œè‡ªå‹•ç¿»è­¯å¯èƒ½åŒ…å«éŒ¯èª¤æˆ–ä¸æº–ç¢ºä¹‹è™•ã€‚åŸå§‹æ–‡ä»¶çš„æ¯èªç‰ˆæœ¬æ‡‰è¢«è¦–ç‚ºæ¬Šå¨ä¾†æºã€‚å°æ–¼é—œéµè³‡è¨Šï¼Œå»ºè­°å°‹æ±‚å°ˆæ¥­äººå·¥ç¿»è­¯ã€‚æˆ‘å€‘å°å› ä½¿ç”¨æ­¤ç¿»è­¯è€Œå¼•èµ·çš„ä»»ä½•èª¤è§£æˆ–éŒ¯èª¤è§£é‡‹ä¸æ‰¿æ“”è²¬ä»»ã€‚