<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-18T14:39:00+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "id"
}
-->
# Bagian 3: Microsoft Olive Optimization Suite

## Daftar Isi
1. [Pendahuluan](../../../Module04)
2. [Apa itu Microsoft Olive?](../../../Module04)
3. [Instalasi](../../../Module04)
4. [Panduan Memulai Cepat](../../../Module04)
5. [Contoh: Mengonversi Qwen3 ke ONNX INT4](../../../Module04)
6. [Penggunaan Lanjutan](../../../Module04)
7. [Praktik Terbaik](../../../Module04)
8. [Pemecahan Masalah](../../../Module04)
9. [Sumber Daya Tambahan](../../../Module04)

## Pendahuluan

Microsoft Olive adalah toolkit optimasi model yang kuat dan mudah digunakan, yang dirancang untuk perangkat keras tertentu. Olive mempermudah proses optimasi model pembelajaran mesin untuk diterapkan di berbagai platform perangkat keras. Baik Anda menargetkan CPU, GPU, atau akselerator AI khusus, Olive membantu Anda mencapai kinerja optimal sambil mempertahankan akurasi model.

## Apa itu Microsoft Olive?

Olive adalah alat optimasi model yang dirancang untuk perangkat keras tertentu, yang menggabungkan teknik-teknik terdepan di industri dalam kompresi model, optimasi, dan kompilasi. Olive bekerja dengan ONNX Runtime sebagai solusi optimasi inferensi E2E.

### Fitur Utama

- **Optimasi Perangkat Keras**: Secara otomatis memilih teknik optimasi terbaik untuk perangkat keras target Anda
- **40+ Komponen Optimasi Bawaan**: Meliputi kompresi model, kuantisasi, optimasi grafik, dan lainnya
- **Antarmuka CLI yang Mudah**: Perintah sederhana untuk tugas optimasi umum
- **Dukungan Multi-Kerangka**: Bekerja dengan PyTorch, model Hugging Face, dan ONNX
- **Dukungan Model Populer**: Olive dapat secara otomatis mengoptimalkan arsitektur model populer seperti Llama, Phi, Qwen, Gemma, dll secara langsung

### Manfaat

- **Pengurangan Waktu Pengembangan**: Tidak perlu bereksperimen secara manual dengan berbagai teknik optimasi
- **Peningkatan Kinerja**: Peningkatan kecepatan yang signifikan (hingga 6x dalam beberapa kasus)
- **Penerapan Lintas Platform**: Model yang dioptimalkan dapat bekerja di berbagai perangkat keras dan sistem operasi
- **Akurasi Terjaga**: Optimasi mempertahankan kualitas model sambil meningkatkan kinerja

## Instalasi

### Prasyarat

- Python 3.8 atau lebih tinggi
- Pengelola paket pip
- Lingkungan virtual (disarankan)

### Instalasi Dasar

Buat dan aktifkan lingkungan virtual:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Instal Olive dengan fitur optimasi otomatis:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Dependensi Opsional

Olive menawarkan berbagai dependensi opsional untuk fitur tambahan:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Verifikasi Instalasi

```bash
olive --help
```

Jika berhasil, Anda akan melihat pesan bantuan CLI Olive.

## Panduan Memulai Cepat

### Optimasi Pertama Anda

Mari kita optimalkan model bahasa kecil menggunakan fitur optimasi otomatis Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Apa yang Dilakukan Perintah Ini

Proses optimasi melibatkan: mendapatkan model dari cache lokal, menangkap Grafik ONNX dan menyimpan bobot dalam file data ONNX, mengoptimalkan Grafik ONNX, dan mengkuantisasi model ke int4 menggunakan metode RTN.

### Penjelasan Parameter Perintah

- `--model_name_or_path`: Identifikasi model Hugging Face atau jalur lokal
- `--output_path`: Direktori tempat model yang dioptimalkan akan disimpan
- `--device`: Perangkat target (cpu, gpu)
- `--provider`: Penyedia eksekusi (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Gunakan ONNX Runtime Generate AI untuk inferensi
- `--precision`: Presisi kuantisasi (int4, int8, fp16)
- `--log_level`: Tingkat detail log (0=minimal, 1=rinci)

## Contoh: Mengonversi Qwen3 ke ONNX INT4

Berdasarkan contoh Hugging Face yang disediakan di [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), berikut cara mengoptimalkan model Qwen3:

### Langkah 1: Unduh Model (Opsional)

Untuk meminimalkan waktu unduh, cache hanya file yang penting:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Langkah 2: Optimalkan Model Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Langkah 3: Uji Model yang Dioptimalkan

Buat skrip Python sederhana untuk menguji model yang dioptimalkan:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktur Output

Setelah optimasi, direktori output Anda akan berisi:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Penggunaan Lanjutan

### File Konfigurasi

Untuk alur kerja optimasi yang lebih kompleks, Anda dapat menggunakan file konfigurasi JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Jalankan dengan konfigurasi:

```bash
olive run --config config.json
```

### Optimasi GPU

Untuk optimasi GPU CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Untuk DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fine-tuning dengan Olive

Olive juga mendukung fine-tuning model:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Praktik Terbaik

### 1. Pemilihan Model
- Mulailah dengan model yang lebih kecil untuk pengujian (misalnya, parameter 0.5B-7B)
- Pastikan arsitektur model target Anda didukung oleh Olive

### 2. Pertimbangan Perangkat Keras
- Sesuaikan target optimasi Anda dengan perangkat keras penerapan
- Gunakan optimasi GPU jika Anda memiliki perangkat keras yang kompatibel dengan CUDA
- Pertimbangkan DirectML untuk mesin Windows dengan grafis terintegrasi

### 3. Pemilihan Presisi
- **INT4**: Kompresi maksimum, sedikit kehilangan akurasi
- **INT8**: Keseimbangan ukuran dan akurasi yang baik
- **FP16**: Kehilangan akurasi minimal, pengurangan ukuran sedang

### 4. Pengujian dan Validasi
- Selalu uji model yang dioptimalkan dengan kasus penggunaan spesifik Anda
- Bandingkan metrik kinerja (latensi, throughput, akurasi)
- Gunakan data input yang representatif untuk evaluasi

### 5. Optimasi Iteratif
- Mulailah dengan optimasi otomatis untuk hasil cepat
- Gunakan file konfigurasi untuk kontrol yang lebih rinci
- Bereksperimen dengan berbagai langkah optimasi

## Pemecahan Masalah

### Masalah Umum

#### 1. Masalah Instalasi
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Masalah CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Masalah Memori
- Gunakan ukuran batch yang lebih kecil selama optimasi
- Coba kuantisasi dengan presisi yang lebih tinggi terlebih dahulu (int8 daripada int4)
- Pastikan ruang disk yang cukup untuk cache model

#### 4. Kesalahan Pemuatan Model
- Verifikasi jalur model dan izin akses
- Periksa apakah model memerlukan `trust_remote_code=True`
- Pastikan semua file model yang diperlukan telah diunduh

### Mendapatkan Bantuan

- **Dokumentasi**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Contoh**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Sumber Daya Tambahan

### Tautan Resmi
- **Repositori GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Dokumentasi ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Contoh Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Contoh Komunitas
- **Jupyter Notebooks**: Tersedia di repositori GitHub Olive
- **Ekstensi VS Code**: Ekstensi AI Toolkit menggunakan Olive untuk optimasi model
- **Postingan Blog**: Blog Open Source Microsoft memiliki tutorial Olive yang mendetail

### Alat Terkait
- **ONNX Runtime**: Mesin inferensi berkinerja tinggi
- **Hugging Face Transformers**: Sumber banyak model yang kompatibel
- **Azure Machine Learning**: Alur kerja optimasi berbasis cloud

## ➡️ Langkah Selanjutnya

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berusaha untuk memberikan hasil yang akurat, harap diperhatikan bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang otoritatif. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa penerjemahan profesional oleh manusia. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang keliru yang timbul dari penggunaan terjemahan ini.