<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "76b134be93f45283a2df8f8a93717d06",
  "translation_date": "2025-10-17T10:08:34+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "ro"
}
-->
# SecÈ›iunea 1: Fundamentele EdgeAI

EdgeAI reprezintÄƒ o schimbare de paradigmÄƒ Ã®n implementarea inteligenÈ›ei artificiale, aducÃ¢nd capabilitÄƒÈ›ile AI direct pe dispozitivele de la marginea reÈ›elei, Ã®n loc sÄƒ se bazeze exclusiv pe procesarea Ã®n cloud. Este important sÄƒ Ã®nÈ›elegem cum EdgeAI permite procesarea localÄƒ a AI pe dispozitive cu resurse limitate, menÈ›inÃ¢nd Ã®n acelaÈ™i timp o performanÈ›Äƒ rezonabilÄƒ È™i abordÃ¢nd provocÄƒri precum confidenÈ›ialitatea, latenÈ›a È™i funcÈ›ionalitatea offline.

## Introducere

Ãn aceastÄƒ lecÈ›ie, vom explora EdgeAI È™i conceptele sale fundamentale. Vom acoperi paradigma tradiÈ›ionalÄƒ de calcul AI, provocÄƒrile calculului la marginea reÈ›elei, tehnologiile cheie care permit EdgeAI È™i aplicaÈ›iile practice Ã®n diverse industrii.

## Obiective de Ã®nvÄƒÈ›are

La finalul acestei lecÈ›ii, veÈ›i putea:

- ÃnÈ›elege diferenÈ›a dintre abordÄƒrile tradiÈ›ionale bazate pe cloud È™i cele bazate pe EdgeAI.
- Identifica tehnologiile cheie care permit procesarea AI pe dispozitivele de la marginea reÈ›elei.
- RecunoaÈ™te beneficiile È™i limitÄƒrile implementÄƒrilor EdgeAI.
- Aplica cunoÈ™tinÈ›ele despre EdgeAI Ã®n scenarii È™i cazuri de utilizare din lumea realÄƒ.

## ÃnÈ›elegerea paradigmei tradiÈ›ionale de calcul AI

Ãn mod tradiÈ›ional, aplicaÈ›iile AI generative se bazeazÄƒ pe infrastructuri de calcul de Ã®naltÄƒ performanÈ›Äƒ pentru a rula eficient modele de limbaj mari (LLMs). OrganizaÈ›iile implementeazÄƒ de obicei aceste modele pe clustere GPU Ã®n medii cloud, accesÃ¢nd capabilitÄƒÈ›ile lor prin interfeÈ›e API.

Acest model centralizat funcÈ›ioneazÄƒ bine pentru multe aplicaÈ›ii, dar are limitÄƒri inerente Ã®n scenariile de calcul la marginea reÈ›elei. Abordarea convenÈ›ionalÄƒ implicÄƒ trimiterea interogÄƒrilor utilizatorului cÄƒtre servere la distanÈ›Äƒ, procesarea acestora folosind hardware puternic È™i returnarea rezultatelor prin internet. DeÈ™i aceastÄƒ metodÄƒ oferÄƒ acces la modele de ultimÄƒ generaÈ›ie, creeazÄƒ dependenÈ›e de conectivitatea la internet, introduce preocupÄƒri legate de latenÈ›Äƒ È™i ridicÄƒ probleme de confidenÈ›ialitate atunci cÃ¢nd datele sensibile trebuie transmise cÄƒtre servere externe.

ExistÄƒ cÃ¢teva concepte de bazÄƒ pe care trebuie sÄƒ le Ã®nÈ›elegem atunci cÃ¢nd lucrÄƒm cu paradigmele tradiÈ›ionale de calcul AI, È™i anume:

- **â˜ï¸ Procesare bazatÄƒ pe cloud**: Modelele AI ruleazÄƒ pe infrastructuri server puternice cu resurse computaÈ›ionale ridicate.
- **ğŸ”Œ Acces bazat pe API**: AplicaÈ›iile acceseazÄƒ capabilitÄƒÈ›ile AI prin apeluri API la distanÈ›Äƒ, mai degrabÄƒ decÃ¢t prin procesare localÄƒ.
- **ğŸ›ï¸ Gestionarea centralizatÄƒ a modelelor**: Modelele sunt Ã®ntreÈ›inute È™i actualizate centralizat, asigurÃ¢nd consistenÈ›a, dar necesitÃ¢nd conectivitate la reÈ›ea.
- **ğŸ“ˆ Scalabilitatea resurselor**: Infrastructura cloud poate scala dinamic pentru a face faÈ›Äƒ cerinÈ›elor computaÈ›ionale variabile.

## ProvocÄƒrile calculului la marginea reÈ›elei

Dispozitivele de la marginea reÈ›elei, cum ar fi laptopurile, telefoanele mobile È™i dispozitivele Internet of Things (IoT), precum Raspberry Pi È™i NVIDIA Orin Nano, prezintÄƒ constrÃ¢ngeri computaÈ›ionale unice. Aceste dispozitive au, de obicei, putere de procesare, memorie È™i resurse energetice limitate comparativ cu infrastructura centrelor de date.

Rularea LLM-urilor tradiÈ›ionale pe astfel de dispozitive a fost istoric o provocare din cauza acestor limitÄƒri hardware. Cu toate acestea, nevoia de procesare AI la marginea reÈ›elei a devenit din ce Ã®n ce mai importantÄƒ Ã®n diverse scenarii. LuaÈ›i Ã®n considerare situaÈ›iile Ã®n care conectivitatea la internet este nesigurÄƒ sau indisponibilÄƒ, cum ar fi site-uri industriale izolate, vehicule Ã®n tranzit sau zone cu acoperire slabÄƒ a reÈ›elei. Ãn plus, aplicaÈ›iile care necesitÄƒ standarde ridicate de securitate, cum ar fi dispozitivele medicale, sistemele financiare sau aplicaÈ›iile guvernamentale, pot necesita procesarea datelor sensibile local pentru a menÈ›ine confidenÈ›ialitatea È™i cerinÈ›ele de conformitate.

### ConstrÃ¢ngeri cheie ale calculului la marginea reÈ›elei

Mediile de calcul la marginea reÈ›elei se confruntÄƒ cu mai multe constrÃ¢ngeri fundamentale pe care soluÈ›iile tradiÈ›ionale de AI bazate pe cloud nu le Ã®ntÃ¢mpinÄƒ:

- **Putere de procesare limitatÄƒ**: Dispozitivele de la marginea reÈ›elei au, de obicei, mai puÈ›ine nuclee CPU È™i viteze de ceas mai mici comparativ cu hardware-ul de nivel server.
- **ConstrÃ¢ngeri de memorie**: RAM-ul disponibil È™i capacitatea de stocare sunt semnificativ reduse pe dispozitivele de la marginea reÈ›elei.
- **LimitÄƒri energetice**: Dispozitivele alimentate de baterii trebuie sÄƒ echilibreze performanÈ›a cu consumul de energie pentru o funcÈ›ionare extinsÄƒ.
- **Gestionarea termicÄƒ**: Formele compacte limiteazÄƒ capacitÄƒÈ›ile de rÄƒcire, afectÃ¢nd performanÈ›a susÈ›inutÄƒ sub sarcinÄƒ.

## Ce este EdgeAI?

### Concept: Definirea Edge AI

Edge AI se referÄƒ la implementarea È™i execuÈ›ia algoritmilor de inteligenÈ›Äƒ artificialÄƒ direct pe dispozitivele de la marginea reÈ›eleiâ€”hardware-ul fizic care existÄƒ la "marginea" reÈ›elei, aproape de locul unde sunt generate È™i colectate datele. Aceste dispozitive includ smartphone-uri, senzori IoT, camere inteligente, vehicule autonome, dispozitive purtabile È™i echipamente industriale. Spre deosebire de sistemele AI tradiÈ›ionale care se bazeazÄƒ pe servere cloud pentru procesare, Edge AI aduce inteligenÈ›a direct la sursa datelor.

Ãn esenÈ›Äƒ, Edge AI Ã®nseamnÄƒ decentralizarea procesÄƒrii AI, mutÃ¢nd-o departe de centrele de date centralizate È™i distribuind-o pe vasta reÈ›ea de dispozitive care alcÄƒtuiesc ecosistemul nostru digital. Aceasta reprezintÄƒ o schimbare arhitecturalÄƒ fundamentalÄƒ Ã®n modul Ã®n care sunt proiectate È™i implementate sistemele AI.

Pilonii conceptuali cheie ai Edge AI includ:

- **Procesare de proximitate**: ComputaÈ›ia are loc fizic aproape de locul unde se origineazÄƒ datele.
- **InteligenÈ›Äƒ descentralizatÄƒ**: CapacitÄƒÈ›ile de luare a deciziilor sunt distribuite pe mai multe dispozitive.
- **Suveranitatea datelor**: InformaÈ›iile rÄƒmÃ¢n sub control local, deseori fÄƒrÄƒ a pÄƒrÄƒsi dispozitivul.
- **FuncÈ›ionare autonomÄƒ**: Dispozitivele pot funcÈ›iona inteligent fÄƒrÄƒ a necesita conectivitate constantÄƒ.
- **AI integrat**: InteligenÈ›a devine o capacitate intrinsecÄƒ a dispozitivelor de zi cu zi.

### Vizualizarea arhitecturii Edge AI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TRADITIONAL AI ARCHITECTURE                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Data Transfer  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   API Response   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Edge Devices â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Cloud Servers â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ End Users â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            EDGE AI ARCHITECTURE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Direct Response  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Edge Devices with Embedded AI         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ End Users â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â”‚ Sensors â”‚â”€>â”‚ SLM Inference â”‚â”€>â”‚ Local Action â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI reprezintÄƒ o schimbare de paradigmÄƒ Ã®n implementarea inteligenÈ›ei artificiale, aducÃ¢nd capabilitÄƒÈ›ile AI direct pe dispozitivele de la marginea reÈ›elei, Ã®n loc sÄƒ se bazeze exclusiv pe procesarea Ã®n cloud. AceastÄƒ abordare permite modelelor AI sÄƒ ruleze local pe dispozitive cu resurse computaÈ›ionale limitate, oferind capabilitÄƒÈ›i de inferenÈ›Äƒ Ã®n timp real fÄƒrÄƒ a necesita conectivitate constantÄƒ la internet.

EdgeAI cuprinde diverse tehnologii È™i tehnici concepute pentru a face modelele AI mai eficiente È™i potrivite pentru implementarea pe dispozitive cu resurse limitate. Scopul este de a menÈ›ine o performanÈ›Äƒ rezonabilÄƒ, reducÃ¢nd semnificativ cerinÈ›ele computaÈ›ionale È™i de memorie ale modelelor AI.

SÄƒ analizÄƒm abordÄƒrile fundamentale care permit implementÄƒrile EdgeAI pe diferite tipuri de dispozitive È™i cazuri de utilizare.

### Principii fundamentale ale EdgeAI

EdgeAI se bazeazÄƒ pe mai multe principii fundamentale care Ã®l diferenÈ›iazÄƒ de AI-ul tradiÈ›ional bazat pe cloud:

- **Procesare localÄƒ**: InferenÈ›a AI are loc direct pe dispozitivul de la marginea reÈ›elei, fÄƒrÄƒ a necesita conectivitate externÄƒ.
- **Optimizarea resurselor**: Modelele sunt optimizate special pentru constrÃ¢ngerile hardware ale dispozitivelor È›intÄƒ.
- **PerformanÈ›Äƒ Ã®n timp real**: Procesarea are loc cu latenÈ›Äƒ minimÄƒ pentru aplicaÈ›iile sensibile la timp.
- **ConfidenÈ›ialitate prin design**: Datele sensibile rÄƒmÃ¢n pe dispozitiv, Ã®mbunÄƒtÄƒÈ›ind securitatea È™i conformitatea.

## Tehnologii cheie care permit EdgeAI

### Cuantificarea modelelor

Una dintre cele mai importante tehnici Ã®n EdgeAI este cuantificarea modelelor. Acest proces implicÄƒ reducerea preciziei parametrilor modelului, de obicei de la numere Ã®n virgulÄƒ mobilÄƒ pe 32 de biÈ›i la Ã®ntregi pe 8 biÈ›i sau chiar formate de precizie mai redusÄƒ. DeÈ™i aceastÄƒ reducere a preciziei poate pÄƒrea Ã®ngrijorÄƒtoare, cercetÄƒrile au arÄƒtat cÄƒ multe modele AI pot menÈ›ine performanÈ›a chiar È™i cu o precizie semnificativ redusÄƒ.

Cuantificarea funcÈ›ioneazÄƒ prin maparea intervalului de valori Ã®n virgulÄƒ mobilÄƒ la un set mai mic de valori discrete. De exemplu, Ã®n loc sÄƒ foloseascÄƒ 32 de biÈ›i pentru a reprezenta fiecare parametru, cuantificarea poate folosi doar 8 biÈ›i, rezultÃ¢nd o reducere de 4x a cerinÈ›elor de memorie È™i, adesea, conducÃ¢nd la timpi de inferenÈ›Äƒ mai rapizi.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

Diferite tehnici de cuantificare includ:

- **Cuantificare post-antrenament (PTQ)**: AplicatÄƒ dupÄƒ antrenarea modelului, fÄƒrÄƒ a necesita reantrenare.
- **Antrenament conÈ™tient de cuantificare (QAT)**: ÃncorporeazÄƒ efectele cuantificÄƒrii Ã®n timpul antrenamentului pentru o acurateÈ›e mai bunÄƒ.
- **Cuantificare dinamicÄƒ**: CuantificÄƒ greutÄƒÈ›ile la int8, dar calculeazÄƒ activÄƒrile dinamic.
- **Cuantificare staticÄƒ**: Pre-calculÄƒ toÈ›i parametrii de cuantificare atÃ¢t pentru greutÄƒÈ›i, cÃ¢t È™i pentru activÄƒri.

Pentru implementÄƒrile EdgeAI, selectarea strategiei de cuantificare adecvate depinde de arhitectura specificÄƒ a modelului, cerinÈ›ele de performanÈ›Äƒ È™i capacitÄƒÈ›ile hardware ale dispozitivului È›intÄƒ.

### Compresia È™i optimizarea modelelor

Dincolo de cuantificare, diverse tehnici de compresie ajutÄƒ la reducerea dimensiunii modelului È™i a cerinÈ›elor computaÈ›ionale. Acestea includ:

**Pruning**: AceastÄƒ tehnicÄƒ eliminÄƒ conexiunile sau neuronii inutili din reÈ›elele neuronale. Prin identificarea È™i eliminarea parametrilor care contribuie puÈ›in la performanÈ›a modelului, pruning-ul poate reduce semnificativ dimensiunea modelului, menÈ›inÃ¢nd Ã®n acelaÈ™i timp acurateÈ›ea.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Distilarea cunoÈ™tinÈ›elor**: AceastÄƒ abordare implicÄƒ antrenarea unui model "student" mai mic pentru a imita comportamentul unui model "teacher" mai mare. Modelul student Ã®nvaÈ›Äƒ sÄƒ aproximeze ieÈ™irile teacher-ului, obÈ›inÃ¢nd adesea performanÈ›e similare cu semnificativ mai puÈ›ini parametri.

**Optimizarea arhitecturii modelului**: CercetÄƒtorii au dezvoltat arhitecturi specializate concepute special pentru implementarea la marginea reÈ›elei, cum ar fi MobileNets, EfficientNets È™i alte arhitecturi uÈ™oare care echilibreazÄƒ performanÈ›a cu eficienÈ›a computaÈ›ionalÄƒ.

### Modele de limbaj mici (SLMs)

O tendinÈ›Äƒ emergentÄƒ Ã®n EdgeAI este dezvoltarea modelelor de limbaj mici (SLMs). Aceste modele sunt concepute de la zero pentru a fi compacte È™i eficiente, oferind Ã®n acelaÈ™i timp capabilitÄƒÈ›i semnificative de procesare a limbajului natural. SLM-urile realizeazÄƒ acest lucru prin alegeri arhitecturale atente, tehnici eficiente de antrenament È™i antrenament concentrat pe domenii sau sarcini specifice.

Spre deosebire de abordÄƒrile tradiÈ›ionale care implicÄƒ comprimarea modelelor mari, SLM-urile sunt adesea antrenate cu seturi de date mai mici È™i arhitecturi optimizate concepute special pentru implementarea la marginea reÈ›elei. AceastÄƒ abordare poate duce la modele care nu doar cÄƒ sunt mai mici, dar È™i mai eficiente pentru cazuri de utilizare specifice.

## Accelerarea hardware pentru EdgeAI

Dispozitivele moderne de la marginea reÈ›elei includ din ce Ã®n ce mai mult hardware specializat conceput pentru a accelera sarcinile AI:

### UnitÄƒÈ›i de procesare neuronalÄƒ (NPUs)

NPUs sunt procesoare specializate concepute special pentru calculele reÈ›elelor neuronale. Aceste cipuri pot efectua sarcini de inferenÈ›Äƒ AI mult mai eficient decÃ¢t procesoarele tradiÈ›ionale, adesea cu un consum mai redus de energie. Multe smartphone-uri moderne, laptopuri È™i dispozitive IoT includ acum NPUs pentru a permite procesarea AI pe dispozitiv.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

Dispozitive cu NPUs includ:

- **Apple**: Cipurile din seria A È™i M cu Neural Engine
- **Qualcomm**: Procesoarele Snapdragon cu Hexagon DSP/NPU
- **Samsung**: Procesoarele Exynos cu NPU
- **Intel**: VPUs Movidius È™i acceleratoare Habana Labs
- **Microsoft**: PC-uri Windows Copilot+ cu NPUs

### ğŸ® Accelerarea GPU

DeÈ™i dispozitivele de la marginea reÈ›elei pot sÄƒ nu aibÄƒ GPU-uri puternice precum cele din centrele de date, multe includ totuÈ™i GPU-uri integrate sau discrete care pot accelera sarcinile AI. GPU-urile mobile moderne È™i procesoarele grafice integrate pot oferi Ã®mbunÄƒtÄƒÈ›iri semnificative ale performanÈ›ei pentru sarcinile de inferenÈ›Äƒ AI.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### Optimizarea CPU

Chiar È™i dispozitivele care au doar CPU pot beneficia de EdgeAI prin implementÄƒri optimizate. CPU-urile moderne includ instrucÈ›iuni specializate pentru sarcinile AI, iar cadrele software au fost dezvoltate pentru a maximiza performanÈ›a CPU pentru inferenÈ›a AI.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

Pentru inginerii software care lucreazÄƒ cu EdgeAI, Ã®nÈ›elegerea modului de a valorifica aceste opÈ›iuni de accelerare hardware este esenÈ›ialÄƒ pentru optimizarea performanÈ›ei inferenÈ›ei È™i eficienÈ›ei energetice pe dispozitivele È›intÄƒ.

## Beneficiile EdgeAI

### ConfidenÈ›ialitate È™i securitate

Unul dintre cele mai mari avantaje ale EdgeAI este Ã®mbunÄƒtÄƒÈ›irea confidenÈ›ialitÄƒÈ›ii È™i securitÄƒÈ›ii. Prin procesarea datelor local pe dispozitiv, informaÈ›iile sensibile nu pÄƒrÄƒsesc niciodatÄƒ controlul utilizatorului. Acest lucru este deosebit de important pentru aplicaÈ›iile care gestioneazÄƒ date personale, informaÈ›ii medicale sau date confidenÈ›iale de afaceri.

### Reducerea latenÈ›ei

EdgeAI eliminÄƒ necesitatea de a trimite date cÄƒtre servere la distanÈ›Äƒ pentru procesare, reducÃ¢nd semnificativ latenÈ›a. Acest lucru este crucial pentru aplicaÈ›iile Ã®n timp real, cum ar fi vehiculele autonome, automatizarea industrialÄƒ sau aplicaÈ›iile interactive care necesitÄƒ rÄƒspunsuri imediate.

### FuncÈ›ionalitate offline

EdgeAI permite funcÈ›ionalitatea AI chiar È™i atunci cÃ¢nd conectivitatea la internet nu este disponibilÄƒ. Acest lucru este valoros pentru aplicaÈ›iile din locaÈ›ii izolate, Ã®n timpul cÄƒlÄƒtoriilor sau Ã®n situaÈ›ii Ã®n care fiabilitatea reÈ›elei este o preocupare.

### EficienÈ›Äƒ costurilor

Prin reducerea dependenÈ›ei de serviciile AI bazate pe cloud, EdgeAI poate ajuta la reducerea costurilor operaÈ›ionale, mai ales pentru aplicaÈ›iile cu volume mari de utilizare. OrganizaÈ›iile pot evita costurile API recurente È™i pot reduce cerinÈ›ele de lÄƒÈ›ime de bandÄƒ.

### Scalabilitate

EdgeAI distribuie sarcina computaÈ›ionalÄƒ pe dispozitivele de la marginea reÈ›elei, Ã®n loc sÄƒ o centralizeze Ã®n centrele de date. Acest lucru poate ajuta la reducerea costurilor infrastructurii È™i la Ã®mbunÄƒtÄƒÈ›irea scalabilitÄƒÈ›ii generale a sistemului.

## AplicaÈ›ii ale EdgeAI

### Dispozitive inteligente È™i IoT

EdgeAI alimenteazÄƒ multe funcÈ›ii ale dispozitivelor inteligente, de la asistenÈ›i vocali care pot procesa comenzi local pÃ¢nÄƒ la camere inteligente care pot identifica obiecte È™i persoane fÄƒrÄƒ a trimite videoclipuri Ã®n cloud. Dispozitivele IoT utilizeazÄƒ EdgeAI pentru Ã®ntreÈ›inere predictivÄƒ, monitorizarea mediului È™i luarea deciziilor automate.

### AplicaÈ›ii mobile

Smartphone-urile È™i tabletele utilizeazÄƒ EdgeAI pentru diverse funcÈ›ii, inclusiv Ã®mbunÄƒtÄƒÈ›irea fotografiilor, traducerea Ã®n timp real, realitatea augmentatÄƒ È™i recomandÄƒrile personalizate. Aceste aplicaÈ›ii beneficiazÄƒ de avantajele latenÈ›ei reduse È™i confidenÈ›ialitÄƒÈ›ii procesÄƒrii locale.

### AplicaÈ›ii industriale

Mediile de producÈ›ie È™i industriale utilizeazÄƒ EdgeAI pentru controlul calitÄƒÈ›ii, Ã®ntreÈ›inerea predictivÄƒ È™i optimizarea proceselor. Aceste aplicaÈ›ii necesitÄƒ adesea procesare Ã®n timp real È™i pot func
- [02: AplicaÈ›ii EdgeAI](02.RealWorldCaseStudies.md)

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa maternÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de oameni. Nu ne asumÄƒm responsabilitatea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.