<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-18T19:03:00+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "ro"
}
-->
# SecÈ›iunea 4: Implementarea modelului pregÄƒtit pentru producÈ›ie

## Prezentare generalÄƒ

Acest tutorial cuprinzÄƒtor te va ghida prin procesul complet de implementare a modelelor cuantificate ajustate folosind Foundry Local. Vom acoperi conversia modelului, optimizarea prin cuantificare È™i configurarea implementÄƒrii de la Ã®nceput pÃ¢nÄƒ la sfÃ¢rÈ™it.

## CerinÈ›e preliminare

Ãnainte de a Ã®ncepe, asigurÄƒ-te cÄƒ ai urmÄƒtoarele:

- âœ… Un model onnx ajustat, pregÄƒtit pentru implementare
- âœ… Computer Windows sau Mac
- âœ… Python 3.10 sau o versiune mai nouÄƒ
- âœ… Cel puÈ›in 8GB RAM disponibil
- âœ… Foundry Local instalat pe sistemul tÄƒu

## Partea 1: Configurarea mediului

### Instalarea instrumentelor necesare

Deschide terminalul (Command Prompt pe Windows, Terminal pe Mac) È™i ruleazÄƒ urmÄƒtoarele comenzi Ã®n ordine:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

âš ï¸ **NotÄƒ importantÄƒ**: Vei avea nevoie È™i de CMake versiunea 3.31 sau mai nouÄƒ, care poate fi descÄƒrcat de la [cmake.org](https://cmake.org/download/).

## Partea 2: Conversia È™i cuantificarea modelului

### Alegerea formatului potrivit

Pentru modelele mici de limbaj ajustate, recomandÄƒm utilizarea formatului **ONNX**, deoarece oferÄƒ:

- ğŸš€ Optimizare mai bunÄƒ a performanÈ›ei
- ğŸ”§ Implementare independentÄƒ de hardware
- ğŸ­ CapacitÄƒÈ›i pregÄƒtite pentru producÈ›ie
- ğŸ“± Compatibilitate cross-platform

### Metoda 1: Conversie cu o singurÄƒ comandÄƒ (RecomandatÄƒ)

FoloseÈ™te urmÄƒtoarea comandÄƒ pentru a converti direct modelul ajustat:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**ExplicaÈ›ia parametrilor:**
- `--model_name_or_path`: Calea cÄƒtre modelul ajustat
- `--device cpu`: UtilizeazÄƒ CPU pentru optimizare
- `--precision int4`: UtilizeazÄƒ cuantificarea INT4 (reducere de aproximativ 75% a dimensiunii)
- `--output_path`: Calea de ieÈ™ire pentru modelul convertit

### Metoda 2: Abordare prin fiÈ™ier de configurare (Utilizatori avansaÈ›i)

CreeazÄƒ un fiÈ™ier de configurare numit `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Apoi ruleazÄƒ:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Compararea opÈ›iunilor de cuantificare

| Precizie | Dimensiunea fiÈ™ierului | Viteza de inferenÈ›Äƒ | Calitatea modelului | Utilizare recomandatÄƒ |
|----------|-------------------------|---------------------|---------------------|-----------------------|
| FP16     | BazÄƒ Ã— 0.5             | Rapid               | Cea mai bunÄƒ       | Hardware de Ã®naltÄƒ performanÈ›Äƒ |
| INT8     | BazÄƒ Ã— 0.25            | Foarte rapid        | BunÄƒ               | Alegere echilibratÄƒ |
| INT4     | BazÄƒ Ã— 0.125           | Cea mai rapidÄƒ      | AcceptabilÄƒ        | Resurse limitate |

ğŸ’¡ **Recomandare**: Ãncepe cu cuantificarea INT4 pentru prima implementare. DacÄƒ calitatea nu este satisfÄƒcÄƒtoare, Ã®ncearcÄƒ INT8 sau FP16.

## Partea 3: Configurarea implementÄƒrii Foundry Local

### Crearea configuraÈ›iei modelului

NavigheazÄƒ la directorul de modele Foundry Local:

```bash
foundry cache cd ./models/
```

CreeazÄƒ structura directorului modelului:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

CreeazÄƒ fiÈ™ierul de configurare `inference_model.json` Ã®n directorul modelului:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### ConfiguraÈ›ii È™ablon specifice modelului

#### Pentru modelele din seria Qwen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Partea 4: Testarea È™i optimizarea modelului

### Verificarea instalÄƒrii modelului

VerificÄƒ dacÄƒ Foundry Local poate recunoaÈ™te modelul tÄƒu:

```bash
foundry cache ls
```

Ar trebui sÄƒ vezi `your-finetuned-model-int4` Ã®n listÄƒ.

### Ãnceperea testÄƒrii modelului

```bash
foundry model run your-finetuned-model-int4
```

### Evaluarea performanÈ›ei

MonitorizeazÄƒ metricile cheie Ã®n timpul testÄƒrii:

1. **Timp de rÄƒspuns**: MÄƒsoarÄƒ timpul mediu per rÄƒspuns
2. **Utilizarea memoriei**: MonitorizeazÄƒ consumul de RAM
3. **Utilizarea CPU**: VerificÄƒ Ã®ncÄƒrcarea procesorului
4. **Calitatea ieÈ™irii**: EvalueazÄƒ relevanÈ›a È™i coerenÈ›a rÄƒspunsurilor

### Lista de verificare pentru validarea calitÄƒÈ›ii

- âœ… Modelul rÄƒspunde corespunzÄƒtor la interogÄƒrile din domeniul ajustat
- âœ… Formatul rÄƒspunsului se potriveÈ™te cu structura de ieÈ™ire aÈ™teptatÄƒ
- âœ… Nu existÄƒ scurgeri de memorie Ã®n timpul utilizÄƒrii prelungite
- âœ… PerformanÈ›Äƒ consistentÄƒ pentru diferite lungimi de intrare
- âœ… Gestionare corespunzÄƒtoare a cazurilor limitÄƒ È™i a intrÄƒrilor invalide

## Rezumat

FelicitÄƒri! Ai finalizat cu succes:

- âœ… Conversia formatului modelului ajustat
- âœ… Optimizarea prin cuantificare a modelului
- âœ… Configurarea implementÄƒrii Foundry Local
- âœ… Reglarea performanÈ›ei È™i depanarea

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ reÈ›ineÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa natalÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm responsabilitatea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.