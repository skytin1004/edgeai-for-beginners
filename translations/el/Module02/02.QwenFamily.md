<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-18T06:42:29+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "el"
}
-->
# Î•Î½ÏŒÏ„Î·Ï„Î± 2: Î˜ÎµÎ¼ÎµÎ»Î¹ÏÎ´ÎµÎ¹Ï‚ Î‘ÏÏ‡Î­Ï‚ Ï„Î·Ï‚ ÎŸÎ¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î±Ï‚ Qwen

Î— Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î± Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Qwen Î±Î½Ï„Î¹Ï€ÏÎ¿ÏƒÏ‰Ï€ÎµÏÎµÎ¹ Ï„Î·Î½ Î¿Î»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î· Ï€ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ· Ï„Î·Ï‚ Alibaba Cloud ÏƒÏ„Î± Î¼ÎµÎ³Î¬Î»Î± Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ¬ Î¼Î¿Î½Ï„Î­Î»Î± ÎºÎ±Î¹ Ï„Î·Î½ Ï€Î¿Î»Ï…Ï„ÏÎ¿Ï€Î¹ÎºÎ® AI, Î±Ï€Î¿Î´ÎµÎ¹ÎºÎ½ÏÎ¿Î½Ï„Î±Ï‚ ÏŒÏ„Î¹ Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± Î±Î½Î¿Î¹Ï‡Ï„Î¿Ï ÎºÏÎ´Î¹ÎºÎ± Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± ÎµÏ€Î¹Ï„ÏÏ‡Î¿Ï…Î½ ÎµÎ¾Î±Î¹ÏÎµÏ„Î¹ÎºÎ® Î±Ï€ÏŒÎ´Î¿ÏƒÎ· ÎµÎ½Ï ÎµÎ¯Î½Î±Î¹ Ï€ÏÎ¿ÏƒÎ²Î¬ÏƒÎ¹Î¼Î± ÏƒÎµ Î´Î¹Î¬Ï†Î¿ÏÎ± ÏƒÎµÎ½Î¬ÏÎ¹Î± Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚. Î•Î¯Î½Î±Î¹ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÏŒ Î½Î± ÎºÎ±Ï„Î±Î½Î¿Î®ÏƒÎ¿Ï…Î¼Îµ Ï€ÏÏ‚ Î· Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î± Qwen ÎµÏ€Î¹Ï„ÏÎ­Ï€ÎµÎ¹ Î¹ÏƒÏ‡Ï…ÏÎ­Ï‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ AI Î¼Îµ ÎµÏ…Î­Î»Î¹ÎºÏ„ÎµÏ‚ ÎµÏ€Î¹Î»Î¿Î³Î­Ï‚ Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚, Î´Î¹Î±Ï„Î·ÏÏÎ½Ï„Î±Ï‚ Ï€Î±ÏÎ¬Î»Î»Î·Î»Î± Î±Î½Ï„Î±Î³Ï‰Î½Î¹ÏƒÏ„Î¹ÎºÎ® Î±Ï€ÏŒÎ´Î¿ÏƒÎ· ÏƒÎµ Ï€Î¿Î¹ÎºÎ¯Î»ÎµÏ‚ ÎµÏÎ³Î±ÏƒÎ¯ÎµÏ‚.

## Î ÏŒÏÎ¿Î¹ Î³Î¹Î± Î ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÏ„Î­Ï‚

### Î‘Ï€Î¿Î¸ÎµÏ„Î®ÏÎ¹Î¿ ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½ Hugging Face
Î•Ï€Î¹Î»ÎµÎ³Î¼Î­Î½Î± Î¼Î¿Î½Ï„Î­Î»Î± Ï„Î·Ï‚ Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î±Ï‚ Qwen ÎµÎ¯Î½Î±Î¹ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î± Î¼Î­ÏƒÏ‰ Ï„Î¿Ï… [Hugging Face](https://huggingface.co/models?search=qwen), Ï€Î±ÏÎ­Ï‡Î¿Î½Ï„Î±Ï‚ Ï€ÏÏŒÏƒÎ²Î±ÏƒÎ· ÏƒÎµ Î¿ÏÎ¹ÏƒÎ¼Î­Î½ÎµÏ‚ Ï€Î±ÏÎ±Î»Î»Î±Î³Î­Ï‚ Î±Ï…Ï„ÏÎ½ Ï„Ï‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½. ÎœÏ€Î¿ÏÎµÎ¯Ï„Îµ Î½Î± ÎµÎ¾ÎµÏÎµÏ…Î½Î®ÏƒÎµÏ„Îµ Ï„Î¹Ï‚ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼ÎµÏ‚ Ï€Î±ÏÎ±Î»Î»Î±Î³Î­Ï‚, Î½Î± Ï„Î¹Ï‚ Ï€ÏÎ¿ÏƒÎ±ÏÎ¼ÏŒÏƒÎµÏ„Îµ ÏƒÏ„Î¹Ï‚ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½ÎµÏ‚ Î±Î½Î¬Î³ÎºÎµÏ‚ ÏƒÎ±Ï‚ ÎºÎ±Î¹ Î½Î± Ï„Î¹Ï‚ Î±Î½Î±Ï€Ï„ÏÎ¾ÎµÏ„Îµ Î¼Î­ÏƒÏ‰ Î´Î¹Î¬Ï†Î¿ÏÏ‰Î½ Ï€Î»Î±Î¹ÏƒÎ¯Ï‰Î½.

### Î•ÏÎ³Î±Î»ÎµÎ¯Î± Î¤Î¿Ï€Î¹ÎºÎ®Ï‚ Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚
Î“Î¹Î± Ï„Î¿Ï€Î¹ÎºÎ® Î±Î½Î¬Ï€Ï„Ï…Î¾Î· ÎºÎ±Î¹ Î´Î¿ÎºÎ¹Î¼Î­Ï‚, Î¼Ï€Î¿ÏÎµÎ¯Ï„Îµ Î½Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎµÏ„Îµ Ï„Î¿ [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) Î³Î¹Î± Î½Î± ÎµÎºÏ„ÎµÎ»Î­ÏƒÎµÏ„Îµ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î± Î¼Î¿Î½Ï„Î­Î»Î± Qwen ÏƒÏ„Î¿Î½ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„Î® Î±Î½Î¬Ï€Ï„Ï…Î¾Î®Ï‚ ÏƒÎ±Ï‚ Î¼Îµ Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î· Î±Ï€ÏŒÎ´Î¿ÏƒÎ·.

### Î ÏŒÏÎ¿Î¹ Î¤ÎµÎºÎ¼Î·ÏÎ¯Ï‰ÏƒÎ·Ï‚
- [Î¤ÎµÎºÎ¼Î·ÏÎ¯Ï‰ÏƒÎ· ÎœÎ¿Î½Ï„Î­Î»Î¿Ï… Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½ Qwen Î³Î¹Î± Î‘Î½Î¬Ï€Ï„Ï…Î¾Î· ÏƒÎµ Edge](https://github.com/microsoft/olive)

## Î•Î¹ÏƒÎ±Î³Ï‰Î³Î®

Î£Îµ Î±Ï…Ï„ÏŒ Ï„Î¿ ÏƒÎµÎ¼Î¹Î½Î¬ÏÎ¹Î¿, Î¸Î± ÎµÎ¾ÎµÏÎµÏ…Î½Î®ÏƒÎ¿Ï…Î¼Îµ Ï„Î·Î½ Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î± Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Qwen Ï„Î·Ï‚ Alibaba ÎºÎ±Î¹ Ï„Î¹Ï‚ Î²Î±ÏƒÎ¹ÎºÎ­Ï‚ Ï„Î·Ï‚ Î­Î½Î½Î¿Î¹ÎµÏ‚. Î˜Î± ÎºÎ±Î»ÏÏˆÎ¿Ï…Î¼Îµ Ï„Î·Î½ ÎµÎ¾Î­Î»Î¹Î¾Î· Ï„Î·Ï‚ Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î±Ï‚ Qwen, Ï„Î¹Ï‚ ÎºÎ±Î¹Î½Î¿Ï„ÏŒÎ¼ÎµÏ‚ Î¼ÎµÎ¸Î¿Î´Î¿Î»Î¿Î³Î¯ÎµÏ‚ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚ Ï€Î¿Ï… ÎºÎ±Î¸Î¹ÏƒÏ„Î¿ÏÎ½ Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± Qwen Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î±Ï„Î¹ÎºÎ¬, Ï„Î¹Ï‚ Î²Î±ÏƒÎ¹ÎºÎ­Ï‚ Ï€Î±ÏÎ±Î»Î»Î±Î³Î­Ï‚ Ï„Î·Ï‚ Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î±Ï‚ ÎºÎ±Î¹ Ï„Î¹Ï‚ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ­Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚ ÏƒÎµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ¬ ÏƒÎµÎ½Î¬ÏÎ¹Î±.

## Î£Ï„ÏŒÏ‡Î¿Î¹ ÎœÎ¬Î¸Î·ÏƒÎ·Ï‚

ÎœÎ­Ï‡ÏÎ¹ Ï„Î¿ Ï„Î­Î»Î¿Ï‚ Î±Ï…Ï„Î¿Ï Ï„Î¿Ï… ÏƒÎµÎ¼Î¹Î½Î±ÏÎ¯Î¿Ï…, Î¸Î± Î¼Ï€Î¿ÏÎµÎ¯Ï„Îµ Î½Î±:

- ÎšÎ±Ï„Î±Î½Î¿Î®ÏƒÎµÏ„Îµ Ï„Î· Ï†Î¹Î»Î¿ÏƒÎ¿Ï†Î¯Î± ÏƒÏ‡ÎµÎ´Î¯Î±ÏƒÎ·Ï‚ ÎºÎ±Î¹ Ï„Î·Î½ ÎµÎ¾Î­Î»Î¹Î¾Î· Ï„Î·Ï‚ Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î±Ï‚ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Qwen Ï„Î·Ï‚ Alibaba
- Î‘Î½Î±Î³Î½Ï‰ÏÎ¯ÏƒÎµÏ„Îµ Ï„Î¹Ï‚ Î²Î±ÏƒÎ¹ÎºÎ­Ï‚ ÎºÎ±Î¹Î½Î¿Ï„Î¿Î¼Î¯ÎµÏ‚ Ï€Î¿Ï… ÎµÏ€Î¹Ï„ÏÎ­Ï€Î¿Ï…Î½ ÏƒÏ„Î± Î¼Î¿Î½Ï„Î­Î»Î± Qwen Î½Î± ÎµÏ€Î¹Ï„Ï…Î³Ï‡Î¬Î½Î¿Ï…Î½ Ï…ÏˆÎ·Î»Î® Î±Ï€ÏŒÎ´Î¿ÏƒÎ· ÏƒÎµ Î´Î¹Î¬Ï†Î¿ÏÎ± Î¼ÎµÎ³Î­Î¸Î· Ï€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½
- Î‘Î½Î±Î³Î½Ï‰ÏÎ¯ÏƒÎµÏ„Îµ Ï„Î± Ï€Î»ÎµÎ¿Î½ÎµÎºÏ„Î®Î¼Î±Ï„Î± ÎºÎ±Î¹ Ï„Î¿Ï…Ï‚ Ï€ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼Î¿ÏÏ‚ Ï„Ï‰Î½ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏÎ½ Ï€Î±ÏÎ±Î»Î»Î±Î³ÏÎ½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Qwen
- Î•Ï†Î±ÏÎ¼ÏŒÏƒÎµÏ„Îµ Ï„Î· Î³Î½ÏÏƒÎ· Ï„Ï‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Qwen Î³Î¹Î± Î½Î± ÎµÏ€Î¹Î»Î­Î¾ÎµÏ„Îµ ÎºÎ±Ï„Î¬Î»Î»Î·Î»ÎµÏ‚ Ï€Î±ÏÎ±Î»Î»Î±Î³Î­Ï‚ Î³Î¹Î± Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ ÏƒÎµÎ½Î¬ÏÎ¹Î±

## ÎšÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ· Ï„Î¿Ï… Î£ÏÎ³Ï‡ÏÎ¿Î½Î¿Ï… Î¤Î¿Ï€Î¯Î¿Ï… ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½ AI

Î¤Î¿ Ï„Î¿Ï€Î¯Î¿ Ï„Î·Ï‚ AI Î­Ï‡ÎµÎ¹ ÎµÎ¾ÎµÎ»Î¹Ï‡Î¸ÎµÎ¯ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÎ¬, Î¼Îµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ¿ÏÏ‚ Î¿ÏÎ³Î±Î½Î¹ÏƒÎ¼Î¿ÏÏ‚ Î½Î± Î±ÎºÎ¿Î»Î¿Ï…Î¸Î¿ÏÎ½ Î´Î¹Î¬Ï†Î¿ÏÎµÏ‚ Ï€ÏÎ¿ÏƒÎµÎ³Î³Î¯ÏƒÎµÎ¹Ï‚ ÏƒÏ„Î·Î½ Î±Î½Î¬Ï€Ï„Ï…Î¾Î· Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÏÎ½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½. Î•Î½Ï ÎºÎ¬Ï€Î¿Î¹Î¿Î¹ ÎµÏ€Î¹ÎºÎµÎ½Ï„ÏÏÎ½Î¿Î½Ï„Î±Î¹ ÏƒÎµ Î¹Î´Î¹ÏŒÎºÏ„Î·Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± ÎºÎ»ÎµÎ¹ÏƒÏ„Î¿Ï ÎºÏÎ´Î¹ÎºÎ±, Î¬Î»Î»Î¿Î¹ Î´Î¯Î½Î¿Ï…Î½ Î­Î¼Ï†Î±ÏƒÎ· ÏƒÏ„Î·Î½ Ï€ÏÎ¿ÏƒÎ²Î±ÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î± ÎºÎ±Î¹ Ï„Î· Î´Î¹Î±Ï†Î¬Î½ÎµÎ¹Î± Î±Î½Î¿Î¹Ï‡Ï„Î¿Ï ÎºÏÎ´Î¹ÎºÎ±. Î— Ï€Î±ÏÎ±Î´Î¿ÏƒÎ¹Î±ÎºÎ® Ï€ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ· Ï€ÎµÏÎ¹Î»Î±Î¼Î²Î¬Î½ÎµÎ¹ ÎµÎ¯Ï„Îµ Ï„ÎµÏÎ¬ÏƒÏ„Î¹Î± Î¹Î´Î¹ÏŒÎºÏ„Î·Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± Ï€Î¿Ï… ÎµÎ¯Î½Î±Î¹ Ï€ÏÎ¿ÏƒÎ²Î¬ÏƒÎ¹Î¼Î± Î¼ÏŒÎ½Î¿ Î¼Î­ÏƒÏ‰ APIs ÎµÎ¯Ï„Îµ Î¼Î¿Î½Ï„Î­Î»Î± Î±Î½Î¿Î¹Ï‡Ï„Î¿Ï ÎºÏÎ´Î¹ÎºÎ± Ï€Î¿Ï… Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï…ÏƒÏ„ÎµÏÎ¿ÏÎ½ ÏƒÎµ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚.

Î‘Ï…Ï„Î® Î· Ï€Î±ÏÎ¬Î´Î¿ÏƒÎ· Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Ï€ÏÎ¿ÎºÎ»Î®ÏƒÎµÎ¹Ï‚ Î³Î¹Î± Î¿ÏÎ³Î±Î½Î¹ÏƒÎ¼Î¿ÏÏ‚ Ï€Î¿Ï… Î±Î½Î±Î¶Î·Ï„Î¿ÏÎ½ Î¹ÏƒÏ‡Ï…ÏÎ­Ï‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ AI, Î´Î¹Î±Ï„Î·ÏÏÎ½Ï„Î±Ï‚ Ï€Î±ÏÎ¬Î»Î»Î·Î»Î± Ï„Î¿Î½ Î­Î»ÎµÎ³Ï‡Î¿ Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï„Î¿Ï…Ï‚, Ï„Î¿Ï… ÎºÏŒÏƒÏ„Î¿Ï…Ï‚ ÎºÎ±Î¹ Ï„Î·Ï‚ ÎµÏ…ÎµÎ»Î¹Î¾Î¯Î±Ï‚ Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚. Î— ÏƒÏ…Î¼Î²Î±Ï„Î¹ÎºÎ® Ï€ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ· ÏƒÏ…Ï‡Î½Î¬ Î±Ï€Î±Î¹Ï„ÎµÎ¯ ÎµÏ€Î¹Î»Î¿Î³Î® Î¼ÎµÏ„Î±Î¾Ï ÎºÎ¿ÏÏ…Ï†Î±Î¯Î±Ï‚ Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚ ÎºÎ±Î¹ Ï€ÏÎ±ÎºÏ„Î¹ÎºÏÎ½ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½ Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚.

## Î— Î ÏÏŒÎºÎ»Î·ÏƒÎ· Ï„Î·Ï‚ Î ÏÎ¿ÏƒÎ²Î¬ÏƒÎ¹Î¼Î·Ï‚ Î‘ÏÎ¹ÏƒÏ„ÎµÎ¯Î±Ï‚ AI

Î— Î±Î½Î¬Î³ÎºÎ· Î³Î¹Î± Ï…ÏˆÎ·Î»Î®Ï‚ Ï€Î¿Î¹ÏŒÏ„Î·Ï„Î±Ï‚, Ï€ÏÎ¿ÏƒÎ²Î¬ÏƒÎ¹Î¼Î· AI Î­Ï‡ÎµÎ¹ Î³Î¯Î½ÎµÎ¹ Î¿Î»Î¿Î­Î½Î± ÎºÎ±Î¹ Ï€Î¹Î¿ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÎ® ÏƒÎµ Î´Î¹Î¬Ï†Î¿ÏÎ± ÏƒÎµÎ½Î¬ÏÎ¹Î±. Î£ÎºÎµÏ†Ï„ÎµÎ¯Ï„Îµ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚ Ï€Î¿Ï… Î±Ï€Î±Î¹Ï„Î¿ÏÎ½ ÎµÏ…Î­Î»Î¹ÎºÏ„ÎµÏ‚ ÎµÏ€Î¹Î»Î¿Î³Î­Ï‚ Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚ Î³Î¹Î± Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Î¿ÏÎ³Î±Î½Ï‰Ï„Î¹ÎºÎ­Ï‚ Î±Î½Î¬Î³ÎºÎµÏ‚, Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ­Ï‚ Ï…Î»Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹Ï‚ ÏŒÏ€Î¿Ï… Ï„Î¿ ÎºÏŒÏƒÏ„Î¿Ï‚ API Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î³Î¯Î½ÎµÎ¹ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÏŒ, Ï€Î¿Î»Ï…Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ­Ï‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ Î³Î¹Î± Ï€Î±Î³ÎºÏŒÏƒÎ¼Î¹ÎµÏ‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚ Î® ÎµÎ¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î· Î³Î½ÏÏƒÎ· ÏƒÎµ Ï„Î¿Î¼ÎµÎ¯Ï‚ ÏŒÏ€Ï‰Ï‚ Î¿ Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒÏ‚ ÎºÎ±Î¹ Ï„Î± Î¼Î±Î¸Î·Î¼Î±Ï„Î¹ÎºÎ¬.

### Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ Î‘Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚ Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚

ÎŸÎ¹ ÏƒÏÎ³Ï‡ÏÎ¿Î½ÎµÏ‚ Î±Î½Î±Ï€Ï„ÏÎ¾ÎµÎ¹Ï‚ AI Î±Î½Ï„Î¹Î¼ÎµÏ„Ï‰Ï€Î¯Î¶Î¿Ï…Î½ Î±ÏÎºÎµÏ„Î­Ï‚ Î¸ÎµÎ¼ÎµÎ»Î¹ÏÎ´ÎµÎ¹Ï‚ Î±Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚ Ï€Î¿Ï… Ï€ÎµÏÎ¹Î¿ÏÎ¯Î¶Î¿Ï…Î½ Ï„Î·Î½ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ® ÎµÏ†Î±ÏÎ¼Î¿ÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î±:

- **Î ÏÎ¿ÏƒÎ²Î±ÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î±**: Î”Î¹Î±Î¸ÎµÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î± Î±Î½Î¿Î¹Ï‡Ï„Î¿Ï ÎºÏÎ´Î¹ÎºÎ± Î³Î¹Î± Î´Î¹Î±Ï†Î¬Î½ÎµÎ¹Î± ÎºÎ±Î¹ Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿Î³Î®
- **ÎŸÎ¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ® Î‘Ï€Î¿Î´Î¿Ï„Î¹ÎºÏŒÏ„Î·Ï„Î±**: Î›Î¿Î³Î¹ÎºÎ­Ï‚ Î±Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„Î¹ÎºÎ®Ï‚ Î¹ÏƒÏ‡ÏÎ¿Ï‚ Î³Î¹Î± Î´Î¹Î¬Ï†Î¿ÏÎ¿Ï…Ï‚ Ï€ÏÎ¿Ï‹Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼Î¿ÏÏ‚
- **Î•Ï…ÎµÎ»Î¹Î¾Î¯Î±**: Î Î¿Î»Î»Î±Ï€Î»Î¬ Î¼ÎµÎ³Î­Î¸Î· Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Î³Î¹Î± Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ¬ ÏƒÎµÎ½Î¬ÏÎ¹Î± Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚
- **Î Î±Î³ÎºÏŒÏƒÎ¼Î¹Î± Î•Î¼Î²Î­Î»ÎµÎ¹Î±**: Î™ÏƒÏ‡Ï…ÏÎ­Ï‚ Ï€Î¿Î»Ï…Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ­Ï‚ ÎºÎ±Î¹ Î´Î¹Î±Ï€Î¿Î»Î¹Ï„Î¹ÏƒÎ¼Î¹ÎºÎ­Ï‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚
- **Î•Î¾ÎµÎ¹Î´Î¯ÎºÎµÏ…ÏƒÎ·**: Î Î±ÏÎ±Î»Î»Î±Î³Î­Ï‚ ÎµÎ¹Î´Î¹ÎºÎ­Ï‚ Î³Î¹Î± Ï„Î¿Î¼ÎµÎ¯Ï‚ Î³Î¹Î± ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½ÎµÏ‚ Ï‡ÏÎ®ÏƒÎµÎ¹Ï‚

## Î— Î¦Î¹Î»Î¿ÏƒÎ¿Ï†Î¯Î± Ï„Ï‰Î½ ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½ Qwen

Î— Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î± Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Qwen Î±Î½Ï„Î¹Ï€ÏÎ¿ÏƒÏ‰Ï€ÎµÏÎµÎ¹ Î¼Î¹Î± Î¿Î»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î· Ï€ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ· ÏƒÏ„Î·Î½ Î±Î½Î¬Ï€Ï„Ï…Î¾Î· Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ AI, Î´Î¯Î½Î¿Î½Ï„Î±Ï‚ Ï€ÏÎ¿Ï„ÎµÏÎ±Î¹ÏŒÏ„Î·Ï„Î± ÏƒÏ„Î·Î½ Ï€ÏÎ¿ÏƒÎ²Î±ÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î± Î±Î½Î¿Î¹Ï‡Ï„Î¿Ï ÎºÏÎ´Î¹ÎºÎ±, Ï„Î¹Ï‚ Ï€Î¿Î»Ï…Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ­Ï‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ ÎºÎ±Î¹ Ï„Î·Î½ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ® Î±Î½Î¬Ï€Ï„Ï…Î¾Î·, Î´Î¹Î±Ï„Î·ÏÏÎ½Ï„Î±Ï‚ Ï€Î±ÏÎ¬Î»Î»Î·Î»Î± Î±Î½Ï„Î±Î³Ï‰Î½Î¹ÏƒÏ„Î¹ÎºÎ¬ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚. Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± Qwen ÎµÏ€Î¹Ï„Ï…Î³Ï‡Î¬Î½Î¿Ï…Î½ Î±Ï…Ï„ÏŒ Î¼Î­ÏƒÏ‰ Ï€Î¿Î¹ÎºÎ¯Î»Ï‰Î½ Î¼ÎµÎ³ÎµÎ¸ÏÎ½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½, Ï…ÏˆÎ·Î»Î®Ï‚ Ï€Î¿Î¹ÏŒÏ„Î·Ï„Î±Ï‚ Î¼ÎµÎ¸Î¿Î´Î¿Î»Î¿Î³Î¹ÏÎ½ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚ ÎºÎ±Î¹ ÎµÎ¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Ï‰Î½ Ï€Î±ÏÎ±Î»Î»Î±Î³ÏÎ½ Î³Î¹Î± Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ¿ÏÏ‚ Ï„Î¿Î¼ÎµÎ¯Ï‚.

Î— Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î± Qwen Ï€ÎµÏÎ¹Î»Î±Î¼Î²Î¬Î½ÎµÎ¹ Î´Î¹Î¬Ï†Î¿ÏÎµÏ‚ Ï€ÏÎ¿ÏƒÎµÎ³Î³Î¯ÏƒÎµÎ¹Ï‚ Ï€Î¿Ï… Î­Ï‡Î¿Ï…Î½ ÏƒÏ‡ÎµÎ´Î¹Î±ÏƒÏ„ÎµÎ¯ Î³Î¹Î± Î½Î± Ï€Î±ÏÎ­Ï‡Î¿Ï…Î½ ÎµÏ€Î¹Î»Î¿Î³Î­Ï‚ ÏƒÎµ ÏŒÎ»Î¿ Ï„Î¿ Ï†Î¬ÏƒÎ¼Î± Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚-Î±Ï€Î¿Î´Î¿Ï„Î¹ÎºÏŒÏ„Î·Ï„Î±Ï‚, ÎµÏ€Î¹Ï„ÏÎ­Ï€Î¿Î½Ï„Î±Ï‚ Ï„Î·Î½ Î±Î½Î¬Ï€Ï„Ï…Î¾Î· Î±Ï€ÏŒ ÎºÎ¹Î½Î·Ï„Î­Ï‚ ÏƒÏ…ÏƒÎºÎµÏ…Î­Ï‚ Î­Ï‰Ï‚ Î´Î¹Î±ÎºÎ¿Î¼Î¹ÏƒÏ„Î­Ï‚ ÎµÏ€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½, ÎµÎ½Ï Ï€Î±ÏÎ­Ï‡Î¿Ï…Î½ Î¿Ï…ÏƒÎ¹Î±ÏƒÏ„Î¹ÎºÎ­Ï‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ AI. ÎŸ ÏƒÏ„ÏŒÏ‡Î¿Ï‚ ÎµÎ¯Î½Î±Î¹ Î½Î± Î´Î·Î¼Î¿ÎºÏÎ±Ï„Î¹ÎºÎ¿Ï€Î¿Î¹Î·Î¸ÎµÎ¯ Î· Ï€ÏÏŒÏƒÎ²Î±ÏƒÎ· ÏƒÎµ AI Ï…ÏˆÎ·Î»Î®Ï‚ Ï€Î¿Î¹ÏŒÏ„Î·Ï„Î±Ï‚, Ï€Î±ÏÎ­Ï‡Î¿Î½Ï„Î±Ï‚ ÎµÏ…ÎµÎ»Î¹Î¾Î¯Î± ÏƒÏ„Î¹Ï‚ ÎµÏ€Î¹Î»Î¿Î³Î­Ï‚ Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚.

### Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ Î‘ÏÏ‡Î­Ï‚ Î£Ï‡ÎµÎ´Î¯Î±ÏƒÎ·Ï‚ Qwen

Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± Qwen Î²Î±ÏƒÎ¯Î¶Î¿Î½Ï„Î±Î¹ ÏƒÎµ Î±ÏÎºÎµÏ„Î­Ï‚ Î¸ÎµÎ¼ÎµÎ»Î¹ÏÎ´ÎµÎ¹Ï‚ Î±ÏÏ‡Î­Ï‚ Ï€Î¿Ï… Ï„Î± Î´Î¹Î±ÎºÏÎ¯Î½Î¿Ï…Î½ Î±Ï€ÏŒ Î¬Î»Î»ÎµÏ‚ Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹ÎµÏ‚ Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÏÎ½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½:

- **Î ÏÏÏ„Î± Î¿ Î‘Î½Î¿Î¹Ï‡Ï„ÏŒÏ‚ ÎšÏÎ´Î¹ÎºÎ±Ï‚**: Î Î»Î®ÏÎ·Ï‚ Î´Î¹Î±Ï†Î¬Î½ÎµÎ¹Î± ÎºÎ±Î¹ Ï€ÏÎ¿ÏƒÎ²Î±ÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î± Î³Î¹Î± Î­ÏÎµÏ…Î½Î± ÎºÎ±Î¹ ÎµÎ¼Ï€Î¿ÏÎ¹ÎºÎ® Ï‡ÏÎ®ÏƒÎ·
- **ÎŸÎ»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î· Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·**: Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· ÏƒÎµ Ï„ÎµÏÎ¬ÏƒÏ„Î¹Î±, Ï€Î¿Î¹ÎºÎ¯Î»Î± ÏƒÏÎ½Î¿Î»Î± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï€Î¿Ï… ÎºÎ±Î»ÏÏ€Ï„Î¿Ï…Î½ Ï€Î¿Î»Î»Î­Ï‚ Î³Î»ÏÏƒÏƒÎµÏ‚ ÎºÎ±Î¹ Ï„Î¿Î¼ÎµÎ¯Ï‚
- **ÎšÎ»Î¹Î¼Î±ÎºÎ¿ÏÎ¼ÎµÎ½Î· Î‘ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ®**: Î Î¿Î»Î»Î±Ï€Î»Î¬ Î¼ÎµÎ³Î­Î¸Î· Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Î³Î¹Î± Î½Î± Ï„Î±Î¹ÏÎ¹Î¬Î¶Î¿Ï…Î½ ÏƒÎµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Î±Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„Î¹ÎºÎ®Ï‚ Î¹ÏƒÏ‡ÏÎ¿Ï‚
- **Î•Î¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î· Î‘ÏÎ¹ÏƒÏ„ÎµÎ¯Î±**: Î Î±ÏÎ±Î»Î»Î±Î³Î­Ï‚ ÎµÎ¹Î´Î¹ÎºÎ­Ï‚ Î³Î¹Î± Ï„Î¿Î¼ÎµÎ¯Ï‚, Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½ÎµÏ‚ Î³Î¹Î± ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½ÎµÏ‚ ÎµÏÎ³Î±ÏƒÎ¯ÎµÏ‚

## Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ Î¤ÎµÏ‡Î½Î¿Î»Î¿Î³Î¯ÎµÏ‚ Ï€Î¿Ï… Î•Î½Î¹ÏƒÏ‡ÏÎ¿Ï…Î½ Ï„Î·Î½ ÎŸÎ¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î± Qwen

### Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· ÏƒÎµ ÎœÎµÎ³Î¬Î»Î· ÎšÎ»Î¯Î¼Î±ÎºÎ±

ÎˆÎ½Î± Î±Ï€ÏŒ Ï„Î± ÎºÎ±Î¸Î¿ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Ï„Î·Ï‚ Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î±Ï‚ Qwen ÎµÎ¯Î½Î±Î¹ Î· ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· ÏƒÎµ Ï„ÎµÏÎ¬ÏƒÏ„Î¹Î± ÎºÎ»Î¯Î¼Î±ÎºÎ± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ ÎºÎ±Î¹ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„Î¹ÎºÏÎ½ Ï€ÏŒÏÏ‰Î½ Ï€Î¿Ï… ÎµÏ€ÎµÎ½Î´ÏÎ¿Î½Ï„Î±Î¹ ÏƒÏ„Î·Î½ Î±Î½Î¬Ï€Ï„Ï…Î¾Î· Î¼Î¿Î½Ï„Î­Î»Ï‰Î½. Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± Qwen Î±Î¾Î¹Î¿Ï€Î¿Î¹Î¿ÏÎ½ Ï€ÏÎ¿ÏƒÎµÎºÏ„Î¹ÎºÎ¬ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î±, Ï€Î¿Î»Ï…Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ¬ ÏƒÏÎ½Î¿Î»Î± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï€Î¿Ï… ÎºÎ±Î»ÏÏ€Ï„Î¿Ï…Î½ Ï„ÏÎ¹ÏƒÎµÎºÎ±Ï„Î¿Î¼Î¼ÏÏÎ¹Î± tokens, ÏƒÏ‡ÎµÎ´Î¹Î±ÏƒÎ¼Î­Î½Î± Î½Î± Ï€Î±ÏÎ­Ï‡Î¿Ï…Î½ Î¿Î»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î· Î³Î½ÏÏƒÎ· Ï„Î¿Ï… ÎºÏŒÏƒÎ¼Î¿Ï… ÎºÎ±Î¹ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ Î»Î¿Î³Î¹ÎºÎ®Ï‚.

Î‘Ï…Ï„Î® Î· Ï€ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ· Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯ ÏƒÏ…Î½Î´Ï…Î¬Î¶Î¿Î½Ï„Î±Ï‚ Ï…ÏˆÎ·Î»Î®Ï‚ Ï€Î¿Î¹ÏŒÏ„Î·Ï„Î±Ï‚ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿ Î¹ÏƒÏ„Î¿Ï, Î±ÎºÎ±Î´Î·Î¼Î±ÏŠÎºÎ® Î²Î¹Î²Î»Î¹Î¿Î³ÏÎ±Ï†Î¯Î±, Î±Ï€Î¿Î¸ÎµÏ„Î®ÏÎ¹Î± ÎºÏÎ´Î¹ÎºÎ± ÎºÎ±Î¹ Ï€Î¿Î»Ï…Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ¿ÏÏ‚ Ï€ÏŒÏÎ¿Ï…Ï‚. Î— Î¼ÎµÎ¸Î¿Î´Î¿Î»Î¿Î³Î¯Î± ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚ Î´Î¯Î½ÎµÎ¹ Î­Î¼Ï†Î±ÏƒÎ· Ï„ÏŒÏƒÎ¿ ÏƒÏ„Î¿ ÎµÏÏÎ¿Ï‚ Ï„Î·Ï‚ Î³Î½ÏÏƒÎ·Ï‚ ÏŒÏƒÎ¿ ÎºÎ±Î¹ ÏƒÏ„Î¿ Î²Î¬Î¸Î¿Ï‚ Ï„Î·Ï‚ ÎºÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ·Ï‚ ÏƒÎµ Î´Î¹Î¬Ï†Î¿ÏÎ¿Ï…Ï‚ Ï„Î¿Î¼ÎµÎ¯Ï‚ ÎºÎ±Î¹ Î³Î»ÏÏƒÏƒÎµÏ‚.

### Î ÏÎ¿Î·Î³Î¼Î­Î½Î· Î›Î¿Î³Î¹ÎºÎ® ÎºÎ±Î¹ Î£ÎºÎ­ÏˆÎ·

Î¤Î± Ï€ÏÏŒÏƒÏ†Î±Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± Qwen ÎµÎ½ÏƒÏ‰Î¼Î±Ï„ÏÎ½Î¿Ï…Î½ ÎµÎ¾ÎµÎ»Î¹Î³Î¼Î­Î½ÎµÏ‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ Î»Î¿Î³Î¹ÎºÎ®Ï‚ Ï€Î¿Ï… ÎµÏ€Î¹Ï„ÏÎ­Ï€Î¿Ï…Î½ Ï„Î·Î½ ÎµÏ€Î¯Î»Ï…ÏƒÎ· ÏƒÏÎ½Î¸ÎµÏ„Ï‰Î½ Ï€ÏÎ¿Î²Î»Î·Î¼Î¬Ï„Ï‰Î½ Ï€Î¿Î»Î»ÏÎ½ Î²Î·Î¼Î¬Ï„Ï‰Î½:

**Î›ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î± Î£ÎºÎ­ÏˆÎ·Ï‚ (Qwen3)**: Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± ÎµÎ¼Ï€Î»Î±ÎºÎ¿ÏÎ½ ÏƒÎµ Î»ÎµÏ€Ï„Î¿Î¼ÎµÏÎ® Î»Î¿Î³Î¹ÎºÎ® Î²Î®Î¼Î± Ï€ÏÎ¿Ï‚ Î²Î®Î¼Î± Ï€ÏÎ¹Î½ Ï€Î±ÏÎ­Ï‡Î¿Ï…Î½ Ï„ÎµÎ»Î¹ÎºÎ­Ï‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚, Ï€Î±ÏÏŒÎ¼Î¿Î¹Î± Î¼Îµ Ï„Î¹Ï‚ Î±Î½Î¸ÏÏÏ€Î¹Î½ÎµÏ‚ Ï€ÏÎ¿ÏƒÎµÎ³Î³Î¯ÏƒÎµÎ¹Ï‚ ÎµÏ€Î¯Î»Ï…ÏƒÎ·Ï‚ Ï€ÏÎ¿Î²Î»Î·Î¼Î¬Ï„Ï‰Î½.

**Î”Î¹Ï€Î»Î® Î›ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î±**: Î”Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„Î± ÎµÎ½Î±Î»Î»Î±Î³Î®Ï‚ Î¼ÎµÏ„Î±Î¾Ï Î³ÏÎ®Î³Î¿ÏÎ·Ï‚ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î±Ï‚ Î±Ï€ÏŒÎºÏÎ¹ÏƒÎ·Ï‚ Î³Î¹Î± Î±Ï€Î»Î¬ ÎµÏÏ‰Ï„Î®Î¼Î±Ï„Î± ÎºÎ±Î¹ Î²Î±Î¸ÏÏ„ÎµÏÎ·Ï‚ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î±Ï‚ ÏƒÎºÎ­ÏˆÎ·Ï‚ Î³Î¹Î± ÏƒÏÎ½Î¸ÎµÏ„Î± Ï€ÏÎ¿Î²Î»Î®Î¼Î±Ï„Î±.

**Î•Î½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· Î‘Î»Ï…ÏƒÎ¯Î´Î±Ï‚ Î£ÎºÎ­ÏˆÎ·Ï‚**: Î¦Ï…ÏƒÎ¹ÎºÎ® ÎµÎ½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· Î²Î·Î¼Î¬Ï„Ï‰Î½ Î»Î¿Î³Î¹ÎºÎ®Ï‚ Ï€Î¿Ï… Î²ÎµÎ»Ï„Î¹ÏÎ½Î¿Ï…Î½ Ï„Î· Î´Î¹Î±Ï†Î¬Î½ÎµÎ¹Î± ÎºÎ±Î¹ Ï„Î·Î½ Î±ÎºÏÎ¯Î²ÎµÎ¹Î± ÏƒÎµ ÏƒÏÎ½Î¸ÎµÏ„ÎµÏ‚ ÎµÏÎ³Î±ÏƒÎ¯ÎµÏ‚.

### ÎšÎ±Î¹Î½Î¿Ï„Î¿Î¼Î¯ÎµÏ‚ Î‘ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ®Ï‚

Î— Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î± Qwen ÎµÎ½ÏƒÏ‰Î¼Î±Ï„ÏÎ½ÎµÎ¹ Î±ÏÎºÎµÏ„Î­Ï‚ Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹Ï‚ Î±ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ®Ï‚ Ï€Î¿Ï… Î­Ï‡Î¿Ï…Î½ ÏƒÏ‡ÎµÎ´Î¹Î±ÏƒÏ„ÎµÎ¯ Ï„ÏŒÏƒÎ¿ Î³Î¹Î± Î±Ï€ÏŒÎ´Î¿ÏƒÎ· ÏŒÏƒÎ¿ ÎºÎ±Î¹ Î³Î¹Î± Î±Ï€Î¿Î´Î¿Ï„Î¹ÎºÏŒÏ„Î·Ï„Î±:

**ÎšÎ»Î¹Î¼Î±ÎºÎ¿ÏÎ¼ÎµÎ½Î¿Ï‚ Î£Ï‡ÎµÎ´Î¹Î±ÏƒÎ¼ÏŒÏ‚**: Î£Ï…Î½ÎµÏ€Î®Ï‚ Î±ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ® ÏƒÎµ ÏŒÎ»Î± Ï„Î± Î¼ÎµÎ³Î­Î¸Î· Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Ï€Î¿Ï… ÎµÏ€Î¹Ï„ÏÎ­Ï€ÎµÎ¹ ÎµÏÎºÎ¿Î»Î· ÎºÎ»Î¹Î¼Î¬ÎºÏ‰ÏƒÎ· ÎºÎ±Î¹ ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ·.

**Î Î¿Î»Ï…Ï„ÏÎ¿Ï€Î¹ÎºÎ® Î•Î½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ·**: Î‘Ï€ÏÏŒÏƒÎºÎ¿Ï€Ï„Î· ÎµÎ½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· Î´Ï…Î½Î±Ï„Î¿Ï„Î®Ï„Ï‰Î½ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…, ÎµÎ¹ÎºÏŒÎ½Î±Ï‚ ÎºÎ±Î¹ Î®Ï‡Î¿Ï… ÏƒÎµ ÎµÎ½Î¹Î±Î¯ÎµÏ‚ Î±ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ­Ï‚.

**Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚**: Î Î¿Î»Î»Î±Ï€Î»Î­Ï‚ ÎµÏ€Î¹Î»Î¿Î³Î­Ï‚ Ï€Î¿ÏƒÎ¿Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚ ÎºÎ±Î¹ Î¼Î¿ÏÏ†Î­Ï‚ Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚ Î³Î¹Î± Î´Î¹Î¬Ï†Î¿ÏÎµÏ‚ Î´Î¹Î±Î¼Î¿ÏÏ†ÏÏƒÎµÎ¹Ï‚ Ï…Î»Î¹ÎºÎ¿Ï.

## ÎœÎ­Î³ÎµÎ¸Î¿Ï‚ ÎœÎ¿Î½Ï„Î­Î»Î¿Ï… ÎºÎ±Î¹ Î•Ï€Î¹Î»Î¿Î³Î­Ï‚ Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚

Î¤Î± ÏƒÏÎ³Ï‡ÏÎ¿Î½Î± Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î± Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚ ÎµÏ€Ï‰Ï†ÎµÎ»Î¿ÏÎ½Ï„Î±Î¹ Î±Ï€ÏŒ Ï„Î·Î½ ÎµÏ…ÎµÎ»Î¹Î¾Î¯Î± Ï„Ï‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Qwen ÏƒÎµ Î´Î¹Î¬Ï†Î¿ÏÎµÏ‚ Î±Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„Î¹ÎºÎ®Ï‚ Î¹ÏƒÏ‡ÏÎ¿Ï‚:

### ÎœÎ¹ÎºÏÎ¬ ÎœÎ¿Î½Ï„Î­Î»Î± (0.5B-3B)

Î¤Î± Qwen Ï€Î±ÏÎ­Ï‡Î¿Ï…Î½ Î±Ï€Î¿Î´Î¿Ï„Î¹ÎºÎ¬ Î¼Î¹ÎºÏÎ¬ Î¼Î¿Î½Ï„Î­Î»Î± ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î± Î³Î¹Î± Î±Î½Î¬Ï€Ï„Ï…Î¾Î· ÏƒÎµ edge, ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚ ÎºÎ¹Î½Î·Ï„ÏÎ½ ÎºÎ±Î¹ Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î± Î¼Îµ Ï€ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼Î­Î½Î¿Ï…Ï‚ Ï€ÏŒÏÎ¿Ï…Ï‚, Î´Î¹Î±Ï„Î·ÏÏÎ½Ï„Î±Ï‚ ÎµÎ½Ï„Ï…Ï€Ï‰ÏƒÎ¹Î±ÎºÎ­Ï‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚.

### ÎœÎµÏƒÎ±Î¯Î± ÎœÎ¿Î½Ï„Î­Î»Î± (7B-32B)

Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± Î¼ÎµÏƒÎ±Î¯Î±Ï‚ ÎºÎ»Î¯Î¼Î±ÎºÎ±Ï‚ Ï€ÏÎ¿ÏƒÏ†Î­ÏÎ¿Ï…Î½ ÎµÎ½Î¹ÏƒÏ‡Ï…Î¼Î­Î½ÎµÏ‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ Î³Î¹Î± ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÎ­Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚, Ï€Î±ÏÎ­Ï‡Î¿Î½Ï„Î±Ï‚ ÎµÎ¾Î±Î¹ÏÎµÏ„Î¹ÎºÎ® Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î¯Î± Î¼ÎµÏ„Î±Î¾Ï Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚ ÎºÎ±Î¹ Î±Ï€Î±Î¹Ï„Î®ÏƒÎµÏ‰Î½ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„Î¹ÎºÎ®Ï‚ Î¹ÏƒÏ‡ÏÎ¿Ï‚.

### ÎœÎµÎ³Î¬Î»Î± ÎœÎ¿Î½Ï„Î­Î»Î± (72B+)

Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± Ï€Î»Î®ÏÎ¿Ï…Ï‚ ÎºÎ»Î¯Î¼Î±ÎºÎ±Ï‚ Ï€Î±ÏÎ­Ï‡Î¿Ï…Î½ ÎºÎ¿ÏÏ…Ï†Î±Î¯Î± Î±Ï€ÏŒÎ´Î¿ÏƒÎ· Î³Î¹Î± Î±Ï€Î±Î¹Ï„Î·Ï„Î¹ÎºÎ­Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚, Î­ÏÎµÏ…Î½Î± ÎºÎ±Î¹ Î±Î½Î±Ï€Ï„ÏÎ¾ÎµÎ¹Ï‚ ÎµÏ€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½ Ï€Î¿Ï… Î±Ï€Î±Î¹Ï„Î¿ÏÎ½ Î¼Î­Î³Î¹ÏƒÏ„ÎµÏ‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚.

## Î Î»ÎµÎ¿Î½ÎµÎºÏ„Î®Î¼Î±Ï„Î± Ï„Î·Ï‚ ÎŸÎ¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î±Ï‚ ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½ Qwen

### Î ÏÎ¿ÏƒÎ²Î±ÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î± Î‘Î½Î¿Î¹Ï‡Ï„Î¿Ï ÎšÏÎ´Î¹ÎºÎ±

Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± Qwen Ï€Î±ÏÎ­Ï‡Î¿Ï…Î½ Ï€Î»Î®ÏÎ· Î´Î¹Î±Ï†Î¬Î½ÎµÎ¹Î± ÎºÎ±Î¹ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿Î³Î®Ï‚, ÎµÏ€Î¹Ï„ÏÎ­Ï€Î¿Î½Ï„Î±Ï‚ ÏƒÏ„Î¿Ï…Ï‚ Î¿ÏÎ³Î±Î½Î¹ÏƒÎ¼Î¿ÏÏ‚ Î½Î± ÎºÎ±Ï„Î±Î½Î¿Î¿ÏÎ½, Î½Î± Ï„ÏÎ¿Ï€Î¿Ï€Î¿Î¹Î¿ÏÎ½ ÎºÎ±Î¹ Î½Î± Ï€ÏÎ¿ÏƒÎ±ÏÎ¼ÏŒÎ¶Î¿Ï…Î½ Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± ÏƒÏ„Î¹Ï‚ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½ÎµÏ‚ Î±Î½Î¬Î³ÎºÎµÏ‚ Ï„Î¿Ï…Ï‚ Ï‡Ï‰ÏÎ¯Ï‚ ÎµÎ¾Î¬ÏÏ„Î·ÏƒÎ· Î±Ï€ÏŒ Ï€ÏÎ¿Î¼Î·Î¸ÎµÏ…Ï„Î­Ï‚.

### Î•Ï…ÎµÎ»Î¹Î¾Î¯Î± Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚

Î— Ï€Î¿Î¹ÎºÎ¹Î»Î¯Î± Î¼ÎµÎ³ÎµÎ¸ÏÎ½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ ÎµÏ€Î¹Ï„ÏÎ­Ï€ÎµÎ¹ Ï„Î·Î½ Î±Î½Î¬Ï€Ï„Ï…Î¾Î· ÏƒÎµ Î´Î¹Î¬Ï†Î¿ÏÎµÏ‚ Î´Î¹Î±Î¼Î¿ÏÏ†ÏÏƒÎµÎ¹Ï‚ Ï…Î»Î¹ÎºÎ¿Ï, Î±Ï€ÏŒ ÎºÎ¹Î½Î·Ï„Î­Ï‚ ÏƒÏ…ÏƒÎºÎµÏ…Î­Ï‚ Î­Ï‰Ï‚ Î´Î¹Î±ÎºÎ¿Î¼Î¹ÏƒÏ„Î­Ï‚ Ï…ÏˆÎ·Î»ÏÎ½ Ï€ÏÎ¿Î´Î¹Î±Î³ÏÎ±Ï†ÏÎ½, Ï€Î±ÏÎ­Ï‡Î¿Î½Ï„Î±Ï‚ ÏƒÏ„Î¿Ï…Ï‚ Î¿ÏÎ³Î±Î½Î¹ÏƒÎ¼Î¿ÏÏ‚ ÎµÏ…ÎµÎ»Î¹Î¾Î¯Î± ÏƒÏ„Î¹Ï‚ ÎµÏ€Î¹Î»Î¿Î³Î­Ï‚ Ï…Ï€Î¿Î´Î¿Î¼Î®Ï‚ AI.

### Î Î¿Î»Ï…Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ® Î‘ÏÎ¹ÏƒÏ„ÎµÎ¯Î±

Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± Qwen Î´Î¹Î±Ï€ÏÎ­Ï€Î¿Ï…Î½ ÏƒÏ„Î·Î½ Ï€Î¿Î»Ï…Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ® ÎºÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ· ÎºÎ±Î¹ Ï€Î±ÏÎ±Î³Ï‰Î³Î®, Ï…Ï€Î¿ÏƒÏ„Î·ÏÎ¯Î¶Î¿Î½Ï„Î±Ï‚ Î´ÎµÎºÎ¬Î´ÎµÏ‚ Î³Î»ÏÏƒÏƒÎµÏ‚ Î¼Îµ Î¹Î´Î¹Î±Î¯Ï„ÎµÏÎ· Î´ÏÎ½Î±Î¼Î· ÏƒÏ„Î± Î‘Î³Î³Î»Î¹ÎºÎ¬ ÎºÎ±Î¹ Ï„Î± ÎšÎ¹Î½Î­Î¶Î¹ÎºÎ±, ÎºÎ±Î¸Î¹ÏƒÏ„ÏÎ½Ï„Î±Ï‚ Ï„Î± ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î± Î³Î¹Î± Ï€Î±Î³ÎºÏŒÏƒÎ¼Î¹ÎµÏ‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚.

### Î‘Î½Ï„Î±Î³Ï‰Î½Î¹ÏƒÏ„Î¹ÎºÎ® Î‘Ï€ÏŒÎ´Î¿ÏƒÎ·

Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± Qwen ÎµÏ€Î¹Ï„Ï…Î³Ï‡Î¬Î½Î¿Ï…Î½ ÏƒÏ„Î±Î¸ÎµÏÎ¬ Î±Î½Ï„Î±Î³Ï‰Î½Î¹ÏƒÏ„Î¹ÎºÎ¬ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± ÏƒÎµ benchmarks, Ï€Î±ÏÎ­Ï‡Î¿Î½Ï„Î±Ï‚ Ï€Î±ÏÎ¬Î»Î»Î·Î»Î± Ï€ÏÎ¿ÏƒÎ²Î±ÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î± Î±Î½Î¿Î¹Ï‡Ï„Î¿Ï ÎºÏÎ´Î¹ÎºÎ±, Î±Ï€Î¿Î´ÎµÎ¹ÎºÎ½ÏÎ¿Î½Ï„Î±Ï‚ ÏŒÏ„Î¹ Ï„Î± Î±Î½Î¿Î¹Ï‡Ï„Î¬ Î¼Î¿Î½Ï„Î­Î»Î± Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± Î±Î½Ï„Î±Î³Ï‰Î½Î¹ÏƒÏ„Î¿ÏÎ½ Î¹Î´Î¹ÏŒÎºÏ„Î·Ï„Î± ÎµÎ½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ¬.

### Î•Î¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½ÎµÏ‚ Î”Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚

Î Î±ÏÎ±Î»Î»Î±Î³Î­Ï‚ ÎµÎ¹Î´Î¹ÎºÎ­Ï‚ Î³Î¹Î± Ï„Î¿Î¼ÎµÎ¯Ï‚ ÏŒÏ€Ï‰Ï‚ Ï„Î¿ Qwen-Coder ÎºÎ±Î¹ Ï„Î¿ Qwen-Math Ï€Î±ÏÎ­Ï‡Î¿Ï…Î½ ÎµÎ¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î· Î³Î½ÏÏƒÎ·, Î´Î¹Î±Ï„Î·ÏÏÎ½Ï„Î±Ï‚ Ï€Î±ÏÎ¬Î»Î»Î·Î»Î± Î³ÎµÎ½Î¹ÎºÎ­Ï‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ ÎºÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ·Ï‚ Î³Î»ÏÏƒÏƒÎ±Ï‚.

## Î ÏÎ±ÎºÏ„Î¹ÎºÎ¬ Î Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î± ÎºÎ±Î¹ Î§ÏÎ®ÏƒÎµÎ¹Ï‚

Î ÏÎ¹Î½ ÎµÎ¼Î²Î±Î¸ÏÎ½Î¿Ï…Î¼Îµ ÏƒÏ„Î¹Ï‚ Ï„ÎµÏ‡Î½Î¹ÎºÎ­Ï‚ Î»ÎµÏ€Ï„Î¿Î¼Î­ÏÎµÎ¹ÎµÏ‚, Î±Ï‚ ÎµÎ¾ÎµÏÎµÏ…Î½Î®ÏƒÎ¿Ï…Î¼Îµ Î¼ÎµÏÎ¹ÎºÎ¬ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î± Ï€Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î± Î³Î¹Î± Ï„Î¿ Ï„Î¹ Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± ÎµÏ€Î¹Ï„ÏÏ‡Î¿Ï…Î½ Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± Qwen:

### Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± ÎœÎ±Î¸Î·Î¼Î±Ï„Î¹ÎºÎ®Ï‚ Î›Î¿Î³Î¹ÎºÎ®Ï‚

Î¤Î¿ Qwen-Math Î´Î¹Î±Ï€ÏÎ­Ï€ÎµÎ¹ ÏƒÏ„Î·Î½ ÎµÏ€Î¯Î»Ï…ÏƒÎ· Î¼Î±Î¸Î·Î¼Î±Ï„Î¹ÎºÏÎ½ Ï€ÏÎ¿Î²Î»Î·Î¼Î¬Ï„Ï‰Î½ Î²Î®Î¼Î± Ï€ÏÎ¿Ï‚ Î²Î®Î¼Î±. Î“Î¹Î± Ï€Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î±, ÏŒÏ„Î±Î½ Ï„Î¿Ï… Î¶Î·Ï„Î·Î¸ÎµÎ¯ Î½Î± Î»ÏÏƒÎµÎ¹ Î­Î½Î± ÏƒÏÎ½Î¸ÎµÏ„Î¿ Ï€ÏÏŒÎ²Î»Î·Î¼Î± Î»Î¿Î³Î¹ÏƒÎ¼Î¿Ï:

```
User: Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = xÂ³ + 2xÂ² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹

Breaking down each term:
- d/dx(xÂ³) = 3xÂ²
- d/dx(2xÂ²) = 2 Â· 2xÂ¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3xÂ² + 4x - 5
```

### Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± Î Î¿Î»Ï…Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ®Ï‚ Î¥Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î·Ï‚

Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± Qwen ÎµÏ€Î¹Î´ÎµÎ¹ÎºÎ½ÏÎ¿Ï…Î½ Î¹ÏƒÏ‡Ï…ÏÎ­Ï‚ Ï€Î¿Î»Ï…Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ­Ï‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ ÏƒÎµ Î´Î¹Î¬Ï†Î¿ÏÎµÏ‚ Î³Î»ÏÏƒÏƒÎµÏ‚:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**ä¸­æ–‡:**
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡æœºå™¨ï¼ˆç‰¹åˆ«æ˜¯è®¡ç®—æœºç³»ç»Ÿï¼‰å¯¹äººç±»æ™ºèƒ½è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚è¿™äº›è¿‡ç¨‹åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€è§£å†³é—®é¢˜ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚äººå·¥æ™ºèƒ½ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»è®¤çŸ¥èƒ½åŠ›çš„ä»»åŠ¡ã€‚
```

### Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± Î Î¿Î»Ï…Ï„ÏÎ¿Ï€Î¹ÎºÏÎ½ Î”Ï…Î½Î±Ï„Î¿Ï„Î®Ï„Ï‰Î½

Î¤Î¿ Qwen-VL Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÏ„ÎµÎ¯ Ï„Î±Ï…Ï„ÏŒÏ‡ÏÎ¿Î½Î± ÎºÎµÎ¯Î¼ÎµÎ½Î¿ ÎºÎ±Î¹ ÎµÎ¹ÎºÏŒÎ½ÎµÏ‚:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± Î Î±ÏÎ±Î³Ï‰Î³Î®Ï‚ ÎšÏÎ´Î¹ÎºÎ±

Î¤Î¿ Qwen-Coder Î´Î¹Î±Ï€ÏÎ­Ï€ÎµÎ¹ ÏƒÏ„Î·Î½ Ï€Î±ÏÎ±Î³Ï‰Î³Î® ÎºÎ±Î¹ ÎµÎ¾Î®Î³Î·ÏƒÎ· ÎºÏÎ´Î¹ÎºÎ± ÏƒÎµ Ï€Î¿Î»Î»Î­Ï‚ Î³Î»ÏÏƒÏƒÎµÏ‚ Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÎ¼Î¿Ï:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Î•ÎºÏ„ÎµÎ»ÎµÎ¯ Î´Ï…Î±Î´Î¹ÎºÎ® Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· ÏƒÎµ Ï„Î±Î¾Î¹Î½Î¿Î¼Î·Î¼Î­Î½Î¿ Ï€Î¯Î½Î±ÎºÎ± Î³Î¹Î± Î½Î± Î²ÏÎµÎ¹ Ï„Î·Î½ Ï„Î¹Î¼Î® ÏƒÏ„ÏŒÏ‡Î¿.
    
    Args:
        arr (list): ÎˆÎ½Î±Ï‚ Ï„Î±Î¾Î¹Î½Î¿Î¼Î·Î¼Î­Î½Î¿Ï‚ Ï€Î¯Î½Î±ÎºÎ±Ï‚ ÏƒÏ…Î³ÎºÏÎ¯ÏƒÎ¹Î¼Ï‰Î½ ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Ï‰Î½
        target: Î— Ï„Î¹Î¼Î® Ï€Î¿Ï… Î±Î½Î±Î¶Î·Ï„ÎµÎ¯Ï„Î±Î¹
        
    Returns:
        int: Î”ÎµÎ¯ÎºÏ„Î·Ï‚ Ï„Î·Ï‚ Ï„Î¹Î¼Î®Ï‚ ÏƒÏ„ÏŒÏ‡Î¿Ï… Î±Î½ Î²ÏÎµÎ¸ÎµÎ¯, -1 Î±Î½ Î´ÎµÎ½ Î²ÏÎµÎ¸ÎµÎ¯
        
    Î§ÏÎ¿Î½Î¿Î»Î¿Î³Î¹ÎºÎ® Î Î¿Î»Ï…Ï€Î»Î¿ÎºÏŒÏ„Î·Ï„Î±: O(log n)
    Î§Ï‰ÏÎ¹ÎºÎ® Î Î¿Î»Ï…Ï€Î»Î¿ÎºÏŒÏ„Î·Ï„Î±: O(1)
    """
    # Î‘ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Î´ÎµÎ¹ÎºÏ„ÏÎ½ Î±ÏÎ¹ÏƒÏ„ÎµÏÎ¬ ÎºÎ±Î¹ Î´ÎµÎ¾Î¹Î¬
    left, right = 0, len(arr) - 1
    
    # Î£Ï…Î½Î­Ï‡Î¹ÏƒÎ· Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚ ÏŒÏƒÎ¿ Ï„Î¿ Î´Î¹Î¬ÏƒÏ„Î·Î¼Î± Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚ ÎµÎ¯Î½Î±Î¹ Î­Î³ÎºÏ…ÏÎ¿
    while left <= right:
        # Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Î¼ÎµÏƒÎ±Î¯Î¿Ï… Î´ÎµÎ¯ÎºÏ„Î· Î³Î¹Î± Î±Ï€Î¿Ï†Ï…Î³Î® Ï…Ï€ÎµÏÏ‡ÎµÎ¯Î»Î¹ÏƒÎ·Ï‚ Î±ÎºÎ­ÏÎ±Î¹Ï‰Î½
        mid = left + (right - left) // 2
        
        # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î±Î½ Î²ÏÎ­Î¸Î·ÎºÎµ Î· Ï„Î¹Î¼Î® ÏƒÏ„ÏŒÏ‡Î¿Ï‚
        if arr[mid] == target:
            return mid
        
        # Î‘Î½ Î· Ï„Î¹Î¼Î® ÏƒÏ„ÏŒÏ‡Î¿Ï‚ ÎµÎ¯Î½Î±Î¹ Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎ·, Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· ÏƒÏ„Î¿ Î±ÏÎ¹ÏƒÏ„ÎµÏÏŒ Î¼Î¹ÏƒÏŒ
        elif arr[mid] > target:
            right = mid - 1
        
        # Î‘Î½ Î· Ï„Î¹Î¼Î® ÏƒÏ„ÏŒÏ‡Î¿Ï‚ ÎµÎ¯Î½Î±Î¹ Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ·, Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· ÏƒÏ„Î¿ Î´ÎµÎ¾Î¯ Î¼Î¹ÏƒÏŒ
        else:
            left = mid + 1
    
    # Î— Ï„Î¹Î¼Î® ÏƒÏ„ÏŒÏ‡Î¿Ï‚ Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ
    return -1

# Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± Ï‡ÏÎ®ÏƒÎ·Ï‚:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Î”ÎµÎ¯ÎºÏ„Î·Ï‚ Ï„Î¿Ï… 7: {result}")  # ÎˆÎ¾Î¿Î´Î¿Ï‚: Î”ÎµÎ¯ÎºÏ„Î·Ï‚ Ï„Î¿Ï… 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚ ÏƒÎµ Edge

Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± Qwen Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± Î±Î½Î±Ï€Ï„Ï…Ï‡Î¸Î¿ÏÎ½ ÏƒÎµ Î´Î¹Î¬Ï†Î¿ÏÎµÏ‚ ÏƒÏ…ÏƒÎºÎµÏ…Î­Ï‚ edge Î¼Îµ Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½ÎµÏ‚ Î´Î¹Î±Î¼Î¿ÏÏ†ÏÏƒÎµÎ¹Ï‚:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Î— Î•Î¾Î­Î»Î¹Î¾Î· Ï„Î·Ï‚ ÎŸÎ¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î±Ï‚ Qwen

### Qwen 1.0 ÎºÎ±Î¹ 1
Î ÏÏ‚ Î½Î± Î¾ÎµÎºÎ¹Î½Î®ÏƒÎµÏ„Îµ Î¼Îµ Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± Qwen Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î· Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎ· Hugging Face Transformers:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Î§ÏÎ®ÏƒÎ· ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½ Qwen2.5

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### Î•Î¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î· Î§ÏÎ®ÏƒÎ· ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½

**Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÎšÏÎ´Î¹ÎºÎ± Î¼Îµ Qwen-Coder:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**Î•Ï€Î¯Î»Ï…ÏƒÎ· ÎœÎ±Î¸Î·Î¼Î±Ï„Î¹ÎºÏÎ½ Î ÏÎ¿Î²Î»Î·Î¼Î¬Ï„Ï‰Î½:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**Î•ÏÎ³Î±ÏƒÎ¯ÎµÏ‚ ÎŒÏÎ±ÏƒÎ·Ï‚-Î“Î»ÏÏƒÏƒÎ±Ï‚:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### Î›ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î± Î£ÎºÎ­ÏˆÎ·Ï‚ (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### ğŸ“± Î‘Î½Î¬Ï€Ï„Ï…Î¾Î· ÏƒÎµ ÎšÎ¹Î½Î·Ï„Î­Ï‚ Î£Ï…ÏƒÎºÎµÏ…Î­Ï‚ ÎºÎ±Î¹ Edge

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚ API

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## Î•Ï€Î¹Î´ÏŒÏƒÎµÎ¹Ï‚ ÎºÎ±Î¹ Î•Ï€Î¹Ï„ÎµÏÎ³Î¼Î±Ï„Î±

Î— Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î± Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Qwen Î­Ï‡ÎµÎ¹ ÎµÏ€Î¹Ï„ÏÏ‡ÎµÎ¹ ÎµÎ¾Î±Î¹ÏÎµÏ„Î¹ÎºÎ­Ï‚ ÎµÏ€Î¹Î´ÏŒÏƒÎµÎ¹Ï‚ ÏƒÎµ Î´Î¹Î¬Ï†Î¿ÏÎ± benchmarks, Î´Î¹Î±Ï„Î·ÏÏÎ½Ï„Î±Ï‚ Ï€Î±ÏÎ¬Î»Î»Î·Î»Î± Ï„Î·Î½ Ï€ÏÎ¿ÏƒÎ²Î±ÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î± Ï‰Ï‚ Î±Î½Î¿Î¹Ï‡Ï„Î¿Ï ÎºÏÎ´Î¹ÎºÎ±:

### Î’Î±ÏƒÎ¹ÎºÎ¬ Î£Î·Î¼ÎµÎ¯Î± Î•Ï€Î¹Î´ÏŒÏƒÎµÏ‰Î½

**Î•Î¾Î±Î¹ÏÎµÏ„Î¹ÎºÎ® Î›Î¿Î³Î¹ÎºÎ®:**
- Î¤Î¿ Qwen3-235B-A22B ÎµÏ€Î¹Ï„Ï…Î³Ï‡Î¬Î½ÎµÎ¹ Î±Î½Ï„Î±Î³Ï‰Î½Î¹ÏƒÏ„Î¹ÎºÎ¬ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± ÏƒÎµ Î±Î¾Î¹Î¿Î»Î¿Î³Î®ÏƒÎµÎ¹Ï‚ benchmarks Î³Î¹Î± ÎºÏÎ´Î¹ÎºÎ±, Î¼Î±Î¸Î·Î¼Î±Ï„Î¹ÎºÎ¬, Î³ÎµÎ½Î¹ÎºÎ­Ï‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚, ÏƒÏ…Î³ÎºÏÎ¹Î½ÏŒÎ¼ÎµÎ½Î¿ Î¼Îµ Î¬Î»Î»Î± ÎºÎ¿ÏÏ…Ï†Î±Î¯Î± Î¼Î¿Î½Ï„Î­Î»Î± ÏŒÏ€Ï‰Ï‚ Ï„Î± DeepSeek-R1, o1, o3-mini, Grok-3 ÎºÎ±Î¹ Gemini-2.5-Pro.
- Î¤Î¿ Qwen3-30B-A3B Ï…Ï€ÎµÏÏ„ÎµÏÎµÎ¯ Ï„Î¿Ï… QwQ-32B Î¼Îµ 10 Ï†Î¿ÏÎ­Ï‚ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎµÏ‚ ÎµÎ½ÎµÏÎ³Î¿Ï€Î¿Î¹Î·Î¼Î­Î½ÎµÏ‚ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÎ¿Ï…Ï‚.
- Î¤Î¿ Qwen3-4B Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î±Î½Ï„Î±Î³Ï‰Î½Î¹ÏƒÏ„ÎµÎ¯ Ï„Î·Î½ Î±Ï€ÏŒÎ´Î¿ÏƒÎ· Ï„Î¿Ï… Qwen2.5-72B-Instruct.

**Î•Ï€Î¹Ï„ÎµÏÎ³Î¼Î±Ï„Î± Î‘Ï€Î¿Î´Î¿Ï„Î¹ÎºÏŒÏ„Î·Ï„Î±Ï‚:**
- Î¤Î± Î²Î±ÏƒÎ¹ÎºÎ¬ Î¼Î¿Î½Ï„Î­Î»Î± Qwen3-MoE ÎµÏ€Î¹Ï„Ï…Î³Ï‡Î¬Î½Î¿Ï…Î½ Ï€Î±ÏÏŒÎ¼Î¿Î¹Î± Î±Ï€ÏŒÎ´Î¿ÏƒÎ· Î¼Îµ Ï„Î± Î²Î±ÏƒÎ¹ÎºÎ¬ Î¼Î¿Î½Ï„Î­Î»Î± Qwen2.5 dense, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Î¼ÏŒÎ½Î¿ Ï„Î¿ 10% Ï„Ï‰Î½ ÎµÎ½ÎµÏÎ³ÏÎ½ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½.
- Î£Î·Î¼Î±Î½Ï„Î¹ÎºÎ® ÎµÎ¾Î¿Î¹ÎºÎ¿Î½ÏŒÎ¼Î·ÏƒÎ· ÎºÏŒÏƒÏ„Î¿Ï…Ï‚ Ï„ÏŒÏƒÎ¿ ÏƒÏ„Î·Î½ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· ÏŒÏƒÎ¿ ÎºÎ±Î¹ ÏƒÏ„Î·Î½ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· ÏƒÎµ ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ· Î¼Îµ dense Î¼Î¿Î½Ï„Î­Î»Î±.

**Î Î¿Î»Ï…Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ­Ï‚ Î”Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚:**
- Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± Qwen3 Ï…Ï€Î¿ÏƒÏ„Î·ÏÎ¯Î¶Î¿Ï…Î½ 119 Î³Î»ÏÏƒÏƒÎµÏ‚ ÎºÎ±Î¹ Î´Î¹Î±Î»Î­ÎºÏ„Î¿Ï…Ï‚.
- Î™ÏƒÏ‡Ï…ÏÎ® Î±Ï€ÏŒÎ´Î¿ÏƒÎ· ÏƒÎµ Ï€Î¿Î¹ÎºÎ¯Î»Î± Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ¬ ÎºÎ±Î¹ Ï€Î¿Î»Î¹Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ Ï€Î»Î±Î¯ÏƒÎ¹Î±.

**ÎšÎ»Î¯Î¼Î±ÎºÎ± Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚:**
- Î¤Î¿ Qwen3 Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ ÏƒÏ‡ÎµÎ´ÏŒÎ½ Î´Î¹Ï€Î»Î¬ÏƒÎ¹Î¿ ÏŒÎ³ÎºÎ¿ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½, Î¼Îµ Ï€ÎµÏÎ¯Ï€Î¿Ï… 36 Ï„ÏÎ¹ÏƒÎµÎºÎ±Ï„Î¿Î¼Î¼ÏÏÎ¹Î± tokens Ï€Î¿Ï… ÎºÎ±Î»ÏÏ€Ï„Î¿Ï…Î½ 119 Î³Î»ÏÏƒÏƒÎµÏ‚ ÎºÎ±Î¹ Î´Î¹Î±Î»Î­ÎºÏ„Î¿Ï…Ï‚, ÏƒÎµ ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ· Î¼Îµ Ï„Î± 18 Ï„ÏÎ¹ÏƒÎµÎºÎ±Ï„Î¿Î¼Î¼ÏÏÎ¹Î± tokens Ï„Î¿Ï… Qwen2.5.

### Î Î¯Î½Î±ÎºÎ±Ï‚ Î£ÏÎ³ÎºÏÎ¹ÏƒÎ·Ï‚ ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½

| Î£ÎµÎ¹ÏÎ¬ ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½ | Î•ÏÏÎ¿Ï‚ Î Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½ | ÎœÎ®ÎºÎ¿Ï‚ Î Î»Î±Î¹ÏƒÎ¯Î¿Ï… | Î’Î±ÏƒÎ¹ÎºÎ¬ Î Î»ÎµÎ¿Î½ÎµÎºÏ„Î®Î¼Î±Ï„Î± | ÎšÎ±Î»ÏÏ„ÎµÏÎµÏ‚ Î§ÏÎ®ÏƒÎµÎ¹Ï‚ |
|----------------|------------------|----------------|-----------------------|-------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | Î™ÏƒÎ¿ÏÏÎ¿Ï€Î·Î¼Î­Î½Î· Î±Ï€ÏŒÎ´Î¿ÏƒÎ·, Ï€Î¿Î»Ï…Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÏŒÏ„Î·Ï„Î± | Î“ÎµÎ½Î¹ÎºÎ­Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚, Î±Î½Î¬Ï€Ï„Ï…Î¾Î· Ï€Î±ÏÎ±Î³Ï‰Î³Î®Ï‚ |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÎºÏÎ´Î¹ÎºÎ±, Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒÏ‚ | Î‘Î½Î¬Ï€Ï„Ï…Î¾Î· Î»Î¿Î³Î¹ÏƒÎ¼Î¹ÎºÎ¿Ï, Î²Î¿Î®Î¸ÎµÎ¹Î± ÏƒÏ„Î¿Î½ Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒ |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | ÎœÎ±Î¸Î·Î¼Î±Ï„Î¹ÎºÎ® Î»Î¿Î³Î¹ÎºÎ® | Î•ÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÎ­Ï‚ Ï€Î»Î±Ï„Ï†ÏŒÏÎ¼ÎµÏ‚, ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚ STEM |
| **Qwen2.5-VL** | Î”Î¹Î¬Ï†Î¿ÏÎ± | ÎœÎµÏ„Î±Î²Î»Î·Ï„ÏŒ | ÎšÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ· ÏŒÏÎ±ÏƒÎ·Ï‚-Î³Î»ÏÏƒÏƒÎ±Ï‚ | Î Î¿Î»Ï…Ï„ÏÎ¿Ï€Î¹ÎºÎ­Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚, Î±Î½Î¬Î»Ï…ÏƒÎ· ÎµÎ¹ÎºÏŒÎ½Î±Ï‚ |
| **Qwen3** | 0.6B-235B | ÎœÎµÏ„Î±Î²Î»Î·Ï„ÏŒ | Î ÏÎ¿Î·Î³Î¼Î­Î½Î· Î»Î¿Î³Î¹ÎºÎ®, Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î± ÏƒÎºÎ­ÏˆÎ·Ï‚ | Î£ÏÎ½Î¸ÎµÏ„Î· Î»Î¿Î³Î¹ÎºÎ®, ÎµÏÎµÏ…Î½Î·Ï„Î¹ÎºÎ­Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚ |
| **Qwen3 MoE** | 30B-235B ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ¬ | ÎœÎµÏ„Î±Î²Î»Î·Ï„ÏŒ | Î‘Ï€Î¿Î´Î¿Ï„Î¹ÎºÎ® Î±Ï€ÏŒÎ´Î¿ÏƒÎ· Î¼ÎµÎ³Î¬Î»Î·Ï‚ ÎºÎ»Î¯Î¼Î±ÎºÎ±Ï‚ | Î•Ï„Î±Î¹ÏÎ¹ÎºÎ­Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚, Î±Î½Î¬Î³ÎºÎµÏ‚ Ï…ÏˆÎ·Î»Î®Ï‚ Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚ |

## ÎŸÎ´Î·Î³ÏŒÏ‚ Î•Ï€Î¹Î»Î¿Î³Î®Ï‚ ÎœÎ¿Î½Ï„Î­Î»Î¿Ï…

### Î“Î¹Î± Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ Î•Ï†Î±ÏÎ¼Î¿Î³Î­Ï‚
- **Qwen2.5-0.5B/1.5B**: Î•Ï†Î±ÏÎ¼Î¿Î³Î­Ï‚ ÎºÎ¹Î½Î·Ï„ÏÎ½, edge ÏƒÏ…ÏƒÎºÎµÏ…Î­Ï‚, ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚ ÏƒÎµ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏŒ Ï‡ÏÏŒÎ½Î¿.
- **Qwen2.5-3B/7B**: Î“ÎµÎ½Î¹ÎºÎ¬ chatbots, Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï€ÎµÏÎ¹ÎµÏ‡Î¿Î¼Î­Î½Î¿Ï…, ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î± ÎµÏÏ‰Ï„Î®ÏƒÎµÏ‰Î½-Î±Ï€Î±Î½Ï„Î®ÏƒÎµÏ‰Î½.

### Î“Î¹Î± ÎœÎ±Î¸Î·Î¼Î±Ï„Î¹ÎºÎ­Ï‚ ÎºÎ±Î¹ Î›Î¿Î³Î¹ÎºÎ­Ï‚ Î•ÏÎ³Î±ÏƒÎ¯ÎµÏ‚
- **Qwen2.5-Math**: Î•Ï€Î¯Î»Ï…ÏƒÎ· Î¼Î±Î¸Î·Î¼Î±Ï„Î¹ÎºÏÎ½ Ï€ÏÎ¿Î²Î»Î·Î¼Î¬Ï„Ï‰Î½ ÎºÎ±Î¹ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· STEM.
- **Qwen3 Î¼Îµ Î›ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î± Î£ÎºÎ­ÏˆÎ·Ï‚**: Î£ÏÎ½Î¸ÎµÏ„Î· Î»Î¿Î³Î¹ÎºÎ® Ï€Î¿Ï… Î±Ï€Î±Î¹Ï„ÎµÎ¯ Î±Î½Î¬Î»Ï…ÏƒÎ· Î²Î®Î¼Î± Ï€ÏÎ¿Ï‚ Î²Î®Î¼Î±.

### Î“Î¹Î± Î ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒ ÎºÎ±Î¹ Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·
- **Qwen2.5-Coder**: Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÎºÏÎ´Î¹ÎºÎ±, ÎµÎ½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½, Î²Î¿Î®Î¸ÎµÎ¹Î± ÏƒÏ„Î¿Î½ Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒ.
- **Qwen3**: Î ÏÎ¿Î·Î³Î¼Î­Î½ÎµÏ‚ ÎµÏÎ³Î±ÏƒÎ¯ÎµÏ‚ Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÎ¼Î¿Ï Î¼Îµ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ Î»Î¿Î³Î¹ÎºÎ®Ï‚.

### Î“Î¹Î± Î Î¿Î»Ï…Ï„ÏÎ¿Ï€Î¹ÎºÎ­Ï‚ Î•Ï†Î±ÏÎ¼Î¿Î³Î­Ï‚
- **Qwen2.5-VL**: ÎšÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ· ÎµÎ¹ÎºÏŒÎ½Î±Ï‚, Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚ ÏƒÎµ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î·Î½ ÏŒÏÎ±ÏƒÎ·.
- **Qwen-Audio**: Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î®Ï‡Î¿Ï… ÎºÎ±Î¹ ÎºÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ· Î¿Î¼Î¹Î»Î¯Î±Ï‚.

### Î“Î¹Î± Î•Ï„Î±Î¹ÏÎ¹ÎºÎ® Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·
- **Qwen2.5-32B/72B**: Î¥ÏˆÎ·Î»Î®Ï‚ Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚ ÎºÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ· Î³Î»ÏÏƒÏƒÎ±Ï‚.
- **Qwen3-235B-A22B**: ÎœÎ­Î³Î¹ÏƒÏ„Î· Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„Î± Î³Î¹Î± Î±Ï€Î±Î¹Ï„Î·Ï„Î¹ÎºÎ­Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚.

## Î Î»Î±Ï„Ï†ÏŒÏÎ¼ÎµÏ‚ Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚ ÎºÎ±Î¹ Î ÏÎ¿ÏƒÎ²Î±ÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î±
### Î Î»Î±Ï„Ï†ÏŒÏÎ¼ÎµÏ‚ Cloud
- **Hugging Face Hub**: ÎŸÎ»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î¿ Î±Ï€Î¿Î¸ÎµÏ„Î®ÏÎ¹Î¿ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Î¼Îµ Ï…Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î· ÎºÎ¿Î¹Î½ÏŒÏ„Î·Ï„Î±Ï‚.
- **ModelScope**: Î Î»Î±Ï„Ï†ÏŒÏÎ¼Î± Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Ï„Î·Ï‚ Alibaba Î¼Îµ ÎµÏÎ³Î±Î»ÎµÎ¯Î± Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚.
- **Î”Î¹Î¬Ï†Î¿ÏÎ¿Î¹ Î Î¬ÏÎ¿Ï‡Î¿Î¹ Cloud**: Î¥Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î· Î¼Î­ÏƒÏ‰ Ï„Ï…Ï€Î¹ÎºÏÎ½ Ï€Î»Î±Ï„Ï†Î¿ÏÎ¼ÏÎ½ ML.

### Î Î»Î±Î¯ÏƒÎ¹Î± Î¤Î¿Ï€Î¹ÎºÎ®Ï‚ Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚
- **Transformers**: Î¤Ï…Ï€Î¹ÎºÎ® ÎµÎ½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· Hugging Face Î³Î¹Î± ÎµÏÎºÎ¿Î»Î· Î±Î½Î¬Ï€Ï„Ï…Î¾Î·.
- **vLLM**: Î¥ÏˆÎ·Î»Î®Ï‚ Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚ ÎµÎ¾Ï…Ï€Î·ÏÎ­Ï„Î·ÏƒÎ· Î³Î¹Î± Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î± Ï€Î±ÏÎ±Î³Ï‰Î³Î®Ï‚.
- **Ollama**: Î‘Ï€Î»Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î· Ï„Î¿Ï€Î¹ÎºÎ® Î±Î½Î¬Ï€Ï„Ï…Î¾Î· ÎºÎ±Î¹ Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ·.
- **ONNX Runtime**: Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î³Î¹Î± Î´Î¹Î¬Ï†Î¿ÏÎ¿ hardware.
- **llama.cpp**: Î‘Ï€Î¿Î´Î¿Ï„Î¹ÎºÎ® Ï…Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ· C++ Î³Î¹Î± Ï€Î¿Î¹ÎºÎ¯Î»ÎµÏ‚ Ï€Î»Î±Ï„Ï†ÏŒÏÎ¼ÎµÏ‚.

### Î Î·Î³Î­Ï‚ ÎœÎ¬Î¸Î·ÏƒÎ·Ï‚
- **Î¤ÎµÎºÎ¼Î·ÏÎ¯Ï‰ÏƒÎ· Qwen**: Î•Ï€Î¯ÏƒÎ·Î¼Î· Ï„ÎµÎºÎ¼Î·ÏÎ¯Ï‰ÏƒÎ· ÎºÎ±Î¹ ÎºÎ¬ÏÏ„ÎµÏ‚ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½.
- **Hugging Face Model Hub**: Î”Î¹Î±Î´ÏÎ±ÏƒÏ„Î¹ÎºÎ¬ demos ÎºÎ±Î¹ Ï€Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î± ÎºÎ¿Î¹Î½ÏŒÏ„Î·Ï„Î±Ï‚.
- **Î•ÏÎµÏ…Î½Î·Ï„Î¹ÎºÎ¬ ÎˆÎ³Î³ÏÎ±Ï†Î±**: Î¤ÎµÏ‡Î½Î¹ÎºÎ¬ Î­Î³Î³ÏÎ±Ï†Î± ÏƒÏ„Î¿ arxiv Î³Î¹Î± Î²Î±Î¸ÏÏ„ÎµÏÎ· ÎºÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ·.
- **Î¦ÏŒÏÎ¿Ï…Î¼ ÎšÎ¿Î¹Î½ÏŒÏ„Î·Ï„Î±Ï‚**: Î•Î½ÎµÏÎ³Î® Ï…Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î· ÎºÎ±Î¹ ÏƒÏ…Î¶Î·Ï„Î®ÏƒÎµÎ¹Ï‚ ÎºÎ¿Î¹Î½ÏŒÏ„Î·Ï„Î±Ï‚.

### ÎÎµÎºÎ¹Î½ÏÎ½Ï„Î±Ï‚ Î¼Îµ Ï„Î± ÎœÎ¿Î½Ï„Î­Î»Î± Qwen

#### Î Î»Î±Ï„Ï†ÏŒÏÎ¼ÎµÏ‚ Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚
1. **Hugging Face Transformers**: ÎÎµÎºÎ¹Î½Î®ÏƒÏ„Îµ Î¼Îµ Ï„Ï…Ï€Î¹ÎºÎ® ÎµÎ½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· Python.
2. **ModelScope**: Î•Î¾ÎµÏÎµÏ…Î½Î®ÏƒÏ„Îµ Ï„Î± ÎµÏÎ³Î±Î»ÎµÎ¯Î± Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚ Ï„Î·Ï‚ Alibaba.
3. **Î¤Î¿Ï€Î¹ÎºÎ® Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·**: Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Ï„Î¿ Ollama Î® Î±Ï€ÎµÏ…Î¸ÎµÎ¯Î±Ï‚ transformers Î³Î¹Î± Ï„Î¿Ï€Î¹ÎºÎ­Ï‚ Î´Î¿ÎºÎ¹Î¼Î­Ï‚.

#### Î”Î¹Î±Î´ÏÎ¿Î¼Î® ÎœÎ¬Î¸Î·ÏƒÎ·Ï‚
1. **ÎšÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ· Î’Î±ÏƒÎ¹ÎºÏÎ½ Î•Î½Î½Î¿Î¹ÏÎ½**: ÎœÎµÎ»ÎµÏ„Î®ÏƒÏ„Îµ Ï„Î·Î½ Î±ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ® ÎºÎ±Î¹ Ï„Î¹Ï‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ Ï„Î·Ï‚ Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î±Ï‚ Qwen.
2. **Î ÎµÎ¹ÏÎ±Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒÏ‚ Î¼Îµ Î Î±ÏÎ±Î»Î»Î±Î³Î­Ï‚**: Î”Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Îµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ¬ Î¼ÎµÎ³Î­Î¸Î· Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Î³Î¹Î± Î½Î± ÎºÎ±Ï„Î±Î½Î¿Î®ÏƒÎµÏ„Îµ Ï„Î¹Ï‚ ÎµÏ€Î¹Î´ÏŒÏƒÎµÎ¹Ï‚.
3. **Î ÏÎ±ÎºÏ„Î¹ÎºÎ® Î¥Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ·**: Î‘Î½Î±Ï€Ï„ÏÎ¾Ï„Îµ Î¼Î¿Î½Ï„Î­Î»Î± ÏƒÎµ Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î± Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚.
4. **Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚**: Î ÏÎ¿ÏƒÎ±ÏÎ¼ÏŒÏƒÏ„Îµ Î³Î¹Î± Ï€ÎµÏÎ¹Ï€Ï„ÏÏƒÎµÎ¹Ï‚ Ï‡ÏÎ®ÏƒÎ·Ï‚ Ï€Î±ÏÎ±Î³Ï‰Î³Î®Ï‚.

#### Î’Î­Î»Ï„Î¹ÏƒÏ„ÎµÏ‚ Î ÏÎ±ÎºÏ„Î¹ÎºÎ­Ï‚
- **ÎÎµÎºÎ¹Î½Î®ÏƒÏ„Îµ ÎœÎ¹ÎºÏÎ¬**: ÎÎµÎºÎ¹Î½Î®ÏƒÏ„Îµ Î¼Îµ Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎ± Î¼Î¿Î½Ï„Î­Î»Î± (1.5B-7B) Î³Î¹Î± Î±ÏÏ‡Î¹ÎºÎ® Î±Î½Î¬Ï€Ï„Ï…Î¾Î·.
- **Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Î ÏÏŒÏ„Ï…Ï€Î± Î£Ï…Î½Î¿Î¼Î¹Î»Î¯Î±Ï‚**: Î•Ï†Î±ÏÎ¼ÏŒÏƒÏ„Îµ ÏƒÏ‰ÏƒÏ„Î® Î¼Î¿ÏÏ†Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î³Î¹Î± Î²Î­Î»Ï„Î¹ÏƒÏ„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±.
- **Î Î±ÏÎ±ÎºÎ¿Î»Î¿Ï…Î¸Î®ÏƒÏ„Îµ Î ÏŒÏÎ¿Ï…Ï‚**: Î Î±ÏÎ±ÎºÎ¿Î»Î¿Ï…Î¸Î®ÏƒÏ„Îµ Ï„Î· Ï‡ÏÎ®ÏƒÎ· Î¼Î½Î®Î¼Î·Ï‚ ÎºÎ±Î¹ Ï„Î·Î½ Ï„Î±Ï‡ÏÏ„Î·Ï„Î± Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·Ï‚.
- **Î•Î¾ÎµÏ„Î¬ÏƒÏ„Îµ Î•Î¾ÎµÎ¹Î´Î¯ÎºÎµÏ…ÏƒÎ·**: Î•Ï€Î¹Î»Î­Î¾Ï„Îµ Ï€Î±ÏÎ±Î»Î»Î±Î³Î­Ï‚ ÎµÎ¹Î´Î¹ÎºÎ­Ï‚ Î³Î¹Î± Ï„Î¿Î½ Ï„Î¿Î¼Î­Î± ÏŒÏ„Î±Î½ ÎµÎ¯Î½Î±Î¹ Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„Î¿.

## Î ÏÎ¿Î·Î³Î¼Î­Î½Î± ÎœÎ¿Ï„Î¯Î²Î± Î§ÏÎ®ÏƒÎ·Ï‚

### Î Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î± Fine-tuning

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### Î•Î¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î· ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ® Î ÏÎ¿Ï„ÏÎ¿Ï€ÏÎ½

**Î“Î¹Î± Î£ÏÎ½Î¸ÎµÏ„ÎµÏ‚ Î•ÏÎ³Î±ÏƒÎ¯ÎµÏ‚ Î›Î¿Î³Î¹ÎºÎ®Ï‚:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**Î“Î¹Î± Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÎšÏÎ´Î¹ÎºÎ± Î¼Îµ Î Î»Î±Î¯ÏƒÎ¹Î¿:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### Î Î¿Î»Ï…Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ­Ï‚ Î•Ï†Î±ÏÎ¼Î¿Î³Î­Ï‚

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ä¸­æ–‡)",
        "es": "Spanish (EspaÃ±ol)",
        "fr": "French (FranÃ§ais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (æ—¥æœ¬èª)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ğŸ”§ ÎœÎ¿Ï„Î¯Î²Î± Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚ Î Î±ÏÎ±Î³Ï‰Î³Î®Ï‚

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## Î£Ï„ÏÎ±Ï„Î·Î³Î¹ÎºÎ­Ï‚ Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚ Î‘Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚

### Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· ÎœÎ½Î®Î¼Î·Ï‚

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î ÏÏŒÎ²Î»ÎµÏˆÎ·Ï‚

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## Î’Î­Î»Ï„Î¹ÏƒÏ„ÎµÏ‚ Î ÏÎ±ÎºÏ„Î¹ÎºÎ­Ï‚ ÎºÎ±Î¹ ÎšÎ±Ï„ÎµÏ…Î¸Ï…Î½Ï„Î®ÏÎ¹ÎµÏ‚ Î“ÏÎ±Î¼Î¼Î­Ï‚

### Î‘ÏƒÏ†Î¬Î»ÎµÎ¹Î± ÎºÎ±Î¹ Î™Î´Î¹Ï‰Ï„Î¹ÎºÏŒÏ„Î·Ï„Î±

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### Î Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· ÎºÎ±Î¹ Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ·

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## Î£Ï…Î¼Ï€Î­ÏÎ±ÏƒÎ¼Î±

Î— Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î± Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Qwen Î±Î½Ï„Î¹Ï€ÏÎ¿ÏƒÏ‰Ï€ÎµÏÎµÎ¹ Î¼Î¹Î± Î¿Î»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î· Ï€ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ· Î³Î¹Î± Ï„Î· Î´Î·Î¼Î¿ÎºÏÎ±Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Î·Ï‚ Ï„ÎµÏ‡Î½Î¿Î»Î¿Î³Î¯Î±Ï‚ AI, Î´Î¹Î±Ï„Î·ÏÏÎ½Ï„Î±Ï‚ Ï€Î±ÏÎ¬Î»Î»Î·Î»Î± Î±Î½Ï„Î±Î³Ï‰Î½Î¹ÏƒÏ„Î¹ÎºÎ® Î±Ï€ÏŒÎ´Î¿ÏƒÎ· ÏƒÎµ Î´Î¹Î¬Ï†Î¿ÏÎµÏ‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚. ÎœÎ­ÏƒÏ‰ Ï„Î·Ï‚ Î´Î­ÏƒÎ¼ÎµÏ…ÏƒÎ®Ï‚ Ï„Î·Ï‚ Î³Î¹Î± Ï€ÏÎ¿ÏƒÎ²Î±ÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î± Î±Î½Î¿Î¹Ï‡Ï„Î¿Ï ÎºÏÎ´Î¹ÎºÎ±, Ï€Î¿Î»Ï…Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ­Ï‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ ÎºÎ±Î¹ ÎµÏ…Î­Î»Î¹ÎºÏ„ÎµÏ‚ ÎµÏ€Î¹Î»Î¿Î³Î­Ï‚ Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚, Ï„Î¿ Qwen ÎµÏ€Î¹Ï„ÏÎ­Ï€ÎµÎ¹ ÏƒÎµ Î¿ÏÎ³Î±Î½Î¹ÏƒÎ¼Î¿ÏÏ‚ ÎºÎ±Î¹ Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÏ„Î­Ï‚ Î½Î± Î±Î¾Î¹Î¿Ï€Î¿Î¹Î®ÏƒÎ¿Ï…Î½ Î¹ÏƒÏ‡Ï…ÏÎ­Ï‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ AI Î±Î½ÎµÎ¾Î±ÏÏ„Î®Ï„Ï‰Ï‚ Ï€ÏŒÏÏ‰Î½ Î® ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Ï‰Î½ Î±Ï€Î±Î¹Ï„Î®ÏƒÎµÏ‰Î½.

### Î’Î±ÏƒÎ¹ÎºÎ¬ Î£Î·Î¼ÎµÎ¯Î±

**Î‘ÏÎ¹ÏƒÏ„ÎµÎ¯Î± Î‘Î½Î¿Î¹Ï‡Ï„Î¿Ï ÎšÏÎ´Î¹ÎºÎ±**: Î¤Î¿ Qwen Î±Ï€Î¿Î´ÎµÎ¹ÎºÎ½ÏÎµÎ¹ ÏŒÏ„Î¹ Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± Î±Î½Î¿Î¹Ï‡Ï„Î¿Ï ÎºÏÎ´Î¹ÎºÎ± Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± ÎµÏ€Î¹Ï„ÏÏ‡Î¿Ï…Î½ Î±Ï€ÏŒÎ´Î¿ÏƒÎ· Î±Î½Ï„Î±Î³Ï‰Î½Î¹ÏƒÏ„Î¹ÎºÎ® Î¼Îµ Î¹Î´Î¹ÏŒÎºÏ„Î·Ï„Î± ÎµÎ½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ¬, Ï€Î±ÏÎ­Ï‡Î¿Î½Ï„Î±Ï‚ Ï€Î±ÏÎ¬Î»Î»Î·Î»Î± Î´Î¹Î±Ï†Î¬Î½ÎµÎ¹Î±, Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿Î³Î® ÎºÎ±Î¹ Î­Î»ÎµÎ³Ï‡Î¿.

**ÎšÎ»Î¹Î¼Î±ÎºÎ¿ÏÎ¼ÎµÎ½Î· Î‘ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ®**: Î¤Î¿ ÎµÏÏÎ¿Ï‚ Î±Ï€ÏŒ 0.5B Î­Ï‰Ï‚ 235B Ï€Î±ÏÎ±Î¼Î­Ï„ÏÎ¿Ï…Ï‚ ÎµÏ€Î¹Ï„ÏÎ­Ï€ÎµÎ¹ Ï„Î·Î½ Î±Î½Î¬Ï€Ï„Ï…Î¾Î· ÏƒÎµ ÏŒÎ»Î¿ Ï„Î¿ Ï†Î¬ÏƒÎ¼Î± Ï„Ï‰Î½ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„Î¹ÎºÏÎ½ Ï€ÎµÏÎ¹Î²Î±Î»Î»ÏŒÎ½Ï„Ï‰Î½, Î±Ï€ÏŒ ÎºÎ¹Î½Î·Ï„Î­Ï‚ ÏƒÏ…ÏƒÎºÎµÏ…Î­Ï‚ Î­Ï‰Ï‚ ÎµÏ„Î±Î¹ÏÎ¹ÎºÎ¬ clusters.

**Î•Î¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½ÎµÏ‚ Î”Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚**: Î Î±ÏÎ±Î»Î»Î±Î³Î­Ï‚ ÎµÎ¹Î´Î¹ÎºÎ­Ï‚ Î³Î¹Î± Ï„Î¿Î½ Ï„Î¿Î¼Î­Î± ÏŒÏ€Ï‰Ï‚ Ï„Î¿ Qwen-Coder, Qwen-Math ÎºÎ±Î¹ Qwen-VL Ï€Î±ÏÎ­Ï‡Î¿Ï…Î½ ÎµÎ¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î· Ï„ÎµÏ‡Î½Î¿Î³Î½Ï‰ÏƒÎ¯Î±, Î´Î¹Î±Ï„Î·ÏÏÎ½Ï„Î±Ï‚ Ï€Î±ÏÎ¬Î»Î»Î·Î»Î± Î³ÎµÎ½Î¹ÎºÎ® ÎºÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ· Î³Î»ÏÏƒÏƒÎ±Ï‚.

**Î Î±Î³ÎºÏŒÏƒÎ¼Î¹Î± Î ÏÎ¿ÏƒÎ²Î±ÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î±**: Î™ÏƒÏ‡Ï…ÏÎ® Ï€Î¿Î»Ï…Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ® Ï…Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î· ÏƒÎµ 119+ Î³Î»ÏÏƒÏƒÎµÏ‚ ÎºÎ±Î¸Î¹ÏƒÏ„Î¬ Ï„Î¿ Qwen ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î¿ Î³Î¹Î± Î´Î¹ÎµÎ¸Î½ÎµÎ¯Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚ ÎºÎ±Î¹ Ï€Î¿Î¹ÎºÎ¯Î»ÎµÏ‚ Î²Î¬ÏƒÎµÎ¹Ï‚ Ï‡ÏÎ·ÏƒÏ„ÏÎ½.

**Î£Ï…Î½ÎµÏ‡Î®Ï‚ ÎšÎ±Î¹Î½Î¿Ï„Î¿Î¼Î¯Î±**: Î— ÎµÎ¾Î­Î»Î¹Î¾Î· Î±Ï€ÏŒ Ï„Î¿ Qwen 1.0 ÏƒÏ„Î¿ Qwen3 Î´ÎµÎ¯Ï‡Î½ÎµÎ¹ ÏƒÏ…Î½ÎµÏ‡Î® Î²ÎµÎ»Ï„Î¯Ï‰ÏƒÎ· ÏƒÏ„Î¹Ï‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚, Ï„Î·Î½ Î±Ï€Î¿Î´Î¿Ï„Î¹ÎºÏŒÏ„Î·Ï„Î± ÎºÎ±Î¹ Ï„Î¹Ï‚ ÎµÏ€Î¹Î»Î¿Î³Î­Ï‚ Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚.

### ÎœÎµÎ»Î»Î¿Î½Ï„Î¹ÎºÎ® Î ÏÎ¿Î¿Ï€Ï„Î¹ÎºÎ®

ÎšÎ±Î¸ÏÏ‚ Î· Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î± Qwen ÏƒÏ…Î½ÎµÏ‡Î¯Î¶ÎµÎ¹ Î½Î± ÎµÎ¾ÎµÎ»Î¯ÏƒÏƒÎµÏ„Î±Î¹, Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± Ï€ÎµÏÎ¹Î¼Î­Î½Î¿Ï…Î¼Îµ:

- **Î’ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î· Î‘Ï€Î¿Î´Î¿Ï„Î¹ÎºÏŒÏ„Î·Ï„Î±**: Î£Ï…Î½ÎµÏ‡Î®Ï‚ Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎµÏ‚ Î±Î½Î±Î»Î¿Î³Î¯ÎµÏ‚ Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚-Ï€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½.
- **Î•Ï€ÎµÎºÏ„Î±Î¼Î­Î½ÎµÏ‚ Î Î¿Î»Ï…Ï„ÏÎ¿Ï€Î¹ÎºÎ­Ï‚ Î”Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚**: Î•Î½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· Ï€Î¹Î¿ ÎµÎ¾ÎµÎ»Î¹Î³Î¼Î­Î½Î·Ï‚ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚ ÏŒÏÎ±ÏƒÎ·Ï‚, Î®Ï‡Î¿Ï… ÎºÎ±Î¹ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï….
- **Î’ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î· Î›Î¿Î³Î¹ÎºÎ®**: Î ÏÎ¿Î·Î³Î¼Î­Î½Î¿Î¹ Î¼Î·Ï‡Î±Î½Î¹ÏƒÎ¼Î¿Î¯ ÏƒÎºÎ­ÏˆÎ·Ï‚ ÎºÎ±Î¹ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ ÎµÏ€Î¯Î»Ï…ÏƒÎ·Ï‚ Ï€ÏÎ¿Î²Î»Î·Î¼Î¬Ï„Ï‰Î½ Ï€Î¿Î»Î»ÏÎ½ Î²Î·Î¼Î¬Ï„Ï‰Î½.
- **ÎšÎ±Î»ÏÏ„ÎµÏÎ± Î•ÏÎ³Î±Î»ÎµÎ¯Î± Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚**: Î•Î½Î¹ÏƒÏ‡Ï…Î¼Î­Î½Î± Ï€Î»Î±Î¯ÏƒÎ¹Î± ÎºÎ±Î¹ ÎµÏÎ³Î±Î»ÎµÎ¯Î± Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚ Î³Î¹Î± Ï€Î¿Î¹ÎºÎ¯Î»Î± ÏƒÎµÎ½Î¬ÏÎ¹Î± Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚.
- **Î‘Î½Î¬Ï€Ï„Ï…Î¾Î· ÎšÎ¿Î¹Î½ÏŒÏ„Î·Ï„Î±Ï‚**: Î”Î¹ÎµÏ…ÏÏ…Î¼Î­Î½Î¿ Î¿Î¹ÎºÎ¿ÏƒÏÏƒÏ„Î·Î¼Î± ÎµÏÎ³Î±Î»ÎµÎ¯Ï‰Î½, ÎµÏ†Î±ÏÎ¼Î¿Î³ÏÎ½ ÎºÎ±Î¹ ÏƒÏ…Î½ÎµÎ¹ÏƒÏ†Î¿ÏÏÎ½ ÎºÎ¿Î¹Î½ÏŒÏ„Î·Ï„Î±Ï‚.

### Î•Ï€ÏŒÎ¼ÎµÎ½Î± Î’Î®Î¼Î±Ï„Î±

Î•Î¯Ï„Îµ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯Ï„Îµ chatbot, Î±Î½Î±Ï€Ï„ÏÏƒÏƒÎµÏ„Îµ ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÎ¬ ÎµÏÎ³Î±Î»ÎµÎ¯Î±, Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯Ï„Îµ Î²Î¿Î·Î¸Î¿ÏÏ‚ Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÎ¼Î¿Ï Î® ÎµÏÎ³Î¬Î¶ÎµÏƒÏ„Îµ ÏƒÎµ Ï€Î¿Î»Ï…Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ­Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚, Î· Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î± Qwen Ï€Î±ÏÎ­Ï‡ÎµÎ¹ ÎºÎ»Î¹Î¼Î±ÎºÎ¿ÏÎ¼ÎµÎ½ÎµÏ‚ Î»ÏÏƒÎµÎ¹Ï‚ Î¼Îµ Î¹ÏƒÏ‡Ï…ÏÎ® Ï…Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î· ÎºÎ¿Î¹Î½ÏŒÏ„Î·Ï„Î±Ï‚ ÎºÎ±Î¹ Î¿Î»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î· Ï„ÎµÎºÎ¼Î·ÏÎ¯Ï‰ÏƒÎ·.

Î“Î¹Î± Ï„Î¹Ï‚ Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯ÎµÏ‚ ÎµÎ½Î·Î¼ÎµÏÏÏƒÎµÎ¹Ï‚, ÎºÏ…ÎºÎ»Î¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ ÎºÎ±Î¹ Î»ÎµÏ€Ï„Î¿Î¼ÎµÏÎ® Ï„ÎµÏ‡Î½Î¹ÎºÎ® Ï„ÎµÎºÎ¼Î·ÏÎ¯Ï‰ÏƒÎ·, ÎµÏ€Î¹ÏƒÎºÎµÏ†Î¸ÎµÎ¯Ï„Îµ Ï„Î± ÎµÏ€Î¯ÏƒÎ·Î¼Î± Î±Ï€Î¿Î¸ÎµÏ„Î®ÏÎ¹Î± Qwen ÏƒÏ„Î¿ Hugging Face ÎºÎ±Î¹ ÎµÎ¾ÎµÏÎµÏ…Î½Î®ÏƒÏ„Îµ Ï„Î¹Ï‚ ÎµÎ½ÎµÏÎ³Î­Ï‚ ÏƒÏ…Î¶Î·Ï„Î®ÏƒÎµÎ¹Ï‚ ÎºÎ±Î¹ Ï€Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î± ÎºÎ¿Î¹Î½ÏŒÏ„Î·Ï„Î±Ï‚.

Î¤Î¿ Î¼Î­Î»Î»Î¿Î½ Ï„Î·Ï‚ Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚ AI Î²ÏÎ¯ÏƒÎºÎµÏ„Î±Î¹ ÏƒÎµ Ï€ÏÎ¿ÏƒÎ²Î¬ÏƒÎ¹Î¼Î±, Î´Î¹Î±Ï†Î±Î½Î® ÎºÎ±Î¹ Î¹ÏƒÏ‡Ï…ÏÎ¬ ÎµÏÎ³Î±Î»ÎµÎ¯Î± Ï€Î¿Ï… ÎµÏ€Î¹Ï„ÏÎ­Ï€Î¿Ï…Î½ Ï„Î·Î½ ÎºÎ±Î¹Î½Î¿Ï„Î¿Î¼Î¯Î± ÏƒÎµ ÏŒÎ»Î¿Ï…Ï‚ Ï„Î¿Ï…Ï‚ Ï„Î¿Î¼ÎµÎ¯Ï‚ ÎºÎ±Î¹ ÎºÎ»Î¯Î¼Î±ÎºÎµÏ‚. Î— Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î± Qwen ÎµÎ½ÏƒÎ±ÏÎºÏÎ½ÎµÎ¹ Î±Ï…Ï„ÏŒ Ï„Î¿ ÏŒÏÎ±Î¼Î±, Ï€Î±ÏÎ­Ï‡Î¿Î½Ï„Î±Ï‚ ÏƒÏ„Î¿Ï…Ï‚ Î¿ÏÎ³Î±Î½Î¹ÏƒÎ¼Î¿ÏÏ‚ ÎºÎ±Î¹ Ï„Î¿Ï…Ï‚ Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÏ„Î­Ï‚ Ï„Î· Î²Î¬ÏƒÎ· Î³Î¹Î± Ï„Î· Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï„Î·Ï‚ ÎµÏ€ÏŒÎ¼ÎµÎ½Î·Ï‚ Î³ÎµÎ½Î¹Î¬Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³ÏÎ½ Î¼Îµ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ AI.

## Î ÏÏŒÏƒÎ¸ÎµÏ„Î¿Î¹ Î ÏŒÏÎ¿Î¹

- **Î•Ï€Î¯ÏƒÎ·Î¼Î· Î¤ÎµÎºÎ¼Î·ÏÎ¯Ï‰ÏƒÎ·**: [Qwen Documentation](https://qwen.readthedocs.io/)
- **Model Hub**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **Î¤ÎµÏ‡Î½Î¹ÎºÎ¬ ÎˆÎ³Î³ÏÎ±Ï†Î±**: [Qwen Research Publications](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **ÎšÎ¿Î¹Î½ÏŒÏ„Î·Ï„Î±**: [GitHub Discussions and Issues](https://github.com/QwenLM/)
- **Î Î»Î±Ï„Ï†ÏŒÏÎ¼Î± ModelScope**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## ÎœÎ±Î¸Î·ÏƒÎ¹Î±ÎºÎ¬ Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±

ÎœÎµÏ„Î¬ Ï„Î·Î½ Î¿Î»Î¿ÎºÎ»Î®ÏÏ‰ÏƒÎ· Î±Ï…Ï„Î®Ï‚ Ï„Î·Ï‚ ÎµÎ½ÏŒÏ„Î·Ï„Î±Ï‚, Î¸Î± Î¼Ï€Î¿ÏÎµÎ¯Ï„Îµ Î½Î±:

1. Î•Î¾Î·Î³Î®ÏƒÎµÏ„Îµ Ï„Î± Î±ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ¬ Ï€Î»ÎµÎ¿Î½ÎµÎºÏ„Î®Î¼Î±Ï„Î± Ï„Î·Ï‚ Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î±Ï‚ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Qwen ÎºÎ±Î¹ Ï„Î·Î½ Ï€ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ® Ï„Î·Ï‚ Ï‰Ï‚ Î±Î½Î¿Î¹Ï‡Ï„Î¿Ï ÎºÏÎ´Î¹ÎºÎ±.
2. Î•Ï€Î¹Î»Î­Î¾ÎµÏ„Îµ Ï„Î·Î½ ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î· Ï€Î±ÏÎ±Î»Î»Î±Î³Î® Qwen Î¼Îµ Î²Î¬ÏƒÎ· ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½ÎµÏ‚ Î±Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î®Ï‚ ÎºÎ±Î¹ Ï€ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼Î¿ÏÏ‚ Ï€ÏŒÏÏ‰Î½.
3. Î¥Î»Î¿Ï€Î¿Î¹Î®ÏƒÎµÏ„Îµ Î¼Î¿Î½Ï„Î­Î»Î± Qwen ÏƒÎµ Î´Î¹Î¬Ï†Î¿ÏÎ± ÏƒÎµÎ½Î¬ÏÎ¹Î± Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚ Î¼Îµ Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½ÎµÏ‚ Î´Î¹Î±Î¼Î¿ÏÏ†ÏÏƒÎµÎ¹Ï‚.
4. Î•Ï†Î±ÏÎ¼ÏŒÏƒÎµÏ„Îµ Ï„ÎµÏ‡Î½Î¹ÎºÎ­Ï‚ Ï€Î¿ÏƒÎ¿Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚ ÎºÎ±Î¹ Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚ Î³Î¹Î± Ï„Î· Î²ÎµÎ»Ï„Î¯Ï‰ÏƒÎ· Ï„Î·Ï‚ Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚ Ï„Ï‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Qwen.
5. Î‘Î¾Î¹Î¿Î»Î¿Î³Î®ÏƒÎµÏ„Îµ Ï„Î± Ï€Î»ÎµÎ¿Î½ÎµÎºÏ„Î®Î¼Î±Ï„Î± ÎºÎ±Î¹ Ï„Î± Î¼ÎµÎ¹Î¿Î½ÎµÎºÏ„Î®Î¼Î±Ï„Î± Î¼ÎµÏ„Î±Î¾Ï Î¼ÎµÎ³Î­Î¸Î¿Ï…Ï‚ Î¼Î¿Î½Ï„Î­Î»Î¿Ï…, Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚ ÎºÎ±Î¹ Î´Ï…Î½Î±Ï„Î¿Ï„Î®Ï„Ï‰Î½ ÏƒÏ„Î·Î½ Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î± Qwen.

## Î¤Î¹ Î±ÎºÎ¿Î»Î¿Ï…Î¸ÎµÎ¯

- [03: Î˜ÎµÎ¼ÎµÎ»Î¹ÏÎ´ÎµÎ¹Ï‚ Î‘ÏÏ‡Î­Ï‚ Ï„Î·Ï‚ ÎŸÎ¹ÎºÎ¿Î³Î­Î½ÎµÎ¹Î±Ï‚ Gemma](03.GemmaFamily.md)

---

**Î‘Ï€Î¿Ï€Î¿Î¯Î·ÏƒÎ· ÎµÏ…Î¸ÏÎ½Î·Ï‚**:  
Î‘Ï…Ï„ÏŒ Ï„Î¿ Î­Î³Î³ÏÎ±Ï†Î¿ Î­Ï‡ÎµÎ¹ Î¼ÎµÏ„Î±Ï†ÏÎ±ÏƒÏ„ÎµÎ¯ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î·Î½ Ï…Ï€Î·ÏÎµÏƒÎ¯Î± Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î·Ï‚ Î¼ÎµÏ„Î¬Ï†ÏÎ±ÏƒÎ·Ï‚ [Co-op Translator](https://github.com/Azure/co-op-translator). Î Î±ÏÏŒÎ»Î¿ Ï€Î¿Ï… ÎºÎ±Ï„Î±Î²Î¬Î»Î»Î¿Ï…Î¼Îµ Ï€ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹ÎµÏ‚ Î³Î¹Î± Î±ÎºÏÎ¯Î²ÎµÎ¹Î±, Ï€Î±ÏÎ±ÎºÎ±Î»Î¿ÏÎ¼Îµ Î½Î± Î­Ï‡ÎµÏ„Îµ Ï…Ï€ÏŒÏˆÎ· ÏŒÏ„Î¹ Î¿Î¹ Î±Ï…Ï„Î¿Î¼Î±Ï„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½ÎµÏ‚ Î¼ÎµÏ„Î±Ï†ÏÎ¬ÏƒÎµÎ¹Ï‚ ÎµÎ½Î´Î­Ï‡ÎµÏ„Î±Î¹ Î½Î± Ï€ÎµÏÎ¹Î­Ï‡Î¿Ï…Î½ ÏƒÏ†Î¬Î»Î¼Î±Ï„Î± Î® Î±Î½Î±ÎºÏÎ¯Î²ÎµÎ¹ÎµÏ‚. Î¤Î¿ Ï€ÏÏ‰Ï„ÏŒÏ„Ï…Ï€Î¿ Î­Î³Î³ÏÎ±Ï†Î¿ ÏƒÏ„Î· Î¼Î·Ï„ÏÎ¹ÎºÎ® Ï„Î¿Ï… Î³Î»ÏÏƒÏƒÎ± Î¸Î± Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î¸ÎµÏ‰ÏÎµÎ¯Ï„Î±Î¹ Î· Î±Ï…Î¸ÎµÎ½Ï„Î¹ÎºÎ® Ï€Î·Î³Î®. Î“Î¹Î± ÎºÏÎ¯ÏƒÎ¹Î¼ÎµÏ‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚, ÏƒÏ…Î½Î¹ÏƒÏ„Î¬Ï„Î±Î¹ ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÎ® Î±Î½Î¸ÏÏÏ€Î¹Î½Î· Î¼ÎµÏ„Î¬Ï†ÏÎ±ÏƒÎ·. Î”ÎµÎ½ Ï†Î­ÏÎ¿Ï…Î¼Îµ ÎµÏ…Î¸ÏÎ½Î· Î³Î¹Î± Ï„Ï…Ï‡ÏŒÎ½ Ï€Î±ÏÎµÎ¾Î·Î³Î®ÏƒÎµÎ¹Ï‚ Î® ÎµÏƒÏ†Î±Î»Î¼Î­Î½ÎµÏ‚ ÎµÏÎ¼Î·Î½ÎµÎ¯ÎµÏ‚ Ï€Î¿Ï… Ï€ÏÎ¿ÎºÏÏ€Ï„Î¿Ï…Î½ Î±Ï€ÏŒ Ï„Î· Ï‡ÏÎ®ÏƒÎ· Î±Ï…Ï„Î®Ï‚ Ï„Î·Ï‚ Î¼ÎµÏ„Î¬Ï†ÏÎ±ÏƒÎ·Ï‚.