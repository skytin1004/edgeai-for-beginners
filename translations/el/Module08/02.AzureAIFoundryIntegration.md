<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "92120c9492ba50fd155393e042143bdb",
  "translation_date": "2025-09-24T22:29:51+00:00",
  "source_file": "Module08/02.AzureAIFoundryIntegration.md",
  "language_code": "el"
}
-->
# Î£Ï…Î½ÎµÎ´ÏÎ¯Î± 2: Î•Î½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· OpenAI SDK Î¼Îµ Ï„Î¿ Azure AI Foundry

## Î•Ï€Î¹ÏƒÎºÏŒÏ€Î·ÏƒÎ·

Î’Î±ÏƒÎ¹Î¶ÏŒÎ¼ÎµÎ½Î¿Î¹ ÏƒÏ„Î· Î²Î¬ÏƒÎ· Ï„Î¿Ï… Foundry Local, Î±Ï…Ï„Î® Î· ÏƒÏ…Î½ÎµÎ´ÏÎ¯Î± ÎµÏ€Î¹ÎºÎµÎ½Ï„ÏÏÎ½ÎµÏ„Î±Î¹ ÏƒÎµ Ï€ÏÎ¿Î·Î³Î¼Î­Î½Î± Î¼Î¿Ï„Î¯Î²Î± ÎµÎ½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ·Ï‚ Ï„Î¿Ï… OpenAI SDK Ï€Î¿Ï… Ï…Ï€Î¿ÏƒÏ„Î·ÏÎ¯Î¶Î¿Ï…Î½ Î±Ï€ÏÏŒÏƒÎºÎ¿Ï€Ï„Î± Ï„ÏŒÏƒÎ¿ Ï„Î¿ Microsoft Foundry Local ÏŒÏƒÎ¿ ÎºÎ±Î¹ Ï„Î¿ Azure OpenAI. Î˜Î± Î¼Î¬Î¸ÎµÏ„Îµ Ï€ÏÏ‚ Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯Ï„Îµ ÎµÏ…Î­Î»Î¹ÎºÏ„ÎµÏ‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚ AI Ï€Î¿Ï… Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¿ÏÎ½ Ï„Î¿Ï€Î¹ÎºÎ¬ Î³Î¹Î± Î»ÏŒÎ³Î¿Ï…Ï‚ Î¹Î´Î¹Ï‰Ï„Î¹ÎºÏŒÏ„Î·Ï„Î±Ï‚ ÎºÎ±Î¹ Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚, ÎµÎ½Ï Ï€Î±ÏÎ­Ï‡Î¿Ï…Î½ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„Î± ÎºÎ»Î¹Î¼Î¬ÎºÏ‰ÏƒÎ·Ï‚ Î¼Î­ÏƒÏ‰ Ï„Î¿Ï… Azure OpenAI ÏŒÏ„Î±Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹.

## Î£Ï„ÏŒÏ‡Î¿Î¹ ÎœÎ¬Î¸Î·ÏƒÎ·Ï‚

ÎœÎ­Ï‡ÏÎ¹ Ï„Î¿ Ï„Î­Î»Î¿Ï‚ Î±Ï…Ï„Î®Ï‚ Ï„Î·Ï‚ ÏƒÏ…Î½ÎµÎ´ÏÎ¯Î±Ï‚, Î¸Î± Î¼Ï€Î¿ÏÎµÎ¯Ï„Îµ:
- ÎÎ± ÎºÎ±Ï„Î±Î½Î¿ÎµÎ¯Ï„Îµ Ï€ÏÎ¿Î·Î³Î¼Î­Î½Î± Î¼Î¿Ï„Î¯Î²Î± ÎµÎ½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ·Ï‚ Ï„Î¿Ï… OpenAI SDK Î¼Îµ Ï„Î¿ Foundry Local ÎºÎ±Î¹ Ï„Î¿ Azure OpenAI
- ÎÎ± Ï…Î»Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Îµ ÏÎ¿Î­Ï‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÏ‰Î½ ÏƒÎµ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏŒ Ï‡ÏÏŒÎ½Î¿ Î³Î¹Î± Î²ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î· ÎµÎ¼Ï€ÎµÎ¹ÏÎ¯Î± Ï‡ÏÎ®ÏƒÏ„Î·
- ÎÎ± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯Ï„Îµ Î¹ÏƒÏ‡Ï…ÏÎ¬ Î¼Î¿Ï„Î¯Î²Î± client factory Î³Î¹Î± Ï…Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î· Ï€Î¿Î»Î»Î±Ï€Î»ÏÎ½ Ï€Î±ÏÏŒÏ‡Ï‰Î½
- ÎÎ± ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î¬Î¶ÎµÏ„Îµ ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î± Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ·Ï‚ ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¹ÏÎ½ Î¼Îµ Î´Î¹Î±Ï„Î®ÏÎ·ÏƒÎ· Ï„Î¿Ï… Ï€Î»Î±Î¹ÏƒÎ¯Î¿Ï…
- ÎÎ± ÎºÎ±Î¸Î¹ÎµÏÏÎ½ÎµÏ„Îµ Î¼ÎµÏ„ÏÎ®ÏƒÎµÎ¹Ï‚ Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚ ÎºÎ±Î¹ Ï€Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· Ï…Î³ÎµÎ¯Î±Ï‚
- ÎÎ± Î±Î½Î±Ï€Ï„ÏÏƒÏƒÎµÏ„Îµ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚ Î­Ï„Î¿Î¹Î¼ÎµÏ‚ Î³Î¹Î± Ï€Î±ÏÎ±Î³Ï‰Î³Î® Î¼Îµ ÏƒÏ‰ÏƒÏ„Î® Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½

## Î ÏÎ¿Î±Ï€Î±Î¹Ï„Î¿ÏÎ¼ÎµÎ½Î±

- ÎŸÎ»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î· Î£Ï…Î½ÎµÎ´ÏÎ¯Î± 1: Î•Î¹ÏƒÎ±Î³Ï‰Î³Î® ÏƒÏ„Î¿ Foundry Local
- Î•Î½ÎµÏÎ³Î® ÎµÎ³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Ï„Î¿Ï… Foundry Local Î¼Îµ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¹ÎºÎ¬ Î¼Î¿Î½Ï„Î­Î»Î±
- Python 3.8 Î® Î½ÎµÏŒÏ„ÎµÏÎ· Î­ÎºÎ´Î¿ÏƒÎ· Î¼Îµ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„Î± ÎµÎ¹ÎºÎ¿Î½Î¹ÎºÎ¿Ï Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î¿Ï‚
- Î•Î³ÎºÎ±Ï„ÎµÏƒÏ„Î·Î¼Î­Î½Î¿ OpenAI Python SDK (`pip install openai foundry-local-sdk`)
- Î›Î¿Î³Î±ÏÎ¹Î±ÏƒÎ¼ÏŒÏ‚ Azure Î¼Îµ Ï…Ï€Î·ÏÎµÏƒÎ¯Î± OpenAI (Ï€ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÏŒ, Î³Î¹Î± ÏƒÎµÎ½Î¬ÏÎ¹Î± cloud)
- Î’Î±ÏƒÎ¹ÎºÎ® ÎºÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ· Ï„Ï‰Î½ Î¼Î¿Ï„Î¯Î²Ï‰Î½ Python async/await

## ÎœÎ­ÏÎ¿Ï‚ 1: ÎœÎ¿Ï„Î¯Î²Î¿ Client Factory Ï„Î¿Ï… OpenAI SDK

### ÎšÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ· Ï„Î·Ï‚ Î‘ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ®Ï‚ Î Î¿Î»Î»Î±Ï€Î»ÏÎ½ Î Î±ÏÏŒÏ‡Ï‰Î½

Î— Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÎµÏ†Î±ÏÎ¼Î¿Î³ÏÎ½ Ï€Î¿Ï… Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¿ÏÎ½ Ï„ÏŒÏƒÎ¿ Î¼Îµ Ï„Î¿ Foundry Local ÏŒÏƒÎ¿ ÎºÎ±Î¹ Î¼Îµ Ï„Î¿ Azure OpenAI Î±Ï€Î±Î¹Ï„ÎµÎ¯ Î­Î½Î± ÎµÏ…Î­Î»Î¹ÎºÏ„Î¿ Î¼Î¿Ï„Î¯Î²Î¿ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î±Ï‚ client:

```python
# sdk_integration.py - Sample 02 pattern
import os
from openai import OpenAI
from typing import Tuple

try:
    from foundry_local import FoundryLocalManager
    FOUNDRY_SDK_AVAILABLE = True
except ImportError:
    FOUNDRY_SDK_AVAILABLE = False


def create_azure_client() -> Tuple[OpenAI, str]:
    """Create Azure OpenAI client."""
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    azure_api_version = os.environ.get("AZURE_OPENAI_API_VERSION", "2024-08-01-preview")
    
    if not azure_endpoint or not azure_api_key:
        raise ValueError("Azure OpenAI endpoint and API key are required")
    
    model = os.environ.get("MODEL", "your-deployment-name")
    client = OpenAI(
        base_url=f"{azure_endpoint}/openai",
        api_key=azure_api_key,
        default_query={"api-version": azure_api_version},
    )
    
    print(f"ğŸŒ Azure OpenAI client created with model: {model}")
    return client, model


def create_foundry_client() -> Tuple[OpenAI, str]:
    """Create Foundry Local client with SDK management."""
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    if FOUNDRY_SDK_AVAILABLE:
        try:
            # Use FoundryLocalManager for proper service management
            manager = FoundryLocalManager(alias)
            model_info = manager.get_model_info(alias)
            
            # Configure OpenAI client to use local Foundry service
            client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            
            print(f"ğŸ  Foundry Local SDK initialized with model: {model_info.id}")
            return client, model_info.id
        except Exception as e:
            print(f"âš ï¸ Could not use Foundry SDK ({e}), falling back to manual configuration")
    
    # Fallback to manual configuration
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    api_key = os.environ.get("API_KEY", "")
    
    client = OpenAI(
        base_url=f"{base_url}/v1",
        api_key=api_key
    )
    
    print(f"ğŸ”§ Manual configuration with model: {alias}")
    return client, alias


def initialize_client() -> Tuple[OpenAI, str, str]:
    """Initialize the appropriate OpenAI client."""
    # Check for Azure OpenAI configuration
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    
    if azure_endpoint and azure_api_key:
        try:
            client, model = create_azure_client()
            return client, model, "azure"
        except Exception as e:
            print(f"âŒ Azure OpenAI initialization failed: {e}")
            print("ğŸ”„ Falling back to Foundry Local...")
    
    # Use Foundry Local
    client, model = create_foundry_client()
    return client, model, "foundry"
```

## ÎœÎ­ÏÎ¿Ï‚ 2: Î¡Î¿Î­Ï‚ Î‘Ï€Î±Î½Ï„Î®ÏƒÎµÏ‰Î½ ÎºÎ±Î¹ Î‘Î»Î»Î·Î»ÎµÏ€Î¯Î´ÏÎ±ÏƒÎ· ÏƒÎµ Î ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏŒ Î§ÏÏŒÎ½Î¿

### Î¥Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î¡Î¿ÏÎ½ Î£Ï…Î½Î¿Î¼Î¹Î»Î¯Î±Ï‚ Î¼Îµ Streaming

Î¤Î¿ streaming Ï€ÏÎ¿ÏƒÏ†Î­ÏÎµÎ¹ ÎºÎ±Î»ÏÏ„ÎµÏÎ· ÎµÎ¼Ï€ÎµÎ¹ÏÎ¯Î± Ï‡ÏÎ®ÏƒÏ„Î·, ÎµÎ¼Ï†Î±Î½Î¯Î¶Î¿Î½Ï„Î±Ï‚ Ï„Î¹Ï‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚ ÎºÎ±Î¸ÏÏ‚ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ½Ï„Î±Î¹:

```python
# streaming_chat.py - Following Sample 02 patterns
def streaming_chat_completion(client: OpenAI, model: str, prompt: str, max_tokens: int = 300):
    """Demonstrate streaming responses for better UX."""
    try:
        print("ğŸ¤– Assistant (streaming):")
        
        # Create streaming completion
        stream = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            stream=True
        )
        
        full_response = ""
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                print(content, end="", flush=True)
                full_response += content
        
        print("\n")  # New line after streaming
        return full_response
    except Exception as e:
        error_msg = f"Error: {e}"
        print(error_msg)
        return error_msg


# Usage example
client, model, provider = initialize_client()
prompt = "Explain the key benefits of using Microsoft Foundry Local for AI development."
response = streaming_chat_completion(client, model, prompt)
```

### Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î£Ï…Î½Î¿Î¼Î¹Î»Î¹ÏÎ½ Î Î¿Î»Î»Î±Ï€Î»ÏÎ½ Î£Ï„ÏÎ¿Ï†ÏÎ½

```python
# conversation_manager.py
class ConversationManager:
    """Manages multi-turn conversations with context preservation."""
    
    def __init__(self, client: OpenAI, model: str, system_prompt: str = None):
        self.client = client
        self.model = model
        self.messages = []
        
        if system_prompt:
            self.messages.append({"role": "system", "content": system_prompt})
    
    def send_message(self, user_message: str, max_tokens: int = 200, stream: bool = False):
        """Send a message and get response while maintaining context."""
        # Add user message to conversation
        self.messages.append({"role": "user", "content": user_message})
        
        try:
            if stream:
                return self._stream_response(max_tokens)
            else:
                return self._regular_response(max_tokens)
        except Exception as e:
            return f"Error: {e}"
    
    def _regular_response(self, max_tokens: int):
        """Get regular (non-streaming) response."""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=self.messages,
            max_tokens=max_tokens
        )
        
        assistant_message = response.choices[0].message.content
        self.messages.append({"role": "assistant", "content": assistant_message})
        return assistant_message
    
    def _stream_response(self, max_tokens: int):
        """Get streaming response."""
        stream = self.client.chat.completions.create(
            model=self.model,
            messages=self.messages,
            max_tokens=max_tokens,
            stream=True
        )
        
        full_response = ""
        for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                print(content, end="", flush=True)
                full_response += content
        
        print()  # New line
        self.messages.append({"role": "assistant", "content": full_response})
        return full_response
    
    def get_conversation_length(self) -> int:
        """Get the number of messages in the conversation."""
        return len(self.messages)
    
    def clear_conversation(self, keep_system: bool = True):
        """Clear conversation history."""
        if keep_system and self.messages and self.messages[0]["role"] == "system":
            self.messages = [self.messages[0]]
        else:
            self.messages = []


# Example usage
client, model, provider = initialize_client()
system_prompt = "You are a helpful AI assistant specialized in explaining AI and machine learning concepts."
conversation = ConversationManager(client, model, system_prompt)

# Multi-turn conversation
conversation_turns = [
    "What is the difference between AI inference on-device vs in the cloud?",
    "Which approach is better for privacy?",
    "What about performance and latency considerations?"
]

for i, turn in enumerate(conversation_turns, 1):
    print(f"\nTurn {i}: {turn}")
    response = conversation.send_message(turn, stream=True)
```

## ÎœÎ­ÏÎ¿Ï‚ 3: ÎœÎµÏ„ÏÎ®ÏƒÎµÎ¹Ï‚ Î‘Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚ ÎºÎ±Î¹ Î‘Î½Î¬Î»Ï…ÏƒÎ·

### ÎœÎ­Ï„ÏÎ·ÏƒÎ· Î§ÏÏŒÎ½Î¿Ï… Î‘Ï€ÏŒÎºÏÎ¹ÏƒÎ·Ï‚

ÎœÎµÏ„ÏÎ®ÏƒÏ„Îµ ÎºÎ±Î¹ ÏƒÏ…Î³ÎºÏÎ¯Î½ÎµÏ„Îµ Ï„Î·Î½ Î±Ï€ÏŒÎ´Î¿ÏƒÎ· ÏƒÎµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Î´Î¹Î±Î¼Î¿ÏÏ†ÏÏƒÎµÎ¹Ï‚:

```python
# performance_benchmark.py - Sample 02 patterns
import time
from typing import Dict, List
from openai import OpenAI

def benchmark_response_time(client: OpenAI, model: str, prompt: str, iterations: int = 3) -> Dict:
    """Benchmark response time for a given prompt."""
    times = []
    responses = []
    
    for i in range(iterations):
        start_time = time.time()
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=50  # Keep responses short for timing
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            times.append(response_time)
            responses.append(response.choices[0].message.content)
            
        except Exception as e:
            print(f"Error in iteration {i+1}: {e}")
    
    if times:
        return {
            "average_time": sum(times) / len(times),
            "min_time": min(times),
            "max_time": max(times),
            "all_times": times,
            "sample_response": responses[0] if responses else None,
            "success_rate": len(times) / iterations * 100
        }
    
    return {"error": "No successful responses"}


def compare_providers(foundry_client: OpenAI, foundry_model: str, 
                     azure_client: OpenAI, azure_model: str, test_prompts: List[str]):
    """Compare performance between Foundry Local and Azure OpenAI."""
    results = {
        "foundry_local": [],
        "azure_openai": []
    }
    
    for prompt in test_prompts:
        print(f"\nTesting prompt: '{prompt}'")
        
        # Test Foundry Local
        foundry_result = benchmark_response_time(foundry_client, foundry_model, prompt)
        results["foundry_local"].append({
            "prompt": prompt,
            "benchmark": foundry_result
        })
        
        # Test Azure OpenAI
        azure_result = benchmark_response_time(azure_client, azure_model, prompt)
        results["azure_openai"].append({
            "prompt": prompt,
            "benchmark": azure_result
        })
        
        # Compare results
        if "error" not in foundry_result and "error" not in azure_result:
            foundry_time = foundry_result["average_time"]
            azure_time = azure_result["average_time"]
            
            print(f"  Foundry Local: {foundry_time:.2f}s")
            print(f"  Azure OpenAI: {azure_time:.2f}s")
            print(f"  Winner: {'Foundry Local' if foundry_time < azure_time else 'Azure OpenAI'}")
    
    return results


# Example usage
benchmark_prompts = [
    "What is AI?",
    "Explain machine learning in simple terms.",
    "List 3 benefits of edge computing."
]

# Initialize clients
foundry_client, foundry_model, _ = initialize_client()
# azure_client, azure_model = create_azure_client()  # Uncomment if Azure is configured

for prompt in benchmark_prompts:
    print(f"\nğŸ“ Benchmarking: '{prompt}'")
    result = benchmark_response_time(foundry_client, foundry_model, prompt)
    
    if "error" not in result:
        print(f"   â° Average time: {result['average_time']:.2f}s")
        print(f"   âš¡ Fastest: {result['min_time']:.2f}s")
        print(f"   ğŸŒ Slowest: {result['max_time']:.2f}s")
        print(f"   âœ… Success rate: {result['success_rate']:.1f}%")
```

### Î”Î¿ÎºÎ¹Î¼Î® Î˜ÎµÏÎ¼Î¿ÎºÏÎ±ÏƒÎ¯Î±Ï‚ ÎºÎ±Î¹ Î Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½

```python
# parameter_testing.py
def test_temperature_effects(client: OpenAI, model: str, prompt: str):
    """Test how different temperature values affect responses."""
    temperatures = [0.1, 0.5, 0.9]
    
    print(f"Testing prompt: '{prompt}'")
    print("=" * 60)
    
    for temp in temperatures:
        print(f"\nğŸŒ¡ï¸ Temperature: {temp}")
        print("-" * 30)
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=100,
                temperature=temp
            )
            
            print(f"Response: {response.choices[0].message.content[:150]}...")
            
        except Exception as e:
            print(f"Error with temperature {temp}: {e}")


# Test creative vs analytical prompts
creative_prompt = "Write a creative short story about AI."
analytical_prompt = "Explain the technical differences between GPT and BERT models."

test_temperature_effects(foundry_client, foundry_model, creative_prompt)
test_temperature_effects(foundry_client, foundry_model, analytical_prompt)
```

## ÎœÎ­ÏÎ¿Ï‚ 4: Î Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· Î¥Î³ÎµÎ¯Î±Ï‚ Î¥Ï€Î·ÏÎµÏƒÎ¯Î±Ï‚ ÎºÎ±Î¹ Î”Î¹Î±Î³Î½Ï‰ÏƒÏ„Î¹ÎºÎ¬

### Î£ÏÏƒÏ„Î·Î¼Î± Î Î»Î®ÏÎ¿Ï…Ï‚ Î•Î»Î­Î³Ï‡Î¿Ï… Î¥Î³ÎµÎ¯Î±Ï‚

```python
# health_monitoring.py - Sample 02 patterns
def comprehensive_health_check(client: OpenAI, model: str, provider: str) -> Dict:
    """Perform comprehensive health check of the AI service."""
    print("ğŸ¥ Comprehensive Health Check")
    print("=" * 50)
    
    health_results = {
        "provider": provider,
        "model": model,
        "timestamp": time.time(),
        "tests": {}
    }
    
    # Test 1: Model listing
    try:
        models_response = client.models.list()
        available_models = [m.id for m in models_response.data]
        health_results["tests"]["model_listing"] = {
            "status": "success",
            "available_models": available_models,
            "current_model_available": model in available_models
        }
        print(f"âœ… Model listing: SUCCESS ({len(available_models)} models)")
    except Exception as e:
        health_results["tests"]["model_listing"] = {
            "status": "failed",
            "error": str(e)
        }
        print(f"âŒ Model listing: FAILED - {e}")
    
    # Test 2: Basic completion
    try:
        start_time = time.time()
        test_response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": "Say 'Health check successful'"}],
            max_tokens=10
        )
        response_time = time.time() - start_time
        
        health_results["tests"]["basic_completion"] = {
            "status": "success",
            "response_time": response_time,
            "response": test_response.choices[0].message.content
        }
        print(f"âœ… Basic completion: SUCCESS ({response_time:.2f}s)")
    except Exception as e:
        health_results["tests"]["basic_completion"] = {
            "status": "failed",
            "error": str(e)
        }
        print(f"âŒ Basic completion: FAILED - {e}")
    
    # Test 3: Streaming
    try:
        start_time = time.time()
        stream = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": "Count to 3"}],
            max_tokens=20,
            stream=True
        )
        
        stream_content = ""
        chunk_count = 0
        for chunk in stream:
            if chunk.choices[0].delta.content:
                stream_content += chunk.choices[0].delta.content
                chunk_count += 1
        
        streaming_time = time.time() - start_time
        
        health_results["tests"]["streaming"] = {
            "status": "success",
            "response_time": streaming_time,
            "chunks_received": chunk_count,
            "content": stream_content.strip()
        }
        print(f"âœ… Streaming: SUCCESS ({streaming_time:.2f}s, {chunk_count} chunks)")
    except Exception as e:
        health_results["tests"]["streaming"] = {
            "status": "failed",
            "error": str(e)
        }
        print(f"âŒ Streaming: FAILED - {e}")
    
    # Overall health score
    successful_tests = sum(1 for test in health_results["tests"].values() if test["status"] == "success")
    total_tests = len(health_results["tests"])
    health_score = (successful_tests / total_tests) * 100
    
    health_results["overall_health"] = {
        "score": health_score,
        "successful_tests": successful_tests,
        "total_tests": total_tests,
        "status": "healthy" if health_score >= 70 else "degraded" if health_score >= 30 else "unhealthy"
    }
    
    print(f"\nğŸ“Š Overall Health: {health_score:.1f}% ({health_results['overall_health']['status'].upper()})")
    
    return health_results


# Usage example
client, model, provider = initialize_client()
health_status = comprehensive_health_check(client, model, provider)
```

### Î•Ï€Î±Î»Î·Î¸ÎµÏ…Ï„Î®Ï‚ Î”Î¹Î±Î¼ÏŒÏÏ†Ï‰ÏƒÎ·Ï‚ Î ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î¿Ï‚

```python
# config_validator.py
def validate_environment_configuration() -> Dict:
    """Validate environment configuration for both providers."""
    validation_results = {
        "foundry_local": {},
        "azure_openai": {},
        "recommendations": []
    }
    
    # Check Foundry Local configuration
    foundry_sdk_available = FOUNDRY_SDK_AVAILABLE
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    
    validation_results["foundry_local"] = {
        "sdk_available": foundry_sdk_available,
        "base_url": base_url,
        "model": os.environ.get("MODEL", "phi-4-mini"),
        "api_key": bool(os.environ.get("API_KEY"))
    }
    
    if not foundry_sdk_available:
        validation_results["recommendations"].append(
            "Install Foundry Local SDK: pip install foundry-local-sdk"
        )
    
    # Check Azure OpenAI configuration
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    azure_api_version = os.environ.get("AZURE_OPENAI_API_VERSION")
    
    validation_results["azure_openai"] = {
        "endpoint_configured": bool(azure_endpoint),
        "api_key_configured": bool(azure_api_key),
        "api_version": azure_api_version or "2024-08-01-preview",
        "model": os.environ.get("MODEL", "your-deployment-name")
    }
    
    if azure_endpoint and not azure_api_key:
        validation_results["recommendations"].append(
            "Azure endpoint is set but API key is missing"
        )
    
    # Overall assessment
    can_use_foundry = foundry_sdk_available or base_url
    can_use_azure = azure_endpoint and azure_api_key
    
    if not can_use_foundry and not can_use_azure:
        validation_results["recommendations"].append(
            "No valid configuration found. Set up either Foundry Local or Azure OpenAI."
        )
    
    validation_results["summary"] = {
        "foundry_ready": can_use_foundry,
        "azure_ready": can_use_azure,
        "total_options": sum([can_use_foundry, can_use_azure])
    }
    
    return validation_results


# Display configuration status
config_status = validate_environment_configuration()
print("âš™ï¸ Environment Configuration Status")
print("=" * 40)
print(f"ğŸ  Foundry Local Ready: {'âœ…' if config_status['summary']['foundry_ready'] else 'âŒ'}")
print(f"ğŸŒ Azure OpenAI Ready: {'âœ…' if config_status['summary']['azure_ready'] else 'âŒ'}")
print(f"ğŸ“‹ Available Options: {config_status['summary']['total_options']}")

if config_status["recommendations"]:
    print("\nğŸ’¡ Recommendations:")
    for rec in config_status["recommendations"]:
        print(f"   â€¢ {rec}")
```

## ÎœÎ­ÏÎ¿Ï‚ 5: ÎœÎµÏ„Î±Î²Î»Î·Ï„Î­Ï‚ Î ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î¿Ï‚ ÎºÎ±Î¹ Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î”Î¹Î±Î¼ÏŒÏÏ†Ï‰ÏƒÎ·Ï‚

### Î‘Î½Î±Ï†Î¿ÏÎ¬ ÎœÎµÏ„Î±Î²Î»Î·Ï„ÏÎ½ Î ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î¿Ï‚

Î Î»Î®ÏÎ·Ï‚ Î±Î½Î±Ï†Î¿ÏÎ¬ Î³Î¹Î± Ï„Î· Î´Î¹Î±Î¼ÏŒÏÏ†Ï‰ÏƒÎ· ÎºÎ±Î¹ Ï„Ï‰Î½ Î´ÏÎ¿ Ï€Î±ÏÏŒÏ‡Ï‰Î½:

```python
# config_reference.py - Sample 02 patterns
import os
from typing import Dict, Optional

class ConfigurationManager:
    """Manages environment configuration for multi-provider setup."""
    
    @staticmethod
    def get_foundry_config() -> Dict[str, Optional[str]]:
        """Get Foundry Local configuration from environment."""
        return {
            "MODEL": os.environ.get("MODEL", "phi-4-mini"),
            "BASE_URL": os.environ.get("BASE_URL", "http://localhost:8000"),
            "API_KEY": os.environ.get("API_KEY", ""),
        }
    
    @staticmethod
    def get_azure_config() -> Dict[str, Optional[str]]:
        """Get Azure OpenAI configuration from environment."""
        return {
            "AZURE_OPENAI_ENDPOINT": os.environ.get("AZURE_OPENAI_ENDPOINT"),
            "AZURE_OPENAI_API_KEY": os.environ.get("AZURE_OPENAI_API_KEY"),
            "AZURE_OPENAI_API_VERSION": os.environ.get("AZURE_OPENAI_API_VERSION", "2024-08-01-preview"),
            "MODEL": os.environ.get("MODEL", "your-deployment-name"),
        }
    
    @staticmethod
    def display_current_config():
        """Display current configuration status."""
        print("âš™ï¸ Current Configuration")
        print("=" * 40)
        
        foundry_config = ConfigurationManager.get_foundry_config()
        azure_config = ConfigurationManager.get_azure_config()
        
        print("ğŸ  Foundry Local:")
        for key, value in foundry_config.items():
            display_value = value if value else "(not set)"
            if key == "API_KEY" and value:
                display_value = "***" + value[-4:] if len(value) > 4 else "***"
            print(f"   {key}: {display_value}")
        
        print("\nğŸŒ Azure OpenAI:")
        for key, value in azure_config.items():
            display_value = value if value else "(not set)"
            if "KEY" in key and value:
                display_value = "***" + value[-4:] if len(value) > 4 else "***"
            print(f"   {key}: {display_value}")
        
        # Determine active provider
        azure_ready = azure_config["AZURE_OPENAI_ENDPOINT"] and azure_config["AZURE_OPENAI_API_KEY"]
        foundry_ready = True  # Foundry can always fallback to defaults
        
        print(f"\nğŸ“Š Provider Status:")
        print(f"   Azure OpenAI: {'âœ… Ready' if azure_ready else 'âŒ Not configured'}")
        print(f"   Foundry Local: {'âœ… Ready' if foundry_ready else 'âŒ Not available'}")
        print(f"   Active: {'Azure OpenAI' if azure_ready else 'Foundry Local'}")


# Display current configuration
config_manager = ConfigurationManager()
config_manager.display_current_config()
```

### Î Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î± Î”Î¹Î±Î¼ÏŒÏÏ†Ï‰ÏƒÎ·Ï‚

**Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Windows Command Prompt:**

```cmd
REM Foundry Local configuration
set MODEL=phi-4-mini
set BASE_URL=http://localhost:8000
set API_KEY=

REM Azure OpenAI configuration (alternative)
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key-here
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name

REM Run the sample
python samples\02\sdk_quickstart.py
```

**Î¡ÏÎ¸Î¼Î¹ÏƒÎ· PowerShell:**

```powershell
# Foundry Local configuration
$env:MODEL = "phi-4-mini"
$env:BASE_URL = "http://localhost:8000"
$env:API_KEY = ""

# Azure OpenAI configuration (alternative)
$env:AZURE_OPENAI_ENDPOINT = "https://your-resource.openai.azure.com"
$env:AZURE_OPENAI_API_KEY = "your-api-key-here"
$env:AZURE_OPENAI_API_VERSION = "2024-08-01-preview"
$env:MODEL = "your-deployment-name"

# Run the sample
python samples/02/sdk_quickstart.py
```

## ÎœÎ­ÏÎ¿Ï‚ 6: Î‘ÏƒÎºÎ®ÏƒÎµÎ¹Ï‚ Î ÏÎ±ÎºÏ„Î¹ÎºÎ®Ï‚

### Î†ÏƒÎºÎ·ÏƒÎ· 1: Î•Î½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· SDK Î Î¿Î»Î»Î±Ï€Î»ÏÎ½ Î Î±ÏÏŒÏ‡Ï‰Î½

Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÏ„Îµ Î¼Î¹Î± Ï€Î»Î®ÏÎ· ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Ï€Î¿Ï… ÎµÎ½Î±Î»Î»Î¬ÏƒÏƒÎµÏ„Î±Î¹ Î±Ï€ÏÏŒÏƒÎºÎ¿Ï€Ï„Î± Î¼ÎµÏ„Î±Î¾Ï Ï€Î±ÏÏŒÏ‡Ï‰Î½:

```python
# exercise_1_multi_provider.py
from openai import OpenAI
from typing import Tuple, Dict, Any
import time

class MultiProviderSDKDemo:
    """Demonstrates seamless switching between Foundry Local and Azure OpenAI."""
    
    def __init__(self):
        self.clients = {}
        self.models = {}
        self.setup_clients()
    
    def setup_clients(self):
        """Initialize all available clients."""
        # Try to initialize Foundry Local
        try:
            foundry_client, foundry_model, _ = initialize_client()
            self.clients["foundry"] = foundry_client
            self.models["foundry"] = foundry_model
            print("âœ… Foundry Local client ready")
        except Exception as e:
            print(f"âŒ Foundry Local setup failed: {e}")
        
        # Try to initialize Azure OpenAI
        try:
            if os.environ.get("AZURE_OPENAI_ENDPOINT") and os.environ.get("AZURE_OPENAI_API_KEY"):
                azure_client, azure_model = create_azure_client()
                self.clients["azure"] = azure_client
                self.models["azure"] = azure_model
                print("âœ… Azure OpenAI client ready")
        except Exception as e:
            print(f"âŒ Azure OpenAI setup failed: {e}")
    
    def compare_providers(self, prompt: str, max_tokens: int = 100) -> Dict[str, Any]:
        """Compare responses from all available providers."""
        results = {}
        
        for provider_name, client in self.clients.items():
            model = self.models[provider_name]
            print(f"\nTesting {provider_name} ({model})...")
            
            start_time = time.time()
            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens
                )
                
                response_time = time.time() - start_time
                
                results[provider_name] = {
                    "model": model,
                    "response": response.choices[0].message.content,
                    "response_time": response_time,
                    "status": "success"
                }
                
                print(f"   âœ… Success ({response_time:.2f}s)")
                
            except Exception as e:
                results[provider_name] = {
                    "model": model,
                    "error": str(e),
                    "status": "failed"
                }
                print(f"   âŒ Failed: {e}")
        
        return results
    
    def streaming_comparison(self, prompt: str, max_tokens: int = 150):
        """Compare streaming responses from providers."""
        for provider_name, client in self.clients.items():
            model = self.models[provider_name]
            print(f"\nğŸŒŠ Streaming from {provider_name} ({model}):")
            print("-" * 50)
            
            try:
                stream = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    stream=True
                )
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        print(chunk.choices[0].delta.content, end="", flush=True)
                
                print("\n")
                
            except Exception as e:
                print(f"Streaming failed: {e}")


# Run Exercise 1
exercise_1 = MultiProviderSDKDemo()

test_prompt = "Explain the benefits of running AI models locally versus in the cloud."
print(f"ğŸ—ºï¸ Exercise 1: Multi-Provider Comparison")
print(f"Prompt: {test_prompt}")
print("=" * 60)

comparison_results = exercise_1.compare_providers(test_prompt)
exercise_1.streaming_comparison(test_prompt)
```

### Î†ÏƒÎºÎ·ÏƒÎ· 2: Î ÏÎ¿Î·Î³Î¼Î­Î½Î· Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î£Ï…Î½Î¿Î¼Î¹Î»Î¹ÏÎ½

```python
# exercise_2_conversation.py
class AdvancedConversationManager:
    """Advanced conversation management with multiple features."""
    
    def __init__(self, client: OpenAI, model: str):
        self.client = client
        self.model = model
        self.conversations = {}  # Multiple conversation sessions
    
    def create_conversation(self, session_id: str, system_prompt: str = None) -> str:
        """Create a new conversation session."""
        self.conversations[session_id] = {
            "messages": [],
            "created_at": time.time(),
            "message_count": 0
        }
        
        if system_prompt:
            self.conversations[session_id]["messages"].append({
                "role": "system", 
                "content": system_prompt
            })
        
        return f"Conversation {session_id} created"
    
    def send_message(self, session_id: str, message: str, 
                    temperature: float = 0.7, max_tokens: int = 200) -> Dict[str, Any]:
        """Send message in a specific conversation session."""
        if session_id not in self.conversations:
            return {"error": f"Conversation {session_id} not found"}
        
        conversation = self.conversations[session_id]
        conversation["messages"].append({"role": "user", "content": message})
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=conversation["messages"],
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            assistant_message = response.choices[0].message.content
            conversation["messages"].append({
                "role": "assistant", 
                "content": assistant_message
            })
            conversation["message_count"] += 2  # User + assistant
            
            return {
                "session_id": session_id,
                "response": assistant_message,
                "message_count": conversation["message_count"],
                "status": "success"
            }
            
        except Exception as e:
            return {"error": str(e), "session_id": session_id}
    
    def get_conversation_summary(self, session_id: str) -> Dict[str, Any]:
        """Get summary of conversation session."""
        if session_id not in self.conversations:
            return {"error": f"Conversation {session_id} not found"}
        
        conversation = self.conversations[session_id]
        
        return {
            "session_id": session_id,
            "message_count": conversation["message_count"],
            "created_at": conversation["created_at"],
            "duration": time.time() - conversation["created_at"],
            "has_system_prompt": len(conversation["messages"]) > 0 and 
                               conversation["messages"][0]["role"] == "system"
        }
    
    def export_conversation(self, session_id: str) -> str:
        """Export conversation as formatted text."""
        if session_id not in self.conversations:
            return f"Conversation {session_id} not found"
        
        conversation = self.conversations[session_id]
        export_text = f"Conversation Export: {session_id}\n"
        export_text += "=" * 50 + "\n\n"
        
        for msg in conversation["messages"]:
            role = msg["role"].title()
            content = msg["content"]
            export_text += f"{role}: {content}\n\n"
        
        return export_text


# Run Exercise 2
client, model, provider = initialize_client()
conv_manager = AdvancedConversationManager(client, model)

# Create multiple conversation sessions
print("ğŸ’¬ Exercise 2: Advanced Conversation Management")
print("=" * 60)

# Technical discussion
conv_manager.create_conversation("tech_discussion", 
    "You are a technical expert explaining AI concepts clearly.")

# Creative session
conv_manager.create_conversation("creative_session", 
    "You are a creative writing assistant helping with storytelling.")

# Test conversations
tech_questions = [
    "What is the difference between inference and training?",
    "How does quantization improve model performance?"
]

creative_prompts = [
    "Start a story about an AI that lives on an edge device.",
    "Continue the story with a plot twist."
]

# Technical conversation
print("\nğŸ”§ Technical Discussion:")
for question in tech_questions:
    result = conv_manager.send_message("tech_discussion", question)
    print(f"Q: {question}")
    print(f"A: {result['response'][:100]}...\n")

# Creative conversation
print("ğŸ¨ Creative Session:")
for prompt in creative_prompts:
    result = conv_manager.send_message("creative_session", prompt, temperature=0.9)
    print(f"Prompt: {prompt}")
    print(f"Response: {result['response'][:100]}...\n")

# Show conversation summaries
print("ğŸ“Š Conversation Summaries:")
for session_id in conv_manager.conversations.keys():
    summary = conv_manager.get_conversation_summary(session_id)
    print(f"   {session_id}: {summary['message_count']} messages, {summary['duration']:.1f}s")
```

### Î†ÏƒÎºÎ·ÏƒÎ· 3: Î Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· Î¥Î³ÎµÎ¯Î±Ï‚ ÎˆÏ„Î¿Î¹Î¼Î· Î³Î¹Î± Î Î±ÏÎ±Î³Ï‰Î³Î®

```python
# exercise_3_monitoring.py
class ProductionHealthMonitor:
    """Production-ready health monitoring for AI services."""
    
    def __init__(self):
        self.health_history = []
        self.alert_thresholds = {
            "response_time": 5.0,
            "error_rate": 10.0,
            "availability": 95.0
        }
    
    def run_comprehensive_check(self, client: OpenAI, model: str, provider: str) -> Dict[str, Any]:
        """Run comprehensive health check with detailed reporting."""
        check_results = {
            "timestamp": time.time(),
            "provider": provider,
            "model": model,
            "tests": {},
            "overall_health": "unknown"
        }
        
        # Test 1: Basic connectivity
        connectivity_result = self._test_connectivity(client)
        check_results["tests"]["connectivity"] = connectivity_result
        
        # Test 2: Model availability
        model_result = self._test_model_availability(client, model)
        check_results["tests"]["model_availability"] = model_result
        
        # Test 3: Response time benchmark
        performance_result = self._test_performance(client, model)
        check_results["tests"]["performance"] = performance_result
        
        # Test 4: Stress test
        stress_result = self._test_stress(client, model)
        check_results["tests"]["stress_test"] = stress_result
        
        # Calculate overall health
        check_results["overall_health"] = self._calculate_health_score(check_results["tests"])
        
        # Store for trending
        self.health_history.append(check_results)
        
        return check_results
    
    def _test_connectivity(self, client: OpenAI) -> Dict[str, Any]:
        """Test basic service connectivity."""
        try:
            start_time = time.time()
            models = client.models.list()
            response_time = time.time() - start_time
            
            return {
                "status": "success",
                "response_time": response_time,
                "models_count": len(models.data)
            }
        except Exception as e:
            return {"status": "failed", "error": str(e)}
    
    def _test_model_availability(self, client: OpenAI, model: str) -> Dict[str, Any]:
        """Test specific model availability."""
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": "Health check"}],
                max_tokens=5
            )
            
            return {
                "status": "success",
                "model": model,
                "response_received": bool(response.choices[0].message.content)
            }
        except Exception as e:
            return {"status": "failed", "error": str(e)}
    
    def _test_performance(self, client: OpenAI, model: str) -> Dict[str, Any]:
        """Test response time performance."""
        response_times = []
        
        for i in range(3):
            try:
                start_time = time.time()
                client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": f"Test {i+1}"}],
                    max_tokens=10
                )
                response_time = time.time() - start_time
                response_times.append(response_time)
            except Exception:
                pass
        
        if response_times:
            avg_time = sum(response_times) / len(response_times)
            return {
                "status": "success",
                "average_response_time": avg_time,
                "min_time": min(response_times),
                "max_time": max(response_times),
                "within_threshold": avg_time < self.alert_thresholds["response_time"]
            }
        else:
            return {"status": "failed", "error": "No successful responses"}
    
    def _test_stress(self, client: OpenAI, model: str) -> Dict[str, Any]:
        """Test service under concurrent requests."""
        import concurrent.futures
        
        def single_request():
            try:
                client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": "Stress test"}],
                    max_tokens=5
                )
                return True
            except Exception:
                return False
        
        # Run 5 concurrent requests
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(single_request) for _ in range(5)]
            results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        success_rate = (sum(results) / len(results)) * 100
        
        return {
            "status": "success" if success_rate > 80 else "degraded",
            "concurrent_requests": len(results),
            "success_rate": success_rate,
            "within_threshold": success_rate >= self.alert_thresholds["availability"]
        }
    
    def _calculate_health_score(self, tests: Dict[str, Any]) -> str:
        """Calculate overall health score."""
        successful_tests = sum(1 for test in tests.values() if test["status"] == "success")
        total_tests = len(tests)
        health_percentage = (successful_tests / total_tests) * 100
        
        if health_percentage >= 90:
            return "healthy"
        elif health_percentage >= 70:
            return "degraded"
        else:
            return "unhealthy"
    
    def generate_health_report(self) -> str:
        """Generate formatted health report."""
        if not self.health_history:
            return "No health data available"
        
        latest = self.health_history[-1]
        report = f"Health Report - {time.ctime(latest['timestamp'])}\n"
        report += "=" * 60 + "\n"
        report += f"Provider: {latest['provider']}\n"
        report += f"Model: {latest['model']}\n"
        report += f"Overall Health: {latest['overall_health'].upper()}\n\n"
        
        for test_name, test_result in latest["tests"].items():
            status_icon = "âœ…" if test_result["status"] == "success" else "âŒ"
            report += f"{status_icon} {test_name.replace('_', ' ').title()}: {test_result['status']}\n"
        
        return report


# Run Exercise 3
client, model, provider = initialize_client()
health_monitor = ProductionHealthMonitor()

print("ğŸ¥ Exercise 3: Production Health Monitoring")
print("=" * 60)

health_results = health_monitor.run_comprehensive_check(client, model, provider)
print(health_monitor.generate_health_report())
```

## ÎœÎ­ÏÎ¿Ï‚ 7: Î ÎµÏÎ¯Î»Î·ÏˆÎ· ÎºÎ±Î¹ Î•Ï€ÏŒÎ¼ÎµÎ½Î± Î’Î®Î¼Î±Ï„Î±

### Î•Ï€Î¹Ï„ÎµÏÎ³Î¼Î±Ï„Î± Î£Ï…Î½ÎµÎ´ÏÎ¯Î±Ï‚

Î£Îµ Î±Ï…Ï„Î® Ï„Î· ÏƒÏ…Î½ÎµÎ´ÏÎ¯Î±, ÎºÎ±Ï„Î±ÎºÏ„Î®ÏƒÎ±Ï„Îµ:
- âœ… **Î•Î½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· OpenAI SDK**: Î ÏÎ¿Î·Î³Î¼Î­Î½Î± Î¼Î¿Ï„Î¯Î²Î± Î³Î¹Î± Ï„Î¿ Foundry Local ÎºÎ±Î¹ Ï„Î¿ Azure OpenAI
- âœ… **Î¡Î¿Î­Ï‚ Î‘Ï€Î±Î½Ï„Î®ÏƒÎµÏ‰Î½**: Î£Ï…Î½Î¿Î¼Î¹Î»Î¯ÎµÏ‚ ÏƒÎµ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏŒ Ï‡ÏÏŒÎ½Î¿ Î³Î¹Î± Î²ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î· ÎµÎ¼Ï€ÎµÎ¹ÏÎ¯Î± Ï‡ÏÎ®ÏƒÏ„Î·
- âœ… **Î¥Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î· Î Î¿Î»Î»Î±Ï€Î»ÏÎ½ Î Î±ÏÏŒÏ‡Ï‰Î½**: Î‘Ï€ÏÏŒÏƒÎºÎ¿Ï€Ï„Î· ÎµÎ½Î±Î»Î»Î±Î³Î® Î¼ÎµÏ„Î±Î¾Ï Ï„Î¿Ï€Î¹ÎºÏÎ½ ÎºÎ±Î¹ cloud Ï…Ï€Î·ÏÎµÏƒÎ¹ÏÎ½ AI
- âœ… **Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î£Ï…Î½Î¿Î¼Î¹Î»Î¹ÏÎ½**: Î£Ï…Î½Î¿Î¼Î¹Î»Î¯ÎµÏ‚ Ï€Î¿Î»Î»Î±Ï€Î»ÏÎ½ ÏƒÏ„ÏÎ¿Ï†ÏÎ½ Î¼Îµ Î´Î¹Î±Ï„Î®ÏÎ·ÏƒÎ· Ï€Î»Î±Î¹ÏƒÎ¯Î¿Ï…
- âœ… **Î Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· Î‘Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚**: ÎœÎµÏ„ÏÎ®ÏƒÎµÎ¹Ï‚ ÎºÎ±Î¹ Î­Î»ÎµÎ³Ï‡Î¿Î¹ Ï…Î³ÎµÎ¯Î±Ï‚ Î³Î¹Î± Ï€Î±ÏÎ±Î³Ï‰Î³Î¹ÎºÎ­Ï‚ Î±Î½Î±Ï€Ï„ÏÎ¾ÎµÎ¹Ï‚
- âœ… **ÎœÎ¿Ï„Î¯Î²Î± Î Î±ÏÎ±Î³Ï‰Î³Î®Ï‚**: Î™ÏƒÏ‡Ï…ÏÎ® Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½ ÎºÎ±Î¹ Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î´Î¹Î±Î¼ÏŒÏÏ†Ï‰ÏƒÎ·Ï‚

### Î’Î±ÏƒÎ¹ÎºÎ¬ Î‘ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ¬ ÎœÎ¿Ï„Î¯Î²Î±

**ÎœÎ¿Ï„Î¯Î²Î¿ Client Factory:**
```
Environment Detection â†’ Provider Selection â†’ Client Creation â†’ Model Configuration
        â†“                    â†“                  â†“                 â†“
    Azure/Local       Azure OpenAI/        OpenAI Client    Model Selection
    Credentials       Foundry Local        Initialization   and Validation
```

**Î¡Î¿Î® Î‘Ï€Î±Î½Ï„Î®ÏƒÎµÏ‰Î½ Î¼Îµ Streaming:**
```
User Input â†’ Chat Completion â†’ Stream Processing â†’ Real-time Display
     â†“             â†“                â†“                 â†“
   Prompt         Stream=True         Token Chunks      Progressive UI
```

### Î ÎµÏÎ¯Î»Î·ÏˆÎ· Î’Î­Î»Ï„Î¹ÏƒÏ„Ï‰Î½ Î ÏÎ±ÎºÏ„Î¹ÎºÏÎ½

1. **ğŸ”„ Î Î¬Î½Ï„Î± Î¥Î»Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Îµ Î•Î½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ­Ï‚ Î›ÏÏƒÎµÎ¹Ï‚**: Azure â†’ Foundry Local â†’ Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½
2. **ğŸŒŠ Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Streaming Î³Î¹Î± ÎœÎµÎ³Î¬Î»ÎµÏ‚ Î‘Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚**: ÎšÎ±Î»ÏÏ„ÎµÏÎ· Î±Î½Ï„Î¹Î»Î·Ï€Ï„Î® Î±Ï€ÏŒÎ´Î¿ÏƒÎ·
3. **ğŸ›¡ï¸ Î¥Î»Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Î Î»Î®ÏÎ· Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î£Ï†Î±Î»Î¼Î¬Ï„Ï‰Î½**: Î¦Î¹Î»Î¹ÎºÎ¬ Î¼Î·Î½ÏÎ¼Î±Ï„Î± ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½ Ï€ÏÎ¿Ï‚ Ï„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î·
4. **ğŸ“ˆ Î Î±ÏÎ±ÎºÎ¿Î»Î¿Ï…Î¸Î®ÏƒÏ„Îµ Î‘Ï€ÏŒÎ´Î¿ÏƒÎ·**: Î Î±ÏÎ±ÎºÎ¿Î»Î¿Ï…Î¸Î®ÏƒÏ„Îµ Ï‡ÏÏŒÎ½Î¿Ï…Ï‚ Î±Ï€ÏŒÎºÏÎ¹ÏƒÎ·Ï‚ ÎºÎ±Î¹ Ï€Î¿ÏƒÎ¿ÏƒÏ„Î¬ ÎµÏ€Î¹Ï„Ï…Ï‡Î¯Î±Ï‚
5. **âš™ï¸ Î”Î¹Î±Î¼ÏŒÏÏ†Ï‰ÏƒÎ· Î’Î¬ÏƒÎµÎ¹ Î ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î¿Ï‚**: Î•ÏÎºÎ¿Î»Î· ÎµÎ½Î±Î»Î»Î±Î³Î® Î¼ÎµÏ„Î±Î¾Ï dev/staging/prod
6. **ğŸ”’ Î‘ÏƒÏ†Î±Î»Î®Ï‚ Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î”Î¹Î±Ï€Î¹ÏƒÏ„ÎµÏ…Ï„Î·ÏÎ¯Ï‰Î½**: Î Î¿Ï„Î­ Î¼Î·Î½ Î±Ï€Î¿Î¸Î·ÎºÎµÏÎµÏ„Îµ API keys ÏƒÏ„Î¿Î½ ÎºÏÎ´Î¹ÎºÎ±

### ÎšÎ±Ï„ÎµÏ…Î¸Ï…Î½Ï„Î®ÏÎ¹ÎµÏ‚ Î“ÏÎ±Î¼Î¼Î­Ï‚ Î•Ï€Î¹Î»Î¿Î³Î®Ï‚ Î Î±ÏÏŒÏ‡Î¿Ï…

| Î£ÎµÎ½Î¬ÏÎ¹Î¿ | Î£Ï…Î½Î¹ÏƒÏ„ÏÎ¼ÎµÎ½Î¿Ï‚ Î Î¬ÏÎ¿Ï‡Î¿Ï‚ | Î‘Î¹Ï„Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· |
|----------|---------------------|----------|
| **Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·** | Foundry Local | Î“ÏÎ®Î³Î¿ÏÎ· ÎµÏ€Î±Î½Î¬Î»Î·ÏˆÎ·, Ï‡Ï‰ÏÎ¯Ï‚ ÎºÏŒÏƒÏ„Î¿Ï‚ API |
| **ÎšÏÎ¯ÏƒÎ¹Î¼Î· Î™Î´Î¹Ï‰Ï„Î¹ÎºÏŒÏ„Î·Ï„Î±** | Foundry Local | Î¤Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î´ÎµÎ½ Ï†ÎµÏÎ³Î¿Ï…Î½ Î±Ï€ÏŒ Ï„Î· ÏƒÏ…ÏƒÎºÎµÏ…Î® |
| **Î Î±ÏÎ±Î³Ï‰Î³Î® Î¥ÏˆÎ·Î»Î¿Ï ÎŒÎ³ÎºÎ¿Ï…** | Azure OpenAI | ÎšÎ±Î»ÏÏ„ÎµÏÎ· ÎºÎ»Î¹Î¼Î¬ÎºÏ‰ÏƒÎ·, SLA Î³Î¹Î± ÎµÏ€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÎ¹Ï‚ |
| **ÎÎµÏŒÏ„ÎµÏÎ± ÎœÎ¿Î½Ï„Î­Î»Î±** | Azure OpenAI | Î ÏÏŒÏƒÎ²Î±ÏƒÎ· ÏƒÏ„Î¹Ï‚ Ï€Î¹Î¿ Ï€ÏÏŒÏƒÏ†Î±Ï„ÎµÏ‚ ÎµÎºÎ´ÏŒÏƒÎµÎ¹Ï‚ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ |
| **Î‘Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚ Î•ÎºÏ„ÏŒÏ‚ Î£ÏÎ½Î´ÎµÏƒÎ·Ï‚** | Foundry Local | Î§Ï‰ÏÎ¯Ï‚ ÎµÎ¾Î¬ÏÏ„Î·ÏƒÎ· Î±Ï€ÏŒ Ï„Î¿ Î´Î¹Î±Î´Î¯ÎºÏ„Ï…Î¿ |
| **Î•Ï…Î±Î¹ÏƒÎ¸Î·ÏƒÎ¯Î± ÎšÏŒÏƒÏ„Î¿Ï…Ï‚** | Foundry Local | Î§Ï‰ÏÎ¯Ï‚ Ï‡ÏÎµÏÏƒÎµÎ¹Ï‚ Î±Î½Î¬ token |

### Î“ÏÎ®Î³Î¿ÏÎ· Î‘Î½Î±Ï†Î¿ÏÎ¬ ÎœÎµÏ„Î±Î²Î»Î·Ï„ÏÎ½ Î ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î¿Ï‚

```cmd
REM Foundry Local (default)
set MODEL=phi-4-mini
set BASE_URL=http://localhost:8000
set API_KEY=

REM Azure OpenAI (cloud)
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name
```

### Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Î³Î¹Î± Ï„Î· Î£Ï…Î½ÎµÎ´ÏÎ¯Î± 3: Î‘Î½Î¿Î¹Ï‡Ï„Î¿Ï ÎšÏÎ´Î¹ÎºÎ± ÎœÎ¿Î½Ï„Î­Î»Î±

1. **Î•Î¾ÎµÏÎµÏ…Î½Î®ÏƒÏ„Îµ Ï„Î¿Î½ ÎšÎ±Ï„Î¬Î»Î¿Î³Î¿ ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½**: Î‘Î½Î±ÏƒÎºÏŒÏ€Î·ÏƒÎ· Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Ï‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ ÏƒÏ„Î¿ Foundry Local
2. **ÎšÎ±Ï„Î±Î½Î¿Î®ÏƒÏ„Îµ Ï„Î¹Ï‚ ÎœÎ¿ÏÏ†Î­Ï‚ ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½**: ÎœÎ¬Î¸ÎµÏ„Îµ Î³Î¹Î± ONNX, Ï€Î¿ÏƒÎ¿Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· ÎºÎ±Î¹ Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ·
3. **Î£ÎºÎµÏ†Ï„ÎµÎ¯Ï„Îµ Î ÏÎ¿ÏƒÎ±ÏÎ¼Î¿ÏƒÎ¼Î­Î½Î± ÎœÎ¿Î½Ï„Î­Î»Î±**: Î£ÎºÎµÏ†Ï„ÎµÎ¯Ï„Îµ Î³Î¹Î± Î±Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ ÎµÎ¹Î´Î¹ÎºÏÎ½ Ï„Î¿Î¼Î­Ï‰Î½

### Î“ÏÎ®Î³Î¿ÏÎ¿Ï‚ ÎŸÎ´Î·Î³ÏŒÏ‚ Î‘Î½Ï„Î¹Î¼ÎµÏ„ÏÏ€Î¹ÏƒÎ·Ï‚ Î ÏÎ¿Î²Î»Î·Î¼Î¬Ï„Ï‰Î½

**Î£Ï…Î½Î·Î¸Î¹ÏƒÎ¼Î­Î½Î± Î ÏÎ¿Î²Î»Î®Î¼Î±Ï„Î±:**
```cmd
REM Issue: Could not use Foundry SDK
pip install foundry-local-sdk

REM Issue: Connection refused
foundry service status
foundry model run phi-4-mini

REM Issue: Azure authentication failed
echo %AZURE_OPENAI_ENDPOINT%
echo %AZURE_OPENAI_API_KEY%

REM Issue: Model not found
foundry model list
curl http://localhost:8000/v1/models
```

## Î‘Î½Î±Ï†Î¿ÏÎ­Ï‚

- **[Î¤ÎµÎºÎ¼Î·ÏÎ¯Ï‰ÏƒÎ· OpenAI Python SDK](https://github.com/openai/openai-python)**: Î•Ï€Î¯ÏƒÎ·Î¼Î· Î±Î½Î±Ï†Î¿ÏÎ¬ SDK
- **[Î¤ÎµÎºÎ¼Î·ÏÎ¯Ï‰ÏƒÎ· Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/)**: ÎŸÎ´Î·Î³ÏŒÏ‚ Ï…Ï€Î·ÏÎµÏƒÎ¯Î±Ï‚ Azure OpenAI
- **[Î‘Î½Î±Ï†Î¿ÏÎ¬ Foundry Local SDK](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Î¤ÎµÎºÎ¼Î·ÏÎ¯Ï‰ÏƒÎ· Ï„Î¿Ï€Î¹ÎºÎ®Ï‚ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·Ï‚
- **[ÎŸÎ´Î·Î³ÏŒÏ‚ Î¡Î¿ÏÎ½ Î£Ï…Î½Î¿Î¼Î¹Î»Î¹ÏÎ½](https://learn.microsoft.com/azure/ai-foundry/foundry-local/how-to/integrate-with-inference-sdks)**: Î ÏÎ¿Î·Î³Î¼Î­Î½Î± Î¼Î¿Ï„Î¯Î²Î± streaming
- **[Î”ÎµÎ¯Î³Î¼Î± 01: Î“ÏÎ®Î³Î¿ÏÎ· Î£Ï…Î½Î¿Î¼Î¹Î»Î¯Î± Î¼Î­ÏƒÏ‰ OpenAI SDK](samples/01/README.md)**: Î’Î±ÏƒÎ¹ÎºÎ¬ Î¼Î¿Ï„Î¯Î²Î± ÎµÎ½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ·Ï‚
- **[Î”ÎµÎ¯Î³Î¼Î± 02: Î ÏÎ¿Î·Î³Î¼Î­Î½Î· Î•Î½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· SDK](samples/02/README.md)**: Î ÏÎ±ÎºÏ„Î¹ÎºÎ¬ Ï€Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î± Î±Ï…Ï„Î®Ï‚ Ï„Î·Ï‚ ÏƒÏ…Î½ÎµÎ´ÏÎ¯Î±Ï‚
- **[Î”ÎµÎ¯Î³Î¼Î± 04: Î•Ï†Î±ÏÎ¼Î¿Î³Î® Chainlit](samples/04/README.md)**: Î‘Î½Î¬Ï€Ï„Ï…Î¾Î· Web UI
- **[Î”ÎµÎ¯Î³Î¼Î± 05: Î£Ï…ÏƒÏ„Î®Î¼Î±Ï„Î± Î Î¿Î»Î»Î±Ï€Î»ÏÎ½ Î ÏÎ±ÎºÏ„ÏŒÏÏ‰Î½](samples/05/README.md)**: Î ÏÎ¿Î·Î³Î¼Î­Î½Î± Î¼Î¿Ï„Î¯Î²Î± Î¿ÏÏ‡Î®ÏƒÏ„ÏÏ‰ÏƒÎ·Ï‚

Î•Î¯ÏƒÏ„Îµ Ï€Î»Î­Î¿Î½ Î­Ï„Î¿Î¹Î¼Î¿Î¹ Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎµÏ„Îµ ÎµÎ¾ÎµÎ»Î¹Î³Î¼Î­Î½ÎµÏ‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚ AI Ï€Î¿Ï… ÎµÎ½ÏƒÏ‰Î¼Î±Ï„ÏÎ½Î¿Ï…Î½ Î±Ï€ÏÏŒÏƒÎºÎ¿Ï€Ï„Î± Ï„Î¿Ï€Î¹ÎºÎ­Ï‚ ÎºÎ±Î¹ cloud Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ AI, Ï€Î±ÏÎ­Ï‡Î¿Î½Ï„Î±Ï‚ Ï„Î·Î½ ÎµÏ…ÎµÎ»Î¹Î¾Î¯Î± Î½Î± ÎµÏ€Î¹Î»Î­Î¾ÎµÏ„Îµ Ï„Î¿Î½ ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î¿ Ï€Î¬ÏÎ¿Ï‡Î¿ Î³Î¹Î± ÎºÎ¬Î¸Îµ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î· Ï€ÎµÏÎ¯Ï€Ï„Ï‰ÏƒÎ· Ï‡ÏÎ®ÏƒÎ·Ï‚, Î´Î¹Î±Ï„Î·ÏÏÎ½Ï„Î±Ï‚ Ï€Î±ÏÎ¬Î»Î»Î·Î»Î± ÏƒÏ…Î½ÎµÏ€ÎµÎ¯Ï‚ Î¼Î¿Ï„Î¯Î²Î± Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚.

---

