{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0382ebeb",
   "metadata": {},
   "source": [
    "# Î”ÎµÎ¯Î³Î¼Î± 04: Î•Ï†Î±ÏÎ¼Î¿Î³Î® Chainlit Web\n",
    "\n",
    "Î‘Ï…Ï„ÏŒ Ï„Î¿ ÏƒÎ·Î¼ÎµÎ¹Ï‰Î¼Î±Ï„Î¬ÏÎ¹Î¿ Î´ÎµÎ¯Ï‡Î½ÎµÎ¹ Ï€ÏÏ‚ Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎµÏ„Îµ ÎºÎ±Î¹ Î½Î± ÎºÎ±Ï„Î±Î½Î¿Î®ÏƒÎµÏ„Îµ Î¼Î¹Î± ÏƒÏÎ³Ï‡ÏÎ¿Î½Î· ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Chainlit web Î³Î¹Î± ÏƒÏ…Î½Î¿Î¼Î¹Î»Î·Ï„Î¹ÎºÎ® AI Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î¿ Microsoft Foundry Local Î¼Îµ Ï„Î¿ OpenAI SDK.\n",
    "\n",
    "## Î•Ï€Î¹ÏƒÎºÏŒÏ€Î·ÏƒÎ·\n",
    "\n",
    "Î¤Î¿ Chainlit ÎµÎ¯Î½Î±Î¹ Î­Î½Î± Î¹ÏƒÏ‡Ï…ÏÏŒ Ï€Î»Î±Î¯ÏƒÎ¹Î¿ Î³Î¹Î± Ï„Î·Î½ Î±Î½Î¬Ï€Ï„Ï…Î¾Î· ÎµÏ†Î±ÏÎ¼Î¿Î³ÏÎ½ ÏƒÏ…Î½Î¿Î¼Î¹Î»Î·Ï„Î¹ÎºÎ®Ï‚ AI Î¼Îµ ÏƒÏÎ³Ï‡ÏÎ¿Î½ÎµÏ‚ Î´Î¹ÎµÏ€Î±Ï†Î­Ï‚ web. Î‘Ï…Ï„ÏŒ Ï„Î¿ Î´ÎµÎ¯Î³Î¼Î± Ï€Î±ÏÎ¿Ï…ÏƒÎ¹Î¬Î¶ÎµÎ¹:\n",
    "\n",
    "- ğŸŒ **Î£ÏÎ³Ï‡ÏÎ¿Î½Î· Î”Î¹ÎµÏ€Î±Ï†Î® Web**: Î•Ï€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÎ® Î´Î¹ÎµÏ€Î±Ï†Î® ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¯Î±Ï‚ Î¼Îµ ÎµÎ½Î·Î¼ÎµÏÏÏƒÎµÎ¹Ï‚ ÏƒÎµ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏŒ Ï‡ÏÏŒÎ½Î¿\n",
    "- ğŸ”„ **Î¡Î¿Î­Ï‚ Î‘Ï€Î±Î½Ï„Î®ÏƒÎµÏ‰Î½**: Î¡Î¿Î® Î¼Î·Î½Ï…Î¼Î¬Ï„Ï‰Î½ ÏƒÎµ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏŒ Ï‡ÏÏŒÎ½Î¿ Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· ÎµÎ¼Ï€ÎµÎ¹ÏÎ¯Î± Ï‡ÏÎ®ÏƒÏ„Î·\n",
    "- ğŸ¯ **Î•Î½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· OpenAI SDK**: Î£Ï‰ÏƒÏ„Î® ÏÏÎ¸Î¼Î¹ÏƒÎ· Ï€ÎµÎ»Î¬Ï„Î· API Î¼Îµ Ï„Î¿ Foundry Local\n",
    "- ğŸ›¡ï¸ **Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î£Ï†Î±Î»Î¼Î¬Ï„Ï‰Î½**: ÎŸÎ¼Î±Î»Î­Ï‚ ÎµÎ½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ­Ï‚ Î»ÏÏƒÎµÎ¹Ï‚ ÎºÎ±Î¹ Ï†Î¹Î»Î¹ÎºÎ¬ Î¼Î·Î½ÏÎ¼Î±Ï„Î± ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½ Ï€ÏÎ¿Ï‚ Ï„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î·\n",
    "- âš™ï¸ **ÎˆÏ„Î¿Î¹Î¼Î¿ Î³Î¹Î± Î Î±ÏÎ±Î³Ï‰Î³Î®**: Î”Î¹Î±Î¼ÏŒÏÏ†Ï‰ÏƒÎ· ÎºÎ±Î¹ Ï€ÏÏŒÏ„Ï…Ï€Î± Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚ ÎµÏ€Î¹Ï€Î­Î´Î¿Ï… ÎµÏ€Î¹Ï‡ÎµÎ¯ÏÎ·ÏƒÎ·Ï‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b131d",
   "metadata": {},
   "source": [
    "## Î ÏÎ¿Î±Ï€Î±Î¹Ï„Î¿ÏÎ¼ÎµÎ½Î± ÎºÎ±Î¹ Î¡ÏÎ¸Î¼Î¹ÏƒÎ·\n",
    "\n",
    "Î ÏÎ¹Î½ ÎµÎºÏ„ÎµÎ»Î­ÏƒÎµÏ„Îµ Î±Ï…Ï„ÏŒ Ï„Î¿ notebook, Î²ÎµÎ²Î±Î¹Ï‰Î¸ÎµÎ¯Ï„Îµ ÏŒÏ„Î¹ Î­Ï‡ÎµÏ„Îµ ÎµÎ³ÎºÎ±Ï„Î±ÏƒÏ„Î®ÏƒÎµÎ¹ Ï„Î± Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„Î± Ï€Î±ÎºÎ­Ï„Î±:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b547d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: chainlit in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: openai in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (1.109.0)\n",
      "Requirement already satisfied: foundry-local-sdk in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: aiofiles<25.0.0,>=23.1.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (24.1.0)\n",
      "Requirement already satisfied: asyncer<0.1.0,>=0.0.8 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.0.8)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (8.3.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.6.7)\n",
      "Requirement already satisfied: fastapi<0.117,>=0.116.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.116.2)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (1.2.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.27.2)\n",
      "Requirement already satisfied: lazify<0.5.0,>=0.4.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.4.0)\n",
      "Requirement already satisfied: literalai==0.1.201 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.1.201)\n",
      "Requirement already satisfied: mcp<2.0.0,>=1.11.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (1.14.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (1.6.0)\n",
      "Requirement already satisfied: packaging>=23.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (24.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.10.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (2.10.1)\n",
      "Requirement already satisfied: pydantic<3,>=2.7.2 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (2.11.9)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (2.10.1)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (1.1.1)\n",
      "Requirement already satisfied: python-multipart<1.0.0,>=0.0.18 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.0.20)\n",
      "Requirement already satisfied: python-socketio<6.0.0,>=5.11.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (5.13.0)\n",
      "Requirement already satisfied: starlette>=0.47.2 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.48.0)\n",
      "Requirement already satisfied: syncer<3.0.0,>=2.0.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (2.0.3)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (2.2.1)\n",
      "Requirement already satisfied: uvicorn>=0.35.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.37.0)\n",
      "Requirement already satisfied: watchfiles<1.0.0,>=0.20.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.24.0)\n",
      "Requirement already satisfied: chevron>=0.14.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from literalai==0.1.201->chainlit) (0.14.0)\n",
      "Requirement already satisfied: traceloop-sdk>=0.33.12 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from click<9.0.0,>=8.1.3->chainlit) (0.4.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->chainlit) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->chainlit) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from httpx>=0.23.0->chainlit) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from httpx>=0.23.0->chainlit) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from httpcore==1.*->httpx>=0.23.0->chainlit) (0.14.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from mcp<2.0.0,>=1.11.0->chainlit) (0.4.1)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from mcp<2.0.0,>=1.11.0->chainlit) (4.23.0)\n",
      "Requirement already satisfied: pywin32>=310 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from mcp<2.0.0,>=1.11.0->chainlit) (311)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from mcp<2.0.0,>=1.11.0->chainlit) (3.0.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from pydantic<3,>=2.7.2->chainlit) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from pydantic<3,>=2.7.2->chainlit) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from pydantic<3,>=2.7.2->chainlit) (0.4.1)\n",
      "Requirement already satisfied: bidict>=0.21.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from python-socketio<6.0.0,>=5.11.0->chainlit) (0.23.1)\n",
      "Requirement already satisfied: python-engineio>=4.11.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from python-socketio<6.0.0,>=5.11.0->chainlit) (4.12.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->chainlit) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->chainlit) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->chainlit) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->chainlit) (0.20.0)\n",
      "Requirement already satisfied: simple-websocket>=0.10.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from python-engineio>=4.11.0->python-socketio<6.0.0,>=5.11.0->chainlit) (1.1.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.11.11 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (3.12.15)\n",
      "Requirement already satisfied: cuid<0.5,>=0.4 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.4)\n",
      "Requirement already satisfied: deprecated<2.0.0,>=1.2.14 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.2.18)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.5 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (3.1.6)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.28.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.28.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.28.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-alephalpha==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-anthropic==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-bedrock==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-chromadb==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-cohere==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-crewai==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-google-generativeai==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-groq==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-haystack==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-lancedb==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-langchain==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-llamaindex==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-logging>=0.57b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-marqo==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-mcp==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-milvus==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-mistralai==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-ollama==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-openai==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-openai-agents==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-pinecone==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-qdrant==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-redis>=0.50b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-replicate==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-requests>=0.50b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-sagemaker==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-sqlalchemy>=0.50b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-threading>=0.50b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-together==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-transformers==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-urllib3>=0.50b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-vertexai==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-watsonx==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-weaviate==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-writer==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.28.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.13 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.4.13)\n",
      "Requirement already satisfied: posthog<4,>3.0.2 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (3.25.0)\n",
      "Requirement already satisfied: tenacity<10.0,>=8.2.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (9.1.2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation>=0.50b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-instrumentation-alephalpha==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions>=0.50b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-instrumentation-alephalpha==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: anthropic>=0.17.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-instrumentation-bedrock==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.68.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-instrumentation-bedrock==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.20.0)\n",
      "Requirement already satisfied: inflection<0.6.0,>=0.5.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-instrumentation-llamaindex==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.5.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp<2.0.0,>=1.34.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-instrumentation-mcp==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.37.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->chainlit) (1.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.11->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.11->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.11->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.11->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.11->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.11->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.20.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from deprecated<2.0.0,>=1.2.14->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.17.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from jinja2<4.0.0,>=3.1.5->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (2.1.5)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.75.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.37.0)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-proto==1.37.0->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (6.32.1)\n",
      "Requirement already satisfied: requests~=2.7 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (2.32.3)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.58b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-instrumentation-requests>=0.50b0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from posthog<4,>3.0.2->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from posthog<4,>3.0.2->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from posthog<4,>3.0.2->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from posthog<4,>3.0.2->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (2.9.0)\n",
      "Requirement already satisfied: wsproto in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio<6.0.0,>=5.11.0->chainlit) (1.2.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from anthropic>=0.17.0->opentelemetry-instrumentation-bedrock==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.16)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (3.20.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (2.2.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from tokenizers>=0.13.0->opentelemetry-instrumentation-bedrock==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.25.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->opentelemetry-instrumentation-bedrock==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->opentelemetry-instrumentation-bedrock==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->opentelemetry-instrumentation-bedrock==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install chainlit openai foundry-local-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc08221b",
   "metadata": {},
   "source": [
    "## ÎšÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ· Ï„Î·Ï‚ Î•Ï†Î±ÏÎ¼Î¿Î³Î®Ï‚ Chainlit\n",
    "\n",
    "Î‘Ï‚ ÎµÎ¾ÎµÏ„Î¬ÏƒÎ¿Ï…Î¼Îµ Ï„Î· Î´Î¿Î¼Î® ÎºÎ±Î¹ Ï„Î± Î²Î±ÏƒÎ¹ÎºÎ¬ ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î± Ï„Î·Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î®Ï‚ Î¼Î±Ï‚ Chainlit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6569e528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Foundry Local SDK is available\n",
      "ğŸ“¦ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import chainlit as cl\n",
    "import os\n",
    "import sys\n",
    "from typing import Optional, Dict, Any\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import asyncio\n",
    "\n",
    "# Check for Foundry Local SDK\n",
    "try:\n",
    "    from foundry_local import FoundryLocalManager\n",
    "    FOUNDRY_SDK_AVAILABLE = True\n",
    "    print(\"âœ… Foundry Local SDK is available\")\n",
    "except ImportError:\n",
    "    FOUNDRY_SDK_AVAILABLE = False\n",
    "    print(\"âš ï¸ Foundry Local SDK not available, will use manual configuration\")\n",
    "\n",
    "print(\"ğŸ“¦ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d20de",
   "metadata": {},
   "source": [
    "## ÎšÎ»Î¬ÏƒÎ· Î¡ÏÎ¸Î¼Î¹ÏƒÎ·Ï‚ Î ÎµÎ»Î¬Ï„Î·\n",
    "\n",
    "Î‘Ï…Ï„Î® Î· ÎºÎ»Î¬ÏƒÎ· Î´Î¹Î±Ï‡ÎµÎ¹ÏÎ¯Î¶ÎµÏ„Î±Î¹ Ï„Î· ÏÏÎ¸Î¼Î¹ÏƒÎ· Ï„Î¿Ï… Ï€ÎµÎ»Î¬Ï„Î· OpenAI Î¼Îµ ÎµÎ½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· Foundry Local:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "638523d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Initializing Foundry Local SDK with model: phi-4-mini...\n",
      "2025-09-23 21:43:21 - HTTP Request: GET http://127.0.0.1:51211/foundry/list \"HTTP/1.1 200 OK\"\n",
      "2025-09-23 21:43:21 - HTTP Request: GET http://127.0.0.1:51211/openai/models \"HTTP/1.1 200 OK\"\n",
      "2025-09-23 21:43:21 - HTTP Request: GET http://127.0.0.1:51211/openai/load/Phi-4-mini-instruct-cuda-gpu?ttl=600&ep= \"HTTP/1.1 200 OK\"\n",
      "âœ… Foundry Local SDK initialized at http://127.0.0.1:51211/v1\n",
      "\n",
      "ğŸ“Š **Client Initialization Result:**\n",
      "   Status: success\n",
      "   Method: foundry_sdk\n",
      "   Base_Url: http://127.0.0.1:51211/v1\n",
      "   Model: phi-4-mini\n"
     ]
    }
   ],
   "source": [
    "class FoundryClientManager:\n",
    "    \"\"\"Manages OpenAI client setup for Foundry Local integration.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"phi-4-mini\"):\n",
    "        self.model_name = model_name\n",
    "        self.client = None\n",
    "        self.async_client = None\n",
    "        self.base_url = None\n",
    "        self.api_key = None\n",
    "        \n",
    "    def _get_fallback_config(self) -> tuple[str, str]:\n",
    "        \"\"\"Get fallback configuration from environment variables.\"\"\"\n",
    "        base_url = os.getenv(\"BASE_URL\", \"http://localhost:8000\")\n",
    "        api_key = os.getenv(\"API_KEY\", \"\")\n",
    "        return base_url, api_key\n",
    "    \n",
    "    def initialize_clients(self) -> Dict[str, Any]:\n",
    "        \"\"\"Initialize both sync and async OpenAI clients.\"\"\"\n",
    "        if FOUNDRY_SDK_AVAILABLE:\n",
    "            try:\n",
    "                print(f\"ğŸ”„ Initializing Foundry Local SDK with model: {self.model_name}...\")\n",
    "                manager = FoundryLocalManager(self.model_name)\n",
    "                \n",
    "                self.base_url = manager.endpoint\n",
    "                self.api_key = manager.api_key\n",
    "                \n",
    "                # Create both sync and async clients\n",
    "                self.client = OpenAI(\n",
    "                    base_url=self.base_url,\n",
    "                    api_key=self.api_key\n",
    "                )\n",
    "                \n",
    "                self.async_client = AsyncOpenAI(\n",
    "                    base_url=self.base_url,\n",
    "                    api_key=self.api_key\n",
    "                )\n",
    "                \n",
    "                print(f\"âœ… Foundry Local SDK initialized at {self.base_url}\")\n",
    "                return {\n",
    "                    \"status\": \"success\",\n",
    "                    \"method\": \"foundry_sdk\",\n",
    "                    \"base_url\": self.base_url,\n",
    "                    \"model\": self.model_name\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Foundry SDK failed ({e}), falling back to manual configuration\")\n",
    "        \n",
    "        # Fallback to manual configuration\n",
    "        self.base_url, self.api_key = self._get_fallback_config()\n",
    "        \n",
    "        self.client = OpenAI(\n",
    "            base_url=f\"{self.base_url}/v1\",\n",
    "            api_key=self.api_key\n",
    "        )\n",
    "        \n",
    "        self.async_client = AsyncOpenAI(\n",
    "            base_url=f\"{self.base_url}/v1\",\n",
    "            api_key=self.api_key\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ”§ Manual configuration at {self.base_url}/v1\")\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"method\": \"manual\",\n",
    "            \"base_url\": f\"{self.base_url}/v1\",\n",
    "            \"model\": self.model_name\n",
    "        }\n",
    "    \n",
    "    async def test_connection(self) -> Dict[str, Any]:\n",
    "        \"\"\"Test the connection to Foundry Local service.\"\"\"\n",
    "        try:\n",
    "            # Try to list available models\n",
    "            models = await self.async_client.models.list()\n",
    "            available_models = [model.id for model in models.data]\n",
    "            \n",
    "            # Test with a simple completion\n",
    "            response = await self.async_client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Hello, are you working?\"}],\n",
    "                max_tokens=50\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"healthy\",\n",
    "                \"available_models\": available_models,\n",
    "                \"current_model\": self.model_name,\n",
    "                \"test_response\": response.choices[0].message.content,\n",
    "                \"base_url\": self.base_url\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "                \"base_url\": self.base_url\n",
    "            }\n",
    "\n",
    "# Create a client manager instance\n",
    "client_manager = FoundryClientManager(\"phi-4-mini\")\n",
    "init_result = client_manager.initialize_clients()\n",
    "\n",
    "print(f\"\\nğŸ“Š **Client Initialization Result:**\")\n",
    "for key, value in init_result.items():\n",
    "    print(f\"   {key.title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05013074",
   "metadata": {},
   "source": [
    "## Î”Î¿ÎºÎ¹Î¼Î® Î£ÏÎ½Î´ÎµÏƒÎ·Ï‚\n",
    "\n",
    "Î‘Ï‚ Î´Î¿ÎºÎ¹Î¼Î¬ÏƒÎ¿Ï…Î¼Îµ Ï„Î· ÏƒÏÎ½Î´ÎµÏƒÎ® Î¼Î±Ï‚ Î¼Îµ Ï„Î·Î½ Ï…Ï€Î·ÏÎµÏƒÎ¯Î± Foundry Local:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96614f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” **Testing Foundry Local Connection**\n",
      "==================================================\n",
      "2025-09-23 21:43:24 - HTTP Request: GET http://127.0.0.1:51211/v1/models \"HTTP/1.1 200 OK\"\n",
      "2025-09-23 21:43:24 - HTTP Request: POST http://127.0.0.1:51211/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-09-23 21:43:24 - HTTP Request: POST http://127.0.0.1:51211/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "âŒ **Connection Status:** Error\n",
      "ğŸ”— **Base URL:** http://127.0.0.1:51211/v1\n",
      "âš ï¸ **Error:** Error code: 400\n",
      "\n",
      "ğŸ”§ **Troubleshooting:**\n",
      "1. Check if Foundry Local is running: foundry service status\n",
      "2. Start a model: foundry model run phi-4-mini\n",
      "3. Verify the endpoint URL and port\n"
     ]
    }
   ],
   "source": [
    "# Test the connection asynchronously\n",
    "async def test_service_connection():\n",
    "    \"\"\"Test connection to Foundry Local service.\"\"\"\n",
    "    print(\"ğŸ” **Testing Foundry Local Connection**\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    health_check = await client_manager.test_connection()\n",
    "    \n",
    "    if health_check[\"status\"] == \"healthy\":\n",
    "        print(\"âœ… **Connection Status:** Healthy\")\n",
    "        print(f\"ğŸ”— **Base URL:** {health_check['base_url']}\")\n",
    "        print(f\"ğŸ¤– **Current Model:** {health_check['current_model']}\")\n",
    "        print(f\"ğŸ’¬ **Test Response:** {health_check['test_response']}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ **Available Models ({len(health_check['available_models'])}):**\")\n",
    "        for i, model in enumerate(health_check['available_models'], 1):\n",
    "            current = \" (current)\" if model == health_check['current_model'] else \"\"\n",
    "            print(f\"   {i}. {model}{current}\")\n",
    "    else:\n",
    "        print(\"âŒ **Connection Status:** Error\")\n",
    "        print(f\"ğŸ”— **Base URL:** {health_check['base_url']}\")\n",
    "        print(f\"âš ï¸ **Error:** {health_check['error']}\")\n",
    "        print(\"\\nğŸ”§ **Troubleshooting:**\")\n",
    "        print(\"1. Check if Foundry Local is running: foundry service status\")\n",
    "        print(\"2. Start a model: foundry model run phi-4-mini\")\n",
    "        print(\"3. Verify the endpoint URL and port\")\n",
    "    \n",
    "    return health_check\n",
    "\n",
    "# Run the connection test\n",
    "connection_result = await test_service_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be87081",
   "metadata": {},
   "source": [
    "## Î”Î¿Î¼Î® Î•Ï†Î±ÏÎ¼Î¿Î³Î®Ï‚ Chainlit\n",
    "\n",
    "Î‘Ï‚ ÎµÎ¾ÎµÏ„Î¬ÏƒÎ¿Ï…Î¼Îµ Ï„ÏÏÎ± Ï„Î± Î²Î±ÏƒÎ¹ÎºÎ¬ ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î± Ï„Î·Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î®Ï‚ Î¼Î±Ï‚ Chainlit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fe8c2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ **Chainlit Application Code:**\n",
      "============================================================\n",
      "#!/usr/bin/env python3\n",
      "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
      "# Licensed under the MIT License.\n",
      "\n",
      "import os\n",
      "import chainlit as cl\n",
      "from openai import OpenAI\n",
      "\n",
      "try:\n",
      "    from foundry_local import FoundryLocalManager\n",
      "    FOUNDRY_SDK_AVAILABLE = True\n",
      "except ImportError:\n",
      "    FOUNDRY_SDK_AVAILABLE = False\n",
      "\n",
      "# Global variables for client and model\n",
      "client = None\n",
      "model_name = None\n",
      "\n",
      "\n",
      "async def initialize_client():\n",
      "    \"\"\"Initialize OpenAI client with Foundry Local or fallback configuration.\"\"\"\n",
      "    global client, model_name\n",
      "    \n",
      "    alias = os.environ.get(\"MODEL\", \"phi-4-mini\")\n",
      "    \n",
      "    if FOUNDRY_SDK_AVAILABLE:\n",
      "        try:\n",
      "            # Use FoundryLocalManager for proper service management\n",
      "            manager = FoundryLocalManager(alias)\n",
      "            model_info = manager.get_model_info(alias)\n",
      "            \n",
      "            # Configure OpenAI client to use local Foundry service\n",
      "            client = OpenAI(\n",
      "                base_url=manager.endpoint,\n",
      "                api_key=manager.api_key or \"not-required\"  # Ensure API key is not None\n",
      "            )\n",
      "            model_name = model_info.id if model_info else alias\n",
      "            print(f\"Initialized Foundry Local with model: {model_name}\")\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            print(f\"Warning: Could not use Foundry SDK ({e}), falling back to manual configuration\")\n",
      "    \n",
      "    # Fallback to manual configuration\n",
      "    base_url = os.environ.get(\"BASE_URL\", \"http://localhost:51211\")\n",
      "    api_key = os.environ.get(\"API_KEY\", \"not-required\")\n",
      "    model_name = alias\n",
      "    \n",
      "    client = OpenAI(\n",
      "        base_url=f\"{base_url}/v1\",\n",
      "        api_key=api_key\n",
      "    )\n",
      "    print(f\"Initialized manual configuration with model: {model_name}\")\n",
      "    return True\n",
      "\n",
      "\n",
      "@cl.on_chat_start\n",
      "async def start():\n",
      "    \"\"\"Initialize the chat session.\"\"\"\n",
      "    global client, model_name\n",
      "    \n",
      "    if client is None:\n",
      "        try:\n",
      "            await initialize_client()\n",
      "        except Exception as e:\n",
      "            error_msg = f\"âŒ **Initialization Error**\\n\\nCould not initialize the AI client. Please ensure Foundry Local is running.\\n\\n**Error:** {str(e)}\"\n",
      "            await cl.Message(content=error_msg).send()\n",
      "            return\n",
      "    \n",
      "    welcome_msg = f\"\"\"ğŸ¤– **Welcome to Foundry Local RAG Chat!**\n",
      "    \n",
      "**Model:** {model_name or 'Unknown'}\n",
      "**Powered by:** Microsoft Foundry Local\n",
      "\n",
      "You can ask me anything and I'll respond using the local AI model. The conversation supports:\n",
      "- âœ… Natural language processing\n",
      "- âœ… Code generation and explanation\n",
      "- âœ… Question answering\n",
      "- âœ… Creative writing\n",
      "\n",
      "Try asking me something!\"\"\"\n",
      "    \n",
      "    await cl.Message(content=welcome_msg).send()\n",
      "\n",
      "\n",
      "@cl.on_message\n",
      "async def main(message: cl.Message):\n",
      "    \"\"\"Handle incoming messages and generate responses.\"\"\"\n",
      "    global client, model_name\n",
      "    \n",
      "    if client is None:\n",
      "        await cl.Message(content=\"âŒ Error: Client not initialized. Please restart the application.\").send()\n",
      "        return\n",
      "    \n",
      "    try:\n",
      "        # Show typing indicator\n",
      "        msg = cl.Message(content=\"\")\n",
      "        await msg.send()\n",
      "        \n",
      "        # Create streaming response\n",
      "        stream = client.chat.completions.create(\n",
      "            model=model_name,\n",
      "            messages=[\n",
      "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant powered by Microsoft Foundry Local. Provide clear, accurate, and helpful responses.\"},\n",
      "                {\"role\": \"user\", \"content\": message.content}\n",
      "            ],\n",
      "            max_tokens=500,\n",
      "            temperature=0.7,\n",
      "            stream=True\n",
      "        )\n",
      "        \n",
      "        # Stream the response\n",
      "        for chunk in stream:\n",
      "            if hasattr(chunk, 'choices') and len(chunk.choices) > 0:\n",
      "                delta_content = chunk.choices[0].delta.content\n",
      "                if delta_content is not None:\n",
      "                    await msg.stream_token(delta_content)\n",
      "        \n",
      "        # Finalize the message\n",
      "        await msg.update()\n",
      "        \n",
      "    except Exception as e:\n",
      "        error_msg = f\"âŒ **Error generating response:**\\n\\n{str(e)}\\n\\nğŸ’¡ **Troubleshooting:**\\n1. Ensure Foundry Local is running: `foundry service status`\\n2. Check if model is loaded: `foundry service ps`\\n3. Verify endpoint: `curl http://localhost:51211/v1/models`\"\n",
      "        await cl.Message(content=error_msg).send()\n",
      "\n",
      "\n",
      "# Note: Client initialization happens in @cl.on_chat_start to ensure async context\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the actual Chainlit application file\n",
    "app_file_path = \"../04/app.py\"\n",
    "\n",
    "try:\n",
    "    with open(app_file_path, 'r', encoding='utf-8') as f:\n",
    "        app_content = f.read()\n",
    "    \n",
    "    print(\"ğŸ“„ **Chainlit Application Code:**\")\n",
    "    print(\"=\" * 60)\n",
    "    print(app_content)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ Application file not found at {app_file_path}\")\n",
    "    print(\"Let's create a sample application structure instead:\")\n",
    "    \n",
    "    sample_app = '''\n",
    "# Chainlit Application Structure\n",
    "\n",
    "import chainlit as cl\n",
    "from openai import AsyncOpenAI\n",
    "from foundry_local import FoundryLocalManager\n",
    "\n",
    "# Global client variable\n",
    "client = None\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    \"\"\"Initialize the chat session.\"\"\"\n",
    "    # Setup client and welcome user\n",
    "    \n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    \"\"\"Handle incoming messages with streaming response.\"\"\"\n",
    "    # Process message and stream response\n",
    "    \n",
    "# Error handling and client setup functions...\n",
    "'''\n",
    "    print(sample_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5674f9",
   "metadata": {},
   "source": [
    "## Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ ÎˆÎ½Î½Î¿Î¹ÎµÏ‚ Ï„Î¿Ï… Chainlit\n",
    "\n",
    "Î‘Ï‚ ÎºÎ±Ï„Î±Î½Î¿Î®ÏƒÎ¿Ï…Î¼Îµ Ï„Î¹Ï‚ ÎºÏÏÎ¹ÎµÏ‚ Î­Î½Î½Î¿Î¹ÎµÏ‚ Ï€Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ½Ï„Î±Î¹ ÏƒÏ„Î¹Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚ Chainlit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab85a613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ **Key Chainlit Concepts**\n",
      "==================================================\n",
      "\n",
      "ğŸ”§ **@cl.on_chat_start**\n",
      "   Purpose: Decorator for session initialization\n",
      "   When Called: When a new chat session begins\n",
      "   Typical Use: Setup client, show welcome message, initialize context\n",
      "\n",
      "ğŸ”§ **@cl.on_message**\n",
      "   Purpose: Decorator for message handling\n",
      "   When Called: When user sends a message\n",
      "   Typical Use: Process user input, generate AI response, stream output\n",
      "\n",
      "ğŸ”§ **cl.Message**\n",
      "   Purpose: Message object containing user input\n",
      "   Properties: content, author, timestamp, elements\n",
      "   Typical Use: Access user's message content and metadata\n",
      "\n",
      "ğŸ”§ **cl.make_async**\n",
      "   Purpose: Convert sync functions to async\n",
      "   When Needed: When using sync OpenAI client in async context\n",
      "   Typical Use: Wrap synchronous API calls for Chainlit compatibility\n",
      "\n",
      "ğŸ”§ **Streaming Response**\n",
      "   Purpose: Real-time message updates\n",
      "   Implementation: Create empty message, update content progressively\n",
      "   Typical Use: Better UX for long responses, real-time feedback\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ¯ **Key Chainlit Concepts**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "chainlit_concepts = {\n",
    "    \"@cl.on_chat_start\": {\n",
    "        \"purpose\": \"Decorator for session initialization\",\n",
    "        \"when_called\": \"When a new chat session begins\",\n",
    "        \"typical_use\": \"Setup client, show welcome message, initialize context\"\n",
    "    },\n",
    "    \"@cl.on_message\": {\n",
    "        \"purpose\": \"Decorator for message handling\",\n",
    "        \"when_called\": \"When user sends a message\",\n",
    "        \"typical_use\": \"Process user input, generate AI response, stream output\"\n",
    "    },\n",
    "    \"cl.Message\": {\n",
    "        \"purpose\": \"Message object containing user input\",\n",
    "        \"properties\": \"content, author, timestamp, elements\",\n",
    "        \"typical_use\": \"Access user's message content and metadata\"\n",
    "    },\n",
    "    \"cl.make_async\": {\n",
    "        \"purpose\": \"Convert sync functions to async\",\n",
    "        \"when_needed\": \"When using sync OpenAI client in async context\",\n",
    "        \"typical_use\": \"Wrap synchronous API calls for Chainlit compatibility\"\n",
    "    },\n",
    "    \"Streaming Response\": {\n",
    "        \"purpose\": \"Real-time message updates\",\n",
    "        \"implementation\": \"Create empty message, update content progressively\",\n",
    "        \"typical_use\": \"Better UX for long responses, real-time feedback\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for concept, details in chainlit_concepts.items():\n",
    "    print(f\"\\nğŸ”§ **{concept}**\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"   {key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36bd257",
   "metadata": {},
   "source": [
    "## Î¥Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î¡Î¿Î®Ï‚ Î‘Ï€Î±Î½Ï„Î®ÏƒÎµÏ‰Î½\n",
    "\n",
    "Î”ÎµÎ¯Ï„Îµ Ï€ÏÏ‚ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¿ÏÎ½ Î¿Î¹ ÏÎ¿Î­Ï‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÏ‰Î½ ÏƒÏ„Î¿ Chainlit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f70bd7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒŠ **Streaming Response Pattern**\n",
      "==================================================\n",
      "ğŸ“ **Streaming Implementation:**\n",
      "\n",
      "# 1. Create an empty message to update progressively\n",
      "msg = cl.Message(content=\"\")\n",
      "await msg.send()\n",
      "\n",
      "# 2. Make streaming API call\n",
      "stream = await client.chat.completions.create(\n",
      "    model=model_name,\n",
      "    messages=messages,\n",
      "    stream=True,\n",
      "    max_tokens=1000\n",
      ")\n",
      "\n",
      "# 3. Process each chunk and update the message\n",
      "async for chunk in stream:\n",
      "    if chunk.choices[0].delta.content is not None:\n",
      "        await msg.stream_token(chunk.choices[0].delta.content)\n",
      "\n",
      "# 4. Finalize the message\n",
      "await msg.update()\n",
      "\n",
      "\n",
      "âœ¨ **Benefits of Streaming:**\n",
      "   ğŸš€ **Real-time feedback**: Users see responses as they're generated\n",
      "   âš¡ **Better perceived performance**: Feels faster than waiting for complete response\n",
      "   ğŸ¯ **User engagement**: Keeps users engaged during long responses\n",
      "   ğŸ›‘ **Early termination**: Users can interrupt if response goes off-track\n",
      "   ğŸ’¡ **Professional UX**: Modern chat interface experience\n",
      "\n",
      "ğŸ”§ **Implementation Notes:**\n",
      "   â€¢ Always use AsyncOpenAI for Chainlit applications\n",
      "   â€¢ Handle streaming errors gracefully with try-catch blocks\n",
      "   â€¢ Check for None content in delta chunks\n",
      "   â€¢ Update message when streaming is complete\n",
      "   â€¢ Consider rate limiting for production deployments\n",
      "\n",
      "==================================================\n",
      "ğŸ“ **Streaming Implementation:**\n",
      "\n",
      "# 1. Create an empty message to update progressively\n",
      "msg = cl.Message(content=\"\")\n",
      "await msg.send()\n",
      "\n",
      "# 2. Make streaming API call\n",
      "stream = await client.chat.completions.create(\n",
      "    model=model_name,\n",
      "    messages=messages,\n",
      "    stream=True,\n",
      "    max_tokens=1000\n",
      ")\n",
      "\n",
      "# 3. Process each chunk and update the message\n",
      "async for chunk in stream:\n",
      "    if chunk.choices[0].delta.content is not None:\n",
      "        await msg.stream_token(chunk.choices[0].delta.content)\n",
      "\n",
      "# 4. Finalize the message\n",
      "await msg.update()\n",
      "\n",
      "\n",
      "âœ¨ **Benefits of Streaming:**\n",
      "   ğŸš€ **Real-time feedback**: Users see responses as they're generated\n",
      "   âš¡ **Better perceived performance**: Feels faster than waiting for complete response\n",
      "   ğŸ¯ **User engagement**: Keeps users engaged during long responses\n",
      "   ğŸ›‘ **Early termination**: Users can interrupt if response goes off-track\n",
      "   ğŸ’¡ **Professional UX**: Modern chat interface experience\n",
      "\n",
      "ğŸ”§ **Implementation Notes:**\n",
      "   â€¢ Always use AsyncOpenAI for Chainlit applications\n",
      "   â€¢ Handle streaming errors gracefully with try-catch blocks\n",
      "   â€¢ Check for None content in delta chunks\n",
      "   â€¢ Update message when streaming is complete\n",
      "   â€¢ Consider rate limiting for production deployments\n"
     ]
    }
   ],
   "source": [
    "async def demonstrate_streaming_pattern():\n",
    "    \"\"\"Demonstrate the streaming response pattern used in Chainlit.\"\"\"\n",
    "    print(\"ğŸŒŠ **Streaming Response Pattern**\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # This is how streaming works in the actual Chainlit app\n",
    "    streaming_code = '''\n",
    "# 1. Create an empty message to update progressively\n",
    "msg = cl.Message(content=\"\")\n",
    "await msg.send()\n",
    "\n",
    "# 2. Make streaming API call\n",
    "stream = await client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "# 3. Process each chunk and update the message\n",
    "async for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        await msg.stream_token(chunk.choices[0].delta.content)\n",
    "\n",
    "# 4. Finalize the message\n",
    "await msg.update()\n",
    "'''\n",
    "    \n",
    "    print(\"ğŸ“ **Streaming Implementation:**\")\n",
    "    print(streaming_code)\n",
    "    \n",
    "    print(\"\\nâœ¨ **Benefits of Streaming:**\")\n",
    "    benefits = [\n",
    "        \"ğŸš€ **Real-time feedback**: Users see responses as they're generated\",\n",
    "        \"âš¡ **Better perceived performance**: Feels faster than waiting for complete response\",\n",
    "        \"ğŸ¯ **User engagement**: Keeps users engaged during long responses\",\n",
    "        \"ğŸ›‘ **Early termination**: Users can interrupt if response goes off-track\",\n",
    "        \"ğŸ’¡ **Professional UX**: Modern chat interface experience\"\n",
    "    ]\n",
    "    \n",
    "    for benefit in benefits:\n",
    "        print(f\"   {benefit}\")\n",
    "    \n",
    "    print(\"\\nğŸ”§ **Implementation Notes:**\")\n",
    "    notes = [\n",
    "        \"Always use AsyncOpenAI for Chainlit applications\",\n",
    "        \"Handle streaming errors gracefully with try-catch blocks\",\n",
    "        \"Check for None content in delta chunks\",\n",
    "        \"Update message when streaming is complete\",\n",
    "        \"Consider rate limiting for production deployments\"\n",
    "    ]\n",
    "    \n",
    "    for note in notes:\n",
    "        print(f\"   â€¢ {note}\")\n",
    "\n",
    "await demonstrate_streaming_pattern()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722fd61b",
   "metadata": {},
   "source": [
    "## Î ÏÏŒÏ„Ï…Ï€Î± Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ·Ï‚ Î£Ï†Î±Î»Î¼Î¬Ï„Ï‰Î½\n",
    "\n",
    "Î— Î¹ÏƒÏ‡Ï…ÏÎ® Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½ ÎµÎ¯Î½Î±Î¹ Î¶Ï‰Ï„Î¹ÎºÎ®Ï‚ ÏƒÎ·Î¼Î±ÏƒÎ¯Î±Ï‚ Î³Î¹Î± ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚ Chainlit ÏƒÎµ Ï€Î±ÏÎ±Î³Ï‰Î³Î®:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a723b71e",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›¡ï¸ **Error Handling Patterns**\n",
      "==================================================\n",
      "\n",
      "âš ï¸ **Client Initialization Failure**\n",
      "   ğŸ” Cause: Foundry Local service not running\n",
      "   ğŸ”§ Handling: Graceful fallback to manual configuration\n",
      "   ğŸ’¬ User Message: 'Service initializing, please wait...'\n",
      "\n",
      "âš ï¸ **Model Not Available**\n",
      "   ğŸ” Cause: Requested model not loaded\n",
      "   ğŸ”§ Handling: Try alternative models or suggest model loading\n",
      "   ğŸ’¬ User Message: 'Model unavailable, trying alternative...'\n",
      "\n",
      "âš ï¸ **Network Connection Error**\n",
      "   ğŸ” Cause: Network issues or service down\n",
      "   ğŸ”§ Handling: Retry with exponential backoff\n",
      "   ğŸ’¬ User Message: 'Connection issue, retrying...'\n",
      "\n",
      "âš ï¸ **Streaming Interruption**\n",
      "   ğŸ” Cause: Stream ends unexpectedly\n",
      "   ğŸ”§ Handling: Complete partial response gracefully\n",
      "   ğŸ’¬ User Message: 'Response completed (partial)'\n",
      "\n",
      "âš ï¸ **Rate Limiting**\n",
      "   ğŸ” Cause: Too many requests\n",
      "   ğŸ”§ Handling: Queue requests or ask user to wait\n",
      "   ğŸ’¬ User Message: 'High traffic, please wait a moment...'\n",
      "\n",
      "ğŸ“‹ **Error Handling Best Practices:**\n",
      "   ğŸ¯ **User-Friendly Messages**: Never show technical errors to users\n",
      "   ğŸ”„ **Automatic Retry**: Implement retry logic for transient failures\n",
      "   ğŸ“Š **Logging**: Log errors for debugging while keeping user experience smooth\n",
      "   ğŸ› ï¸ **Graceful Degradation**: Provide limited functionality when services are down\n",
      "   ğŸ’¡ **Helpful Suggestions**: Guide users on how to resolve issues\n",
      "   âš¡ **Fast Failure**: Fail quickly rather than letting users wait indefinitely\n",
      "\n",
      "ğŸ’» **Example Error Handling Code:**\n",
      "\n",
      "try:\n",
      "    stream = await client.chat.completions.create(\n",
      "        model=model_name,\n",
      "        messages=messages,\n",
      "        stream=True,\n",
      "        max_tokens=1000\n",
      "    )\n",
      "    \n",
      "    async for chunk in stream:\n",
      "        if chunk.choices[0].delta.content is not None:\n",
      "            await msg.stream_token(chunk.choices[0].delta.content)\n",
      "            \n",
      "except Exception as e:\n",
      "    error_msg = \"I encountered an issue. Please try again.\"\n",
      "    await cl.Message(content=error_msg, author=\"System\").send()\n",
      "    # Log the actual error for debugging\n",
      "    print(f\"Error: {e}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def demonstrate_error_handling():\n",
    "    \"\"\"Show error handling patterns for Chainlit applications.\"\"\"\n",
    "    print(\"ğŸ›¡ï¸ **Error Handling Patterns**\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    error_scenarios = {\n",
    "        \"Client Initialization Failure\": {\n",
    "            \"cause\": \"Foundry Local service not running\",\n",
    "            \"handling\": \"Graceful fallback to manual configuration\",\n",
    "            \"user_message\": \"Service initializing, please wait...\"\n",
    "        },\n",
    "        \"Model Not Available\": {\n",
    "            \"cause\": \"Requested model not loaded\",\n",
    "            \"handling\": \"Try alternative models or suggest model loading\",\n",
    "            \"user_message\": \"Model unavailable, trying alternative...\"\n",
    "        },\n",
    "        \"Network Connection Error\": {\n",
    "            \"cause\": \"Network issues or service down\",\n",
    "            \"handling\": \"Retry with exponential backoff\",\n",
    "            \"user_message\": \"Connection issue, retrying...\"\n",
    "        },\n",
    "        \"Streaming Interruption\": {\n",
    "            \"cause\": \"Stream ends unexpectedly\",\n",
    "            \"handling\": \"Complete partial response gracefully\",\n",
    "            \"user_message\": \"Response completed (partial)\"\n",
    "        },\n",
    "        \"Rate Limiting\": {\n",
    "            \"cause\": \"Too many requests\",\n",
    "            \"handling\": \"Queue requests or ask user to wait\",\n",
    "            \"user_message\": \"High traffic, please wait a moment...\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for scenario, details in error_scenarios.items():\n",
    "        print(f\"\\nâš ï¸ **{scenario}**\")\n",
    "        print(f\"   ğŸ” Cause: {details['cause']}\")\n",
    "        print(f\"   ğŸ”§ Handling: {details['handling']}\")\n",
    "        print(f\"   ğŸ’¬ User Message: '{details['user_message']}'\")\n",
    "    \n",
    "    print(\"\\nğŸ“‹ **Error Handling Best Practices:**\")\n",
    "    best_practices = [\n",
    "        \"ğŸ¯ **User-Friendly Messages**: Never show technical errors to users\",\n",
    "        \"ğŸ”„ **Automatic Retry**: Implement retry logic for transient failures\",\n",
    "        \"ğŸ“Š **Logging**: Log errors for debugging while keeping user experience smooth\",\n",
    "        \"ğŸ› ï¸ **Graceful Degradation**: Provide limited functionality when services are down\",\n",
    "        \"ğŸ’¡ **Helpful Suggestions**: Guide users on how to resolve issues\",\n",
    "        \"âš¡ **Fast Failure**: Fail quickly rather than letting users wait indefinitely\"\n",
    "    ]\n",
    "    \n",
    "    for practice in best_practices:\n",
    "        print(f\"   {practice}\")\n",
    "    \n",
    "    # Show example error handling code\n",
    "    print(\"\\nğŸ’» **Example Error Handling Code:**\")\n",
    "    error_code = '''\n",
    "try:\n",
    "    stream = await client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    async for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            await msg.stream_token(chunk.choices[0].delta.content)\n",
    "            \n",
    "except Exception as e:\n",
    "    error_msg = \"I encountered an issue. Please try again.\"\n",
    "    await cl.Message(content=error_msg, author=\"System\").send()\n",
    "    # Log the actual error for debugging\n",
    "    print(f\"Error: {e}\")\n",
    "'''\n",
    "    print(error_code)\n",
    "\n",
    "await demonstrate_error_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6269dfef",
   "metadata": {},
   "source": [
    "## Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Ï„Î·Ï‚ Î•Ï†Î±ÏÎ¼Î¿Î³Î®Ï‚ Chainlit\n",
    "\n",
    "Î‘ÎºÎ¿Î»Î¿Ï…Î¸Î¿ÏÎ½ Î¿Î¹ Î¿Î´Î·Î³Î¯ÎµÏ‚ Î³Î¹Î± Ï„Î·Î½ ÎµÎºÏ„Î­Î»ÎµÏƒÎ· ÎºÎ±Î¹ Ï„Î·Î½ Î±Î½Î¬Ï€Ï„Ï…Î¾Î· Ï„Î·Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î®Ï‚ Chainlit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60fa850b",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ **Running Chainlit Application**\n",
      "==================================================\n",
      "\n",
      "ğŸ”§ **Development Mode**\n",
      "   Command: chainlit run app.py --watch\n",
      "   Description: Auto-reload on file changes\n",
      "   Port: 8000 (default)\n",
      "   Use_Case: Local development and testing\n",
      "\n",
      "ğŸ”§ **Production Mode**\n",
      "   Command: chainlit run app.py --host 0.0.0.0 --port 8080\n",
      "   Description: Production deployment\n",
      "   Port: 8080 (configurable)\n",
      "   Use_Case: Server deployment\n",
      "\n",
      "ğŸ”§ **Custom Configuration**\n",
      "   Command: chainlit run app.py -h 0.0.0.0 -p 3000 --no-cache\n",
      "   Description: Custom host, port, and caching options\n",
      "   Port: 3000 (custom)\n",
      "   Use_Case: Specific deployment requirements\n",
      "\n",
      "ğŸŒ **Access Points:**\n",
      "   ğŸ“± **Local Development**: http://localhost:8000\n",
      "   ğŸŒ **Network Access**: http://YOUR_IP:8000\n",
      "   ğŸ”— **Production**: https://your-domain.com\n",
      "   ğŸ“Š **Health Check**: Add /health endpoint for monitoring\n",
      "\n",
      "âš™ï¸ **Environment Variables for Production:**\n",
      "   BASE_URL=http://localhost:8000 (Foundry Local endpoint)\n",
      "   API_KEY=your-api-key (if required)\n",
      "   MODEL_NAME=phi-4-mini (default model)\n",
      "   MAX_TOKENS=1000 (response length limit)\n",
      "   CHAINLIT_HOST=0.0.0.0 (production host)\n",
      "   CHAINLIT_PORT=8080 (production port)\n",
      "\n",
      "==================================================\n",
      "\n",
      "ğŸ”§ **Development Mode**\n",
      "   Command: chainlit run app.py --watch\n",
      "   Description: Auto-reload on file changes\n",
      "   Port: 8000 (default)\n",
      "   Use_Case: Local development and testing\n",
      "\n",
      "ğŸ”§ **Production Mode**\n",
      "   Command: chainlit run app.py --host 0.0.0.0 --port 8080\n",
      "   Description: Production deployment\n",
      "   Port: 8080 (configurable)\n",
      "   Use_Case: Server deployment\n",
      "\n",
      "ğŸ”§ **Custom Configuration**\n",
      "   Command: chainlit run app.py -h 0.0.0.0 -p 3000 --no-cache\n",
      "   Description: Custom host, port, and caching options\n",
      "   Port: 3000 (custom)\n",
      "   Use_Case: Specific deployment requirements\n",
      "\n",
      "ğŸŒ **Access Points:**\n",
      "   ğŸ“± **Local Development**: http://localhost:8000\n",
      "   ğŸŒ **Network Access**: http://YOUR_IP:8000\n",
      "   ğŸ”— **Production**: https://your-domain.com\n",
      "   ğŸ“Š **Health Check**: Add /health endpoint for monitoring\n",
      "\n",
      "âš™ï¸ **Environment Variables for Production:**\n",
      "   BASE_URL=http://localhost:8000 (Foundry Local endpoint)\n",
      "   API_KEY=your-api-key (if required)\n",
      "   MODEL_NAME=phi-4-mini (default model)\n",
      "   MAX_TOKENS=1000 (response length limit)\n",
      "   CHAINLIT_HOST=0.0.0.0 (production host)\n",
      "   CHAINLIT_PORT=8080 (production port)\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ **Running Chainlit Application**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "running_instructions = {\n",
    "    \"Development Mode\": {\n",
    "        \"command\": \"chainlit run app.py --watch\",\n",
    "        \"description\": \"Auto-reload on file changes\",\n",
    "        \"port\": \"8000 (default)\",\n",
    "        \"use_case\": \"Local development and testing\"\n",
    "    },\n",
    "    \"Production Mode\": {\n",
    "        \"command\": \"chainlit run app.py --host 0.0.0.0 --port 8080\",\n",
    "        \"description\": \"Production deployment\",\n",
    "        \"port\": \"8080 (configurable)\",\n",
    "        \"use_case\": \"Server deployment\"\n",
    "    },\n",
    "    \"Custom Configuration\": {\n",
    "        \"command\": \"chainlit run app.py -h 0.0.0.0 -p 3000 --no-cache\",\n",
    "        \"description\": \"Custom host, port, and caching options\",\n",
    "        \"port\": \"3000 (custom)\",\n",
    "        \"use_case\": \"Specific deployment requirements\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for mode, config in running_instructions.items():\n",
    "    print(f\"\\nğŸ”§ **{mode}**\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"   {key.title()}: {value}\")\n",
    "\n",
    "print(\"\\nğŸŒ **Access Points:**\")\n",
    "access_info = [\n",
    "    \"ğŸ“± **Local Development**: http://localhost:8000\",\n",
    "    \"ğŸŒ **Network Access**: http://YOUR_IP:8000\",\n",
    "    \"ğŸ”— **Production**: https://your-domain.com\",\n",
    "    \"ğŸ“Š **Health Check**: Add /health endpoint for monitoring\"\n",
    "]\n",
    "\n",
    "for info in access_info:\n",
    "    print(f\"   {info}\")\n",
    "\n",
    "print(\"\\nâš™ï¸ **Environment Variables for Production:**\")\n",
    "env_vars = {\n",
    "    \"BASE_URL\": \"http://localhost:8000 (Foundry Local endpoint)\",\n",
    "    \"API_KEY\": \"your-api-key (if required)\",\n",
    "    \"MODEL_NAME\": \"phi-4-mini (default model)\",\n",
    "    \"MAX_TOKENS\": \"1000 (response length limit)\",\n",
    "    \"CHAINLIT_HOST\": \"0.0.0.0 (production host)\",\n",
    "    \"CHAINLIT_PORT\": \"8080 (production port)\"\n",
    "}\n",
    "\n",
    "for var, desc in env_vars.items():\n",
    "    print(f\"   {var}={desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928e0dc1",
   "metadata": {},
   "source": [
    "## Î•Ï€Î¹Î»Î¿Î³Î­Ï‚ Î ÏÎ¿ÏƒÎ±ÏÎ¼Î¿Î³Î®Ï‚\n",
    "\n",
    "Î¤Î¿ Chainlit Ï€ÏÎ¿ÏƒÏ†Î­ÏÎµÎ¹ ÎµÎºÏ„ÎµÏ„Î±Î¼Î­Î½ÎµÏ‚ ÎµÏ€Î¹Î»Î¿Î³Î­Ï‚ Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿Î³Î®Ï‚:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e0ecc0c",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¨ **Chainlit Customization Options**\n",
      "==================================================\n",
      "\n",
      "ğŸ”§ **UI Customization**\n",
      "   ğŸ“„ Config: .chainlit/config.toml\n",
      "   âš™ï¸ Options:\n",
      "      â€¢ Custom theme colors and fonts\n",
      "      â€¢ Company logo and branding\n",
      "      â€¢ Custom CSS styling\n",
      "      â€¢ Dark/light mode preferences\n",
      "\n",
      "ğŸ”§ **Chat Features**\n",
      "   ğŸ“„ Config: app.py (programmatic)\n",
      "   âš™ï¸ Options:\n",
      "      â€¢ File upload support\n",
      "      â€¢ Image and media handling\n",
      "      â€¢ Custom message elements\n",
      "      â€¢ Action buttons and quick replies\n",
      "\n",
      "ğŸ”§ **Authentication**\n",
      "   ğŸ“„ Config: auth.py + config.toml\n",
      "   âš™ï¸ Options:\n",
      "      â€¢ OAuth integration (Google, GitHub)\n",
      "      â€¢ LDAP/Active Directory\n",
      "      â€¢ Custom authentication providers\n",
      "      â€¢ Role-based access control\n",
      "\n",
      "ğŸ”§ **Deployment**\n",
      "   ğŸ“„ Config: docker-compose.yml / Dockerfile\n",
      "   âš™ï¸ Options:\n",
      "      â€¢ Docker containerization\n",
      "      â€¢ Kubernetes deployment\n",
      "      â€¢ Cloud platform integration\n",
      "      â€¢ Reverse proxy configuration\n",
      "\n",
      "ğŸ“ **Sample .chainlit/config.toml:**\n",
      "\n",
      "[project]\n",
      "name = \"Foundry Local Chat\"\n",
      "author = \"Your Organization\"\n",
      "description = \"AI Chat powered by Foundry Local\"\n",
      "\n",
      "[UI]\n",
      "name = \"Foundry AI Assistant\"\n",
      "show_readme_as_default = true\n",
      "show_cloud_icon = false\n",
      "\n",
      "[theme]\n",
      "primary_color = \"#0078d4\"\n",
      "background_color = \"#ffffff\"\n",
      "text_color = \"#323130\"\n",
      "\n",
      "[features]\n",
      "allow_unsafe_html = false\n",
      "max_message_size = 4096\n",
      "max_file_size_mb = 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ¨ **Chainlit Customization Options**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "customization_areas = {\n",
    "    \"UI Customization\": {\n",
    "        \"config_file\": \".chainlit/config.toml\",\n",
    "        \"options\": [\n",
    "            \"Custom theme colors and fonts\",\n",
    "            \"Company logo and branding\",\n",
    "            \"Custom CSS styling\",\n",
    "            \"Dark/light mode preferences\"\n",
    "        ]\n",
    "    },\n",
    "    \"Chat Features\": {\n",
    "        \"config_file\": \"app.py (programmatic)\",\n",
    "        \"options\": [\n",
    "            \"File upload support\",\n",
    "            \"Image and media handling\",\n",
    "            \"Custom message elements\",\n",
    "            \"Action buttons and quick replies\"\n",
    "        ]\n",
    "    },\n",
    "    \"Authentication\": {\n",
    "        \"config_file\": \"auth.py + config.toml\",\n",
    "        \"options\": [\n",
    "            \"OAuth integration (Google, GitHub)\",\n",
    "            \"LDAP/Active Directory\",\n",
    "            \"Custom authentication providers\",\n",
    "            \"Role-based access control\"\n",
    "        ]\n",
    "    },\n",
    "    \"Deployment\": {\n",
    "        \"config_file\": \"docker-compose.yml / Dockerfile\",\n",
    "        \"options\": [\n",
    "            \"Docker containerization\",\n",
    "            \"Kubernetes deployment\",\n",
    "            \"Cloud platform integration\",\n",
    "            \"Reverse proxy configuration\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for area, details in customization_areas.items():\n",
    "    print(f\"\\nğŸ”§ **{area}**\")\n",
    "    print(f\"   ğŸ“„ Config: {details['config_file']}\")\n",
    "    print(f\"   âš™ï¸ Options:\")\n",
    "    for option in details['options']:\n",
    "        print(f\"      â€¢ {option}\")\n",
    "\n",
    "# Show sample configuration\n",
    "print(\"\\nğŸ“ **Sample .chainlit/config.toml:**\")\n",
    "sample_config = '''\n",
    "[project]\n",
    "name = \"Foundry Local Chat\"\n",
    "author = \"Your Organization\"\n",
    "description = \"AI Chat powered by Foundry Local\"\n",
    "\n",
    "[UI]\n",
    "name = \"Foundry AI Assistant\"\n",
    "show_readme_as_default = true\n",
    "show_cloud_icon = false\n",
    "\n",
    "[theme]\n",
    "primary_color = \"#0078d4\"\n",
    "background_color = \"#ffffff\"\n",
    "text_color = \"#323130\"\n",
    "\n",
    "[features]\n",
    "allow_unsafe_html = false\n",
    "max_message_size = 4096\n",
    "max_file_size_mb = 10\n",
    "'''\n",
    "print(sample_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33562e8",
   "metadata": {},
   "source": [
    "## Î ÏÎ¿Î·Î³Î¼Î­Î½ÎµÏ‚ Î”Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚\n",
    "\n",
    "Î•Î¾ÎµÏÎµÏ…Î½Î®ÏƒÏ„Îµ Ï€ÏÎ¿Î·Î³Î¼Î­Î½ÎµÏ‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ Ï„Î¿Ï… Chainlit Î³Î¹Î± ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚ Ï€Î±ÏÎ±Î³Ï‰Î³Î®Ï‚:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86670e5d",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ **Advanced Chainlit Features**\n",
      "==================================================\n",
      "\n",
      "ğŸ¯ **Session Management**\n",
      "   ğŸ“ Description: Maintain conversation context across messages\n",
      "   ğŸ”§ Implementation: cl.user_session for storing state\n",
      "   ğŸ’¡ Use Cases:\n",
      "      â€¢ Multi-turn conversations\n",
      "      â€¢ User preferences\n",
      "      â€¢ Context persistence\n",
      "\n",
      "ğŸ¯ **File Uploads**\n",
      "   ğŸ“ Description: Handle document uploads and processing\n",
      "   ğŸ”§ Implementation: @cl.on_file_upload decorator\n",
      "   ğŸ’¡ Use Cases:\n",
      "      â€¢ Document analysis\n",
      "      â€¢ Image processing\n",
      "      â€¢ Data ingestion\n",
      "\n",
      "ğŸ¯ **Action Buttons**\n",
      "   ğŸ“ Description: Interactive buttons for user actions\n",
      "   ğŸ”§ Implementation: cl.Action elements\n",
      "   ğŸ’¡ Use Cases:\n",
      "      â€¢ Quick replies\n",
      "      â€¢ Workflow triggers\n",
      "      â€¢ Menu navigation\n",
      "\n",
      "ğŸ¯ **Data Persistence**\n",
      "   ğŸ“ Description: Store conversation history and user data\n",
      "   ğŸ”§ Implementation: Database integration\n",
      "   ğŸ’¡ Use Cases:\n",
      "      â€¢ Chat history\n",
      "      â€¢ User analytics\n",
      "      â€¢ Feedback collection\n",
      "\n",
      "ğŸ¯ **Multi-modal Support**\n",
      "   ğŸ“ Description: Handle text, images, and other media\n",
      "   ğŸ”§ Implementation: cl.Image, cl.File elements\n",
      "   ğŸ’¡ Use Cases:\n",
      "      â€¢ Visual Q&A\n",
      "      â€¢ Document chat\n",
      "      â€¢ Media analysis\n",
      "\n",
      "ğŸ’» **Session Management Example:**\n",
      "\n",
      "@cl.on_chat_start\n",
      "async def on_chat_start():\n",
      "    # Initialize session state\n",
      "    cl.user_session.set(\"conversation_history\", [])\n",
      "    cl.user_session.set(\"user_preferences\", {\"temperature\": 0.7})\n",
      "\n",
      "@cl.on_message\n",
      "async def on_message(message: cl.Message):\n",
      "    # Get session state\n",
      "    history = cl.user_session.get(\"conversation_history\", [])\n",
      "    preferences = cl.user_session.get(\"user_preferences\", {})\n",
      "    \n",
      "    # Add current message to history\n",
      "    history.append({\"role\": \"user\", \"content\": message.content})\n",
      "    \n",
      "    # Use full conversation context\n",
      "    response = await client.chat.completions.create(\n",
      "        model=\"phi-4-mini\",\n",
      "        messages=history,\n",
      "        temperature=preferences.get(\"temperature\", 0.7)\n",
      "    )\n",
      "    \n",
      "    # Update session with AI response\n",
      "    history.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
      "    cl.user_session.set(\"conversation_history\", history)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ **Advanced Chainlit Features**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "advanced_features = {\n",
    "    \"Session Management\": {\n",
    "        \"description\": \"Maintain conversation context across messages\",\n",
    "        \"implementation\": \"cl.user_session for storing state\",\n",
    "        \"use_cases\": [\"Multi-turn conversations\", \"User preferences\", \"Context persistence\"]\n",
    "    },\n",
    "    \"File Uploads\": {\n",
    "        \"description\": \"Handle document uploads and processing\",\n",
    "        \"implementation\": \"@cl.on_file_upload decorator\",\n",
    "        \"use_cases\": [\"Document analysis\", \"Image processing\", \"Data ingestion\"]\n",
    "    },\n",
    "    \"Action Buttons\": {\n",
    "        \"description\": \"Interactive buttons for user actions\",\n",
    "        \"implementation\": \"cl.Action elements\",\n",
    "        \"use_cases\": [\"Quick replies\", \"Workflow triggers\", \"Menu navigation\"]\n",
    "    },\n",
    "    \"Data Persistence\": {\n",
    "        \"description\": \"Store conversation history and user data\",\n",
    "        \"implementation\": \"Database integration\",\n",
    "        \"use_cases\": [\"Chat history\", \"User analytics\", \"Feedback collection\"]\n",
    "    },\n",
    "    \"Multi-modal Support\": {\n",
    "        \"description\": \"Handle text, images, and other media\",\n",
    "        \"implementation\": \"cl.Image, cl.File elements\",\n",
    "        \"use_cases\": [\"Visual Q&A\", \"Document chat\", \"Media analysis\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "for feature, details in advanced_features.items():\n",
    "    print(f\"\\nğŸ¯ **{feature}**\")\n",
    "    print(f\"   ğŸ“ Description: {details['description']}\")\n",
    "    print(f\"   ğŸ”§ Implementation: {details['implementation']}\")\n",
    "    print(f\"   ğŸ’¡ Use Cases:\")\n",
    "    for use_case in details['use_cases']:\n",
    "        print(f\"      â€¢ {use_case}\")\n",
    "\n",
    "# Show example code for session management\n",
    "print(\"\\nğŸ’» **Session Management Example:**\")\n",
    "session_code = '''\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    # Initialize session state\n",
    "    cl.user_session.set(\"conversation_history\", [])\n",
    "    cl.user_session.set(\"user_preferences\", {\"temperature\": 0.7})\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    # Get session state\n",
    "    history = cl.user_session.get(\"conversation_history\", [])\n",
    "    preferences = cl.user_session.get(\"user_preferences\", {})\n",
    "    \n",
    "    # Add current message to history\n",
    "    history.append({\"role\": \"user\", \"content\": message.content})\n",
    "    \n",
    "    # Use full conversation context\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"phi-4-mini\",\n",
    "        messages=history,\n",
    "        temperature=preferences.get(\"temperature\", 0.7)\n",
    "    )\n",
    "    \n",
    "    # Update session with AI response\n",
    "    history.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "    cl.user_session.set(\"conversation_history\", history)\n",
    "'''\n",
    "print(session_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be192a",
   "metadata": {},
   "source": [
    "## Î›Î¯ÏƒÏ„Î± Î•Î»Î­Î³Ï‡Î¿Ï… Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚ ÏƒÎµ Î Î±ÏÎ±Î³Ï‰Î³Î®\n",
    "\n",
    "Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ Ï€Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹ Î³Î¹Î± Ï„Î·Î½ Î±Î½Î¬Ï€Ï„Ï…Î¾Î· ÎµÏ†Î±ÏÎ¼Î¿Î³ÏÎ½ Chainlit ÏƒÎµ Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½ Ï€Î±ÏÎ±Î³Ï‰Î³Î®Ï‚:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b18750c",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… **Production Deployment Checklist**\n",
      "==================================================\n",
      "\n",
      "ğŸ”’ Security\n",
      "   â˜ Enable authentication and authorization\n",
      "   â˜ Use HTTPS with proper SSL certificates\n",
      "   â˜ Implement rate limiting and request validation\n",
      "   â˜ Sanitize user inputs and prevent injection attacks\n",
      "   â˜ Set up proper CORS policies\n",
      "   â˜ Use environment variables for sensitive configuration\n",
      "\n",
      "âš¡ Performance\n",
      "   â˜ Configure connection pooling for database\n",
      "   â˜ Implement caching for frequent responses\n",
      "   â˜ Set up load balancing for multiple instances\n",
      "   â˜ Monitor memory usage and optimize where needed\n",
      "   â˜ Configure appropriate timeout values\n",
      "   â˜ Use CDN for static assets\n",
      "\n",
      "ğŸ“Š Monitoring\n",
      "   â˜ Set up application logging and monitoring\n",
      "   â˜ Configure health checks and uptime monitoring\n",
      "   â˜ Track user engagement and conversation metrics\n",
      "   â˜ Monitor API response times and error rates\n",
      "   â˜ Set up alerting for critical issues\n",
      "   â˜ Implement user feedback collection\n",
      "\n",
      "ğŸ› ï¸ Maintenance\n",
      "   â˜ Regular backups of conversation data\n",
      "   â˜ Automated deployment pipelines\n",
      "   â˜ Version control for configuration changes\n",
      "   â˜ Documentation for troubleshooting\n",
      "   â˜ Capacity planning and scaling procedures\n",
      "   â˜ Update procedures for dependencies\n",
      "\n",
      "ğŸŒ Infrastructure\n",
      "   â˜ Container orchestration (Docker/Kubernetes)\n",
      "   â˜ Reverse proxy configuration (nginx/Apache)\n",
      "   â˜ Database setup and optimization\n",
      "   â˜ Network security and firewall rules\n",
      "   â˜ Backup and disaster recovery plans\n",
      "   â˜ Multi-region deployment for redundancy\n",
      "\n",
      "ğŸš€ **Quick Production Setup Commands:**\n",
      "\n",
      "ğŸ’¡ # Build Docker image\n",
      "   docker build -t chainlit-app .\n",
      "\n",
      "\n",
      "ğŸ’¡ # Run with production settings\n",
      "   docker run -d -p 8080:8080 -e NODE_ENV=production chainlit-app\n",
      "\n",
      "\n",
      "ğŸ’¡ # Health check\n",
      "   curl http://localhost:8080/health\n",
      "\n",
      "\n",
      "ğŸ’¡ # Monitor logs\n",
      "   docker logs -f chainlit-app\n"
     ]
    }
   ],
   "source": [
    "print(\"âœ… **Production Deployment Checklist**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "deployment_checklist = {\n",
    "    \"ğŸ”’ Security\": [\n",
    "        \"Enable authentication and authorization\",\n",
    "        \"Use HTTPS with proper SSL certificates\",\n",
    "        \"Implement rate limiting and request validation\",\n",
    "        \"Sanitize user inputs and prevent injection attacks\",\n",
    "        \"Set up proper CORS policies\",\n",
    "        \"Use environment variables for sensitive configuration\"\n",
    "    ],\n",
    "    \"âš¡ Performance\": [\n",
    "        \"Configure connection pooling for database\",\n",
    "        \"Implement caching for frequent responses\",\n",
    "        \"Set up load balancing for multiple instances\",\n",
    "        \"Monitor memory usage and optimize where needed\",\n",
    "        \"Configure appropriate timeout values\",\n",
    "        \"Use CDN for static assets\"\n",
    "    ],\n",
    "    \"ğŸ“Š Monitoring\": [\n",
    "        \"Set up application logging and monitoring\",\n",
    "        \"Configure health checks and uptime monitoring\",\n",
    "        \"Track user engagement and conversation metrics\",\n",
    "        \"Monitor API response times and error rates\",\n",
    "        \"Set up alerting for critical issues\",\n",
    "        \"Implement user feedback collection\"\n",
    "    ],\n",
    "    \"ğŸ› ï¸ Maintenance\": [\n",
    "        \"Regular backups of conversation data\",\n",
    "        \"Automated deployment pipelines\",\n",
    "        \"Version control for configuration changes\",\n",
    "        \"Documentation for troubleshooting\",\n",
    "        \"Capacity planning and scaling procedures\",\n",
    "        \"Update procedures for dependencies\"\n",
    "    ],\n",
    "    \"ğŸŒ Infrastructure\": [\n",
    "        \"Container orchestration (Docker/Kubernetes)\",\n",
    "        \"Reverse proxy configuration (nginx/Apache)\",\n",
    "        \"Database setup and optimization\",\n",
    "        \"Network security and firewall rules\",\n",
    "        \"Backup and disaster recovery plans\",\n",
    "        \"Multi-region deployment for redundancy\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in deployment_checklist.items():\n",
    "    print(f\"\\n{category}\")\n",
    "    for item in items:\n",
    "        print(f\"   â˜ {item}\")\n",
    "\n",
    "print(\"\\nğŸš€ **Quick Production Setup Commands:**\")\n",
    "commands = [\n",
    "    \"# Build Docker image\",\n",
    "    \"docker build -t chainlit-app .\",\n",
    "    \"\",\n",
    "    \"# Run with production settings\",\n",
    "    \"docker run -d -p 8080:8080 -e NODE_ENV=production chainlit-app\",\n",
    "    \"\",\n",
    "    \"# Health check\",\n",
    "    \"curl http://localhost:8080/health\",\n",
    "    \"\",\n",
    "    \"# Monitor logs\",\n",
    "    \"docker logs -f chainlit-app\"\n",
    "]\n",
    "\n",
    "for cmd in commands:\n",
    "    if cmd.startswith(\"#\"):\n",
    "        print(f\"\\nğŸ’¡ {cmd}\")\n",
    "    elif cmd:\n",
    "        print(f\"   {cmd}\")\n",
    "    else:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab2130",
   "metadata": {},
   "source": [
    "## Î ÎµÏÎ¯Î»Î·ÏˆÎ· ÎºÎ±Î¹ Î’Î­Î»Ï„Î¹ÏƒÏ„ÎµÏ‚ Î ÏÎ±ÎºÏ„Î¹ÎºÎ­Ï‚\n",
    "\n",
    "Î‘Ï…Ï„ÏŒ Ï„Î¿ ÏƒÎ·Î¼ÎµÎ¹Ï‰Î¼Î±Ï„Î¬ÏÎ¹Î¿ ÎºÎ¬Î»Ï…ÏˆÎµ Ï„Î· Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚ Î¼Î¹Î±Ï‚ Ï€Î»Î®ÏÎ¿Ï…Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î®Ï‚ Chainlit:\n",
    "\n",
    "### âœ… Î’Î±ÏƒÎ¹ÎºÎ¬ Î£Ï„Î¿Î¹Ï‡ÎµÎ¯Î± Ï€Î¿Ï… ÎšÎ±Î»ÏÏ†Î¸Î·ÎºÎ±Î½\n",
    "\n",
    "1. **ğŸ”§ Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Î ÎµÎ»Î¬Ï„Î·**: Î•Î½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· Ï„Î¿Ï… Foundry Local SDK Î¼Îµ ÎµÎ½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ® Î´Î¹Î±Î¼ÏŒÏÏ†Ï‰ÏƒÎ·\n",
    "2. **ğŸŒŠ Î¡Î¿Î­Ï‚ Î‘Ï€Î±Î½Ï„Î®ÏƒÎµÏ‰Î½**: Î•Î½Î·Î¼ÎµÏÏÏƒÎµÎ¹Ï‚ Î¼Î·Î½Ï…Î¼Î¬Ï„Ï‰Î½ ÏƒÎµ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏŒ Ï‡ÏÏŒÎ½Î¿ Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· ÎµÎ¼Ï€ÎµÎ¹ÏÎ¯Î± Ï‡ÏÎ®ÏƒÏ„Î·\n",
    "3. **ğŸ›¡ï¸ Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î£Ï†Î±Î»Î¼Î¬Ï„Ï‰Î½**: Î§ÎµÎ¹ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î±Ï€Î¿Ï„Ï…Ï‡Î¹ÏÎ½ Î¼Îµ Ï†Î¹Î»Î¹ÎºÏŒ Ï„ÏÏŒÏ€Î¿ ÎºÎ±Î¹ Î¼Î·Î½ÏÎ¼Î±Ï„Î± Ï€ÏÎ¿Ï‚ Ï„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î·\n",
    "4. **âš™ï¸ Î”Î¹Î±Î¼ÏŒÏÏ†Ï‰ÏƒÎ·**: Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ Î²Î¬ÏƒÎµÎ¹ Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î¿Ï‚ ÎºÎ±Î¹ ÎµÏ€Î¹Î»Î¿Î³Î­Ï‚ Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿Î³Î®Ï‚\n",
    "5. **ğŸš€ Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·**: Î ÏÏŒÏ„Ï…Ï€Î± ÎºÎ±Î¹ Î²Î­Î»Ï„Î¹ÏƒÏ„ÎµÏ‚ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ­Ï‚ Î³Î¹Î± Î­Ï„Î¿Î¹Î¼Î· Ï€Î±ÏÎ±Î³Ï‰Î³Î®\n",
    "\n",
    "### ğŸ¯ Î‘ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ® Î•Ï†Î±ÏÎ¼Î¿Î³Î®Ï‚ Chainlit\n",
    "\n",
    "```\n",
    "User Browser â†â†’ Chainlit UI â†â†’ Python Backend â†â†’ Foundry Local â†â†’ AI Model\n",
    "      â†“              â†“              â†“              â†“            â†“\n",
    "   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU\n",
    "```\n",
    "\n",
    "### ğŸ’¡ Î ÎµÏÎ¯Î»Î·ÏˆÎ· Î’Î­Î»Ï„Î¹ÏƒÏ„Ï‰Î½ Î ÏÎ±ÎºÏ„Î¹ÎºÏÎ½\n",
    "\n",
    "- **ğŸ”„ Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Îµ Î Î¬Î½Ï„Î± Async**: Î¤Î¿ Chainlit Î±Ï€Î±Î¹Ï„ÎµÎ¯ Î±ÏƒÏÎ³Ï‡ÏÎ¿Î½ÎµÏ‚ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯ÎµÏ‚ Î³Î¹Î± Î¼Î· Î¼Ï€Î»Î¿ÎºÎ±ÏÎ¹ÏƒÎ¼Î­Î½ÎµÏ‚ Î´Î¹ÎµÏÎ³Î±ÏƒÎ¯ÎµÏ‚\n",
    "- **ğŸŒŠ Î•Ï†Î±ÏÎ¼ÏŒÏƒÏ„Îµ Î¡Î¿Î­Ï‚**: Î Î±ÏÎ­Ï‡ÎµÎ¹ ÎºÎ±Î»ÏÏ„ÎµÏÎ· ÎµÎ¼Ï€ÎµÎ¹ÏÎ¯Î± Ï‡ÏÎ®ÏƒÏ„Î· Î±Ï€ÏŒ Ï„Î·Î½ Î±Î½Î±Î¼Î¿Î½Î® Î³Î¹Î± Ï€Î»Î®ÏÎµÎ¹Ï‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚\n",
    "- **ğŸ›¡ï¸ Î§ÎµÎ¹ÏÎ¹ÏƒÏ„ÎµÎ¯Ï„Îµ Î£Ï†Î¬Î»Î¼Î±Ï„Î± Î¼Îµ Î§Î¬ÏÎ·**: ÎœÎ·Î½ ÎµÎ¼Ï†Î±Î½Î¯Î¶ÎµÏ„Îµ Ï„ÎµÏ‡Î½Î¹ÎºÎ¬ ÏƒÏ†Î¬Î»Î¼Î±Ï„Î± ÏƒÏ„Î¿Ï…Ï‚ Ï„ÎµÎ»Î¹ÎºÎ¿ÏÏ‚ Ï‡ÏÎ®ÏƒÏ„ÎµÏ‚\n",
    "- **ğŸ“Š Î Î±ÏÎ±ÎºÎ¿Î»Î¿Ï…Î¸Î®ÏƒÏ„Îµ Ï„Î·Î½ Î‘Ï€ÏŒÎ´Î¿ÏƒÎ·**: Î•Î»Î­Î³Î¾Ï„Îµ Ï„Î¿Ï…Ï‚ Ï‡ÏÏŒÎ½Î¿Ï…Ï‚ Î±Ï€ÏŒÎºÏÎ¹ÏƒÎ·Ï‚ ÎºÎ±Î¹ Ï„Î± ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ Î±Î»Î»Î·Î»ÎµÏ€Î¯Î´ÏÎ±ÏƒÎ·Ï‚ Ï‡ÏÎ·ÏƒÏ„ÏÎ½\n",
    "- **ğŸ”’ Î‘ÏƒÏ†Î¬Î»ÎµÎ¹Î± Î±Ï€ÏŒ Î ÏÎ¿ÎµÏ€Î¹Î»Î¿Î³Î®**: Î•Ï†Î±ÏÎ¼ÏŒÏƒÏ„Îµ Î±Ï…Î¸ÎµÎ½Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· ÎºÎ±Î¹ Î­Î»ÎµÎ³Ï‡Î¿ ÎµÎ¹ÏƒÏŒÎ´Î¿Ï… Î±Ï€ÏŒ Ï„Î·Î½ Î±ÏÏ‡Î®\n",
    "- **âš¡ Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Î³Î¹Î± ÎšÎ»Î¯Î¼Î±ÎºÎ±**: Î£Ï‡ÎµÎ´Î¹Î¬ÏƒÏ„Îµ Î³Î¹Î± Ï€Î¿Î»Î»Î¿ÏÏ‚ Ï„Î±Ï…Ï„ÏŒÏ‡ÏÎ¿Î½Î¿Ï…Ï‚ Ï‡ÏÎ®ÏƒÏ„ÎµÏ‚ Î±Ï€ÏŒ Ï„Î·Î½ Ï€ÏÏÏ„Î· Î¼Î­ÏÎ±\n",
    "\n",
    "### ğŸš€ Î•Ï€ÏŒÎ¼ÎµÎ½Î± Î’Î®Î¼Î±Ï„Î±\n",
    "\n",
    "- **ğŸ“± Î¥Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î· Î Î¿Î»Î»Î±Ï€Î»ÏÎ½ ÎœÎ¿ÏÏ†ÏÎ½**: Î ÏÎ¿ÏƒÎ¸Î­ÏƒÏ„Îµ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚ ÎµÎ¹ÎºÏŒÎ½Ï‰Î½ ÎºÎ±Î¹ ÎµÎ³Î³ÏÎ¬Ï†Ï‰Î½\n",
    "- **ğŸ¤– Î•Î½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· Î ÏÎ±ÎºÏ„ÏŒÏÏ‰Î½**: Î£Ï…Î½Î´ÎµÎ¸ÎµÎ¯Ï„Îµ Î¼Îµ ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î± Ï€Î¿Î»Î»Î±Ï€Î»ÏÎ½ Ï€ÏÎ±ÎºÏ„ÏŒÏÏ‰Î½ Î³Î¹Î± ÏƒÏÎ½Î¸ÎµÏ„ÎµÏ‚ ÏÎ¿Î­Ï‚ ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚\n",
    "- **ğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚ Î‘Î½Î±Î»ÏÏƒÎµÏ‰Î½**: Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÏ„Îµ Î´Î¹ÎµÏ€Î±Ï†Î­Ï‚ Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ·Ï‚ Î³Î¹Î± Ï€Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· ÎºÎ±Î¹ Î­Î»ÎµÎ³Ï‡Î¿\n",
    "- **ğŸ”§ Î ÏÎ¿ÏƒÎ±ÏÎ¼Î¿ÏƒÎ¼Î­Î½Î± Î ÏÏŒÏƒÎ¸ÎµÏ„Î±**: Î‘Î½Î±Ï€Ï„ÏÎ¾Ï„Îµ Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿ÏƒÎ¼Î­Î½Î± ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î± ÎºÎ±Î¹ ÎµÎ½ÏƒÏ‰Î¼Î±Ï„ÏÏƒÎµÎ¹Ï‚ Chainlit\n",
    "- **ğŸŒ Î•Î½ÏƒÏ‰Î¼Î¬Ï„Ï‰ÏƒÎ· API**: Î£Ï…Î½Î´ÎµÎ¸ÎµÎ¯Ï„Îµ Î¼Îµ ÎµÎ¾Ï‰Ï„ÎµÏÎ¹ÎºÎ­Ï‚ Ï…Ï€Î·ÏÎµÏƒÎ¯ÎµÏ‚ ÎºÎ±Î¹ Î²Î¬ÏƒÎµÎ¹Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½\n",
    "\n",
    "Î‘Ï…Ï„Î® Î· ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Chainlit Î´ÎµÎ¯Ï‡Î½ÎµÎ¹ Ï€ÏÏ‚ Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎµÏ„Îµ ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¹Î±ÎºÎ­Ï‚ Î´Î¹ÎµÏ€Î±Ï†Î­Ï‚ AI Î­Ï„Î¿Î¹Î¼ÎµÏ‚ Î³Î¹Î± Ï€Î±ÏÎ±Î³Ï‰Î³Î®, Î±Î¾Î¹Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î· Î´ÏÎ½Î±Î¼Î· Ï„Ï‰Î½ Ï„Î¿Ï€Î¹ÎºÏÎ½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ AI Î¼Î­ÏƒÏ‰ Ï„Î¿Ï… Microsoft Foundry Local, ÎµÎ½Ï Ï€Î±ÏÎ­Ï‡ÎµÎ¹ Î¼Î¹Î± ÏƒÏÎ³Ï‡ÏÎ¿Î½Î· ÎºÎ±Î¹ ÎµÏ…Î­Î»Î¹ÎºÏ„Î· ÎµÎ¼Ï€ÎµÎ¹ÏÎ¯Î± Ï‡ÏÎ®ÏƒÏ„Î·.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "coopTranslator": {
   "original_hash": "eed20b9ecdd7cd5f88db77bfda326883",
   "translation_date": "2025-09-24T23:01:54+00:00",
   "source_file": "Module08/samples/04/chainlit_app.ipynb",
   "language_code": "el"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}