{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49ba9a1",
   "metadata": {},
   "source": [
    "# Î£Ï…Î½ÎµÎ´ÏÎ¯Î± 4 â€“ Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· SLM ÎºÎ±Î¹ LLM\n",
    "\n",
    "Î£Ï…Î³ÎºÏÎ¯Î½ÎµÏ„Îµ Ï„Î·Î½ ÎºÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ· ÎºÎ±Î¹ Ï„Î·Î½ Ï€Î¿Î¹ÏŒÏ„Î·Ï„Î± Ï„Ï‰Î½ Ï€Î±ÏÎ±Î´ÎµÎ¹Î³Î¼Î±Ï„Î¹ÎºÏÎ½ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÏ‰Î½ Î¼ÎµÏ„Î±Î¾Ï ÎµÎ½ÏŒÏ‚ ÎœÎ¹ÎºÏÎ¿Ï Î“Î»Ï‰ÏƒÏƒÎ¹ÎºÎ¿Ï ÎœÎ¿Î½Ï„Î­Î»Î¿Ï… ÎºÎ±Î¹ ÎµÎ½ÏŒÏ‚ Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï… Ï€Î¿Ï… Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯ Î¼Î­ÏƒÏ‰ Ï„Î¿Ï… Foundry Local.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9330f",
   "metadata": {},
   "source": [
    "## âš¡ Î“ÏÎ®Î³Î¿ÏÎ· Î•ÎºÎºÎ¯Î½Î·ÏƒÎ·\n",
    "\n",
    "**Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î·Ï‚ ÎœÎ½Î®Î¼Î·Ï‚ (Î•Î½Î·Î¼ÎµÏÏ‰Î¼Î­Î½Î·):**\n",
    "1. Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± ÎµÏ€Î¹Î»Î­Î³Î¿Ï…Î½ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± Ï€Î±ÏÎ±Î»Î»Î±Î³Î­Ï‚ CPU (Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯ ÏƒÎµ Î¿Ï€Î¿Î¹Î¿Î´Î®Ï€Î¿Ï„Îµ Ï…Î»Î¹ÎºÏŒ)\n",
    "2. Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î¿ `qwen2.5-3b` Î±Î½Ï„Î¯ Î³Î¹Î± 7B (ÎµÎ¾Î¿Î¹ÎºÎ¿Î½Î¿Î¼ÎµÎ¯ ~4GB RAM)\n",
    "3. Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· Î±Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Î¸ÏÏÎ±Ï‚ (Ï‡Ï‰ÏÎ¯Ï‚ Ï‡ÎµÎ¹ÏÎ¿ÎºÎ¯Î½Î·Ï„Î· ÏÏÎ¸Î¼Î¹ÏƒÎ·)\n",
    "4. Î£Ï…Î½Î¿Î»Î¹ÎºÎ® Î±Ï€Î±Î¹Ï„Î¿ÏÎ¼ÎµÎ½Î· RAM: ~8GB Ï€ÏÎ¿Ï„ÎµÎ¹Î½ÏŒÎ¼ÎµÎ½Î· (Î¼Î¿Î½Ï„Î­Î»Î± + Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¹ÎºÏŒ ÏƒÏÏƒÏ„Î·Î¼Î±)\n",
    "\n",
    "**Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Î¤ÎµÏÎ¼Î±Ï„Î¹ÎºÎ¿Ï (30 Î´ÎµÏ…Ï„ÎµÏÏŒÎ»ÎµÏ€Ï„Î±):**\n",
    "```bash\n",
    "foundry service start\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-3b\n",
    "```\n",
    "\n",
    "Î£Ï„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î±, ÎµÎºÏ„ÎµÎ»Î­ÏƒÏ„Îµ Î±Ï…Ï„ÏŒ Ï„Î¿ notebook! ğŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941ea8c",
   "metadata": {},
   "source": [
    "### Î•Ï€ÎµÎ¾Î®Î³Î·ÏƒÎ·: Î•Î³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Î•Î¾Î±ÏÏ„Î®ÏƒÎµÏ‰Î½\n",
    "Î•Î³ÎºÎ±Î¸Î¹ÏƒÏ„Î¬ Ï„Î± ÎµÎ»Î¬Ï‡Î¹ÏƒÏ„Î± Ï€Î±ÎºÎ­Ï„Î± (`foundry-local-sdk`, `openai`, `numpy`) Ï€Î¿Ï… Î±Ï€Î±Î¹Ï„Î¿ÏÎ½Ï„Î±Î¹ Î³Î¹Î± Ï‡ÏÎ¿Î½Î¹ÏƒÎ¼ÏŒ ÎºÎ±Î¹ Î±Î¹Ï„Î®Î¼Î±Ï„Î± ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¯Î±Ï‚. ÎœÏ€Î¿ÏÎµÎ¯ Î½Î± ÎµÎºÏ„ÎµÎ»ÎµÏƒÏ„ÎµÎ¯ Î¾Î±Î½Î¬ Î¼Îµ Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î± Ï‡Ï‰ÏÎ¯Ï‚ Î½Î± ÎµÏ€Î·ÏÎµÎ¬ÏƒÎµÎ¹ Ï„Î¿ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î±.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4690c7c7",
   "metadata": {},
   "source": [
    "# Î£ÎµÎ½Î¬ÏÎ¹Î¿\n",
    "Î£Ï…Î³ÎºÏÎ¯Î½ÎµÏ„Îµ Î­Î½Î± Î±Î½Ï„Î¹Ï€ÏÎ¿ÏƒÏ‰Ï€ÎµÏ…Ï„Î¹ÎºÏŒ ÎœÎ¹ÎºÏÏŒ ÎœÎ¿Î½Ï„Î­Î»Î¿ Î“Î»ÏÏƒÏƒÎ±Ï‚ (SLM) Î¼Îµ Î­Î½Î± Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ¿ Î¼Î¿Î½Ï„Î­Î»Î¿ ÏƒÎµ Î¼Î¯Î± Î¼ÏŒÎ½Î¿ Ï€ÏÎ¿Ï„ÏÎ¿Ï€Î® Î³Î¹Î± Î½Î± Î±Î½Î±Î´ÎµÎ¯Î¾ÎµÏ„Îµ Ï„Î± Ï€Î»ÎµÎ¿Î½ÎµÎºÏ„Î®Î¼Î±Ï„Î± ÎºÎ±Î¹ Ï„Î± Î¼ÎµÎ¹Î¿Î½ÎµÎºÏ„Î®Î¼Î±Ï„Î±:\n",
    "- **Î”Î¹Î±Ï†Î¿ÏÎ¬ ÎºÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ·Ï‚** (Î´ÎµÏ…Ï„ÎµÏÏŒÎ»ÎµÏ€Ï„Î± Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¿Ï Ï‡ÏÏŒÎ½Î¿Ï…)\n",
    "- **Î§ÏÎ®ÏƒÎ· tokens** (Î±Î½ ÎµÎ¯Î½Î±Î¹ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î·) Ï‰Ï‚ Î´ÎµÎ¯ÎºÏ„Î·Ï‚ Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚\n",
    "- **Î”ÎµÎ¯Î³Î¼Î± Ï€Î¿Î¹Î¿Ï„Î¹ÎºÎ®Ï‚ ÎµÎ¾ÏŒÎ´Î¿Ï…** Î³Î¹Î± Î³ÏÎ®Î³Î¿ÏÎ· Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ·\n",
    "- **Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ ÎµÏ€Î¹Ï„Î¬Ï‡Ï…Î½ÏƒÎ·Ï‚** Î³Î¹Î± Ï€Î¿ÏƒÎ¿Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Ï‰Î½ ÎºÎµÏÎ´ÏÎ½ Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚\n",
    "\n",
    "**ÎœÎµÏ„Î±Î²Î»Î·Ï„Î­Ï‚ Î ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î¿Ï‚:**\n",
    "- `SLM_ALIAS` - ÎœÎ¹ÎºÏÏŒ Î¼Î¿Î½Ï„Î­Î»Î¿ Î³Î»ÏÏƒÏƒÎ±Ï‚ (Ï€ÏÎ¿ÎµÏ€Î¹Î»Î¿Î³Î®: phi-4-mini, ~4GB RAM)\n",
    "- `LLM_ALIAS` - ÎœÎµÎ³Î±Î»ÏÏ„ÎµÏÎ¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î³Î»ÏÏƒÏƒÎ±Ï‚ (Ï€ÏÎ¿ÎµÏ€Î¹Î»Î¿Î³Î®: qwen2.5-7b, ~7GB RAM)\n",
    "- `COMPARE_PROMPT` - Î ÏÎ¿Ï„ÏÎ¿Ï€Î® Î´Î¿ÎºÎ¹Î¼Î®Ï‚ Î³Î¹Î± ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ·\n",
    "- `COMPARE_RETRIES` - Î ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹ÎµÏ‚ ÎµÏ€Î±Î½Î¬Î»Î·ÏˆÎ·Ï‚ Î³Î¹Î± Î±Î½Î¸ÎµÎºÏ„Î¹ÎºÏŒÏ„Î·Ï„Î± (Ï€ÏÎ¿ÎµÏ€Î¹Î»Î¿Î³Î®: 2)\n",
    "- `FOUNDRY_LOCAL_ENDPOINT` - Î¥Ï€Î­ÏÎ²Î±ÏƒÎ· Ï„Î¿Ï… Ï„ÎµÎ»Î¹ÎºÎ¿Ï ÏƒÎ·Î¼ÎµÎ¯Î¿Ï… Ï…Ï€Î·ÏÎµÏƒÎ¯Î±Ï‚ (ÎµÎ½Ï„Î¿Ï€Î¯Î¶ÎµÏ„Î±Î¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± Î±Î½ Î´ÎµÎ½ Î­Ï‡ÎµÎ¹ Î¿ÏÎ¹ÏƒÏ„ÎµÎ¯)\n",
    "\n",
    "**Î ÏÏ‚ Î›ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯ (Î•Ï€Î¯ÏƒÎ·Î¼Î¿ ÎœÎ¿Ï„Î¯Î²Î¿ SDK):**\n",
    "1. ÎŸ **FoundryLocalManager** Î±ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¹ÎµÎ¯ ÎºÎ±Î¹ Î´Î¹Î±Ï‡ÎµÎ¹ÏÎ¯Î¶ÎµÏ„Î±Î¹ Ï„Î·Î½ Ï„Î¿Ï€Î¹ÎºÎ® Ï…Ï€Î·ÏÎµÏƒÎ¯Î± Foundry\n",
    "2. Î— Ï…Ï€Î·ÏÎµÏƒÎ¯Î± Î¾ÎµÎºÎ¹Î½Î¬ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± Î±Î½ Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ ÎµÎ½ÎµÏÎ³Î® (Î´ÎµÎ½ Î±Ï€Î±Î¹Ï„ÎµÎ¯Ï„Î±Î¹ Ï‡ÎµÎ¹ÏÎ¿ÎºÎ¯Î½Î·Ï„Î· ÏÏÎ¸Î¼Î¹ÏƒÎ·)\n",
    "3. Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± ÎµÏ€Î¹Î»ÏÎ¿Î½Ï„Î±Î¹ Î±Ï€ÏŒ ÏˆÎµÏ…Î´ÏÎ½Ï…Î¼Î± ÏƒÎµ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î± IDs Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î±\n",
    "4. Î•Ï€Î¹Î»Î­Î³Î¿Î½Ï„Î±Î¹ Ï€Î±ÏÎ±Î»Î»Î±Î³Î­Ï‚ Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½ÎµÏ‚ Î³Î¹Î± Ï…Î»Î¹ÎºÏŒ (CUDA, NPU Î® CPU)\n",
    "5. ÎŸ Ï€ÎµÎ»Î¬Ï„Î·Ï‚ ÏƒÏ…Î¼Î²Î±Ï„ÏŒÏ‚ Î¼Îµ OpenAI ÎµÎºÏ„ÎµÎ»ÎµÎ¯ Î¿Î»Î¿ÎºÎ»Î·ÏÏÏƒÎµÎ¹Ï‚ ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¯Î±Ï‚\n",
    "6. ÎšÎ±Ï„Î±Î³ÏÎ¬Ï†Î¿Î½Ï„Î±Î¹ Î¼ÎµÏ„ÏÎ®ÏƒÎµÎ¹Ï‚: ÎºÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ·, tokens, Ï€Î¿Î¹ÏŒÏ„Î·Ï„Î± ÎµÎ¾ÏŒÎ´Î¿Ï…\n",
    "7. Î¤Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± ÏƒÏ…Î³ÎºÏÎ¯Î½Î¿Î½Ï„Î±Î¹ Î³Î¹Î± Ï„Î¿Î½ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒ Ï„Î¿Ï… Î»ÏŒÎ³Î¿Ï… ÎµÏ€Î¹Ï„Î¬Ï‡Ï…Î½ÏƒÎ·Ï‚\n",
    "\n",
    "Î‘Ï…Ï„Î® Î· Î¼Î¹ÎºÏÎ¿-ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ· Î²Î¿Î·Î¸Î¬ Î½Î± Î±Ï€Î¿Ï†Î±ÏƒÎ¯ÏƒÎµÏ„Îµ Ï€ÏŒÏ„Îµ ÎµÎ¯Î½Î±Î¹ Î´Î¹ÎºÎ±Î¹Î¿Î»Î¿Î³Î·Î¼Î­Î½Î· Î· Ï‡ÏÎ®ÏƒÎ· ÎµÎ½ÏŒÏ‚ Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï… Î³Î¹Î± Ï„Î·Î½ Ï€ÎµÏÎ¯Ï€Ï„Ï‰ÏƒÎ® ÏƒÎ±Ï‚.\n",
    "\n",
    "**Î‘Î½Î±Ï†Î¿ÏÎ¬ SDK:** \n",
    "- Python SDK: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "- Workshop Utils: Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î¿ ÎµÏ€Î¯ÏƒÎ·Î¼Î¿ Î¼Î¿Ï„Î¯Î²Î¿ Î±Ï€ÏŒ ../samples/workshop_utils.py\n",
    "\n",
    "**ÎšÏÏÎ¹Î± ÎŸÏ†Î­Î»Î·:**\n",
    "- âœ… Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· Î±Î½Î±ÎºÎ¬Î»Ï…ÏˆÎ· ÎºÎ±Î¹ Î±ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï…Ï€Î·ÏÎµÏƒÎ¯Î±Ï‚\n",
    "- âœ… Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· ÎµÎºÎºÎ¯Î½Î·ÏƒÎ· Ï…Ï€Î·ÏÎµÏƒÎ¯Î±Ï‚ Î±Î½ Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ ÎµÎ½ÎµÏÎ³Î®\n",
    "- âœ… Î•Î½ÏƒÏ‰Î¼Î±Ï„Ï‰Î¼Î­Î½Î· ÎµÏ€Î¯Î»Ï…ÏƒÎ· ÎºÎ±Î¹ Ï€ÏÎ¿ÏƒÏ‰ÏÎ¹Î½Î® Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Ï‰Î½\n",
    "- âœ… Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï…Î»Î¹ÎºÎ¿Ï (CUDA/NPU/CPU)\n",
    "- âœ… Î£Ï…Î¼Î²Î±Ï„ÏŒÏ„Î·Ï„Î± Î¼Îµ Ï„Î¿ SDK Ï„Î¿Ï… OpenAI\n",
    "- âœ… Î‘Î½Î¸ÎµÎºÏ„Î¹ÎºÏŒÏ‚ Ï‡ÎµÎ¹ÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½ Î¼Îµ ÎµÏ€Î±Î½Î±Î»Î®ÏˆÎµÎ¹Ï‚\n",
    "- âœ… Î¤Î¿Ï€Î¹ÎºÎ® ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± (Î´ÎµÎ½ Î±Ï€Î±Î¹Ï„ÎµÎ¯Ï„Î±Î¹ API cloud)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4813a6",
   "metadata": {},
   "source": [
    "## ğŸš¨ Î ÏÎ¿Î±Ï€Î±Î¹Ï„Î¿ÏÎ¼ÎµÎ½Î±: Î¤Î¿ Foundry Local Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯!\n",
    "\n",
    "**Î ÏÎ¹Î½ ÎµÎºÏ„ÎµÎ»Î­ÏƒÎµÏ„Îµ Î±Ï…Ï„ÏŒ Ï„Î¿ notebook**, Î²ÎµÎ²Î±Î¹Ï‰Î¸ÎµÎ¯Ï„Îµ ÏŒÏ„Î¹ Î· Ï…Ï€Î·ÏÎµÏƒÎ¯Î± Foundry Local Î­Ï‡ÎµÎ¹ ÏÏ…Î¸Î¼Î¹ÏƒÏ„ÎµÎ¯:\n",
    "\n",
    "### Î•Î½Ï„Î¿Î»Î­Ï‚ Î“ÏÎ®Î³Î¿ÏÎ·Ï‚ Î•ÎºÎºÎ¯Î½Î·ÏƒÎ·Ï‚ (Î•ÎºÏ„ÎµÎ»Î­ÏƒÏ„Îµ ÏƒÏ„Î¿ Î¤ÎµÏÎ¼Î±Ï„Î¹ÎºÏŒ):\n",
    "\n",
    "```bash\n",
    "# 1. Start the Foundry Local service\n",
    "foundry service start\n",
    "\n",
    "# 2. Load the default models used in this comparison (CPU-optimized)\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-3b\n",
    "\n",
    "# 3. Verify models are loaded\n",
    "foundry model ls\n",
    "\n",
    "# 4. Check service health\n",
    "foundry service status\n",
    "```\n",
    "\n",
    "### Î•Î½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ¬ ÎœÎ¿Î½Ï„Î­Î»Î± (Î±Î½ Ï„Î± Ï€ÏÎ¿ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î± Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î±):\n",
    "\n",
    "```bash\n",
    "# Even smaller alternatives (if memory is very limited)\n",
    "foundry model run phi-3.5-mini\n",
    "foundry model run qwen2.5-0.5b\n",
    "\n",
    "# Or update the environment variables in this notebook:\n",
    "# SLM_ALIAS = 'phi-3.5-mini'\n",
    "# LLM_ALIAS = 'qwen2.5-1.5b'  # Or qwen2.5-0.5b for minimum memory\n",
    "```\n",
    "\n",
    "âš ï¸ **Î‘Î½ Ï€Î±ÏÎ±Î»ÎµÎ¯ÏˆÎµÏ„Îµ Î±Ï…Ï„Î¬ Ï„Î± Î²Î®Î¼Î±Ï„Î±**, Î¸Î± Î´ÎµÎ¯Ï„Îµ `APIConnectionError` ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎµÎºÏ„Î­Î»ÎµÏƒÎ· Ï„Ï‰Î½ ÎºÎµÎ»Î¹ÏÎ½ Ï„Î¿Ï… notebook Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8ea63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai numpy requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d2ae1",
   "metadata": {},
   "source": [
    "### Î•Ï€ÎµÎ¾Î®Î³Î·ÏƒÎ·: Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ Î•Î¹ÏƒÎ±Î³Ï‰Î³Î­Ï‚\n",
    "Î ÎµÏÎ¹Î»Î±Î¼Î²Î¬Î½ÎµÎ¹ ÎµÏÎ³Î±Î»ÎµÎ¯Î± Ï‡ÏÎ¿Î½Î¹ÏƒÎ¼Î¿Ï ÎºÎ±Î¹ Ï„Î¿Ï…Ï‚ Ï„Î¿Ï€Î¹ÎºÎ¿ÏÏ‚ / OpenAI Ï€ÎµÎ»Î¬Ï„ÎµÏ‚ Ï„Î¿Ï… Foundry Ï€Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ½Ï„Î±Î¹ Î³Î¹Î± Ï„Î·Î½ Î±Î½Î¬ÎºÏ„Î·ÏƒÎ· Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¹ÏÎ½ Î¼Î¿Î½Ï„Î­Î»Î¿Ï… ÎºÎ±Î¹ Ï„Î·Î½ ÎµÎºÏ„Î­Î»ÎµÏƒÎ· ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¹ÏÎ½.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1233c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "sys.path.append('../samples')\n",
    "from workshop_utils import get_client, chat_once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b6aa79",
   "metadata": {},
   "source": [
    "### Î•Ï€ÎµÎ¾Î®Î³Î·ÏƒÎ·: Î¨ÎµÏ…Î´ÏÎ½Ï…Î¼Î± & Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Î ÏÎ¿Ï„ÏÎ¿Ï€Î®Ï‚\n",
    "ÎŸÏÎ¯Î¶ÎµÎ¹ ÏˆÎµÏ…Î´ÏÎ½Ï…Î¼Î± Ï€Î¿Ï… Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿ÏƒÏ„Î¿ÏÎ½ ÏƒÏ„Î¿ Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½ Î³Î¹Î± Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎ± ÎºÎ±Î¹ Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ± Î¼Î¿Î½Ï„Î­Î»Î±, ÎºÎ±Î¸ÏÏ‚ ÎºÎ±Î¹ Î¼Î¹Î± Ï€ÏÎ¿Ï„ÏÎ¿Ï€Î® ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ·Ï‚. Î¡Ï…Î¸Î¼Î¯ÏƒÏ„Îµ Ï„Î¹Ï‚ Î¼ÎµÏ„Î±Î²Î»Î·Ï„Î­Ï‚ Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î¿Ï‚ Î³Î¹Î± Î½Î± Ï€ÎµÎ¹ÏÎ±Î¼Î±Ï„Î¹ÏƒÏ„ÎµÎ¯Ï„Îµ Î¼Îµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Î¿Î¹ÎºÎ¿Î³Î­Î½ÎµÎ¹ÎµÏ‚ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Î® ÎµÏÎ³Î±ÏƒÎ¯ÎµÏ‚.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f46b14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default to CPU models for better memory efficiency\n",
    "SLM = os.getenv('SLM_ALIAS', 'phi-4-mini')  # Auto-selects CPU variant\n",
    "LLM = os.getenv('LLM_ALIAS', 'qwen2.5-3b')  # Smaller LLM, more memory-friendly\n",
    "PROMPT = os.getenv('COMPARE_PROMPT', 'List 5 benefits of local AI inference.')\n",
    "# Endpoint is now managed by FoundryLocalManager - it auto-detects or can be overridden\n",
    "ENDPOINT = os.getenv('FOUNDRY_LOCAL_ENDPOINT', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29375d5",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Î”Î¹Î±Î¼ÏŒÏÏ†Ï‰ÏƒÎ· Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î·Ï‚ ÎœÎ½Î®Î¼Î·Ï‚\n",
    "\n",
    "**Î‘Ï…Ï„ÏŒ Ï„Î¿ ÏƒÎ·Î¼ÎµÎ¹Ï‰Î¼Î±Ï„Î¬ÏÎ¹Î¿ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Î¼Î¿Î½Ï„Î­Î»Î± Ï€Î¿Ï… ÎµÎ¾Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¿ÏÎ½ Î¼Î½Î®Î¼Î· Î±Ï€ÏŒ Ï€ÏÎ¿ÎµÏ€Î¹Î»Î¿Î³Î®:**\n",
    "- `phi-4-mini` â†’ ~4GB RAM (Î¤Î¿ Foundry Local ÎµÏ€Î¹Î»Î­Î³ÎµÎ¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± Ï„Î·Î½ Ï€Î±ÏÎ±Î»Î»Î±Î³Î® CPU)\n",
    "- `qwen2.5-3b` â†’ ~3GB RAM (Î±Î½Ï„Î¯ Î³Î¹Î± 7B Ï€Î¿Ï… Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ ~7GB+)\n",
    "\n",
    "**Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· Î‘Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Î˜ÏÏÎ±Ï‚:**\n",
    "- Î¤Î¿ Foundry Local Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Î¸ÏÏÎµÏ‚ (ÏƒÏ…Î½Î®Î¸Ï‰Ï‚ 55769 Î® 59959)\n",
    "- Î¤Î¿ Î´Î¹Î±Î³Î½Ï‰ÏƒÏ„Î¹ÎºÏŒ ÎºÎµÎ»Î¯ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Î±Î½Î¹Ï‡Î½ÎµÏÎµÎ¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± Ï„Î· ÏƒÏ‰ÏƒÏ„Î® Î¸ÏÏÎ±\n",
    "- Î”ÎµÎ½ Î±Ï€Î±Î¹Ï„ÎµÎ¯Ï„Î±Î¹ Ï‡ÎµÎ¹ÏÎ¿ÎºÎ¯Î½Î·Ï„Î· Î´Î¹Î±Î¼ÏŒÏÏ†Ï‰ÏƒÎ·!\n",
    "\n",
    "**Î‘Î½ Î­Ï‡ÎµÏ„Îµ Ï€ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼Î­Î½Î· RAM (<8GB), Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Î±ÎºÏŒÎ¼Î· Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎ± Î¼Î¿Î½Ï„Î­Î»Î±:**\n",
    "```python\n",
    "SLM = 'phi-3.5-mini'      # ~2GB\n",
    "LLM = 'qwen2.5-0.5b'      # ~500MB\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c17fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CURRENT CONFIGURATION\n",
      "============================================================\n",
      "SLM Model:     phi-4-mini\n",
      "LLM Model:     qwen2.5-7b\n",
      "SDK Pattern:   FoundryLocalManager (official)\n",
      "Endpoint:      Auto-detect\n",
      "Test Prompt:   List 5 benefits of local AI inference....\n",
      "Retry Count:   2\n",
      "============================================================\n",
      "\n",
      "ğŸ’¡ Using official Foundry SDK pattern from workshop_utils\n",
      "   â†’ FoundryLocalManager handles service lifecycle\n",
      "   â†’ Automatic model resolution and hardware optimization\n",
      "   â†’ OpenAI-compatible API for inference\n"
     ]
    }
   ],
   "source": [
    "# Display current configuration\n",
    "print(\"=\"*60)\n",
    "print(\"CURRENT CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"SLM Model:     {SLM}\")\n",
    "print(f\"LLM Model:     {LLM}\")\n",
    "print(f\"SDK Pattern:   FoundryLocalManager (official)\")\n",
    "print(f\"Endpoint:      {ENDPOINT or 'Auto-detect'}\")\n",
    "print(f\"Test Prompt:   {PROMPT[:50]}...\")\n",
    "print(f\"Retry Count:   2\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nğŸ’¡ Using official Foundry SDK pattern from workshop_utils\")\n",
    "print(\"   â†’ FoundryLocalManager handles service lifecycle\")\n",
    "print(\"   â†’ Automatic model resolution and hardware optimization\")\n",
    "print(\"   â†’ OpenAI-compatible API for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638df58",
   "metadata": {},
   "source": [
    "### Î•Ï€ÎµÎ¾Î®Î³Î·ÏƒÎ·: Î’Î¿Î·Î¸Î·Ï„Î¹ÎºÎ¬ Î•ÏÎ³Î±Î»ÎµÎ¯Î± Î•ÎºÏ„Î­Î»ÎµÏƒÎ·Ï‚ (ÎœÎ¿Ï„Î¯Î²Î¿ Foundry SDK)\n",
    "Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î¿ ÎµÏ€Î¯ÏƒÎ·Î¼Î¿ Î¼Î¿Ï„Î¯Î²Î¿ Foundry Local SDK ÏŒÏ€Ï‰Ï‚ Ï„ÎµÎºÎ¼Î·ÏÎ¹ÏÎ½ÎµÏ„Î±Î¹ ÏƒÏ„Î± Î´ÎµÎ¯Î³Î¼Î±Ï„Î± Ï„Î¿Ï… Workshop:\n",
    "\n",
    "**Î ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ·:**\n",
    "- **FoundryLocalManager** - Î‘ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¹ÎµÎ¯ ÎºÎ±Î¹ Î´Î¹Î±Ï‡ÎµÎ¹ÏÎ¯Î¶ÎµÏ„Î±Î¹ Ï„Î·Î½ Ï…Ï€Î·ÏÎµÏƒÎ¯Î± Foundry Local\n",
    "- **Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· Î‘Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ·** - Î•Î½Ï„Î¿Ï€Î¯Î¶ÎµÎ¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± Ï„Î¿ endpoint ÎºÎ±Î¹ Î´Î¹Î±Ï‡ÎµÎ¹ÏÎ¯Î¶ÎµÏ„Î±Î¹ Ï„Î¿Î½ ÎºÏÎºÎ»Î¿ Î¶Ï‰Î®Ï‚ Ï„Î·Ï‚ Ï…Ï€Î·ÏÎµÏƒÎ¯Î±Ï‚\n",
    "- **Î•Ï€Î¯Î»Ï…ÏƒÎ· ÎœÎ¿Î½Ï„Î­Î»Î¿Ï…** - ÎœÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ ÏˆÎµÏ…Î´ÏÎ½Ï…Î¼Î± ÏƒÎµ Ï€Î»Î®ÏÎ· IDs Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ (Ï€.Ï‡., phi-4-mini â†’ phi-4-mini-instruct-cpu)\n",
    "- **Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î¥Î»Î¹ÎºÎ¿Ï** - Î•Ï€Î¹Î»Î­Î³ÎµÎ¹ Ï„Î·Î½ ÎºÎ±Î»ÏÏ„ÎµÏÎ· Ï€Î±ÏÎ±Î»Î»Î±Î³Î® Î³Î¹Î± Ï„Î¿ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î¿ Ï…Î»Î¹ÎºÏŒ (CUDA, NPU Î® CPU)\n",
    "- **OpenAI Client** - Î”Î¹Î±Î¼Î¿ÏÏ†ÏÎ½ÎµÏ„Î±Î¹ Î¼Îµ Ï„Î¿ endpoint Ï„Î¿Ï… Î´Î¹Î±Ï‡ÎµÎ¹ÏÎ¹ÏƒÏ„Î® Î³Î¹Î± Ï€ÏÏŒÏƒÎ²Î±ÏƒÎ· ÏƒÎµ API ÏƒÏ…Î¼Î²Î±Ï„ÏŒ Î¼Îµ OpenAI\n",
    "\n",
    "**Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î‘Î½Î¸ÎµÎºÏ„Î¹ÎºÏŒÏ„Î·Ï„Î±Ï‚:**\n",
    "- Î›Î¿Î³Î¹ÎºÎ® ÎµÏ€Î±Î½Î±Ï€ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹Î±Ï‚ Î¼Îµ ÎµÎºÎ¸ÎµÏ„Î¹ÎºÎ® ÎºÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ· (ÏÏ…Î¸Î¼Î¹Î¶ÏŒÎ¼ÎµÎ½Î· Î¼Î­ÏƒÏ‰ Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î¿Ï‚)\n",
    "- Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· ÎµÎºÎºÎ¯Î½Î·ÏƒÎ· Ï…Ï€Î·ÏÎµÏƒÎ¯Î±Ï‚ Î±Î½ Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ ÎµÎ½ÎµÏÎ³Î®\n",
    "- Î•Ï€Î±Î»Î®Î¸ÎµÏ…ÏƒÎ· ÏƒÏÎ½Î´ÎµÏƒÎ·Ï‚ Î¼ÎµÏ„Î¬ Ï„Î·Î½ Î±ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ·\n",
    "- ÎŸÎ¼Î±Î»Î® Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½ Î¼Îµ Î»ÎµÏ€Ï„Î¿Î¼ÎµÏÎ® Î±Î½Î±Ï†Î¿ÏÎ¬ ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½\n",
    "- Cache Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Î³Î¹Î± Î±Ï€Î¿Ï†Ï…Î³Î® ÎµÏ€Î±Î½Î±Î»Î±Î¼Î²Î±Î½ÏŒÎ¼ÎµÎ½Î·Ï‚ Î±ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚\n",
    "\n",
    "**Î”Î¿Î¼Î® Î‘Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½:**\n",
    "- ÎœÎ­Ï„ÏÎ·ÏƒÎ· ÎºÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ·Ï‚ (Ï‡ÏÏŒÎ½Î¿Ï‚ Ï„Î¿Î¯Ï‡Î¿Ï…)\n",
    "- Î Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· Ï‡ÏÎ®ÏƒÎ·Ï‚ tokens (Î±Î½ ÎµÎ¯Î½Î±Î¹ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î·)\n",
    "- Î”ÎµÎ¯Î³Î¼Î± ÎµÎ¾ÏŒÎ´Î¿Ï… (Ï€ÎµÏÎ¹ÎºÎ¿Î¼Î¼Î­Î½Î¿ Î³Î¹Î± ÎµÏ…Î±Î½Î¬Î³Î½Ï‰ÏƒÏ„Î· Î¼Î¿ÏÏ†Î®)\n",
    "- Î›ÎµÏ€Ï„Î¿Î¼Î­ÏÎµÎ¹ÎµÏ‚ ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½ Î³Î¹Î± Î±Ï€Î¿Ï„Ï…Ï‡Î·Î¼Î­Î½Î± Î±Î¹Ï„Î®Î¼Î±Ï„Î±\n",
    "\n",
    "Î‘Ï…Ï„ÏŒ Ï„Î¿ Î¼Î¿Ï„Î¯Î²Î¿ Î±Î¾Î¹Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î· Î¼Î¿Î½Î¬Î´Î± workshop_utils Ï€Î¿Ï… Î±ÎºÎ¿Î»Î¿Ï…Î¸ÎµÎ¯ Ï„Î¿ ÎµÏ€Î¯ÏƒÎ·Î¼Î¿ Î¼Î¿Ï„Î¯Î²Î¿ SDK.\n",
    "\n",
    "**Î‘Î½Î±Ï†Î¿ÏÎ¬ SDK:**\n",
    "- ÎšÏÏÎ¹Î¿ Î‘Ï€Î¿Î¸ÎµÏ„Î®ÏÎ¹Î¿: https://github.com/microsoft/Foundry-Local\n",
    "- Python SDK: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "- Workshop Utils: ../samples/workshop_utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e646fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Execution helpers defined: setup(), run()\n",
      "   â†’ Uses workshop_utils for proper SDK integration\n",
      "   â†’ setup() initializes with FoundryLocalManager\n",
      "   â†’ run() executes inference via OpenAI-compatible API\n",
      "   â†’ Token counting: Uses API data or estimates if unavailable\n"
     ]
    }
   ],
   "source": [
    "def setup(alias: str, endpoint: str = None, retries: int = 3):\n",
    "    \"\"\"\n",
    "    Initialize a Foundry Local model connection using official SDK pattern.\n",
    "    \n",
    "    This follows the workshop_utils pattern which uses FoundryLocalManager\n",
    "    to properly initialize the Foundry Local service and resolve models.\n",
    "    \n",
    "    Args:\n",
    "        alias: Model alias (e.g., 'phi-4-mini', 'qwen2.5-3b')\n",
    "        endpoint: Optional endpoint override (usually auto-detected)\n",
    "        retries: Number of connection attempts (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (manager, client, model_id, metadata) or (None, None, alias, error_metadata) if failed\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    last_err = None\n",
    "    current_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            print(f\"[Init] Connecting to '{alias}' (attempt {attempt}/{retries})...\")\n",
    "            \n",
    "            # Use the workshop utility which follows the official SDK pattern\n",
    "            manager, client, model_id = get_client(alias, endpoint=endpoint)\n",
    "            \n",
    "            print(f\"[OK] Connected to '{alias}' -> {model_id}\")\n",
    "            print(f\"     Endpoint: {manager.endpoint}\")\n",
    "            \n",
    "            return manager, client, model_id, {\n",
    "                'endpoint': manager.endpoint,\n",
    "                'resolved': model_id,\n",
    "                'attempts': attempt,\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Provide helpful error messages\n",
    "            if \"Connection error\" in error_msg or \"connection refused\" in error_msg.lower():\n",
    "                print(f\"[ERROR] Cannot connect to Foundry Local service\")\n",
    "                print(f\"        â†’ Is the service running? Try: foundry service start\")\n",
    "                print(f\"        â†’ Is the model loaded? Try: foundry model run {alias}\")\n",
    "            elif \"not found\" in error_msg.lower():\n",
    "                print(f\"[ERROR] Model '{alias}' not found in catalog\")\n",
    "                print(f\"        â†’ Available models: Run 'foundry model ls' in terminal\")\n",
    "                print(f\"        â†’ Download model: Run 'foundry model download {alias}'\")\n",
    "            else:\n",
    "                print(f\"[ERROR] Setup failed: {type(e).__name__}: {error_msg}\")\n",
    "            \n",
    "            if attempt < retries:\n",
    "                print(f\"[Retry] Waiting {current_delay:.1f}s before retry...\")\n",
    "                time.sleep(current_delay)\n",
    "                current_delay *= 2  # Exponential backoff\n",
    "    \n",
    "    # All retries failed - provide actionable guidance\n",
    "    print(f\"\\nâŒ Failed to initialize '{alias}' after {retries} attempts\")\n",
    "    print(f\"   Last error: {type(last_err).__name__}: {str(last_err)}\")\n",
    "    print(f\"\\nğŸ’¡ Troubleshooting steps:\")\n",
    "    print(f\"   1. Ensure Foundry Local service is running:\")\n",
    "    print(f\"      â†’ foundry service status\")\n",
    "    print(f\"      â†’ foundry service start (if not running)\")\n",
    "    print(f\"   2. Ensure model is loaded:\")\n",
    "    print(f\"      â†’ foundry model run {alias}\")\n",
    "    print(f\"   3. Check available models:\")\n",
    "    print(f\"      â†’ foundry model ls\")\n",
    "    print(f\"   4. Try alternative models if '{alias}' isn't available\")\n",
    "    \n",
    "    return None, None, alias, {\n",
    "        'error': f\"{type(last_err).__name__}: {str(last_err)}\",\n",
    "        'endpoint': endpoint or 'auto-detect',\n",
    "        'attempts': retries,\n",
    "        'status': 'failed'\n",
    "    }\n",
    "\n",
    "\n",
    "def run(client, model_id: str, prompt: str, max_tokens: int = 180, temperature: float = 0.5):\n",
    "    \"\"\"\n",
    "    Run inference with the configured model using OpenAI SDK.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance (configured for Foundry Local)\n",
    "        model_id: Model identifier (resolved from alias)\n",
    "        prompt: Input prompt\n",
    "        max_tokens: Maximum response tokens (default: 180)\n",
    "        temperature: Sampling temperature (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Response with timing, tokens, and content\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # Extract response details\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Try to extract token usage from multiple possible locations\n",
    "        usage_info = {}\n",
    "        if hasattr(response, 'usage') and response.usage:\n",
    "            usage_info['prompt_tokens'] = getattr(response.usage, 'prompt_tokens', None)\n",
    "            usage_info['completion_tokens'] = getattr(response.usage, 'completion_tokens', None)\n",
    "            usage_info['total_tokens'] = getattr(response.usage, 'total_tokens', None)\n",
    "        \n",
    "        # Calculate approximate token count if API doesn't provide it\n",
    "        # Rough estimate: ~4 characters per token for English text\n",
    "        if not usage_info.get('total_tokens'):\n",
    "            estimated_prompt_tokens = len(prompt) // 4\n",
    "            estimated_completion_tokens = len(content) // 4\n",
    "            estimated_total = estimated_prompt_tokens + estimated_completion_tokens\n",
    "            usage_info['estimated_tokens'] = estimated_total\n",
    "            usage_info['estimated_prompt_tokens'] = estimated_prompt_tokens\n",
    "            usage_info['estimated_completion_tokens'] = estimated_completion_tokens\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'content': content,\n",
    "            'elapsed_sec': elapsed,\n",
    "            'tokens': usage_info.get('total_tokens') or usage_info.get('estimated_tokens'),\n",
    "            'usage': usage_info,\n",
    "            'model': model_id\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f\"{type(e).__name__}: {str(e)}\",\n",
    "            'elapsed_sec': elapsed,\n",
    "            'model': model_id\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"âœ… Execution helpers defined: setup(), run()\")\n",
    "print(\"   â†’ Uses workshop_utils for proper SDK integration\")\n",
    "print(\"   â†’ setup() initializes with FoundryLocalManager\")\n",
    "print(\"   â†’ run() executes inference via OpenAI-compatible API\")\n",
    "print(\"   â†’ Token counting: Uses API data or estimates if unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba23b5",
   "metadata": {},
   "source": [
    "### Î•Ï€ÎµÎ¾Î®Î³Î·ÏƒÎ·: Î‘Ï…Ï„Î¿Î­Î»ÎµÎ³Ï‡Î¿Ï‚ Ï€ÏÎ¹Î½ Ï„Î·Î½ ÎµÎºÎºÎ¯Î½Î·ÏƒÎ·\n",
    "Î•ÎºÏ„ÎµÎ»ÎµÎ¯ Î­Î½Î±Î½ ÎµÎ»Î±Ï†ÏÏ Î­Î»ÎµÎ³Ï‡Î¿ ÏƒÏ…Î½Î´ÎµÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î±Ï‚ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î¿ FoundryLocalManager ÎºÎ±Î¹ Î³Î¹Î± Ï„Î± Î´ÏÎ¿ Î¼Î¿Î½Ï„Î­Î»Î±. Î‘Ï…Ï„ÏŒ ÎµÏ€Î±Î»Î·Î¸ÎµÏÎµÎ¹:\n",
    "- Î— Ï…Ï€Î·ÏÎµÏƒÎ¯Î± ÎµÎ¯Î½Î±Î¹ Ï€ÏÎ¿ÏƒÎ²Î¬ÏƒÎ¹Î¼Î·\n",
    "- Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± Î±ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¹Î·Î¸Î¿ÏÎ½\n",
    "- Î¤Î± ÏˆÎµÏ…Î´ÏÎ½Ï…Î¼Î± Î±Î½Ï„Î¹ÏƒÏ„Î¿Î¹Ï‡Î¿ÏÎ½ ÏƒÎµ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ IDs Î¼Î¿Î½Ï„Î­Î»Ï‰Î½\n",
    "- Î— ÏƒÏÎ½Î´ÎµÏƒÎ· ÎµÎ¯Î½Î±Î¹ ÏƒÏ„Î±Î¸ÎµÏÎ® Ï€ÏÎ¹Î½ Ï„Î·Î½ ÎµÎºÏ„Î­Î»ÎµÏƒÎ· Ï„Î·Ï‚ ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ·Ï‚\n",
    "\n",
    "Î— ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· setup() Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î¿ ÎµÏ€Î¯ÏƒÎ·Î¼Î¿ Ï€ÏÏŒÏ„Ï…Ï€Î¿ SDK Î±Ï€ÏŒ Ï„Î¿ workshop_utils.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e251bd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diagnostic] Checking Foundry Local service...\n",
      "\n",
      "âŒ Foundry Local service not found!\n",
      "\n",
      "ğŸ’¡ To fix this:\n",
      "   1. Open a terminal\n",
      "   2. Run: foundry service start\n",
      "   3. Run: foundry model run phi-4-mini\n",
      "   4. Run: foundry model run qwen2.5-3b\n",
      "   5. Re-run this notebook\n",
      "\n",
      "âš ï¸  No service detected - FoundryLocalManager will attempt to start it\n"
     ]
    }
   ],
   "source": [
    "# Simplified diagnostic: Just verify service is accessible\n",
    "import requests\n",
    "\n",
    "def check_foundry_service():\n",
    "    \"\"\"Quick diagnostic to verify Foundry Local is running.\"\"\"\n",
    "    # Try common ports\n",
    "    endpoints_to_try = [\n",
    "        \"http://localhost:59959\",\n",
    "        \"http://127.0.0.1:59959\", \n",
    "        \"http://localhost:55769\",\n",
    "        \"http://127.0.0.1:55769\",\n",
    "    ]\n",
    "    \n",
    "    print(\"[Diagnostic] Checking Foundry Local service...\")\n",
    "    \n",
    "    for endpoint in endpoints_to_try:\n",
    "        try:\n",
    "            response = requests.get(f\"{endpoint}/health\", timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"âœ… Service is running at {endpoint}\")\n",
    "                \n",
    "                # Try to list models\n",
    "                try:\n",
    "                    models_response = requests.get(f\"{endpoint}/v1/models\", timeout=2)\n",
    "                    if models_response.status_code == 200:\n",
    "                        models_data = models_response.json()\n",
    "                        model_count = len(models_data.get('data', []))\n",
    "                        print(f\"âœ… Found {model_count} models available\")\n",
    "                        if model_count > 0:\n",
    "                            print(\"   Models:\", [m.get('id', 'unknown') for m in models_data.get('data', [])[:5]])\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸  Could not list models: {e}\")\n",
    "                \n",
    "                return endpoint\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error checking {endpoint}: {e}\")\n",
    "    \n",
    "    print(\"\\nâŒ Foundry Local service not found!\")\n",
    "    print(\"\\nğŸ’¡ To fix this:\")\n",
    "    print(\"   1. Open a terminal\")\n",
    "    print(\"   2. Run: foundry service start\")\n",
    "    print(\"   3. Run: foundry model run phi-4-mini\")\n",
    "    print(\"   4. Run: foundry model run qwen2.5-3b\")\n",
    "    print(\"   5. Re-run this notebook\")\n",
    "    return None\n",
    "\n",
    "# Run diagnostic\n",
    "discovered_endpoint = check_foundry_service()\n",
    "\n",
    "if discovered_endpoint:\n",
    "    print(f\"\\nâœ… Service detected (will be managed by FoundryLocalManager)\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  No service detected - FoundryLocalManager will attempt to start it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35317769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  The commands above are commented out.\n",
      "Uncomment them if you want to start the service from the notebook.\n",
      "\n",
      "ğŸ’¡ Recommended: Run these commands in a separate terminal instead:\n",
      "   foundry service start\n",
      "   foundry model run phi-4-mini\n",
      "   foundry model run qwen2.5-3b\n"
     ]
    }
   ],
   "source": [
    "# Quick Fix: Start service and load models from notebook\n",
    "# Uncomment the commands you need:\n",
    "\n",
    "# !foundry service start\n",
    "# !foundry model run phi-4-mini\n",
    "# !foundry model run qwen2.5-3b\n",
    "# !foundry model ls\n",
    "\n",
    "print(\"âš ï¸  The commands above are commented out.\")\n",
    "print(\"Uncomment them if you want to start the service from the notebook.\")\n",
    "print(\"\")\n",
    "print(\"ğŸ’¡ Recommended: Run these commands in a separate terminal instead:\")\n",
    "print(\"   foundry service start\")\n",
    "print(\"   foundry model run phi-4-mini\")\n",
    "print(\"   foundry model run qwen2.5-3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2d0a5",
   "metadata": {},
   "source": [
    "### ğŸ› ï¸ Î“ÏÎ®Î³Î¿ÏÎ· Î•Ï€Î¹Î´Î¹ÏŒÏÎ¸Ï‰ÏƒÎ·: Î•ÎºÎºÎ¯Î½Î·ÏƒÎ· Ï„Î¿Ï… Foundry Local Î±Ï€ÏŒ Ï„Î¿ Notebook (Î ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÏŒ)\n",
    "\n",
    "Î‘Î½ Î· Ï€Î±ÏÎ±Ï€Î¬Î½Ï‰ Î´Î¹Î¬Î³Î½Ï‰ÏƒÎ· Î´ÎµÎ¯Ï‡Î½ÎµÎ¹ ÏŒÏ„Î¹ Î· Ï…Ï€Î·ÏÎµÏƒÎ¯Î± Î´ÎµÎ½ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯, Î¼Ï€Î¿ÏÎµÎ¯Ï„Îµ Î½Î± Î´Î¿ÎºÎ¹Î¼Î¬ÏƒÎµÏ„Îµ Î½Î± Ï„Î·Î½ ÎµÎºÎºÎ¹Î½Î®ÏƒÎµÏ„Îµ Î±Ï€ÏŒ ÎµÎ´Ï:\n",
    "\n",
    "**Î£Î·Î¼ÎµÎ¯Ï‰ÏƒÎ·:** Î‘Ï…Ï„ÏŒ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯ ÎºÎ±Î»ÏÏ„ÎµÏÎ± ÏƒÏ„Î± Windows. Î£Îµ Î¬Î»Î»ÎµÏ‚ Ï€Î»Î±Ï„Ï†ÏŒÏÎ¼ÎµÏ‚, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ ÎµÎ½Ï„Î¿Î»Î­Ï‚ Ï„ÎµÏÎ¼Î±Ï„Î¹ÎºÎ¿Ï.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c0bd2",
   "metadata": {},
   "source": [
    "### âš ï¸ Î‘Î½Ï„Î¹Î¼ÎµÏ„ÏÏ€Î¹ÏƒÎ· Î£Ï†Î±Î»Î¼Î¬Ï„Ï‰Î½ Î£ÏÎ½Î´ÎµÏƒÎ·Ï‚\n",
    "\n",
    "Î‘Î½ Î²Î»Î­Ï€ÎµÏ„Îµ `APIConnectionError`, Î· Ï…Ï€Î·ÏÎµÏƒÎ¯Î± Foundry Local Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î¼Î·Î½ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯ Î® Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± Î´ÎµÎ½ Î­Ï‡Î¿Ï…Î½ Ï†Î¿ÏÏ„Ï‰Î¸ÎµÎ¯. Î”Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Îµ Ï„Î± Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Î²Î®Î¼Î±Ï„Î±:\n",
    "\n",
    "**1. Î•Î»Î­Î³Î¾Ï„Îµ Ï„Î·Î½ ÎšÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Ï„Î·Ï‚ Î¥Ï€Î·ÏÎµÏƒÎ¯Î±Ï‚:**\n",
    "```bash\n",
    "# In a terminal (not in notebook):\n",
    "foundry service status\n",
    "```\n",
    "\n",
    "**2. ÎÎµÎºÎ¹Î½Î®ÏƒÏ„Îµ Ï„Î·Î½ Î¥Ï€Î·ÏÎµÏƒÎ¯Î± (Î±Î½ Î´ÎµÎ½ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯):**\n",
    "```bash\n",
    "foundry service start\n",
    "```\n",
    "\n",
    "**3. Î¦Î¿ÏÏ„ÏÏƒÏ„Îµ Ï„Î± Î‘Ï€Î±Î¹Ï„Î¿ÏÎ¼ÎµÎ½Î± ÎœÎ¿Î½Ï„Î­Î»Î±:**\n",
    "```bash\n",
    "# Load the models needed for comparison\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-7b\n",
    "\n",
    "# Or use alternative models:\n",
    "foundry model run phi-3.5-mini\n",
    "foundry model run qwen2.5-3b\n",
    "```\n",
    "\n",
    "**4. Î•Ï€Î±Î»Î·Î¸ÎµÏÏƒÏ„Îµ ÏŒÏ„Î¹ Ï„Î± ÎœÎ¿Î½Ï„Î­Î»Î± ÎµÎ¯Î½Î±Î¹ Î”Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î±:**\n",
    "```bash\n",
    "foundry model ls\n",
    "```\n",
    "\n",
    "**Î£Ï…Î½Î·Î¸Î¹ÏƒÎ¼Î­Î½Î± Î ÏÎ¿Î²Î»Î®Î¼Î±Ï„Î±:**\n",
    "- âŒ Î— Ï…Ï€Î·ÏÎµÏƒÎ¯Î± Î´ÎµÎ½ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯ â†’ Î•ÎºÏ„ÎµÎ»Î­ÏƒÏ„Îµ `foundry service start`\n",
    "- âŒ Î¤Î± Î¼Î¿Î½Ï„Î­Î»Î± Î´ÎµÎ½ Î­Ï‡Î¿Ï…Î½ Ï†Î¿ÏÏ„Ï‰Î¸ÎµÎ¯ â†’ Î•ÎºÏ„ÎµÎ»Î­ÏƒÏ„Îµ `foundry model run <model-name>`\n",
    "- âŒ Î£Ï…Î³ÎºÏÎ¿ÏÏƒÎµÎ¹Ï‚ ÏƒÏ„Î¹Ï‚ Î¸ÏÏÎµÏ‚ â†’ Î•Î»Î­Î³Î¾Ï„Îµ Î±Î½ ÎºÎ¬Ï€Î¿Î¹Î± Î¬Î»Î»Î· Ï…Ï€Î·ÏÎµÏƒÎ¯Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î· Î¸ÏÏÎ±\n",
    "- âŒ Î¤Î¿ Ï„ÎµÎ¯Ï‡Î¿Ï‚ Ï€ÏÎ¿ÏƒÏ„Î±ÏƒÎ¯Î±Ï‚ Î¼Ï€Î»Î¿ÎºÎ¬ÏÎµÎ¹ â†’ Î’ÎµÎ²Î±Î¹Ï‰Î¸ÎµÎ¯Ï„Îµ ÏŒÏ„Î¹ ÎµÏ€Î¹Ï„ÏÎ­Ï€Î¿Î½Ï„Î±Î¹ Î¿Î¹ Ï„Î¿Ï€Î¹ÎºÎ­Ï‚ ÏƒÏ…Î½Î´Î­ÏƒÎµÎ¹Ï‚\n",
    "\n",
    "**Î“ÏÎ®Î³Î¿ÏÎ· Î•Ï€Î¹Î´Î¹ÏŒÏÎ¸Ï‰ÏƒÎ·:** Î•ÎºÏ„ÎµÎ»Î­ÏƒÏ„Îµ Ï„Î¿ Î´Î¹Î±Î³Î½Ï‰ÏƒÏ„Î¹ÎºÏŒ ÎºÎµÎ»Î¯ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Ï€ÏÎ¹Î½ Î±Ï€ÏŒ Ï„Î¿Î½ Î­Î»ÎµÎ³Ï‡Î¿ Ï€ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î±Ï‚.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a607078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Connecting to 'phi-4-mini' (attempt 1/2)...\n",
      "[OK] Connected to 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "\n",
      "[Pre-flight Check]\n",
      "  âœ… phi-4-mini: success - Phi-4-mini-instruct-cuda-gpu:4\n",
      "  âœ… qwen2.5-7b: success - qwen2.5-7b-instruct-cuda-gpu:3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'phi-4-mini': {'endpoint': 'http://127.0.0.1:59959/v1',\n",
       "  'resolved': 'Phi-4-mini-instruct-cuda-gpu:4',\n",
       "  'attempts': 1,\n",
       "  'status': 'success'},\n",
       " 'qwen2.5-7b': {'endpoint': 'http://127.0.0.1:59959/v1',\n",
       "  'resolved': 'qwen2.5-7b-instruct-cuda-gpu:3',\n",
       "  'attempts': 1,\n",
       "  'status': 'success'}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preflight = {}\n",
    "retries = 2  # Number of retry attempts\n",
    "\n",
    "for a in (SLM, LLM):\n",
    "    mgr, c, mid, info = setup(a, endpoint=ENDPOINT, retries=retries)\n",
    "    # Keep the original status from info (either 'success' or 'failed')\n",
    "    preflight[a] = info\n",
    "\n",
    "print('\\n[Pre-flight Check]')\n",
    "for alias, details in preflight.items():\n",
    "    status_icon = 'âœ…' if details['status'] == 'success' else 'âŒ'\n",
    "    print(f\"  {status_icon} {alias}: {details['status']} - {details.get('resolved', details.get('error', 'unknown'))}\")\n",
    "\n",
    "preflight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec76741a",
   "metadata": {},
   "source": [
    "### âœ… ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î ÏÎ¹Î½ Ï„Î·Î½ Î•ÎºÎºÎ¯Î½Î·ÏƒÎ·: Î”Î¹Î±Î¸ÎµÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î± ÎœÎ¿Î½Ï„Î­Î»Î¿Ï…\n",
    "\n",
    "Î‘Ï…Ï„ÏŒ Ï„Î¿ ÎºÎµÎ»Î¯ ÎµÏ€Î¹Î²ÎµÎ²Î±Î¹ÏÎ½ÎµÎ¹ ÏŒÏ„Î¹ ÎºÎ±Î¹ Ï„Î± Î´ÏÎ¿ Î¼Î¿Î½Ï„Î­Î»Î± ÎµÎ¯Î½Î±Î¹ Ï€ÏÎ¿ÏƒÎ²Î¬ÏƒÎ¹Î¼Î± ÏƒÏ„Î¿ ÎºÎ±Î¸Î¿ÏÎ¹ÏƒÎ¼Î­Î½Î¿ endpoint Ï€ÏÎ¹Î½ Ï„Î·Î½ ÎµÎºÏ„Î­Î»ÎµÏƒÎ· Ï„Î·Ï‚ ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ·Ï‚.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22961df",
   "metadata": {},
   "source": [
    "### Î•Ï€ÎµÎ¾Î®Î³Î·ÏƒÎ·: Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· Î•ÎºÏ„Î­Î»ÎµÏƒÎ·Ï‚ & Î£Ï…Î»Î»Î¿Î³Î® Î‘Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½\n",
    "Î•Ï€Î±Î½Î±Î»Î±Î¼Î²Î¬Î½ÎµÏ„Î±Î¹ Î³Î¹Î± ÎºÎ¬Î¸Îµ ÏˆÎµÏ…Î´ÏÎ½Ï…Î¼Î¿ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î¿ ÎµÏ€Î¯ÏƒÎ·Î¼Î¿ Ï€ÏÏŒÏ„Ï…Ï€Î¿ Ï„Î¿Ï… Foundry SDK:\n",
    "1. Î‘ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ ÎºÎ¬Î¸Îµ Î¼Î¿Î½Ï„Î­Î»Î¿ Î¼Îµ Ï„Î· Î¼Î­Î¸Î¿Î´Î¿ setup() (Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î¿ FoundryLocalManager)\n",
    "2. Î•ÎºÏ„ÎµÎ»Î­ÏƒÏ„Îµ Ï„Î·Î½ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· Î¼Î­ÏƒÏ‰ Ï„Î¿Ï… API ÏƒÏ…Î¼Î²Î±Ï„Î¿Ï Î¼Îµ OpenAI\n",
    "3. ÎšÎ±Ï„Î±Î³ÏÎ¬ÏˆÏ„Îµ Ï„Î·Î½ ÎºÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ·, Ï„Î± tokens ÎºÎ±Î¹ Ï„Î¿ Î´ÎµÎ¯Î³Î¼Î± ÎµÎ¾ÏŒÎ´Î¿Ï…\n",
    "4. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÏ„Îµ ÏƒÏÎ½Î¿ÏˆÎ· JSON Î¼Îµ ÏƒÏ…Î³ÎºÏÎ¹Ï„Î¹ÎºÎ® Î±Î½Î¬Î»Ï…ÏƒÎ·\n",
    "\n",
    "Î‘ÎºÎ¿Î»Î¿Ï…Î¸ÎµÎ¯ Ï„Î¿ Î¯Î´Î¹Î¿ Ï€ÏÏŒÏ„Ï…Ï€Î¿ ÏŒÏ€Ï‰Ï‚ Ï„Î± Î´ÎµÎ¯Î³Î¼Î±Ï„Î± Ï„Î¿Ï… Workshop ÏƒÏ„Î¿ session04/model_compare.py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6f12b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Connecting to 'phi-4-mini' (attempt 1/2)...\n",
      "[OK] Connected to 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[\n",
      "  {\n",
      "    \"alias\": \"phi-4-mini\",\n",
      "    \"status\": \"success\",\n",
      "    \"content\": \"1. Reduced Latency: Local AI inference can significantly reduce latency by processing data closer to the source, which is particularly beneficial for real-time applications such as autonomous vehicles or augmented reality.\\n\\n2. Enhanced Privacy: By keeping data processing local, sensitive information is less likely to be exposed to external networks, thereby enhancing privacy and security.\\n\\n3. Lower Bandwidth Usage: Local AI inference reduces the need for data transmission over the network, which can save bandwidth and reduce the risk of network congestion.\\n\\n4. Improved Reliability: Local processing can be more reliable, as it is less dependent on network connectivity. This is particularly important in scenarios where network connectivity is unreliable or intermittent.\\n\\n5. Scalability: Local AI inference can be easily scaled by adding more local processing units, making it easier to handle increasing data volumes or more complex AI models.\",\n",
      "    \"elapsed_sec\": 51.97519612312317,\n",
      "    \"tokens\": 247,\n",
      "    \"usage\": {\n",
      "      \"estimated_tokens\": 247,\n",
      "      \"estimated_prompt_tokens\": 9,\n",
      "      \"estimated_completion_tokens\": 238\n",
      "    },\n",
      "    \"model\": \"Phi-4-mini-instruct-cuda-gpu:4\"\n",
      "  },\n",
      "  {\n",
      "    \"alias\": \"qwen2.5-7b\",\n",
      "    \"status\": \"success\",\n",
      "    \"content\": \"Local AI inference offers several advantages over cloud-based or remote inference solutions. Here are five key benefits:\\n\\n1. **Latency Reduction**: Local AI inference reduces latency because the data does not need to be sent to a remote server for processing. This is particularly important in applications where real-time response is critical, such as autonomous vehicles, medical imaging, and real-time analytics.\\n\\n2. **Data Privacy and Security**: Processing data locally can enhance privacy and security by keeping sensitive information within the user's control. This is especially important in industries like healthcare, finance, and government where data breaches can have severe consequences.\\n\\n3. **Cost Efficiency**: For certain applications, local inference can be more cost-effective than cloud inference. While initial setup costs for hardware might be higher, ongoing costs for cloud services can add up over time, especially for high-frequency or large-scale operations.\\n\\n4. **Offline Capabilities**: Devices\",\n",
      "    \"elapsed_sec\": 329.4796121120453,\n",
      "    \"tokens\": 264,\n",
      "    \"usage\": {\n",
      "      \"estimated_tokens\": 264,\n",
      "      \"estimated_prompt_tokens\": 9,\n",
      "      \"estimated_completion_tokens\": 255\n",
      "    },\n",
      "    \"model\": \"qwen2.5-7b-instruct-cuda-gpu:3\"\n",
      "  }\n",
      "]\n",
      "\n",
      "================================================================================\n",
      "COMPARISON SUMMARY\n",
      "================================================================================\n",
      "Alias                Status          Latency(s)      Tokens         \n",
      "--------------------------------------------------------------------------------\n",
      "âœ… phi-4-mini         success         51.975          ~247 (est.)    \n",
      "âœ… qwen2.5-7b         success         329.480         ~264 (est.)    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Detailed Token Usage:\n",
      "\n",
      "  phi-4-mini:\n",
      "    Estimated prompt:     9\n",
      "    Estimated completion: 238\n",
      "    Estimated total:      247\n",
      "    (API did not provide token counts - using ~4 chars/token estimate)\n",
      "\n",
      "  qwen2.5-7b:\n",
      "    Estimated prompt:     9\n",
      "    Estimated completion: 255\n",
      "    Estimated total:      264\n",
      "    (API did not provide token counts - using ~4 chars/token estimate)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ SLM is 6.34x faster than LLM for this prompt\n",
      "   SLM throughput: 4.8 tokens/sec\n",
      "   LLM throughput: 0.8 tokens/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'alias': 'phi-4-mini',\n",
       "  'status': 'success',\n",
       "  'content': '1. Reduced Latency: Local AI inference can significantly reduce latency by processing data closer to the source, which is particularly beneficial for real-time applications such as autonomous vehicles or augmented reality.\\n\\n2. Enhanced Privacy: By keeping data processing local, sensitive information is less likely to be exposed to external networks, thereby enhancing privacy and security.\\n\\n3. Lower Bandwidth Usage: Local AI inference reduces the need for data transmission over the network, which can save bandwidth and reduce the risk of network congestion.\\n\\n4. Improved Reliability: Local processing can be more reliable, as it is less dependent on network connectivity. This is particularly important in scenarios where network connectivity is unreliable or intermittent.\\n\\n5. Scalability: Local AI inference can be easily scaled by adding more local processing units, making it easier to handle increasing data volumes or more complex AI models.',\n",
       "  'elapsed_sec': 51.97519612312317,\n",
       "  'tokens': 247,\n",
       "  'usage': {'estimated_tokens': 247,\n",
       "   'estimated_prompt_tokens': 9,\n",
       "   'estimated_completion_tokens': 238},\n",
       "  'model': 'Phi-4-mini-instruct-cuda-gpu:4'},\n",
       " {'alias': 'qwen2.5-7b',\n",
       "  'status': 'success',\n",
       "  'content': \"Local AI inference offers several advantages over cloud-based or remote inference solutions. Here are five key benefits:\\n\\n1. **Latency Reduction**: Local AI inference reduces latency because the data does not need to be sent to a remote server for processing. This is particularly important in applications where real-time response is critical, such as autonomous vehicles, medical imaging, and real-time analytics.\\n\\n2. **Data Privacy and Security**: Processing data locally can enhance privacy and security by keeping sensitive information within the user's control. This is especially important in industries like healthcare, finance, and government where data breaches can have severe consequences.\\n\\n3. **Cost Efficiency**: For certain applications, local inference can be more cost-effective than cloud inference. While initial setup costs for hardware might be higher, ongoing costs for cloud services can add up over time, especially for high-frequency or large-scale operations.\\n\\n4. **Offline Capabilities**: Devices\",\n",
       "  'elapsed_sec': 329.4796121120453,\n",
       "  'tokens': 264,\n",
       "  'usage': {'estimated_tokens': 264,\n",
       "   'estimated_prompt_tokens': 9,\n",
       "   'estimated_completion_tokens': 255},\n",
       "  'model': 'qwen2.5-7b-instruct-cuda-gpu:3'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "retries = 2  # Number of retry attempts\n",
    "\n",
    "for alias in (SLM, LLM):\n",
    "    mgr, client, mid, info = setup(alias, endpoint=ENDPOINT, retries=retries)\n",
    "    if client:\n",
    "        r = run(client, mid, PROMPT)\n",
    "        results.append({'alias': alias, **r})\n",
    "    else:\n",
    "        # If setup failed, record error\n",
    "        results.append({\n",
    "            'alias': alias,\n",
    "            'status': 'error',\n",
    "            'error': info.get('error', 'Setup failed'),\n",
    "            'elapsed_sec': 0,\n",
    "            'tokens': None,\n",
    "            'model': alias\n",
    "        })\n",
    "\n",
    "# Display results\n",
    "print(json.dumps(results, indent=2))\n",
    "\n",
    "# Quick comparative view\n",
    "print('\\n' + '='*80)\n",
    "print('COMPARISON SUMMARY')\n",
    "print('='*80)\n",
    "print(f\"{'Alias':<20} {'Status':<15} {'Latency(s)':<15} {'Tokens':<15}\")\n",
    "print('-'*80)\n",
    "\n",
    "for row in results:\n",
    "    status = row.get('status', 'unknown')\n",
    "    status_icon = 'âœ…' if status == 'success' else 'âŒ'\n",
    "    latency_str = f\"{row.get('elapsed_sec', 0):.3f}\" if row.get('elapsed_sec') else 'N/A'\n",
    "    \n",
    "    # Handle token display - show if available or indicate estimated\n",
    "    tokens = row.get('tokens')\n",
    "    usage = row.get('usage', {})\n",
    "    if tokens:\n",
    "        if 'estimated_tokens' in usage:\n",
    "            tokens_str = f\"~{tokens} (est.)\"\n",
    "        else:\n",
    "            tokens_str = str(tokens)\n",
    "    else:\n",
    "        tokens_str = 'N/A'\n",
    "    \n",
    "    print(f\"{status_icon} {row['alias']:<18} {status:<15} {latency_str:<15} {tokens_str:<15}\")\n",
    "\n",
    "print('-'*80)\n",
    "\n",
    "# Show detailed token breakdown if available\n",
    "print(\"\\nDetailed Token Usage:\")\n",
    "for row in results:\n",
    "    if row.get('status') == 'success' and row.get('usage'):\n",
    "        usage = row['usage']\n",
    "        print(f\"\\n  {row['alias']}:\")\n",
    "        if 'prompt_tokens' in usage and usage['prompt_tokens']:\n",
    "            print(f\"    Prompt tokens:     {usage['prompt_tokens']}\")\n",
    "            print(f\"    Completion tokens: {usage['completion_tokens']}\")\n",
    "            print(f\"    Total tokens:      {usage['total_tokens']}\")\n",
    "        elif 'estimated_tokens' in usage:\n",
    "            print(f\"    Estimated prompt:     {usage['estimated_prompt_tokens']}\")\n",
    "            print(f\"    Estimated completion: {usage['estimated_completion_tokens']}\")\n",
    "            print(f\"    Estimated total:      {usage['estimated_tokens']}\")\n",
    "            print(f\"    (API did not provide token counts - using ~4 chars/token estimate)\")\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "\n",
    "# Calculate speedup if both succeeded\n",
    "if len(results) == 2 and all(r.get('status') == 'success' and r.get('elapsed_sec') for r in results):\n",
    "    speedup = results[1]['elapsed_sec'] / results[0]['elapsed_sec']\n",
    "    print(f\"\\nğŸ’¡ SLM is {speedup:.2f}x faster than LLM for this prompt\")\n",
    "    \n",
    "    # Compare token throughput if available\n",
    "    slm_tokens = results[0].get('tokens', 0)\n",
    "    llm_tokens = results[1].get('tokens', 0)\n",
    "    if slm_tokens and llm_tokens:\n",
    "        slm_tps = slm_tokens / results[0]['elapsed_sec']\n",
    "        llm_tps = llm_tokens / results[1]['elapsed_sec']\n",
    "        print(f\"   SLM throughput: {slm_tps:.1f} tokens/sec\")\n",
    "        print(f\"   LLM throughput: {llm_tps:.1f} tokens/sec\")\n",
    "        \n",
    "elif any(r.get('status') == 'error' for r in results):\n",
    "    print(f\"\\nâš ï¸  Some models failed - check errors above\")\n",
    "    print(\"   Ensure Foundry Local is running: foundry service start\")\n",
    "    print(\"   Ensure models are loaded: foundry model run <model-name>\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d052c",
   "metadata": {},
   "source": [
    "### Î•ÏÎ¼Î·Î½ÎµÎ¯Î± Î‘Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½\n",
    "\n",
    "**Î’Î±ÏƒÎ¹ÎºÎ¿Î¯ Î”ÎµÎ¯ÎºÏ„ÎµÏ‚:**\n",
    "- **ÎšÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ·**: ÎŒÏƒÎ¿ Ï‡Î±Î¼Î·Î»ÏŒÏ„ÎµÏÎ·, Ï„ÏŒÏƒÎ¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ± - Ï…Ï€Î¿Î´ÎµÎ¹ÎºÎ½ÏÎµÎ¹ Ï„Î±Ï‡ÏÏ„ÎµÏÎ¿ Ï‡ÏÏŒÎ½Î¿ Î±Ï€ÏŒÎºÏÎ¹ÏƒÎ·Ï‚\n",
    "- **Tokens**: Î¥ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ· Î±Ï€ÏŒÎ´Î¿ÏƒÎ· = Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ± tokens ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¼Î­Î½Î±\n",
    "- **Î”Î¹Î±Î´ÏÎ¿Î¼Î®**: Î•Ï€Î¹Î²ÎµÎ²Î±Î¹ÏÎ½ÎµÎ¹ Ï€Î¿Î¹Î¿ API endpoint Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®Î¸Î·ÎºÎµ\n",
    "\n",
    "**Î ÏŒÏ„Îµ Î½Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎµÏ„Îµ SLM Î±Î½Ï„Î¯ Î³Î¹Î± LLM:**\n",
    "- **SLM (ÎœÎ¹ÎºÏÏŒ Î“Î»Ï‰ÏƒÏƒÎ¹ÎºÏŒ ÎœÎ¿Î½Ï„Î­Î»Î¿)**: Î“ÏÎ®Î³Î¿ÏÎµÏ‚ Î±Ï€Î¿ÎºÏÎ¯ÏƒÎµÎ¹Ï‚, Ï‡Î±Î¼Î·Î»ÏŒÏ„ÎµÏÎ· Ï‡ÏÎ®ÏƒÎ· Ï€ÏŒÏÏ‰Î½, ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î¿ Î³Î¹Î± Î±Ï€Î»Î­Ï‚ ÎµÏÎ³Î±ÏƒÎ¯ÎµÏ‚\n",
    "- **LLM (ÎœÎµÎ³Î¬Î»Î¿ Î“Î»Ï‰ÏƒÏƒÎ¹ÎºÏŒ ÎœÎ¿Î½Ï„Î­Î»Î¿)**: Î¥ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ· Ï€Î¿Î¹ÏŒÏ„Î·Ï„Î±, ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î»Î¿Î³Î¹ÎºÎ®, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Ï„Î¿ ÏŒÏ„Î±Î½ Î· Ï€Î¿Î¹ÏŒÏ„Î·Ï„Î± Î­Ï‡ÎµÎ¹ Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ· ÏƒÎ·Î¼Î±ÏƒÎ¯Î±\n",
    "\n",
    "**Î•Ï€ÏŒÎ¼ÎµÎ½Î± Î’Î®Î¼Î±Ï„Î±:**\n",
    "1. Î”Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Îµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ¬ prompts Î³Î¹Î± Î½Î± Î´ÎµÎ¯Ï„Îµ Ï€ÏÏ‚ Î· Ï€Î¿Î»Ï…Ï€Î»Î¿ÎºÏŒÏ„Î·Ï„Î± ÎµÏ€Î·ÏÎµÎ¬Î¶ÎµÎ¹ Ï„Î· ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ·\n",
    "2. Î ÎµÎ¹ÏÎ±Î¼Î±Ï„Î¹ÏƒÏ„ÎµÎ¯Ï„Îµ Î¼Îµ Î¬Î»Î»Î± Î¶ÎµÏÎ³Î· Î¼Î¿Î½Ï„Î­Î»Ï‰Î½\n",
    "3. Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Ï„Î± Î´ÎµÎ¯Î³Î¼Î±Ï„Î± Î´ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚ Workshop (Î£Ï…Î½ÎµÎ´ÏÎ¯Î± 06) Î³Î¹Î± Î­Î¾Ï…Ï€Î½Î· Î´ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ· Î²Î¬ÏƒÎµÎ¹ Ï„Î·Ï‚ Ï€Î¿Î»Ï…Ï€Î»Î¿ÎºÏŒÏ„Î·Ï„Î±Ï‚ Ï„Î·Ï‚ ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8caed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION SUMMARY\n",
      "======================================================================\n",
      "âœ… SLM Model: phi-4-mini\n",
      "âœ… LLM Model: qwen2.5-7b\n",
      "âœ… Using Foundry SDK Pattern: workshop_utils with FoundryLocalManager\n",
      "âœ… Pre-flight passed: True\n",
      "âœ… Comparison completed: True\n",
      "âœ… Both models responded: True\n",
      "======================================================================\n",
      "ğŸ‰ ALL CHECKS PASSED! Notebook completed successfully.\n",
      "   SLM (phi-4-mini) vs LLM (qwen2.5-7b) comparison completed.\n",
      "   Performance: SLM is 5.14x faster\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Validation Check\n",
    "print(\"=\"*70)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"âœ… SLM Model: {SLM}\")\n",
    "print(f\"âœ… LLM Model: {LLM}\")\n",
    "print(f\"âœ… Using Foundry SDK Pattern: workshop_utils with FoundryLocalManager\")\n",
    "print(f\"âœ… Pre-flight passed: {all(v['status'] == 'success' for v in preflight.values()) if 'preflight' in dir() else 'Not run yet'}\")\n",
    "print(f\"âœ… Comparison completed: {len(results) == 2 if 'results' in dir() else 'Not run yet'}\")\n",
    "print(f\"âœ… Both models responded: {all(r.get('status') == 'success' for r in results) if 'results' in dir() and results else 'Not run yet'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for common configuration issues\n",
    "issues = []\n",
    "if 'LLM' in dir() and LLM not in ['qwen2.5-3b', 'qwen2.5-0.5b', 'qwen2.5-1.5b', 'qwen2.5-7b', 'phi-3.5-mini']:\n",
    "    issues.append(f\"âš ï¸  LLM is '{LLM}' - expected qwen2.5-3b for memory efficiency\")\n",
    "if 'preflight' in dir() and not all(v['status'] == 'success' for v in preflight.values()):\n",
    "    issues.append(\"âš ï¸  Pre-flight check failed - models not accessible\")\n",
    "if 'results' in dir() and results and not all(r.get('status') == 'success' for r in results):\n",
    "    issues.append(\"âš ï¸  Comparison incomplete - check for errors above\")\n",
    "\n",
    "if not issues and 'results' in dir() and results and all(r.get('status') == 'success' for r in results):\n",
    "    print(\"ğŸ‰ ALL CHECKS PASSED! Notebook completed successfully.\")\n",
    "    print(f\"   SLM ({SLM}) vs LLM ({LLM}) comparison completed.\")\n",
    "    if len(results) == 2:\n",
    "        speedup = results[1]['elapsed_sec'] / results[0]['elapsed_sec'] if results[0]['elapsed_sec'] > 0 else 0\n",
    "        print(f\"   Performance: SLM is {speedup:.2f}x faster\")\n",
    "elif issues:\n",
    "    print(\"\\nâš ï¸  Issues detected:\")\n",
    "    for issue in issues:\n",
    "        print(f\"   {issue}\")\n",
    "    print(\"\\nğŸ’¡ Troubleshooting:\")\n",
    "    print(\"   1. Ensure service is running: foundry service start\")\n",
    "    print(\"   2. Load models: foundry model run phi-4-mini && foundry model run qwen2.5-7b\")\n",
    "    print(\"   3. Check model list: foundry model ls\")\n",
    "else:\n",
    "    print(\"\\nğŸ’¡ Run all cells above first, then re-run this validation.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Î‘Ï€Î¿Ï€Î¿Î¯Î·ÏƒÎ· ÎµÏ…Î¸ÏÎ½Î·Ï‚**:  \nÎ‘Ï…Ï„ÏŒ Ï„Î¿ Î­Î³Î³ÏÎ±Ï†Î¿ Î­Ï‡ÎµÎ¹ Î¼ÎµÏ„Î±Ï†ÏÎ±ÏƒÏ„ÎµÎ¯ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î·Î½ Ï…Ï€Î·ÏÎµÏƒÎ¯Î± Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î·Ï‚ Î¼ÎµÏ„Î¬Ï†ÏÎ±ÏƒÎ·Ï‚ [Co-op Translator](https://github.com/Azure/co-op-translator). Î Î±ÏÏŒÎ»Î¿ Ï€Î¿Ï… ÎºÎ±Ï„Î±Î²Î¬Î»Î»Î¿Ï…Î¼Îµ Ï€ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹ÎµÏ‚ Î³Î¹Î± Î±ÎºÏÎ¯Î²ÎµÎ¹Î±, Ï€Î±ÏÎ±ÎºÎ±Î»Î¿ÏÎ¼Îµ Î½Î± Î³Î½Ï‰ÏÎ¯Î¶ÎµÏ„Îµ ÏŒÏ„Î¹ Î¿Î¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„ÎµÏ‚ Î¼ÎµÏ„Î±Ï†ÏÎ¬ÏƒÎµÎ¹Ï‚ ÎµÎ½Î´Î­Ï‡ÎµÏ„Î±Î¹ Î½Î± Ï€ÎµÏÎ¹Î­Ï‡Î¿Ï…Î½ Î»Î¬Î¸Î· Î® Î±Î½Î±ÎºÏÎ¯Î²ÎµÎ¹ÎµÏ‚. Î¤Î¿ Ï€ÏÏ‰Ï„ÏŒÏ„Ï…Ï€Î¿ Î­Î³Î³ÏÎ±Ï†Î¿ ÏƒÏ„Î· Î¼Î·Ï„ÏÎ¹ÎºÎ® Ï„Î¿Ï… Î³Î»ÏÏƒÏƒÎ± Î¸Î± Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î¸ÎµÏ‰ÏÎµÎ¯Ï„Î±Î¹ Î· Î±Ï…Î¸ÎµÎ½Ï„Î¹ÎºÎ® Ï€Î·Î³Î®. Î“Î¹Î± ÎºÏÎ¯ÏƒÎ¹Î¼ÎµÏ‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚, ÏƒÏ…Î½Î¹ÏƒÏ„Î¬Ï„Î±Î¹ ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÎ® Î±Î½Î¸ÏÏÏ€Î¹Î½Î· Î¼ÎµÏ„Î¬Ï†ÏÎ±ÏƒÎ·. Î”ÎµÎ½ Ï†Î­ÏÎ¿Ï…Î¼Îµ ÎµÏ…Î¸ÏÎ½Î· Î³Î¹Î± Ï„Ï…Ï‡ÏŒÎ½ Ï€Î±ÏÎµÎ¾Î·Î³Î®ÏƒÎµÎ¹Ï‚ Î® ÎµÏƒÏ†Î±Î»Î¼Î­Î½ÎµÏ‚ ÎµÏÎ¼Î·Î½ÎµÎ¯ÎµÏ‚ Ï€Î¿Ï… Ï€ÏÎ¿ÎºÏÏ€Ï„Î¿Ï…Î½ Î±Ï€ÏŒ Ï„Î· Ï‡ÏÎ®ÏƒÎ· Î±Ï…Ï„Î®Ï‚ Ï„Î·Ï‚ Î¼ÎµÏ„Î¬Ï†ÏÎ±ÏƒÎ·Ï‚.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "8220e33feaffbd35ece0f93e8214c24f",
   "translation_date": "2025-10-09T13:32:15+00:00",
   "source_file": "Workshop/notebooks/session04_model_compare.ipynb",
   "language_code": "el"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}