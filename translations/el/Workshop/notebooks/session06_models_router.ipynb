{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af028554",
   "metadata": {},
   "source": [
    "# Î”ÏÎ¿Î¼Î¿Î»Î¿Î³Î·Ï„Î®Ï‚ ÎœÎ¿Î½Ï„Î­Î»Î¿Ï… Î’Î±ÏƒÎ¹ÏƒÎ¼Î­Î½Î¿Ï‚ ÏƒÏ„Î·Î½ Î ÏÏŒÎ¸ÎµÏƒÎ· Î¼Îµ Ï„Î¿ Foundry Local SDK\n",
    "\n",
    "**Î£ÏÏƒÏ„Î·Î¼Î± Î”ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚ Î Î¿Î»Î»Î±Ï€Î»ÏÎ½ ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½ Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿ Î³Î¹Î± CPU**\n",
    "\n",
    "Î‘Ï…Ï„ÏŒ Ï„Î¿ ÏƒÎ·Î¼ÎµÎ¹Ï‰Î¼Î±Ï„Î¬ÏÎ¹Î¿ Ï€Î±ÏÎ¿Ï…ÏƒÎ¹Î¬Î¶ÎµÎ¹ Î­Î½Î± Î­Î¾Ï…Ï€Î½Î¿ ÏƒÏÏƒÏ„Î·Î¼Î± Î´ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚ Ï€Î¿Ï… ÎµÏ€Î¹Î»Î­Î³ÎµÎ¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± Ï„Î¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ Î¼Î¹ÎºÏÏŒ Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÏŒ Î¼Î¿Î½Ï„Î­Î»Î¿ Î²Î¬ÏƒÎµÎ¹ Ï„Î·Ï‚ Ï€ÏÏŒÎ¸ÎµÏƒÎ·Ï‚ Ï„Î¿Ï… Ï‡ÏÎ®ÏƒÏ„Î·. Î™Î´Î±Î½Î¹ÎºÏŒ Î³Î¹Î± ÏƒÎµÎ½Î¬ÏÎ¹Î± Î±Î½Î¬Ï€Ï„Ï…Î¾Î·Ï‚ ÏƒÎµ Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î± edge, ÏŒÏ€Î¿Ï… Î¸Î­Î»ÎµÏ„Îµ Î½Î± Î±Î¾Î¹Î¿Ï€Î¿Î¹Î®ÏƒÎµÏ„Îµ Ï€Î¿Î»Î»Î±Ï€Î»Î¬ ÎµÎ¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î± Î¼Î¿Î½Ï„Î­Î»Î± Î¼Îµ Î±Ï€Î¿Î´Î¿Ï„Î¹ÎºÏŒÏ„Î·Ï„Î±.\n",
    "\n",
    "## ğŸ¯ Î¤Î¹ Î˜Î± ÎœÎ¬Î¸ÎµÏ„Îµ\n",
    "\n",
    "- **Î‘Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Î ÏÏŒÎ¸ÎµÏƒÎ·Ï‚**: Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· Ï€ÏÎ¿Ï„ÏÎ¿Ï€ÏÎ½ (ÎºÏÎ´Î¹ÎºÎ±Ï‚, Ï€ÎµÏÎ¯Î»Î·ÏˆÎ·, Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ·, Î³ÎµÎ½Î¹ÎºÎ¬)\n",
    "- **ÎˆÎ¾Ï…Ï€Î½Î· Î•Ï€Î¹Î»Î¿Î³Î® ÎœÎ¿Î½Ï„Î­Î»Î¿Ï…**: Î”ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ· ÏƒÏ„Î¿ Ï€Î¹Î¿ Î¹ÎºÎ±Î½ÏŒ Î¼Î¿Î½Ï„Î­Î»Î¿ Î³Î¹Î± ÎºÎ¬Î¸Îµ ÎµÏÎ³Î±ÏƒÎ¯Î±\n",
    "- **Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· CPU**: ÎœÎ¿Î½Ï„Î­Î»Î± Î¼Îµ Î±Ï€Î¿Î´Î¿Ï„Î¹ÎºÎ® Ï‡ÏÎ®ÏƒÎ· Î¼Î½Î®Î¼Î·Ï‚ Ï€Î¿Ï… Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¿ÏÎ½ ÏƒÎµ Î¿Ï€Î¿Î¹Î¿Î´Î®Ï€Î¿Ï„Îµ Ï…Î»Î¹ÎºÏŒ\n",
    "- **Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î Î¿Î»Î»Î±Ï€Î»ÏÎ½ ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½**: Î”Î¹Î±Ï„Î®ÏÎ·ÏƒÎ· Ï€Î¿Î»Î»Î±Ï€Î»ÏÎ½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Ï†Î¿ÏÏ„Ï‰Î¼Î­Î½Ï‰Î½ Î¼Îµ `--retain true`\n",
    "- **Î Î±ÏÎ±Î³Ï‰Î³Î¹ÎºÎ¬ Î ÏÏŒÏ„Ï…Ï€Î±**: Î›Î¿Î³Î¹ÎºÎ® ÎµÏ€Î±Î½Î±Ï€ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹Î±Ï‚, Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½ ÎºÎ±Î¹ Ï€Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· tokens\n",
    "\n",
    "## ğŸ“‹ Î•Ï€Î¹ÏƒÎºÏŒÏ€Î·ÏƒÎ· Î£ÎµÎ½Î±ÏÎ¯Î¿Ï…\n",
    "\n",
    "Î‘Ï…Ï„ÏŒ Ï„Î¿ Ï€ÏÏŒÏ„Ï…Ï€Î¿ Ï€Î±ÏÎ¿Ï…ÏƒÎ¹Î¬Î¶ÎµÎ¹:\n",
    "\n",
    "1. **Î‘Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Î ÏÏŒÎ¸ÎµÏƒÎ·Ï‚**: Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· ÎºÎ¬Î¸Îµ Ï€ÏÎ¿Ï„ÏÎ¿Ï€Î®Ï‚ Ï‡ÏÎ®ÏƒÏ„Î· (ÎºÏÎ´Î¹ÎºÎ±Ï‚, Ï€ÎµÏÎ¯Î»Î·ÏˆÎ·, Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· Î® Î³ÎµÎ½Î¹ÎºÎ¬)\n",
    "2. **Î•Ï€Î¹Î»Î¿Î³Î® ÎœÎ¿Î½Ï„Î­Î»Î¿Ï…**: Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· ÎµÏ€Î¹Î»Î¿Î³Î® Ï„Î¿Ï… Ï€Î¹Î¿ ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î¿Ï… Î¼Î¹ÎºÏÎ¿Ï Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ¿Ï Î¼Î¿Î½Ï„Î­Î»Î¿Ï… Î²Î¬ÏƒÎµÎ¹ Î´Ï…Î½Î±Ï„Î¿Ï„Î®Ï„Ï‰Î½\n",
    "3. **Î¤Î¿Ï€Î¹ÎºÎ® Î•ÎºÏ„Î­Î»ÎµÏƒÎ·**: Î”ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ· ÏƒÎµ Î¼Î¿Î½Ï„Î­Î»Î± Ï€Î¿Ï… ÎµÎºÏ„ÎµÎ»Î¿ÏÎ½Ï„Î±Î¹ Ï„Î¿Ï€Î¹ÎºÎ¬ Î¼Î­ÏƒÏ‰ Ï„Î·Ï‚ Ï…Ï€Î·ÏÎµÏƒÎ¯Î±Ï‚ Foundry Local\n",
    "4. **Î•Î½Î¹Î±Î¯Î± Î”Î¹ÎµÏ€Î±Ï†Î®**: Î•Î½Î¹Î±Î¯Î¿ ÏƒÎ·Î¼ÎµÎ¯Î¿ ÎµÎ¹ÏƒÏŒÎ´Î¿Ï… ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¯Î±Ï‚ Ï€Î¿Ï… Î´ÏÎ¿Î¼Î¿Î»Î¿Î³ÎµÎ¯ ÏƒÎµ Ï€Î¿Î»Î»Î±Ï€Î»Î¬ ÎµÎ¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î± Î¼Î¿Î½Ï„Î­Î»Î±\n",
    "\n",
    "**Î™Î´Î±Î½Î¹ÎºÏŒ Î³Î¹Î±**: Î‘Î½Î±Ï€Ï„ÏÎ¾ÎµÎ¹Ï‚ ÏƒÎµ Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î± edge Î¼Îµ Ï€Î¿Î»Î»Î±Ï€Î»Î¬ ÎµÎ¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î± Î¼Î¿Î½Ï„Î­Î»Î±, ÏŒÏ€Î¿Ï… Î¸Î­Î»ÎµÏ„Îµ Î­Î¾Ï…Ï€Î½Î· Î´ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ· Î±Î¹Ï„Î·Î¼Î¬Ï„Ï‰Î½ Ï‡Ï‰ÏÎ¯Ï‚ Ï‡ÎµÎ¹ÏÎ¿ÎºÎ¯Î½Î·Ï„Î· ÎµÏ€Î¹Î»Î¿Î³Î® Î¼Î¿Î½Ï„Î­Î»Î¿Ï….\n",
    "\n",
    "## ğŸ”§ Î ÏÎ¿Î±Ï€Î±Î¹Ï„Î¿ÏÎ¼ÎµÎ½Î±\n",
    "\n",
    "- Î•Î³ÎºÎ±Ï„ÎµÏƒÏ„Î·Î¼Î­Î½Î¿ ÎºÎ±Î¹ ÎµÎ½ÎµÏÎ³ÏŒ **Foundry Local**\n",
    "- **Python 3.8+** Î¼Îµ pip\n",
    "- **8GB+ RAM** (ÏƒÏ…Î½Î¹ÏƒÏ„Î¬Ï„Î±Î¹ 16GB+ Î³Î¹Î± Ï€Î¿Î»Î»Î±Ï€Î»Î¬ Î¼Î¿Î½Ï„Î­Î»Î±)\n",
    "- Î¤Î¿ module **workshop_utils** (ÏƒÏ„Î¿ ../samples/)\n",
    "\n",
    "## ğŸš€ Î“ÏÎ®Î³Î¿ÏÎ· Î•ÎºÎºÎ¯Î½Î·ÏƒÎ·\n",
    "\n",
    "Î¤Î¿ ÏƒÎ·Î¼ÎµÎ¹Ï‰Î¼Î±Ï„Î¬ÏÎ¹Î¿ Î¸Î±:\n",
    "1. Î•Î½Ï„Î¿Ï€Î¯ÏƒÎµÎ¹ Ï„Î· Î¼Î½Î®Î¼Î· Ï„Î¿Ï… ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„ÏŒÏ‚ ÏƒÎ±Ï‚\n",
    "2. Î ÏÎ¿Ï„ÎµÎ¯Î½ÎµÎ¹ ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î± Î¼Î¿Î½Ï„Î­Î»Î± CPU\n",
    "3. Î¦Î¿ÏÏ„ÏÏƒÎµÎ¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± Î¼Îµ `--retain true`\n",
    "4. Î•Ï€Î±Î»Î·Î¸ÎµÏÏƒÎµÎ¹ ÏŒÏ„Î¹ ÏŒÎ»Î± Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± ÎµÎ¯Î½Î±Î¹ Î­Ï„Î¿Î¹Î¼Î±\n",
    "5. Î”ÏÎ¿Î¼Î¿Î»Î¿Î³Î®ÏƒÎµÎ¹ Î´Î¿ÎºÎ¹Î¼Î±ÏƒÏ„Î¹ÎºÎ­Ï‚ Ï€ÏÎ¿Ï„ÏÎ¿Ï€Î­Ï‚ ÏƒÎµ ÎµÎ¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î± Î¼Î¿Î½Ï„Î­Î»Î±\n",
    "\n",
    "**Î•ÎºÏ„Î¹Î¼ÏÎ¼ÎµÎ½Î¿Ï‚ Ï‡ÏÏŒÎ½Î¿Ï‚ ÎµÎ³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ·Ï‚**: 5-7 Î»ÎµÏ€Ï„Î¬ (Ï€ÎµÏÎ¹Î»Î±Î¼Î²Î¬Î½ÎµÎ¹ Ï„Î· Ï†ÏŒÏÏ„Ï‰ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Ï‰Î½)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa55f0",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Î’Î®Î¼Î± 1: Î•Î³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Î•Î¾Î±ÏÏ„Î®ÏƒÎµÏ‰Î½\n",
    "\n",
    "Î•Î³ÎºÎ±Ï„Î±ÏƒÏ„Î®ÏƒÏ„Îµ Ï„Î¿ ÎµÏ€Î¯ÏƒÎ·Î¼Î¿ Foundry Local SDK ÎºÎ±Î¹ Ï„Î¹Ï‚ Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„ÎµÏ‚ Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚:\n",
    "\n",
    "- **foundry-local-sdk**: Î•Ï€Î¯ÏƒÎ·Î¼Î¿ Python SDK Î³Î¹Î± Ï„Î· Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Ï„Î¿Ï€Î¹ÎºÏÎ½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½\n",
    "- **openai**: API ÏƒÏ…Î¼Î²Î±Ï„ÏŒ Î¼Îµ OpenAI Î³Î¹Î± ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¯ÎµÏ‚\n",
    "- **psutil**: Î‘Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· ÎºÎ±Î¹ Ï€Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· Î¼Î½Î®Î¼Î·Ï‚ ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î¿Ï‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2929c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "!pip install -q foundry-local-sdk openai psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990799e7",
   "metadata": {},
   "source": [
    "## ğŸ’» Î’Î®Î¼Î± 2: Î‘Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· ÎœÎ½Î®Î¼Î·Ï‚ Î£Ï…ÏƒÏ„Î®Î¼Î±Ï„Î¿Ï‚\n",
    "\n",
    "Î‘Î½Î¹Ï‡Î½ÎµÏÏƒÏ„Îµ Ï„Î· Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î· Î¼Î½Î®Î¼Î· Ï„Î¿Ï… ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î¿Ï‚ Î³Î¹Î± Î½Î± ÎºÎ±Î¸Î¿ÏÎ¯ÏƒÎµÏ„Îµ Ï€Î¿Î¹Î± Î¼Î¿Î½Ï„Î­Î»Î± CPU Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¿ÏÎ½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î±Ï„Î¹ÎºÎ¬. Î‘Ï…Ï„ÏŒ ÎµÎ¾Î±ÏƒÏ†Î±Î»Î¯Î¶ÎµÎ¹ Ï„Î·Î½ Î¹Î´Î±Î½Î¹ÎºÎ® ÎµÏ€Î¹Î»Î¿Î³Î® Î¼Î¿Î½Ï„Î­Î»Î¿Ï… Î³Î¹Î± Ï„Î¿ Ï…Î»Î¹ÎºÏŒ ÏƒÎ±Ï‚.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ff58f1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸  System Memory Information\n",
      "======================================================================\n",
      "Total Memory:     63.30 GB\n",
      "Available Memory: 16.19 GB\n",
      "\n",
      "âœ… High Memory System (32GB+)\n",
      "   Can run 3-4 models simultaneously\n",
      "\n",
      "ğŸ“‹ Recommended Model Aliases for Your System:\n",
      "   â€¢ phi-4-mini\n",
      "   â€¢ phi-3.5-mini\n",
      "   â€¢ qwen2.5-0.5b\n",
      "   â€¢ qwen2.5-coder-0.5b\n",
      "\n",
      "ğŸ’¡ About Model Aliases:\n",
      "   âœ“ Use base alias (e.g., phi-4-mini, not phi-4-mini-cpu)\n",
      "   âœ“ Foundry Local automatically selects CPU variant for your hardware\n",
      "   âœ“ No GPU required - optimized for CPU inference\n",
      "   âœ“ Predictable memory usage and consistent performance\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get system memory information\n",
    "total_memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "\n",
    "print('ğŸ–¥ï¸  System Memory Information')\n",
    "print('=' * 70)\n",
    "print(f'Total Memory:     {total_memory_gb:.2f} GB')\n",
    "print(f'Available Memory: {available_memory_gb:.2f} GB')\n",
    "print()\n",
    "\n",
    "# Recommend models based on available memory\n",
    "# Using model aliases - Foundry Local will automatically select CPU variant\n",
    "model_aliases = []\n",
    "\n",
    "if total_memory_gb >= 32:\n",
    "    model_aliases = ['phi-4-mini', 'phi-3.5-mini', 'qwen2.5-0.5b', 'qwen2.5-coder-0.5b']\n",
    "    print('âœ… High Memory System (32GB+)')\n",
    "    print('   Can run 3-4 models simultaneously')\n",
    "elif total_memory_gb >= 16:\n",
    "    model_aliases = ['phi-4-mini', 'qwen2.5-0.5b', 'phi-3.5-mini']\n",
    "    print('âœ… Medium Memory System (16-32GB)')\n",
    "    print('   Can run 2-3 models simultaneously')\n",
    "elif total_memory_gb >= 8:\n",
    "    model_aliases = ['qwen2.5-0.5b', 'phi-3.5-mini']\n",
    "    print('âš ï¸  Lower Memory System (8-16GB)')\n",
    "    print('   Recommended: 2 smaller models')\n",
    "else:\n",
    "    model_aliases = ['qwen2.5-0.5b']\n",
    "    print('âš ï¸  Limited Memory System (<8GB)')\n",
    "    print('   Recommended: Use only smallest model')\n",
    "\n",
    "print()\n",
    "print('ğŸ“‹ Recommended Model Aliases for Your System:')\n",
    "for model in model_aliases:\n",
    "    print(f'   â€¢ {model}')\n",
    "\n",
    "print()\n",
    "print('ğŸ’¡ About Model Aliases:')\n",
    "print('   âœ“ Use base alias (e.g., phi-4-mini, not phi-4-mini-cpu)')\n",
    "print('   âœ“ Foundry Local automatically selects CPU variant for your hardware')\n",
    "print('   âœ“ No GPU required - optimized for CPU inference')\n",
    "print('   âœ“ Predictable memory usage and consistent performance')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69590b94",
   "metadata": {},
   "source": [
    "## ğŸ¤– Î’Î®Î¼Î± 3: Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½\n",
    "\n",
    "Î‘Ï…Ï„ÏŒ Ï„Î¿ ÎºÎµÎ»Î¯ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î±:\n",
    "1. ÎÎµÎºÎ¹Î½Î¬ Ï„Î·Î½ Ï…Ï€Î·ÏÎµÏƒÎ¯Î± Foundry Local (Î±Î½ Î´ÎµÎ½ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯)\n",
    "2. Î¦Î¿ÏÏ„ÏÎ½ÎµÎ¹ Ï„Î± Ï€ÏÎ¿Ï„ÎµÎ¹Î½ÏŒÎ¼ÎµÎ½Î± Î¼Î¿Î½Ï„Î­Î»Î± Î¼Îµ `--retain true` (Î´Î¹Î±Ï„Î·ÏÎµÎ¯ Ï€Î¿Î»Î»Î±Ï€Î»Î¬ Î¼Î¿Î½Ï„Î­Î»Î± ÏƒÏ„Î· Î¼Î½Î®Î¼Î·)\n",
    "3. Î•Ï€Î±Î»Î·Î¸ÎµÏÎµÎ¹ ÏŒÏ„Î¹ ÏŒÎ»Î± Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± ÎµÎ¯Î½Î±Î¹ Î­Ï„Î¿Î¹Î¼Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î¿ SDK\n",
    "\n",
    "â±ï¸ **Î‘Î½Î±Î¼ÎµÎ½ÏŒÎ¼ÎµÎ½Î¿Ï‚ Ï‡ÏÏŒÎ½Î¿Ï‚**: 3-5 Î»ÎµÏ€Ï„Î¬ Î³Î¹Î± ÏŒÎ»Î± Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "543fd976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Automatic Model Loading with SDK Verification\n",
      "======================================================================\n",
      "ğŸ“‹ Loading 3 models: ['phi-4-mini', 'phi-3.5-mini', 'qwen2.5-0.5b']\n",
      "ğŸ’¡ Using model aliases - Foundry will load CPU variants automatically\n",
      "\n",
      "ğŸ“¡ Step 1: Checking Foundry Local service...\n",
      "   âœ… Service is already running\n",
      "\n",
      "ğŸ¤– Step 2: Loading models with retention...\n",
      "   [1/3] Starting phi-4-mini...\n",
      "       âœ… phi-4-mini loading in background\n",
      "   [2/3] Starting phi-3.5-mini...\n",
      "       âœ… phi-3.5-mini loading in background\n",
      "   [3/3] Starting qwen2.5-0.5b...\n",
      "       âœ… qwen2.5-0.5b loading in background\n",
      "\n",
      "âœ… Step 3: Verifying models (this may take 2-3 minutes)...\n",
      "======================================================================\n",
      "\n",
      "   Attempt 1/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 2/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 3/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 4/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 5/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 6/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 7/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 8/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 9/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 10/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 11/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 12/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 13/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 14/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 15/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 16/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 17/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 18/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 19/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 20/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 21/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 22/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 23/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 24/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 25/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 26/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 27/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 28/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 29/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 30/30...\n",
      "   âš ï¸  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   âš ï¸  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "======================================================================\n",
      "ğŸ“¦ Final Status: 0/3 models ready\n",
      "   âŒ phi-4-mini - NOT READY\n",
      "   âŒ phi-3.5-mini - NOT READY\n",
      "   âŒ qwen2.5-0.5b - NOT READY\n",
      "\n",
      "âš ï¸  Some models not ready. Check: foundry model ls\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add samples directory for workshop_utils (Foundry SDK pattern)\n",
    "sys.path.append(os.path.join('..', 'samples'))\n",
    "\n",
    "print('ğŸš€ Automatic Model Loading with SDK Verification')\n",
    "print('=' * 70)\n",
    "\n",
    "# Use top 3 recommended models (aliases)\n",
    "# Foundry will automatically load CPU variants\n",
    "REQUIRED_MODELS = model_aliases[:3]\n",
    "print(f'ğŸ“‹ Loading {len(REQUIRED_MODELS)} models: {REQUIRED_MODELS}')\n",
    "print('ğŸ’¡ Using model aliases - Foundry will load CPU variants automatically')\n",
    "print()\n",
    "\n",
    "# Step 1: Ensure Foundry Local service is running\n",
    "print('ğŸ“¡ Step 1: Checking Foundry Local service...')\n",
    "try:\n",
    "    result = subprocess.run(['foundry', 'service', 'status'], \n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print('   âœ… Service is already running')\n",
    "    else:\n",
    "        print('   âš™ï¸  Starting Foundry Local service...')\n",
    "        subprocess.run(['foundry', 'service', 'start'], \n",
    "                      capture_output=True, text=True, timeout=30)\n",
    "        time.sleep(5)\n",
    "        print('   âœ… Service started')\n",
    "except Exception as e:\n",
    "    print(f'   âš ï¸  Could not verify service: {e}')\n",
    "    print('   ğŸ’¡ Try manually: foundry service start')\n",
    "\n",
    "# Step 2: Load each model with --retain true\n",
    "print(f'\\nğŸ¤– Step 2: Loading models with retention...')\n",
    "for i, model in enumerate(REQUIRED_MODELS, 1):\n",
    "    print(f'   [{i}/{len(REQUIRED_MODELS)}] Starting {model}...')\n",
    "    try:\n",
    "        subprocess.Popen(['foundry', 'model', 'run', model, '--retain', 'true'],\n",
    "                        stdout=subprocess.DEVNULL,\n",
    "                        stderr=subprocess.DEVNULL)\n",
    "        print(f'       âœ… {model} loading in background')\n",
    "    except Exception as e:\n",
    "        print(f'       âŒ Error starting {model}: {e}')\n",
    "\n",
    "# Step 3: Verify models are ready\n",
    "print(f'\\nâœ… Step 3: Verifying models (this may take 2-3 minutes)...')\n",
    "print('=' * 70)\n",
    "\n",
    "try:\n",
    "    from workshop_utils import get_client\n",
    "    \n",
    "    ready_models = []\n",
    "    max_attempts = 30\n",
    "    attempt = 0\n",
    "    \n",
    "    while len(ready_models) < len(REQUIRED_MODELS) and attempt < max_attempts:\n",
    "        attempt += 1\n",
    "        print(f'\\n   Attempt {attempt}/{max_attempts}...')\n",
    "        \n",
    "        for model in REQUIRED_MODELS:\n",
    "            if model in ready_models:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                manager, client, model_id = get_client(model, None)\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "                    max_tokens=5,\n",
    "                    temperature=0\n",
    "                )\n",
    "                \n",
    "                if response and response.choices:\n",
    "                    ready_models.append(model)\n",
    "                    print(f'   âœ… {model} is READY')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_msg = str(e).lower()\n",
    "                if 'connection' in error_msg or 'timeout' in error_msg:\n",
    "                    print(f'   â³ {model} still loading...')\n",
    "                else:\n",
    "                    print(f'   âš ï¸  {model} error: {str(e)[:60]}...')\n",
    "        \n",
    "        if len(ready_models) == len(REQUIRED_MODELS):\n",
    "            break\n",
    "            \n",
    "        if len(ready_models) < len(REQUIRED_MODELS):\n",
    "            time.sleep(10)\n",
    "    \n",
    "    # Final status\n",
    "    print('\\n' + '=' * 70)\n",
    "    print(f'ğŸ“¦ Final Status: {len(ready_models)}/{len(REQUIRED_MODELS)} models ready')\n",
    "    \n",
    "    for model in REQUIRED_MODELS:\n",
    "        if model in ready_models:\n",
    "            print(f'   âœ… {model} - READY (retained in memory)')\n",
    "        else:\n",
    "            print(f'   âŒ {model} - NOT READY')\n",
    "    \n",
    "    if len(ready_models) == len(REQUIRED_MODELS):\n",
    "        print('\\nğŸ‰ All models loaded and verified!')\n",
    "        print('   âœ… Ready for intent-based routing')\n",
    "    else:\n",
    "        print(f'\\nâš ï¸  Some models not ready. Check: foundry model ls')\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f'\\nâŒ Cannot import workshop_utils: {e}')\n",
    "    print('   ğŸ’¡ Ensure workshop_utils.py is in ../samples/')\n",
    "except Exception as e:\n",
    "    print(f'\\nâŒ Verification error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682909b",
   "metadata": {},
   "source": [
    "## ğŸ¯ Î’Î®Î¼Î± 4: Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Î‘Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ·Ï‚ Î ÏÎ¿Î¸Î­ÏƒÎµÏ‰Î½ & ÎšÎ±Ï„Î±Î»ÏŒÎ³Î¿Ï… ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½\n",
    "\n",
    "Î¡Ï…Î¸Î¼Î¯ÏƒÏ„Îµ Ï„Î¿ ÏƒÏÏƒÏ„Î·Î¼Î± Î´ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚ Î¼Îµ:\n",
    "- **ÎšÎ±Î½ÏŒÎ½ÎµÏ‚ Î ÏÎ¿Î¸Î­ÏƒÎµÏ‰Î½**: Î ÏÏŒÏ„Ï…Ï€Î± Regex Î³Î¹Î± Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· Î±Î¹Ï„Î·Î¼Î¬Ï„Ï‰Î½\n",
    "- **ÎšÎ±Ï„Î¬Î»Î¿Î³Î¿ ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½**: Î‘Î½Ï„Î¹ÏƒÏ„Î¿Î¹Ï‡Î¯Î¶ÎµÎ¹ Ï„Î¹Ï‚ Î´Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚ Ï„Ï‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ ÏƒÏ„Î¹Ï‚ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚ Ï€ÏÎ¿Î¸Î­ÏƒÎµÏ‰Î½\n",
    "- **Î£ÏÏƒÏ„Î·Î¼Î± Î ÏÎ¿Ï„ÎµÏÎ±Î¹ÏŒÏ„Î·Ï„Î±Ï‚**: ÎšÎ±Î¸Î¿ÏÎ¯Î¶ÎµÎ¹ Ï„Î·Î½ ÎµÏ€Î¹Î»Î¿Î³Î® Î¼Î¿Î½Ï„Î­Î»Î¿Ï… ÏŒÏ„Î±Î½ Ï„Î±Î¹ÏÎ¹Î¬Î¶Î¿Ï…Î½ Ï€Î¿Î»Î»Î±Ï€Î»Î¬ Î¼Î¿Î½Ï„Î­Î»Î±\n",
    "\n",
    "**Î Î»ÎµÎ¿Î½ÎµÎºÏ„Î®Î¼Î±Ï„Î± ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½ CPU**:\n",
    "- âœ… Î”ÎµÎ½ Î±Ï€Î±Î¹Ï„ÎµÎ¯Ï„Î±Î¹ GPU\n",
    "- âœ… Î£Ï„Î±Î¸ÎµÏÎ® Î±Ï€ÏŒÎ´Î¿ÏƒÎ·\n",
    "- âœ… Î§Î±Î¼Î·Î»ÏŒÏ„ÎµÏÎ· ÎºÎ±Ï„Î±Î½Î¬Î»Ï‰ÏƒÎ· ÎµÎ½Î­ÏÎ³ÎµÎ¹Î±Ï‚\n",
    "- âœ… Î ÏÎ¿Î²Î»Î­ÏˆÎ¹Î¼Î· Ï‡ÏÎ®ÏƒÎ· Î¼Î½Î®Î¼Î·Ï‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3620a4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Active Model Catalog (Hardware-Optimized Aliases)\n",
      "======================================================================\n",
      "ğŸ’¡ Using model aliases - Foundry automatically selects CPU variants\n",
      "\n",
      "   â€¢ phi-4-mini\n",
      "     Capabilities: general, summarize, reasoning\n",
      "     Priority: 3\n",
      "\n",
      "   â€¢ qwen2.5-0.5b\n",
      "     Capabilities: classification, fast, general\n",
      "     Priority: 1\n",
      "\n",
      "   â€¢ phi-3.5-mini\n",
      "     Capabilities: code, refactor, technical\n",
      "     Priority: 2\n",
      "\n",
      "   â€¢ qwen2.5-coder-0.5b\n",
      "     Capabilities: code, programming, debug\n",
      "     Priority: 1\n",
      "\n",
      "âœ… Intent detection and model selection configured\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ğŸ’¡ Using model aliases - Foundry automatically selects CPU variants\n",
      "\n",
      "   â€¢ phi-4-mini\n",
      "     Capabilities: general, summarize, reasoning\n",
      "     Priority: 3\n",
      "\n",
      "   â€¢ qwen2.5-0.5b\n",
      "     Capabilities: classification, fast, general\n",
      "     Priority: 1\n",
      "\n",
      "   â€¢ phi-3.5-mini\n",
      "     Capabilities: code, refactor, technical\n",
      "     Priority: 2\n",
      "\n",
      "   â€¢ qwen2.5-coder-0.5b\n",
      "     Capabilities: code, programming, debug\n",
      "     Priority: 1\n",
      "\n",
      "âœ… Intent detection and model selection configured\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Model capability catalog (maps model aliases to capabilities)\n",
    "# Use base aliases - Foundry Local will automatically select CPU variants\n",
    "CATALOG = {\n",
    "    'phi-4-mini': {\n",
    "        'capabilities': ['general', 'summarize', 'reasoning'],\n",
    "        'priority': 3\n",
    "    },\n",
    "    'qwen2.5-0.5b': {\n",
    "        'capabilities': ['classification', 'fast', 'general'],\n",
    "        'priority': 1\n",
    "    },\n",
    "    'phi-3.5-mini': {\n",
    "        'capabilities': ['code', 'refactor', 'technical'],\n",
    "        'priority': 2\n",
    "    },\n",
    "    'qwen2.5-coder-0.5b': {\n",
    "        'capabilities': ['code', 'programming', 'debug'],\n",
    "        'priority': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Filter to only include models recommended for this system\n",
    "CATALOG = {k: v for k, v in CATALOG.items() if k in model_aliases}\n",
    "\n",
    "print('ğŸ“‹ Active Model Catalog (Hardware-Optimized Aliases)')\n",
    "print('=' * 70)\n",
    "print('ğŸ’¡ Using model aliases - Foundry automatically selects CPU variants')\n",
    "print()\n",
    "for model, info in CATALOG.items():\n",
    "    caps = ', '.join(info['capabilities'])\n",
    "    print(f'   â€¢ {model}')\n",
    "    print(f'     Capabilities: {caps}')\n",
    "    print(f'     Priority: {info[\"priority\"]}')\n",
    "    print()\n",
    "\n",
    "# Intent detection rules (regex pattern -> intent label)\n",
    "INTENT_RULES = [\n",
    "    (re.compile(r'code|refactor|function|debug|program', re.I), 'code'),\n",
    "    (re.compile(r'summar|abstract|tl;?dr|brief', re.I), 'summarize'),\n",
    "    (re.compile(r'classif|categor|label|sentiment', re.I), 'classification'),\n",
    "    (re.compile(r'explain|teach|describe', re.I), 'general'),\n",
    "]\n",
    "\n",
    "def detect_intent(prompt: str) -> str:\n",
    "    \"\"\"Detect intent from prompt using regex patterns.\n",
    "    \n",
    "    Args:\n",
    "        prompt: User input text\n",
    "        \n",
    "    Returns:\n",
    "        Intent label: 'code', 'summarize', 'classification', or 'general'\n",
    "    \"\"\"\n",
    "    for pattern, intent in INTENT_RULES:\n",
    "        if pattern.search(prompt):\n",
    "            return intent\n",
    "    return 'general'\n",
    "\n",
    "def pick_model(intent: str) -> str:\n",
    "    \"\"\"Select best model for intent based on capabilities and priority.\n",
    "    \n",
    "    Args:\n",
    "        intent: Detected intent category\n",
    "        \n",
    "    Returns:\n",
    "        Model alias string, or first available model if no match\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        (alias, info['priority']) \n",
    "        for alias, info in CATALOG.items() \n",
    "        if intent in info['capabilities']\n",
    "    ]\n",
    "    \n",
    "    if candidates:\n",
    "        # Sort by priority (higher = better)\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return candidates[0][0]\n",
    "    \n",
    "    # Fallback to first available model\n",
    "    return list(CATALOG.keys())[0] if CATALOG else None\n",
    "\n",
    "print('âœ… Intent detection and model selection configured')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb6d09",
   "metadata": {},
   "source": [
    "## ğŸ§ª Î’Î®Î¼Î± 5: Î”Î¿ÎºÎ¹Î¼Î® Î‘Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ·Ï‚ Î ÏÏŒÎ¸ÎµÏƒÎ·Ï‚\n",
    "\n",
    "Î•Ï€Î¹Î²ÎµÎ²Î±Î¹ÏÏƒÏ„Îµ ÏŒÏ„Î¹ Ï„Î¿ ÏƒÏÏƒÏ„Î·Î¼Î± Î±Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ·Ï‚ Ï€ÏÏŒÎ¸ÎµÏƒÎ·Ï‚ Ï„Î±Î¾Î¹Î½Î¿Î¼ÎµÎ¯ ÏƒÏ‰ÏƒÏ„Î¬ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ¿ÏÏ‚ Ï„ÏÏ€Î¿Ï…Ï‚ ÎµÏÏ‰Ï„Î·Î¼Î¬Ï„Ï‰Î½.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0fd85468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing Intent Detection\n",
      "======================================================================\n",
      "\n",
      "Prompt: Refactor this Python function for better readabili...\n",
      "   Intent: code            â†’ Model: phi-3.5-mini\n",
      "\n",
      "Prompt: Summarize the key points of this article...\n",
      "   Intent: summarize       â†’ Model: phi-4-mini\n",
      "\n",
      "Prompt: Classify this customer feedback as positive or neg...\n",
      "   Intent: classification  â†’ Model: qwen2.5-0.5b\n",
      "\n",
      "Prompt: Explain how edge AI differs from cloud AI...\n",
      "   Intent: general         â†’ Model: phi-4-mini\n",
      "\n",
      "Prompt: Write a function to calculate fibonacci numbers...\n",
      "   Intent: code            â†’ Model: phi-3.5-mini\n",
      "\n",
      "Prompt: Give me a brief overview of small language models...\n",
      "   Intent: summarize       â†’ Model: phi-4-mini\n",
      "\n",
      "======================================================================\n",
      "âœ… Intent detection working correctly\n"
     ]
    }
   ],
   "source": [
    "# Test intent detection with sample prompts\n",
    "test_prompts = [\n",
    "    'Refactor this Python function for better readability',\n",
    "    'Summarize the key points of this article',\n",
    "    'Classify this customer feedback as positive or negative',\n",
    "    'Explain how edge AI differs from cloud AI',\n",
    "    'Write a function to calculate fibonacci numbers',\n",
    "    'Give me a brief overview of small language models'\n",
    "]\n",
    "\n",
    "print('ğŸ§ª Testing Intent Detection')\n",
    "print('=' * 70)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    intent = detect_intent(prompt)\n",
    "    model = pick_model(intent)\n",
    "    print(f'\\nPrompt: {prompt[:50]}...')\n",
    "    print(f'   Intent: {intent:15s} â†’ Model: {model}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('âœ… Intent detection working correctly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6a08b",
   "metadata": {},
   "source": [
    "## ğŸš€ Î’Î®Î¼Î± 6: Î¥Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Î·Ï‚ Î›ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î±Ï‚ Î”ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚\n",
    "\n",
    "Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÏ„Îµ Ï„Î· Î²Î±ÏƒÎ¹ÎºÎ® Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î± Î´ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚ Ï€Î¿Ï…:\n",
    "1. Î‘Î½Î¹Ï‡Î½ÎµÏÎµÎ¹ Ï„Î·Î½ Ï€ÏÏŒÎ¸ÎµÏƒÎ· Î±Ï€ÏŒ Ï„Î¿ prompt\n",
    "2. Î•Ï€Î¹Î»Î­Î³ÎµÎ¹ Ï„Î¿ Î²Î­Î»Ï„Î¹ÏƒÏ„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿\n",
    "3. Î•ÎºÏ„ÎµÎ»ÎµÎ¯ Ï„Î¿ Î±Î¯Ï„Î·Î¼Î± Î¼Î­ÏƒÏ‰ Ï„Î¿Ï… Foundry Local SDK\n",
    "4. Î Î±ÏÎ±ÎºÎ¿Î»Î¿Ï…Î¸ÎµÎ¯ Ï„Î· Ï‡ÏÎ®ÏƒÎ· Ï„Ï‰Î½ tokens ÎºÎ±Î¹ Ï„Î± ÏƒÏ†Î¬Î»Î¼Î±Ï„Î±\n",
    "\n",
    "**Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î¿ Î¼Î¿Ï„Î¯Î²Î¿ workshop_utils**:\n",
    "- Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· ÎµÏ€Î±Î½Î¬Î»Î·ÏˆÎ· Î¼Îµ ÎµÎºÎ¸ÎµÏ„Î¹ÎºÎ® ÎºÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ·\n",
    "- Î£Ï…Î¼Î²Î±Ï„ÏŒ API Î¼Îµ OpenAI\n",
    "- Î Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· tokens ÎºÎ±Î¹ Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "24cc251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Routing function ready\n",
      "   Using Foundry Local SDK via workshop_utils\n",
      "   Token tracking: Enabled\n",
      "   Retry logic: Automatic with exponential backoff\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from workshop_utils import chat_once\n",
    "\n",
    "# Fix RETRY_BACKOFF environment variable if it has comments\n",
    "if 'RETRY_BACKOFF' in os.environ:\n",
    "    retry_val = os.environ['RETRY_BACKOFF'].strip().split()[0]\n",
    "    try:\n",
    "        float(retry_val)\n",
    "        os.environ['RETRY_BACKOFF'] = retry_val\n",
    "    except ValueError:\n",
    "        os.environ['RETRY_BACKOFF'] = '1.0'\n",
    "\n",
    "def route(prompt: str, max_tokens: int = 200, temperature: float = 0.7):\n",
    "    \"\"\"Route prompt to appropriate model based on intent.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Detect intent using regex patterns\n",
    "    2. Select best model by capability + priority\n",
    "    3. Execute via Foundry Local SDK\n",
    "    \n",
    "    Args:\n",
    "        prompt: User input text\n",
    "        max_tokens: Maximum tokens in response\n",
    "        temperature: Sampling temperature (0-1)\n",
    "        \n",
    "    Returns:\n",
    "        Dict with: intent, model, output, tokens, usage, error\n",
    "    \"\"\"\n",
    "    intent = detect_intent(prompt)\n",
    "    model_alias = pick_model(intent)\n",
    "    \n",
    "    if not model_alias:\n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'model': None,\n",
    "            'output': '',\n",
    "            'tokens': None,\n",
    "            'usage': {},\n",
    "            'error': 'No suitable model found'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Call Foundry Local via workshop_utils\n",
    "        text, usage = chat_once(\n",
    "            model_alias,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        # Extract token information\n",
    "        usage_info = {}\n",
    "        if usage:\n",
    "            usage_info['prompt_tokens'] = getattr(usage, 'prompt_tokens', None)\n",
    "            usage_info['completion_tokens'] = getattr(usage, 'completion_tokens', None)\n",
    "            usage_info['total_tokens'] = getattr(usage, 'total_tokens', None)\n",
    "        \n",
    "        # Estimate if not provided\n",
    "        if not usage_info.get('total_tokens'):\n",
    "            est_prompt = len(prompt) // 4\n",
    "            est_completion = len(text or '') // 4\n",
    "            usage_info['estimated_tokens'] = est_prompt + est_completion\n",
    "        \n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'model': model_alias,\n",
    "            'output': (text or '').strip(),\n",
    "            'tokens': usage_info.get('total_tokens') or usage_info.get('estimated_tokens'),\n",
    "            'usage': usage_info,\n",
    "            'error': None\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'model': model_alias,\n",
    "            'output': '',\n",
    "            'tokens': None,\n",
    "            'usage': {},\n",
    "            'error': f'{type(e).__name__}: {str(e)}'\n",
    "        }\n",
    "\n",
    "print('âœ… Routing function ready')\n",
    "print('   Using Foundry Local SDK via workshop_utils')\n",
    "print('   Token tracking: Enabled')\n",
    "print('   Retry logic: Automatic with exponential backoff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5c915",
   "metadata": {},
   "source": [
    "## ğŸ¯ Î’Î®Î¼Î± 7: Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Î”Î¿ÎºÎ¹Î¼ÏÎ½ Î”ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚\n",
    "\n",
    "Î”Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Îµ Ï„Î¿ Ï€Î»Î®ÏÎµÏ‚ ÏƒÏÏƒÏ„Î·Î¼Î± Î´ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚ Î¼Îµ Î´Î¹Î¬Ï†Î¿ÏÎµÏ‚ Ï€ÏÎ¿Ï„ÏÎ¿Ï€Î­Ï‚ Î³Î¹Î± Î½Î± ÎµÏ€Î¹Î´ÎµÎ¯Î¾ÎµÏ„Îµ:\n",
    "- Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· Î±Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Ï€ÏÎ¿Î¸Î­ÏƒÎµÏ‰Î½\n",
    "- ÎˆÎ¾Ï…Ï€Î½Î· ÎµÏ€Î¹Î»Î¿Î³Î® Î¼Î¿Î½Ï„Î­Î»Î¿Ï…\n",
    "- Î”ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ· Ï€Î¿Î»Î»Î±Ï€Î»ÏÎ½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Î¼Îµ Î´Î¹Î±Ï„Î®ÏÎ·ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Ï‰Î½\n",
    "- Î Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· tokens ÎºÎ±Î¹ Î¼ÎµÏ„ÏÎ®ÏƒÎµÎ¹Ï‚ Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c46ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Running Intent-Based Routing Tests\n",
      "================================================================================\n",
      "\n",
      "[1/6] Testing prompt...\n",
      "Prompt: Refactor this Python function to make it more efficient and readable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Expected Intent: code\n",
      "   Detected Intent: code âœ…\n",
      "   Selected Model:  phi-3.5-mini\n",
      "   âœ… Response: To refactor a Python function for efficiency and readability, I would need to see the specific funct...\n",
      "   ğŸ“Š Tokens: ~158 (estimated)\n",
      "\n",
      "[2/6] Testing prompt...\n",
      "Prompt: Summarize the key benefits of using small language models at the edge\n",
      "   Expected Intent: summarize\n",
      "   Detected Intent: summarize âœ…\n",
      "   Selected Model:  phi-4-mini\n",
      "   âŒ Error: APIConnectionError: Connection error.\n",
      "\n",
      "[3/6] Testing prompt...\n",
      "Prompt: Classify this user feedback: The app is slow but the UI looks great\n",
      "   Expected Intent: classification\n",
      "   Detected Intent: classification âœ…\n",
      "   Selected Model:  qwen2.5-0.5b\n",
      "   âŒ Error: APIConnectionError: Connection error.\n",
      "\n",
      "[4/6] Testing prompt...\n",
      "Prompt: Explain the difference between local and cloud inference\n",
      "   Expected Intent: general\n",
      "   Detected Intent: general âœ…\n",
      "   Selected Model:  phi-4-mini\n",
      "   âŒ Error: APIConnectionError: Connection error.\n",
      "\n",
      "[5/6] Testing prompt...\n",
      "Prompt: Write a Python function to calculate the Fibonacci sequence\n"
     ]
    }
   ],
   "source": [
    "# Test prompts covering all intent categories\n",
    "test_cases = [\n",
    "    {\n",
    "        'prompt': 'Refactor this Python function to make it more efficient and readable',\n",
    "        'expected_intent': 'code'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Summarize the key benefits of using small language models at the edge',\n",
    "        'expected_intent': 'summarize'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Classify this user feedback: The app is slow but the UI looks great',\n",
    "        'expected_intent': 'classification'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Explain the difference between local and cloud inference',\n",
    "        'expected_intent': 'general'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Write a Python function to calculate the Fibonacci sequence',\n",
    "        'expected_intent': 'code'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Give me a brief overview of the Phi model family',\n",
    "        'expected_intent': 'summarize'\n",
    "    }\n",
    "]\n",
    "\n",
    "print('ğŸ¯ Running Intent-Based Routing Tests')\n",
    "print('=' * 80)\n",
    "\n",
    "results = []\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f'\\n[{i}/{len(test_cases)}] Testing prompt...')\n",
    "    print(f'Prompt: {test[\"prompt\"]}')\n",
    "    \n",
    "    result = route(test['prompt'], max_tokens=150)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f'   Expected Intent: {test[\"expected_intent\"]}')\n",
    "    print(f'   Detected Intent: {result[\"intent\"]} {\"âœ…\" if result[\"intent\"] == test[\"expected_intent\"] else \"âš ï¸\"}')\n",
    "    print(f'   Selected Model:  {result[\"model\"]}')\n",
    "    \n",
    "    if result['error']:\n",
    "        print(f'   âŒ Error: {result[\"error\"]}')\n",
    "    else:\n",
    "        output_preview = result['output'][:100] + '...' if len(result['output']) > 100 else result['output']\n",
    "        print(f'   âœ… Response: {output_preview}')\n",
    "        \n",
    "        tokens = result.get('tokens', 0)\n",
    "        if tokens:\n",
    "            usage = result.get('usage', {})\n",
    "            if 'estimated_tokens' in usage:\n",
    "                print(f'   ğŸ“Š Tokens: ~{tokens} (estimated)')\n",
    "            else:\n",
    "                print(f'   ğŸ“Š Tokens: {tokens}')\n",
    "\n",
    "# Summary statistics\n",
    "print('\\n' + '=' * 80)\n",
    "print('ğŸ“Š ROUTING SUMMARY')\n",
    "print('=' * 80)\n",
    "\n",
    "success_count = sum(1 for r in results if not r['error'])\n",
    "total_tokens = sum(r.get('tokens', 0) or 0 for r in results if not r['error'])\n",
    "intent_accuracy = sum(1 for i, r in enumerate(results) if r['intent'] == test_cases[i]['expected_intent'])\n",
    "\n",
    "print(f'Total Prompts:        {len(results)}')\n",
    "print(f'âœ… Successful:         {success_count}/{len(results)}')\n",
    "print(f'âŒ Failed:             {len(results) - success_count}')\n",
    "print(f'ğŸ¯ Intent Accuracy:    {intent_accuracy}/{len(results)} ({intent_accuracy/len(results)*100:.1f}%)')\n",
    "print(f'ğŸ“Š Total Tokens Used:  {total_tokens}')\n",
    "\n",
    "# Model usage distribution\n",
    "print('\\nğŸ“‹ Model Usage Distribution:')\n",
    "model_counts = {}\n",
    "for r in results:\n",
    "    if r['model']:\n",
    "        model_counts[r['model']] = model_counts.get(r['model'], 0) + 1\n",
    "\n",
    "for model, count in sorted(model_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / len(results)) * 100\n",
    "    print(f'   â€¢ {model}: {count} requests ({percentage:.1f}%)')\n",
    "\n",
    "if success_count == len(results):\n",
    "    print('\\nğŸ‰ All routing tests passed successfully!')\n",
    "else:\n",
    "    print(f'\\nâš ï¸  {len(results) - success_count} test(s) failed')\n",
    "    print('   Check Foundry Local service: foundry service status')\n",
    "    print('   Verify models loaded: foundry model ls')\n",
    "\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764811e",
   "metadata": {},
   "source": [
    "## ğŸ”§ Î’Î®Î¼Î± 8: Î”Î¹Î±Î´ÏÎ±ÏƒÏ„Î¹ÎºÎ® Î”Î¿ÎºÎ¹Î¼Î®\n",
    "\n",
    "Î”Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Îµ Ï„Î¹Ï‚ Î´Î¹ÎºÎ­Ï‚ ÏƒÎ±Ï‚ Ï€ÏÎ¿Ï„ÏÎ¿Ï€Î­Ï‚ Î³Î¹Î± Î½Î± Î´ÎµÎ¯Ï„Îµ Ï„Î¿ ÏƒÏÏƒÏ„Î·Î¼Î± Î´ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚ ÏƒÎµ Î´ÏÎ¬ÏƒÎ·!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fdd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Interactive Routing Test\n",
      "================================================================================\n",
      "Your prompt: Explain how model quantization reduces memory usage\n",
      "\n",
      "Detected Intent: general\n",
      "Selected Model:  phi-4-mini\n",
      "\n",
      "âœ… Response:\n",
      "--------------------------------------------------------------------------------\n",
      "Model quantization is a technique used to reduce the memory footprint of a machine learning model, particularly deep learning models. It works by converting the high-precision weights of a neural network, typically represented as 32-bit floating-point numbers, into lower-precision representations, such as 8-bit integers or even binary values.\n",
      "\n",
      "\n",
      "The primary reason for quantization is to decrease the amount of memory required to store the model's parameters. Since floating-point numbers take up more space than integers, by quantizing the weights, we can significantly reduce the model's size. This reduction in size not only saves memory but also can lead to faster computation during inference, as integer operations are generally faster than floating-point operations on many hardware platforms.\n",
      "\n",
      "\n",
      "However, quantization can introduce some loss of accuracy because the lower precision representation may not capture the full range of values that the floating-point representation can. To mitigate this, techniques such as quantization-aware training can be used, where the model is trained with quantization in mind,\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“Š Tokens used: 292\n",
      "\n",
      "ğŸ’¡ Try different prompts to test routing behavior!\n",
      "Detected Intent: general\n",
      "Selected Model:  phi-4-mini\n",
      "\n",
      "âœ… Response:\n",
      "--------------------------------------------------------------------------------\n",
      "Model quantization is a technique used to reduce the memory footprint of a machine learning model, particularly deep learning models. It works by converting the high-precision weights of a neural network, typically represented as 32-bit floating-point numbers, into lower-precision representations, such as 8-bit integers or even binary values.\n",
      "\n",
      "\n",
      "The primary reason for quantization is to decrease the amount of memory required to store the model's parameters. Since floating-point numbers take up more space than integers, by quantizing the weights, we can significantly reduce the model's size. This reduction in size not only saves memory but also can lead to faster computation during inference, as integer operations are generally faster than floating-point operations on many hardware platforms.\n",
      "\n",
      "\n",
      "However, quantization can introduce some loss of accuracy because the lower precision representation may not capture the full range of values that the floating-point representation can. To mitigate this, techniques such as quantization-aware training can be used, where the model is trained with quantization in mind,\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“Š Tokens used: 292\n",
      "\n",
      "ğŸ’¡ Try different prompts to test routing behavior!\n"
     ]
    }
   ],
   "source": [
    "# Interactive testing - modify the prompt and run this cell\n",
    "custom_prompt = \"Explain how model quantization reduces memory usage\"\n",
    "\n",
    "print('ğŸ¯ Interactive Routing Test')\n",
    "print('=' * 80)\n",
    "print(f'Your prompt: {custom_prompt}')\n",
    "print()\n",
    "\n",
    "result = route(custom_prompt, max_tokens=200)\n",
    "\n",
    "print(f'Detected Intent: {result[\"intent\"]}')\n",
    "print(f'Selected Model:  {result[\"model\"]}')\n",
    "print()\n",
    "\n",
    "if result['error']:\n",
    "    print(f'âŒ Error: {result[\"error\"]}')\n",
    "else:\n",
    "    print('âœ… Response:')\n",
    "    print('-' * 80)\n",
    "    print(result['output'])\n",
    "    print('-' * 80)\n",
    "    \n",
    "    if result['tokens']:\n",
    "        print(f'\\nğŸ“Š Tokens used: {result[\"tokens\"]}')\n",
    "\n",
    "print('\\nğŸ’¡ Try different prompts to test routing behavior!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c17226c",
   "metadata": {},
   "source": [
    "## ğŸ“Š Î’Î®Î¼Î± 9: Î‘Î½Î¬Î»Ï…ÏƒÎ· Î‘Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚\n",
    "\n",
    "Î‘Î½Î±Î»ÏÏƒÏ„Îµ Ï„Î·Î½ Î±Ï€ÏŒÎ´Î¿ÏƒÎ· Ï„Î¿Ï… ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î¿Ï‚ Î´ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚ ÎºÎ±Î¹ Ï„Î· Ï‡ÏÎ®ÏƒÎ· Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï….\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Performance Benchmark\n",
      "================================================================================\n",
      "\n",
      "Prompt: Write a hello world function...\n",
      "   Model: phi-3.5-mini\n",
      "   Time: 3.31s\n",
      "   Tokens: 60\n",
      "\n",
      "Prompt: Write a hello world function...\n",
      "   Model: phi-3.5-mini\n",
      "   Time: 3.31s\n",
      "   Tokens: 60\n",
      "\n",
      "Prompt: Summarize: AI at the edge is powerful...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 84\n",
      "\n",
      "Prompt: Summarize: AI at the edge is powerful...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 84\n",
      "\n",
      "Prompt: Classify: Good product...\n",
      "   Model: qwen2.5-0.5b\n",
      "   Time: 7.21s\n",
      "   Tokens: 69\n",
      "\n",
      "Prompt: Classify: Good product...\n",
      "   Model: qwen2.5-0.5b\n",
      "   Time: 7.21s\n",
      "   Tokens: 69\n",
      "\n",
      "Prompt: Explain edge computing...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 72\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š Performance Statistics:\n",
      "   Average response time: 27.46s\n",
      "   Fastest response:      3.31s\n",
      "   Slowest response:      49.67s\n",
      "\n",
      "ğŸ’¡ Note: First request may be slower due to model initialization\n",
      "================================================================================\n",
      "\n",
      "Prompt: Explain edge computing...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 72\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š Performance Statistics:\n",
      "   Average response time: 27.46s\n",
      "   Fastest response:      3.31s\n",
      "   Slowest response:      49.67s\n",
      "\n",
      "ğŸ’¡ Note: First request may be slower due to model initialization\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Performance benchmark\n",
    "benchmark_prompts = [\n",
    "    'Write a hello world function',\n",
    "    'Summarize: AI at the edge is powerful',\n",
    "    'Classify: Good product',\n",
    "    'Explain edge computing'\n",
    "]\n",
    "\n",
    "print('âš¡ Performance Benchmark')\n",
    "print('=' * 80)\n",
    "\n",
    "timings = []\n",
    "for prompt in benchmark_prompts:\n",
    "    start = time.time()\n",
    "    result = route(prompt, max_tokens=50)\n",
    "    duration = time.time() - start\n",
    "    timings.append(duration)\n",
    "    \n",
    "    print(f'\\nPrompt: {prompt[:40]}...')\n",
    "    print(f'   Model: {result[\"model\"]}')\n",
    "    print(f'   Time: {duration:.2f}s')\n",
    "    if result.get('tokens'):\n",
    "        print(f'   Tokens: {result[\"tokens\"]}')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('ğŸ“Š Performance Statistics:')\n",
    "print(f'   Average response time: {sum(timings)/len(timings):.2f}s')\n",
    "print(f'   Fastest response:      {min(timings):.2f}s')\n",
    "print(f'   Slowest response:      {max(timings):.2f}s')\n",
    "print('\\nğŸ’¡ Note: First request may be slower due to model initialization')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db64ff",
   "metadata": {},
   "source": [
    "## ğŸ“ Î’Î±ÏƒÎ¹ÎºÎ¬ Î£Ï…Î¼Ï€ÎµÏÎ¬ÏƒÎ¼Î±Ï„Î± & Î•Ï€ÏŒÎ¼ÎµÎ½Î± Î’Î®Î¼Î±Ï„Î±\n",
    "\n",
    "### âœ… Î¤Î¹ ÎˆÏ‡ÎµÏ„Îµ ÎœÎ¬Î¸ÎµÎ¹\n",
    "\n",
    "1. **Î”ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ· Î’Î¬ÏƒÎµÎ¹ Î ÏÏŒÎ¸ÎµÏƒÎ·Ï‚**: Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· Ï€ÏÎ¿Ï„ÏÎ¿Ï€ÏÎ½ ÎºÎ±Î¹ Î´ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ· ÏƒÎµ ÎµÎ¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î± Î¼Î¿Î½Ï„Î­Î»Î±  \n",
    "2. **Î•Ï€Î¹Î»Î¿Î³Î® ÎœÎµ Î“Î½ÏÎ¼Î¿Î½Î± Î¤Î· ÎœÎ½Î®Î¼Î·**: Î•Ï€Î¹Î»Î¿Î³Î® Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ CPU Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î· Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î· Î¼Î½Î®Î¼Î· ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î¿Ï‚  \n",
    "3. **Î”Î¹Î±Ï„Î®ÏÎ·ÏƒÎ· Î Î¿Î»Î»Î±Ï€Î»ÏÎ½ ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½**: Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ `--retain true` Î³Î¹Î± Î½Î± Î´Î¹Î±Ï„Î·ÏÎµÎ¯Ï„Îµ Ï€Î¿Î»Î»Î¬ Î¼Î¿Î½Ï„Î­Î»Î± Ï†Î¿ÏÏ„Ï‰Î¼Î­Î½Î±  \n",
    "4. **Î Î±ÏÎ±Î³Ï‰Î³Î¹ÎºÎ¬ ÎœÎ¿Ï„Î¯Î²Î±**: Î›Î¿Î³Î¹ÎºÎ® ÎµÏ€Î±Î½Î±Ï€ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹Î±Ï‚, Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½ ÎºÎ±Î¹ Ï€Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· tokens  \n",
    "5. **Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· CPU**: Î‘Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î±Ï„Î¹ÎºÎ® Î±Î½Î¬Ï€Ï„Ï…Î¾Î· Ï‡Ï‰ÏÎ¯Ï‚ Î±Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚ GPU  \n",
    "\n",
    "### ğŸš€ Î™Î´Î­ÎµÏ‚ Î³Î¹Î± Î ÎµÎ¹ÏÎ±Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒ\n",
    "\n",
    "1. **Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Î ÏÎ¿ÏƒÎ±ÏÎ¼Î¿ÏƒÎ¼Î­Î½Ï‰Î½ Î ÏÎ¿Î¸Î­ÏƒÎµÏ‰Î½**:  \n",
    "   ```python\n",
    "   INTENT_RULES.append(\n",
    "       (re.compile(r'translate|convert', re.I), 'translation')\n",
    "   )\n",
    "   ```\n",
    "  \n",
    "2. **Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î•Ï€Î¹Ï€Î»Î­Î¿Î½ ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½**:  \n",
    "   ```bash\n",
    "   foundry model run llama-3.2-1b-cpu --retain true\n",
    "   ```\n",
    "  \n",
    "3. **Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Î•Ï€Î¹Î»Î¿Î³Î®Ï‚ ÎœÎ¿Î½Ï„Î­Î»Î¿Ï…**:  \n",
    "   - Î ÏÎ¿ÏƒÎ±ÏÎ¼ÏŒÏƒÏ„Îµ Ï„Î¹Ï‚ Ï„Î¹Î¼Î­Ï‚ Ï€ÏÎ¿Ï„ÎµÏÎ±Î¹ÏŒÏ„Î·Ï„Î±Ï‚ ÏƒÏ„Î¿Î½ CATALOG  \n",
    "   - Î ÏÎ¿ÏƒÎ¸Î­ÏƒÏ„Îµ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎµÏ‚ ÎµÏ„Î¹ÎºÎ­Ï„ÎµÏ‚ Î´Ï…Î½Î±Ï„Î¿Ï„Î®Ï„Ï‰Î½  \n",
    "   - Î•Ï†Î±ÏÎ¼ÏŒÏƒÏ„Îµ ÏƒÏ„ÏÎ±Ï„Î·Î³Î¹ÎºÎ­Ï‚ ÎµÏ†ÎµÎ´ÏÎµÎ¯Î±Ï‚  \n",
    "\n",
    "4. **Î Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· Î‘Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚**:  \n",
    "   ```python\n",
    "   import psutil\n",
    "   print(f\"Memory: {psutil.virtual_memory().percent}%\")\n",
    "   ```\n",
    "  \n",
    "\n",
    "### ğŸ“š Î ÏÏŒÏƒÎ¸ÎµÏ„Î¿Î¹ Î ÏŒÏÎ¿Î¹\n",
    "\n",
    "- **Foundry Local SDK**: https://github.com/microsoft/Foundry-Local  \n",
    "- **Î”ÎµÎ¯Î³Î¼Î±Ï„Î± Î•ÏÎ³Î±ÏƒÏ„Î·ÏÎ¯Î¿Ï…**: ../samples/  \n",
    "- **ÎœÎ¬Î¸Î·Î¼Î± Edge AI**: ../../Module08/  \n",
    "\n",
    "### ğŸ’¡ Î’Î­Î»Ï„Î¹ÏƒÏ„ÎµÏ‚ Î ÏÎ±ÎºÏ„Î¹ÎºÎ­Ï‚\n",
    "\n",
    "âœ… Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Î¼Î¿Î½Ï„Î­Î»Î± CPU Î³Î¹Î± ÏƒÏ…Î½ÎµÏ€Î® ÏƒÏ…Î¼Ï€ÎµÏÎ¹Ï†Î¿ÏÎ¬ ÏƒÎµ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ Ï€Î»Î±Ï„Ï†ÏŒÏÎ¼ÎµÏ‚  \n",
    "âœ… Î•Î»Î­Î³Ï‡ÎµÏ„Îµ Ï€Î¬Î½Ï„Î± Ï„Î· Î¼Î½Î®Î¼Î· ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î¿Ï‚ Ï€ÏÎ¹Î½ Ï†Î¿ÏÏ„ÏÏƒÎµÏ„Îµ Ï€Î¿Î»Î»Î±Ï€Î»Î¬ Î¼Î¿Î½Ï„Î­Î»Î±  \n",
    "âœ… Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ `--retain true` Î³Î¹Î± ÏƒÎµÎ½Î¬ÏÎ¹Î± Î´ÏÎ¿Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚  \n",
    "âœ… Î•Ï†Î±ÏÎ¼ÏŒÏƒÏ„Îµ ÏƒÏ‰ÏƒÏ„Î® Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½ ÎºÎ±Î¹ ÎµÏ€Î±Î½Î±Ï€ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹ÎµÏ‚  \n",
    "âœ… Î Î±ÏÎ±ÎºÎ¿Î»Î¿Ï…Î¸Î®ÏƒÏ„Îµ Ï„Î· Ï‡ÏÎ®ÏƒÎ· tokens Î³Î¹Î± Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· ÎºÏŒÏƒÏ„Î¿Ï…Ï‚/Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚  \n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Î£Ï…Î³Ï‡Î±ÏÎ·Ï„Î®ÏÎ¹Î±!** ÎˆÏ‡ÎµÏ„Îµ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎµÎ¹ Î­Î½Î±Î½ Î´ÏÎ¿Î¼Î¿Î»Î¿Î³Î·Ï„Î® Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Î²Î¬ÏƒÎµÎ¹ Ï€ÏÏŒÎ¸ÎµÏƒÎ·Ï‚ Î­Ï„Î¿Î¹Î¼Î¿ Î³Î¹Î± Ï€Î±ÏÎ±Î³Ï‰Î³Î®, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î¿ Foundry Local SDK Î¼Îµ Î¼Î¿Î½Ï„Î­Î»Î± Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î± Î³Î¹Î± CPU!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Î‘Ï€Î¿Ï€Î¿Î¯Î·ÏƒÎ· ÎµÏ…Î¸ÏÎ½Î·Ï‚**:  \nÎ‘Ï…Ï„ÏŒ Ï„Î¿ Î­Î³Î³ÏÎ±Ï†Î¿ Î­Ï‡ÎµÎ¹ Î¼ÎµÏ„Î±Ï†ÏÎ±ÏƒÏ„ÎµÎ¯ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î·Î½ Ï…Ï€Î·ÏÎµÏƒÎ¯Î± Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î·Ï‚ Î¼ÎµÏ„Î¬Ï†ÏÎ±ÏƒÎ·Ï‚ [Co-op Translator](https://github.com/Azure/co-op-translator). Î Î±ÏÏŒÎ»Î¿ Ï€Î¿Ï… ÎºÎ±Ï„Î±Î²Î¬Î»Î»Î¿Ï…Î¼Îµ Ï€ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹ÎµÏ‚ Î³Î¹Î± Î±ÎºÏÎ¯Î²ÎµÎ¹Î±, Ï€Î±ÏÎ±ÎºÎ±Î»Î¿ÏÎ¼Îµ Î½Î± Î­Ï‡ÎµÏ„Îµ Ï…Ï€ÏŒÏˆÎ· ÏŒÏ„Î¹ Î¿Î¹ Î±Ï…Ï„Î¿Î¼Î±Ï„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½ÎµÏ‚ Î¼ÎµÏ„Î±Ï†ÏÎ¬ÏƒÎµÎ¹Ï‚ ÎµÎ½Î´Î­Ï‡ÎµÏ„Î±Î¹ Î½Î± Ï€ÎµÏÎ¹Î­Ï‡Î¿Ï…Î½ Î»Î¬Î¸Î· Î® Î±Î½Î±ÎºÏÎ¯Î²ÎµÎ¹ÎµÏ‚. Î¤Î¿ Ï€ÏÏ‰Ï„ÏŒÏ„Ï…Ï€Î¿ Î­Î³Î³ÏÎ±Ï†Î¿ ÏƒÏ„Î· Î¼Î·Ï„ÏÎ¹ÎºÎ® Ï„Î¿Ï… Î³Î»ÏÏƒÏƒÎ± Î¸Î± Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î¸ÎµÏ‰ÏÎµÎ¯Ï„Î±Î¹ Î· Î±Ï…Î¸ÎµÎ½Ï„Î¹ÎºÎ® Ï€Î·Î³Î®. Î“Î¹Î± ÎºÏÎ¯ÏƒÎ¹Î¼ÎµÏ‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚, ÏƒÏ…Î½Î¹ÏƒÏ„Î¬Ï„Î±Î¹ ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÎ® Î±Î½Î¸ÏÏÏ€Î¹Î½Î· Î¼ÎµÏ„Î¬Ï†ÏÎ±ÏƒÎ·. Î”ÎµÎ½ Ï†Î­ÏÎ¿Ï…Î¼Îµ ÎµÏ…Î¸ÏÎ½Î· Î³Î¹Î± Ï„Ï…Ï‡ÏŒÎ½ Ï€Î±ÏÎµÎ¾Î·Î³Î®ÏƒÎµÎ¹Ï‚ Î® ÎµÏƒÏ†Î±Î»Î¼Î­Î½ÎµÏ‚ ÎµÏÎ¼Î·Î½ÎµÎ¯ÎµÏ‚ Ï€Î¿Ï… Ï€ÏÎ¿ÎºÏÏ€Ï„Î¿Ï…Î½ Î±Ï€ÏŒ Ï„Î· Ï‡ÏÎ®ÏƒÎ· Î±Ï…Ï„Î®Ï‚ Ï„Î·Ï‚ Î¼ÎµÏ„Î¬Ï†ÏÎ±ÏƒÎ·Ï‚.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "149e1ff0f023ecf1f4221663a7928ff5",
   "translation_date": "2025-10-09T13:26:11+00:00",
   "source_file": "Workshop/notebooks/session06_models_router.ipynb",
   "language_code": "el"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}