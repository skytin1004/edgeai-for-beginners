<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6503a980cb3bf2b2de2d2bc4ac6acc4c",
  "translation_date": "2025-09-24T15:26:31+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "ne"
}
-->
# ‡§∏‡§§‡•ç‡§∞ ‡•ß: Foundry Local ‡§∏‡•Å‡§∞‡•Å ‡§ó‡§∞‡•ç‡§®‡•á

## ‡§™‡§∞‡§ø‡§ö‡§Ø

Microsoft Foundry Local ‡§≤‡•á Azure AI Foundry ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ‡§π‡§∞‡•Ç ‡§∏‡§ø‡§ß‡•à ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã Windows 11 ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§µ‡§æ‡§§‡§æ‡§µ‡§∞‡§£‡§Æ‡§æ ‡§≤‡•ç‡§Ø‡§æ‡§â‡§Å‡§õ, ‡§ú‡§∏‡§≤‡•á ‡§ó‡•ã‡§™‡§®‡•Ä‡§Ø‡§§‡§æ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§ó‡§∞‡•ç‡§®‡•á, ‡§ï‡§Æ ‡§µ‡§ø‡§≤‡§Æ‡•ç‡§¨‡§§‡§æ AI ‡§µ‡§ø‡§ï‡§æ‡§∏‡§≤‡§æ‡§à ‡§â‡§¶‡•ç‡§Ø‡§Æ-‡§∏‡•ç‡§§‡§∞‡§ï‡§æ ‡§â‡§™‡§ï‡§∞‡§£‡§π‡§∞‡•Ç‡§∏‡§Å‡§ó ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§¨‡§®‡§æ‡§â‡§Å‡§õ‡•§ ‡§Ø‡•ã ‡§∏‡§§‡•ç‡§∞‡§≤‡•á ‡§≤‡•ã‡§ï‡§™‡•ç‡§∞‡§ø‡§Ø ‡§Æ‡•ã‡§°‡•á‡§≤‡§π‡§∞‡•Ç ‡§ú‡§∏‡•ç‡§§‡•à phi, qwen, deepseek, ‡§∞ GPT-OSS-20B ‡§ï‡•ã ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§∏‡•ç‡§•‡§æ‡§™‡§®‡§æ, ‡§ï‡§®‡•ç‡§´‡§ø‡§ó‡§∞‡•á‡§∏‡§®, ‡§∞ ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§™‡§∞‡§ø‡§®‡§ø‡§Ø‡•ã‡§ú‡§® ‡§∏‡§Æ‡•á‡§ü‡•ç‡§õ‡•§

## ‡§∏‡§ø‡§ï‡§æ‡§á ‡§â‡§¶‡•ç‡§¶‡•á‡§∂‡•ç‡§Ø‡§π‡§∞‡•Ç

‡§Ø‡•ã ‡§∏‡§§‡•ç‡§∞‡§ï‡•ã ‡§Ö‡§®‡•ç‡§§‡•ç‡§Ø‡§∏‡§Æ‡•ç‡§Æ‡§Æ‡§æ, ‡§§‡§™‡§æ‡§à‡§Ç:
- Windows 11 ‡§Æ‡§æ Foundry Local ‡§∏‡•ç‡§•‡§æ‡§™‡§®‡§æ ‡§∞ ‡§ï‡§®‡•ç‡§´‡§ø‡§ó‡§∞ ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ
- CLI ‡§ï‡§Æ‡§æ‡§£‡•ç‡§°‡§π‡§∞‡•Ç ‡§∞ ‡§ï‡§®‡•ç‡§´‡§ø‡§ó‡§∞‡•á‡§∏‡§® ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™‡§π‡§∞‡•Ç‡§Æ‡§æ ‡§Æ‡§π‡§æ‡§∞‡§§ ‡§π‡§æ‡§∏‡§ø‡§≤ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ
- ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®‡§≤‡§æ‡§à ‡§Ö‡§®‡•Å‡§ï‡•Ç‡§≤ ‡§¨‡§®‡§æ‡§â‡§® ‡§Æ‡•ã‡§°‡•á‡§≤ ‡§ï‡•ç‡§Ø‡§æ‡§∏‡§ø‡§ô ‡§∞‡§£‡§®‡•Ä‡§§‡§ø‡§π‡§∞‡•Ç ‡§¨‡•Å‡§ù‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ
- phi, qwen, deepseek, ‡§∞ GPT-OSS-20B ‡§Æ‡•ã‡§°‡•á‡§≤‡§π‡§∞‡•Ç ‡§∏‡§´‡§≤‡§§‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡§ï ‡§ö‡§≤‡§æ‡§â‡§® ‡§∏‡§ï‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ
- Foundry Local ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•á‡§∞ ‡§Ü‡§´‡•ç‡§®‡•ã ‡§™‡§π‡§ø‡§≤‡•ã AI ‡§è‡§™‡•ç‡§≤‡§ø‡§ï‡•á‡§∏‡§® ‡§¨‡§®‡§æ‡§â‡§® ‡§∏‡§ï‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ

## ‡§™‡•Ç‡§∞‡•ç‡§µ‡§∂‡§∞‡•ç‡§§‡§π‡§∞‡•Ç

### ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï‡§§‡§æ‡§π‡§∞‡•Ç
- **Windows 11**: ‡§∏‡§Ç‡§∏‡•ç‡§ï‡§∞‡§£ 22H2 ‡§µ‡§æ ‡§™‡§õ‡§ø‡§≤‡•ç‡§≤‡•ã
- **RAM**: ‡§®‡•ç‡§Ø‡•Ç‡§®‡§§‡§Æ 16GB, ‡§∏‡§ø‡§´‡§æ‡§∞‡§ø‡§∏ ‡§ó‡§∞‡§ø‡§è‡§ï‡•ã 32GB
- **‡§∏‡•ç‡§ü‡•ã‡§∞‡•á‡§ú**: ‡§Æ‡•ã‡§°‡•á‡§≤ ‡§∞ ‡§ï‡•ç‡§Ø‡§æ‡§∏‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø 50GB ‡§ñ‡§æ‡§≤‡•Ä ‡§†‡§æ‡§â‡§Å
- **‡§π‡§æ‡§∞‡•ç‡§°‡§µ‡•á‡§Ø‡§∞**: NPU- ‡§µ‡§æ GPU-‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§â‡§™‡§ï‡§∞‡§£ ‡§∏‡§ø‡§´‡§æ‡§∞‡§ø‡§∏ ‡§ó‡§∞‡§ø‡§è‡§ï‡•ã (Copilot+ PC ‡§µ‡§æ NVIDIA GPU)
- **‡§®‡•á‡§ü‡§µ‡§∞‡•ç‡§ï**: ‡§Æ‡•ã‡§°‡•á‡§≤ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§°‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø ‡§â‡§ö‡•ç‡§ö ‡§ó‡§§‡§ø‡§ï‡•ã ‡§á‡§®‡•ç‡§ü‡§∞‡§®‡•á‡§ü

### ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§µ‡§æ‡§§‡§æ‡§µ‡§∞‡§£
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion

# Set up Python environment (recommended)
py -m venv .venv
.venv\Scripts\activate

# Install required dependencies
pip install openai foundry-local-sdk
```

## ‡§≠‡§æ‡§ó ‡•ß: ‡§∏‡•ç‡§•‡§æ‡§™‡§®‡§æ ‡§∞ ‡§∏‡•á‡§ü‡§Ö‡§™

### ‡§ö‡§∞‡§£ ‡•ß: Foundry Local ‡§∏‡•ç‡§•‡§æ‡§™‡§®‡§æ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç

Winget ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•á‡§∞ Foundry Local ‡§∏‡•ç‡§•‡§æ‡§™‡§®‡§æ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç ‡§µ‡§æ GitHub ‡§¨‡§æ‡§ü ‡§á‡§®‡•ç‡§∏‡•ç‡§ü‡§≤‡§∞ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### ‡§ö‡§∞‡§£ ‡•®: ‡§∏‡•ç‡§•‡§æ‡§™‡§®‡§æ ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡§ø‡§§ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## ‡§≠‡§æ‡§ó ‡•®: CLI ‡§¨‡•Å‡§ù‡•ç‡§®‡•á

### ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ï‡§Æ‡§æ‡§£‡•ç‡§° ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## ‡§≠‡§æ‡§ó ‡•©: ‡§Æ‡•ã‡§°‡•á‡§≤ ‡§ï‡•ç‡§Ø‡§æ‡§∏‡§ø‡§ô ‡§∞ ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ‡§™‡§®

Foundry Local ‡§≤‡•á ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§∞ ‡§∏‡•ç‡§ü‡•ã‡§∞‡•á‡§ú‡§≤‡§æ‡§à ‡§Ö‡§®‡•Å‡§ï‡•Ç‡§≤ ‡§¨‡§®‡§æ‡§â‡§® ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§æ‡§® ‡§Æ‡•ã‡§°‡•á‡§≤ ‡§ï‡•ç‡§Ø‡§æ‡§∏‡§ø‡§ô ‡§≤‡§æ‡§ó‡•Ç ‡§ó‡§∞‡•ç‡§¶‡§õ:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## ‡§≠‡§æ‡§ó ‡•™: ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§Æ‡•ã‡§°‡•á‡§≤ ‡§™‡§∞‡§ø‡§®‡§ø‡§Ø‡•ã‡§ú‡§®

### Microsoft Phi ‡§Æ‡•ã‡§°‡•á‡§≤‡§π‡§∞‡•Ç ‡§ö‡§≤‡§æ‡§â‡§®‡•á

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### Qwen ‡§Æ‡•ã‡§°‡•á‡§≤‡§π‡§∞‡•Ç‡§∏‡§Å‡§ó ‡§ï‡§æ‡§Æ ‡§ó‡§∞‡•ç‡§®‡•á

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b-instruct
foundry model run qwen2.5-14b-instruct
```

### DeepSeek ‡§Æ‡•ã‡§°‡•á‡§≤‡§π‡§∞‡•Ç ‡§ö‡§≤‡§æ‡§â‡§®‡•á

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-distill-qwen-7b
```

### GPT-OSS-20B ‡§ö‡§≤‡§æ‡§â‡§®‡•á

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## ‡§≠‡§æ‡§ó ‡•´: ‡§Ü‡§´‡•ç‡§®‡•ã ‡§™‡§π‡§ø‡§≤‡•ã ‡§è‡§™‡•ç‡§≤‡§ø‡§ï‡•á‡§∏‡§® ‡§¨‡§®‡§æ‡§â‡§®‡•á

### ‡§Ü‡§ß‡•Å‡§®‡§ø‡§ï ‡§ö‡•ç‡§Ø‡§æ‡§ü ‡§è‡§™‡•ç‡§≤‡§ø‡§ï‡•á‡§∏‡§® (OpenAI SDK + Foundry Local)

OpenAI SDK ‡§∞ Foundry Local ‡§è‡§ï‡•Ä‡§ï‡§∞‡§£ ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•á‡§∞ ‡§â‡§§‡•ç‡§™‡§æ‡§¶‡§®-‡§§‡§Ø‡§æ‡§∞ ‡§ö‡•ç‡§Ø‡§æ‡§ü ‡§è‡§™‡•ç‡§≤‡§ø‡§ï‡•á‡§∏‡§® ‡§¨‡§®‡§æ‡§â‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§π‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§®‡§Æ‡•Ç‡§®‡§æ ‡•¶‡•ß ‡§¨‡§æ‡§ü ‡§¢‡§æ‡§Å‡§ö‡§æ‡§π‡§∞‡•Ç ‡§Ö‡§®‡•Å‡§∏‡§∞‡§£ ‡§ó‡§∞‡•ç‡§¶‡•à‡•§

```python
# chat_quickstart.py (Sample 01 pattern)
import os
import sys
from openai import OpenAI

try:
    from foundry_local import FoundryLocalManager
    FOUNDRY_SDK_AVAILABLE = True
except ImportError:
    FOUNDRY_SDK_AVAILABLE = False
    print("‚ö†Ô∏è Install foundry-local-sdk: pip install foundry-local-sdk")

def create_client():
    """Create OpenAI client with Foundry Local or Azure OpenAI."""
    # Check for Azure OpenAI configuration
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    
    if azure_endpoint and azure_api_key:
        # Azure OpenAI path
        model = os.environ.get("MODEL", "your-deployment-name")
        client = OpenAI(
            base_url=f"{azure_endpoint}/openai",
            api_key=azure_api_key,
            default_query={"api-version": "2024-08-01-preview"},
        )
        print(f"üåê Using Azure OpenAI with model: {model}")
        return client, model
    
    # Foundry Local path with SDK management
    alias = os.environ.get("MODEL", "phi-4-mini")
    if FOUNDRY_SDK_AVAILABLE:
        try:
            # Use FoundryLocalManager for proper service management
            manager = FoundryLocalManager(alias)
            model_info = manager.get_model_info(alias)
            
            client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            model = model_info.id
            print(f"üè† Using Foundry Local SDK with model: {model}")
            return client, model
        except Exception as e:
            print(f"‚ö†Ô∏è Foundry SDK failed ({e}), using manual configuration")
    
    # Fallback to manual configuration
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    api_key = os.environ.get("API_KEY", "")
    model = alias
    
    client = OpenAI(
        base_url=f"{base_url}/v1",
        api_key=api_key
    )
    print(f"üîß Manual configuration with model: {model}")
    return client, model

def main():
    """Main chat function."""
    client, model = create_client()
    
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    conversation_history = []
    
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        
        try:
            # Add user message to history
            conversation_history.append({"role": "user", "content": user_input})
            
            # Create chat completion
            response = client.chat.completions.create(
                model=model,
                messages=conversation_history,
                max_tokens=500,
                temperature=0.7
            )
            
            assistant_message = response.choices[0].message.content
            conversation_history.append({"role": "assistant", "content": assistant_message})
            
            print(f"Assistant: {assistant_message}\n")
            
        except Exception as e:
            print(f"Error: {e}\n")

if __name__ == "__main__":
    main()
```

### ‡§ö‡•ç‡§Ø‡§æ‡§ü ‡§è‡§™‡•ç‡§≤‡§ø‡§ï‡•á‡§∏‡§® ‡§ö‡§≤‡§æ‡§â‡§®‡•Å‡§π‡•ã‡§∏‡•ç

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Option 1: Using FoundryLocalManager (recommended)
python chat_quickstart.py "Explain what Foundry Local is"

# Option 2: Manual configuration with environment variables
set BASE_URL=http://localhost:8000
set MODEL=phi-4-mini
set API_KEY=
python chat_quickstart.py "Write a welcome message"

# Option 3: Azure OpenAI configuration
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name
python chat_quickstart.py "Hello from Azure OpenAI"
```

## ‡§≠‡§æ‡§ó ‡•¨: ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§® ‡§∞ ‡§â‡§§‡•ç‡§ï‡•É‡§∑‡•ç‡§ü ‡§Ö‡§≠‡•ç‡§Ø‡§æ‡§∏‡§π‡§∞‡•Ç

### ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ‡§π‡§∞‡•Ç ‡§∞ ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§®‡§π‡§∞‡•Ç

```powershell
# Issue: "Could not use Foundry SDK" warning
pip install foundry-local-sdk
# Or set environment variables for manual configuration

# Issue: Connection refused
foundry service status
foundry service ps  # Check loaded models

# Issue: Model not found
foundry model list
foundry model run phi-4-mini

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal

# Test API endpoint
curl http://localhost:8000/v1/models
```

### ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä ‡§∏‡•ç‡§∞‡•ã‡§§‡§π‡§∞‡•Ç ‡§Ö‡§®‡•Å‡§ó‡§Æ‡§® ‡§ó‡§∞‡•ç‡§®‡•á (Windows)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### ‡§µ‡§æ‡§§‡§æ‡§µ‡§∞‡§£ ‡§ö‡§∞‡§π‡§∞‡•Ç

| ‡§ö‡§∞ | ‡§µ‡§ø‡§µ‡§∞‡§£ | ‡§°‡§ø‡§´‡§≤‡•ç‡§ü | ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï |
|-----|--------|--------|---------|
| `MODEL` | ‡§Æ‡•ã‡§°‡•á‡§≤ ‡§â‡§™‡§®‡§æ‡§Æ ‡§µ‡§æ ‡§®‡§æ‡§Æ | `phi-4-mini` | ‡§π‡•ã‡§á‡§® |
| `BASE_URL` | Foundry Local ‡§Ü‡§ß‡§æ‡§∞ URL | `http://localhost:8000` | ‡§π‡•ã‡§á‡§® |
| `API_KEY` | API ‡§ï‡•Å‡§û‡•ç‡§ú‡•Ä (‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø‡§§‡§Ø‡§æ ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§õ‡•à‡§®) | `""` | ‡§π‡•ã‡§á‡§® |
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI ‡§Ö‡§®‡•ç‡§§ ‡§¨‡§ø‡§®‡•ç‡§¶‡•Å | - | Azure ‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø |
| `AZURE_OPENAI_API_KEY` | Azure OpenAI API ‡§ï‡•Å‡§û‡•ç‡§ú‡•Ä | - | Azure ‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø |
| `AZURE_OPENAI_API_VERSION` | Azure API ‡§∏‡§Ç‡§∏‡•ç‡§ï‡§∞‡§£ | `2024-08-01-preview` | ‡§π‡•ã‡§á‡§® |

### ‡§â‡§§‡•ç‡§ï‡•É‡§∑‡•ç‡§ü ‡§Ö‡§≠‡•ç‡§Ø‡§æ‡§∏‡§π‡§∞‡•Ç

- **OpenAI SDK ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç**: ‡§∞‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§Æ‡§∞‡•ç‡§Æ‡§§‡§Ø‡•ã‡§ó‡•ç‡§Ø‡§§‡§æ‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø OpenAI SDK ‡§≤‡§æ‡§à ‡§ï‡§ö‡•ç‡§ö‡§æ HTTP ‡§Ö‡§®‡•Å‡§∞‡•ã‡§ß‡§π‡§∞‡•Ç ‡§≠‡§®‡•ç‡§¶‡§æ ‡§™‡•ç‡§∞‡§æ‡§•‡§Æ‡§ø‡§ï‡§§‡§æ ‡§¶‡§ø‡§®‡•Å‡§π‡•ã‡§∏‡•ç
- **FoundryLocalManager**: ‡§∏‡•á‡§µ‡§æ ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ‡§™‡§®‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø ‡§Ü‡§ß‡§ø‡§ï‡§æ‡§∞‡§ø‡§ï SDK ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç ‡§ú‡§¨ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§õ
- **‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§π‡•ç‡§Ø‡§æ‡§®‡•ç‡§°‡§≤‡§ø‡§ô**: ‡§â‡§§‡•ç‡§™‡§æ‡§¶‡§® ‡§è‡§™‡•ç‡§≤‡§ø‡§ï‡•á‡§∏‡§®‡§π‡§∞‡•Ç‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§â‡§ö‡§ø‡§§ ‡§´‡§≤‡§¨‡•ç‡§Ø‡§æ‡§ï ‡§∞‡§£‡§®‡•Ä‡§§‡§ø‡§π‡§∞‡•Ç ‡§≤‡§æ‡§ó‡•Ç ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç
- **‡§®‡§ø‡§Ø‡§Æ‡§ø‡§§ ‡§∞‡•Ç‡§™‡§Æ‡§æ ‡§Ö‡§™‡§°‡•á‡§ü ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç**: ‡§®‡§Ø‡§æ‡§Å ‡§Æ‡•ã‡§°‡•á‡§≤ ‡§∞ ‡§∏‡•Å‡§ß‡§æ‡§∞‡§π‡§∞‡•Ç ‡§™‡§π‡•Å‡§Å‡§ö ‡§ó‡§∞‡•ç‡§® Foundry Local ‡§Ö‡§¶‡•ç‡§Ø‡§æ‡§µ‡§ß‡§ø‡§ï ‡§∞‡§æ‡§ñ‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç
- **‡§∏‡§æ‡§®‡•ã‡§¨‡§æ‡§ü ‡§∏‡•Å‡§∞‡•Å ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç**: ‡§∏‡§æ‡§®‡§æ ‡§Æ‡•ã‡§°‡•á‡§≤‡§π‡§∞‡•Ç (Phi mini, Qwen 7B) ‡§¨‡§æ‡§ü ‡§∏‡•Å‡§∞‡•Å ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç ‡§∞ ‡§µ‡§ø‡§∏‡•ç‡§§‡§æ‡§∞ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç
- **‡§∏‡•ç‡§∞‡•ã‡§§‡§π‡§∞‡•Ç ‡§Ö‡§®‡•Å‡§ó‡§Æ‡§® ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç**: CPU/GPU/‡§Æ‡•á‡§Æ‡•ã‡§∞‡•Ä ‡§ü‡•ç‡§∞‡•ç‡§Ø‡§æ‡§ï ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç ‡§ú‡§¨ ‡§™‡•ç‡§∞‡§Æ‡•ç‡§™‡•ç‡§ü‡§π‡§∞‡•Ç ‡§∞ ‡§∏‡•á‡§ü‡§ø‡§ô‡§π‡§∞‡•Ç ‡§ü‡•ç‡§Ø‡•Å‡§® ‡§ó‡§∞‡•ç‡§¶‡•à

## ‡§≠‡§æ‡§ó ‡•≠: ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§Ö‡§≠‡•ç‡§Ø‡§æ‡§∏‡§π‡§∞‡•Ç

### ‡§Ö‡§≠‡•ç‡§Ø‡§æ‡§∏ ‡•ß: ‡§õ‡§ø‡§ü‡•ã ‡§¨‡§π‡•Å-‡§Æ‡•ã‡§°‡•á‡§≤ ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b-instruct"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### ‡§Ö‡§≠‡•ç‡§Ø‡§æ‡§∏ ‡•®: OpenAI SDK ‡§è‡§ï‡•Ä‡§ï‡§∞‡§£ ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£

```python
# sdk_integration_test.py (matching Sample 01 pattern)
import os
from openai import OpenAI
from foundry_local import FoundryLocalManager

def test_model_integration(model_alias):
    """Test OpenAI SDK integration with different models."""
    try:
        # Use FoundryLocalManager for proper setup
        manager = FoundryLocalManager(model_alias)
        model_info = manager.get_model_info(model_alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Test basic completion
        response = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Say hello and state your model name."}],
            max_tokens=50
        )
        
        print(f"‚úÖ {model_alias}: {response.choices[0].message.content}")
        return True
    except Exception as e:
        print(f"‚ùå {model_alias}: {e}")
        return False

# Test multiple models
models_to_test = ["phi-4-mini", "qwen2.5-7b-instruct"]
for model in models_to_test:
    test_model_integration(model)
```

### ‡§Ö‡§≠‡•ç‡§Ø‡§æ‡§∏ ‡•©: ‡§µ‡•ç‡§Ø‡§æ‡§™‡§ï ‡§∏‡•á‡§µ‡§æ ‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø ‡§ú‡§æ‡§Å‡§ö

```python
# health_check.py
from openai import OpenAI
from foundry_local import FoundryLocalManager

def comprehensive_health_check():
    """Perform comprehensive health check of Foundry Local service."""
    try:
        # Initialize with a common model
        manager = FoundryLocalManager("phi-4-mini")
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # 1. Check service connectivity
        models_response = client.models.list()
        available_models = [model.id for model in models_response.data]
        print(f"‚úÖ Service healthy - {len(available_models)} models available")
        
        # 2. Test each available model
        for model_id in available_models:
            try:
                response = client.chat.completions.create(
                    model=model_id,
                    messages=[{"role": "user", "content": "Test"}],
                    max_tokens=10
                )
                print(f"‚úÖ {model_id}: Working")
            except Exception as e:
                print(f"‚ùå {model_id}: {e}")
        
        return True
    except Exception as e:
        print(f"‚ùå Service check failed: {e}")
        return False

comprehensive_health_check()
```

## ‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠‡§π‡§∞‡•Ç

- **Foundry Local ‡§∏‡•Å‡§∞‡•Å ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- **CLI ‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠ ‡§∞ ‡§ï‡§Æ‡§æ‡§£‡•ç‡§°‡§π‡§∞‡•Ç‡§ï‡•ã ‡§Ö‡§µ‡§≤‡•ã‡§ï‡§®**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- **OpenAI SDK ‡§è‡§ï‡•Ä‡§ï‡§∞‡§£**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- **Hugging Face ‡§Æ‡•ã‡§°‡•á‡§≤‡§π‡§∞‡•Ç ‡§ï‡§Æ‡•ç‡§™‡§æ‡§á‡§≤ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- **Microsoft Foundry Local GitHub**: https://github.com/microsoft/Foundry-Local
- **OpenAI Python SDK**: https://github.com/openai/openai-python
- **‡§®‡§Æ‡•Ç‡§®‡§æ ‡•¶‡•ß: OpenAI SDK ‡§Æ‡§æ‡§∞‡•ç‡§´‡§§ ‡§õ‡§ø‡§ü‡•ã ‡§ö‡•ç‡§Ø‡§æ‡§ü**: samples/01/README.md
- **‡§®‡§Æ‡•Ç‡§®‡§æ ‡•¶‡•®: ‡§â‡§®‡•ç‡§®‡§§ SDK ‡§è‡§ï‡•Ä‡§ï‡§∞‡§£**: samples/02/README.md

---

