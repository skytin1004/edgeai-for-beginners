{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af028554",
   "metadata": {},
   "source": [
    "# рдЗрдВрдЯреЗрдиреНрдЯ-рдЖрдзрд╛рд░рд┐рдд рдореЛрдбреЗрд▓ рд░рд╛рдЙрдЯрд░ рдлрд╛рдЙрдиреНрдбреНрд░реА рд▓реЛрдХрд▓ SDK рд╕рдВрдЧ\n",
    "\n",
    "**CPU-рдЕрдкреНрдЯрд┐рдорд╛рдЗрдЬреНрдб рдорд▓реНрдЯреА-рдореЛрдбреЗрд▓ рд░рд╛рдЙрдЯрд┐рдЩ рдкреНрд░рдгрд╛рд▓реА**\n",
    "\n",
    "рдпреЛ рдиреЛрдЯрдмреБрдХрд▓реЗ рдПрдХ рдмреБрджреНрдзрд┐рдорд╛рди рд░рд╛рдЙрдЯрд┐рдЩ рдкреНрд░рдгрд╛рд▓реА рдкреНрд░рджрд░реНрд╢рди рдЧрд░реНрджрдЫ рдЬрд╕рд▓реЗ рдкреНрд░рдпреЛрдЧрдХрд░реНрддрд╛рдХреЛ рдЙрджреНрджреЗрд╢реНрдпрдХреЛ рдЖрдзрд╛рд░рдорд╛ рд╕реНрд╡рддрдГ рд╕рдмреИрднрдиреНрджрд╛ рдЙрдкрдпреБрдХреНрдд рд╕рд╛рдиреЛ рднрд╛рд╖рд╛ рдореЛрдбреЗрд▓ рдЪрдпрди рдЧрд░реНрджрдЫред рдпреЛ рдХрд┐рдирд╛рд░рд╛рдорд╛ рддреИрдирд╛рддреА рдкрд░рд┐рджреГрд╢реНрдпрд╣рд░реВрдХреЛ рд▓рд╛рдЧрд┐ рдЙрдкрдпреБрдХреНрдд рдЫ рдЬрд╣рд╛рдБ рддрдкрд╛рдИрдВ рдзреЗрд░реИ рд╡рд┐рд╢реЗрд╖ рдореЛрдбреЗрд▓рд╣рд░реВрд▓рд╛рдИ рдкреНрд░рднрд╛рд╡рдХрд╛рд░реА рд░реВрдкрдорд╛ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрди рдЪрд╛рд╣рдиреБрд╣реБрдиреНрдЫред\n",
    "\n",
    "## ЁЯОп рддрдкрд╛рдИрдВрд▓реЗ рдХреЗ рд╕рд┐рдХреНрдиреБрд╣реБрдиреЗрдЫ\n",
    "\n",
    "- **рдЙрджреНрджреЗрд╢реНрдп рдкрддреНрддрд╛ рд▓рдЧрд╛рдЙрдиреЗ**: рд╕реНрд╡рддрдГ рдкреНрд░рдореНрдкреНрдЯрд╣рд░реВ рд╡рд░реНрдЧреАрдХрд░рдг рдЧрд░реНрдиреЗ (рдХреЛрдб, рд╕рдВрдХреНрд╖реЗрдк, рд╡рд░реНрдЧреАрдХрд░рдг, рд╕рд╛рдорд╛рдиреНрдп)\n",
    "- **рд╕реНрдорд╛рд░реНрдЯ рдореЛрдбреЗрд▓ рдЪрдпрди**: рдкреНрд░рддреНрдпреЗрдХ рдХрд╛рд░реНрдпрдХреЛ рд▓рд╛рдЧрд┐ рд╕рдмреИрднрдиреНрджрд╛ рд╕рдХреНрд╖рдо рдореЛрдбреЗрд▓рдорд╛ рд░рд╛рдЙрдЯ рдЧрд░реНрдиреЗ\n",
    "- **CPU рдЕрдкреНрдЯрд┐рдорд╛рдЗрдЬреЗрд╕рди**: рдореЗрдореЛрд░реА-рдХреБрд╢рд▓ рдореЛрдбреЗрд▓рд╣рд░реВ рдЬреБрди рдХреБрдиреИ рдкрдирд┐ рд╣рд╛рд░реНрдбрд╡реЗрдпрд░рдорд╛ рдХрд╛рдо рдЧрд░реНрдЫрдиреН\n",
    "- **рдорд▓реНрдЯреА-рдореЛрдбреЗрд▓ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди**: `--retain true` рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдзреЗрд░реИ рдореЛрдбреЗрд▓рд╣рд░реВ рд▓реЛрдб рд░рд╛рдЦреНрдиреЗ\n",
    "- **рдЙрддреНрдкрд╛рджрди рдврд╛рдБрдЪрд╛**: рдкреБрди: рдкреНрд░рдпрд╛рд╕ рдЧрд░реНрдиреЗ рддрд░реНрдХ, рддреНрд░реБрдЯрд┐ рд╣реНрдпрд╛рдиреНрдбрд▓рд┐рдЩ, рд░ рдЯреЛрдХрди рдЯреНрд░реНрдпрд╛рдХрд┐рдЩ\n",
    "\n",
    "## ЁЯУЛ рдкрд░рд┐рджреГрд╢реНрдпрдХреЛ рдЕрд╡рд▓реЛрдХрди\n",
    "\n",
    "рдпреЛ рдврд╛рдБрдЪрд╛рд▓реЗ рдкреНрд░рджрд░реНрд╢рди рдЧрд░реНрджрдЫ:\n",
    "\n",
    "1. **рдЙрджреНрджреЗрд╢реНрдп рдкрддреНрддрд╛ рд▓рдЧрд╛рдЙрдиреЗ**: рдкреНрд░рддреНрдпреЗрдХ рдкреНрд░рдпреЛрдЧрдХрд░реНрддрд╛ рдкреНрд░рдореНрдкреНрдЯрд▓рд╛рдИ рд╡рд░реНрдЧреАрдХрд░рдг рдЧрд░реНрдиреЗ (рдХреЛрдб, рд╕рдВрдХреНрд╖реЗрдк, рд╡рд░реНрдЧреАрдХрд░рдг, рд╡рд╛ рд╕рд╛рдорд╛рдиреНрдп)\n",
    "2. **рдореЛрдбреЗрд▓ рдЪрдпрди**: рдХреНрд╖рдорддрд╛рдХреЛ рдЖрдзрд╛рд░рдорд╛ рд╕реНрд╡рддрдГ рд╕рдмреИрднрдиреНрджрд╛ рдЙрдкрдпреБрдХреНрдд рд╕рд╛рдиреЛ рднрд╛рд╖рд╛ рдореЛрдбреЗрд▓ рдЪрдпрди рдЧрд░реНрдиреЗ\n",
    "3. **рд╕реНрдерд╛рдиреАрдп рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди**: рдлрд╛рдЙрдиреНрдбреНрд░реА рд▓реЛрдХрд▓ рд╕реЗрд╡рд╛рдХреЛ рдорд╛рдзреНрдпрдордмрд╛рдЯ рд╕реНрдерд╛рдиреАрдп рд░реВрдкрдорд╛ рдЪрд▓реНрдиреЗ рдореЛрдбреЗрд▓рд╣рд░реВрдорд╛ рд░рд╛рдЙрдЯ рдЧрд░реНрдиреЗ\n",
    "4. **рдПрдХреАрдХреГрдд рдЗрдиреНрдЯрд░рдлреЗрд╕**: рдзреЗрд░реИ рд╡рд┐рд╢реЗрд╖ рдореЛрдбреЗрд▓рд╣рд░реВрдорд╛ рд░рд╛рдЙрдЯ рдЧрд░реНрдиреЗ рдПрдХрд▓ рдЪреНрдпрд╛рдЯ рдкреНрд░рд╡реЗрд╢ рдмрд┐рдиреНрджреБ\n",
    "\n",
    "**рдЖрджрд░реНрд╢**: рдзреЗрд░реИ рд╡рд┐рд╢реЗрд╖ рдореЛрдбреЗрд▓рд╣рд░реВ рднрдПрдХреЛ рдХрд┐рдирд╛рд░рд╛рдорд╛ рддреИрдирд╛рддреАрд╣рд░реВрдХрд╛ рд▓рд╛рдЧрд┐ рдЬрд╣рд╛рдБ рддрдкрд╛рдИрдВ рдмреБрджреНрдзрд┐рдорд╛рди рдЕрдиреБрд░реЛрдз рд░рд╛рдЙрдЯрд┐рдЩ рдЪрд╛рд╣рдиреБрд╣реБрдиреНрдЫ рдмрд┐рдирд╛ рдореНрдпрд╛рдиреБрдЕрд▓ рдореЛрдбреЗрд▓ рдЪрдпрдиред\n",
    "\n",
    "## ЁЯФз рдкреВрд░реНрд╡рд╛рдкреЗрдХреНрд╖рд╛рд╣рд░реВ\n",
    "\n",
    "- **Foundry Local** рд╕реНрдерд╛рдкрдирд╛ рдЧрд░рд┐рдПрдХреЛ рд░ рд╕реЗрд╡рд╛ рдЪрд▓рд┐рд░рд╣реЗрдХреЛ\n",
    "- **Python 3.8+** рдкрд┐рдкрд╕рд╣рд┐рдд\n",
    "- **8GB+ RAM** (рдзреЗрд░реИ рдореЛрдбреЗрд▓рд╣рд░реВрдХреЛ рд▓рд╛рдЧрд┐ 16GB+ рд╕рд┐рдлрд╛рд░рд┐рд╕ рдЧрд░рд┐рдПрдХреЛ)\n",
    "- **workshop_utils** рдореЛрдбреНрдпреБрд▓ (../samples/ рдорд╛)\n",
    "\n",
    "## ЁЯЪА рдЫрд┐рдЯреЛ рд╕реБрд░реБ\n",
    "\n",
    "рдиреЛрдЯрдмреБрдХрд▓реЗ:\n",
    "1. рддрдкрд╛рдИрдВрдХреЛ рдкреНрд░рдгрд╛рд▓реА рдореЗрдореЛрд░реА рдкрддреНрддрд╛ рд▓рдЧрд╛рдЙрдиреЗрдЫ\n",
    "2. рдЙрдкрдпреБрдХреНрдд CPU рдореЛрдбреЗрд▓рд╣рд░реВ рд╕рд┐рдлрд╛рд░рд┐рд╕ рдЧрд░реНрдиреЗрдЫ\n",
    "3. `--retain true` рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рд╕реНрд╡рддрдГ рдореЛрдбреЗрд▓рд╣рд░реВ рд▓реЛрдб рдЧрд░реНрдиреЗрдЫ\n",
    "4. рд╕рдмреИ рдореЛрдбреЗрд▓рд╣рд░реВ рддрдпрд╛рд░ рдЫрдиреН рднрдиреА рдкреНрд░рдорд╛рдгрд┐рдд рдЧрд░реНрдиреЗрдЫ\n",
    "5. рдкрд░реАрдХреНрд╖рдг рдкреНрд░рдореНрдкреНрдЯрд╣рд░реВ рд╡рд┐рд╢реЗрд╖ рдореЛрдбреЗрд▓рд╣рд░реВрдорд╛ рд░рд╛рдЙрдЯ рдЧрд░реНрдиреЗрдЫ\n",
    "\n",
    "**рдЕрдиреБрдорд╛рдирд┐рдд рд╕реЗрдЯрдЕрдк рд╕рдордп**: рел-рен рдорд┐рдиреЗрдЯ (рдореЛрдбреЗрд▓ рд▓реЛрдбрд┐рдЩ рд╕рдорд╛рд╡реЗрд╢ рдЧрд░реНрджрдЫ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa55f0",
   "metadata": {},
   "source": [
    "## ЁЯУж рдЪрд░рдг рез: рдЖрд╡рд╢реНрдпрдХрддрд╛рд╣рд░реВ рд╕реНрдерд╛рдкрдирд╛ рдЧрд░реНрдиреБрд╣реЛрд╕реН\n",
    "\n",
    "рдФрдкрдЪрд╛рд░рд┐рдХ Foundry Local SDK рд░ рдЖрд╡рд╢реНрдпрдХ рдкреБрд╕реНрддрдХрд╛рд▓рдпрд╣рд░реВ рд╕реНрдерд╛рдкрдирд╛ рдЧрд░реНрдиреБрд╣реЛрд╕реН:\n",
    "\n",
    "- **foundry-local-sdk**: рд╕реНрдерд╛рдиреАрдп рдореЛрдбреЗрд▓ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрдирдХрд╛ рд▓рд╛рдЧрд┐ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ Python SDK\n",
    "- **openai**: рдЪреНрдпрд╛рдЯ рдХрдореНрдкреНрд▓рд┐рд╕рдирдХрд╛ рд▓рд╛рдЧрд┐ OpenAI-рд╕рдВрдЧрдд API\n",
    "- **psutil**: рдкреНрд░рдгрд╛рд▓реАрдХреЛ рдореЗрдореЛрд░реА рдкрддреНрддрд╛ рд▓рдЧрд╛рдЙрдиреЗ рд░ рдирд┐рдЧрд░рд╛рдиреА рдЧрд░реНрдиреЗ рдЙрдкрдХрд░рдг\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2929c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "!pip install -q foundry-local-sdk openai psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990799e7",
   "metadata": {},
   "source": [
    "## ЁЯТ╗ рдЪрд░рдг реи: рдкреНрд░рдгрд╛рд▓реА рдореЗрдореЛрд░реА рдкрддреНрддрд╛ рд▓рдЧрд╛рдЙрдиреЗ\n",
    "\n",
    "рдЙрдкрд▓рдмреНрдз рдкреНрд░рдгрд╛рд▓реА рдореЗрдореЛрд░реА рдкрддреНрддрд╛ рд▓рдЧрд╛рдПрд░ рдХреБрди CPU рдореЛрдбреЗрд▓рд╣рд░реВ рдкреНрд░рднрд╛рд╡рдХрд╛рд░реА рд░реВрдкрдорд╛ рдЪрд▓реНрди рд╕рдХреНрдЫрдиреН рднрдиреЗрд░ рдирд┐рд░реНрдзрд╛рд░рдг рдЧрд░реНрдиреБрд╣реЛрд╕реНред рдпрд╕рд▓реЗ рддрдкрд╛рдИрдВрдХреЛ рд╣рд╛рд░реНрдбрд╡реЗрдпрд░рдХрд╛ рд▓рд╛рдЧрд┐ рдЙрдкрдпреБрдХреНрдд рдореЛрдбреЗрд▓ рдЪрдпрди рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдЧрд░реНрджрдЫред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ff58f1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯЦея╕П  System Memory Information\n",
      "======================================================================\n",
      "Total Memory:     63.30 GB\n",
      "Available Memory: 16.19 GB\n",
      "\n",
      "тЬЕ High Memory System (32GB+)\n",
      "   Can run 3-4 models simultaneously\n",
      "\n",
      "ЁЯУЛ Recommended Model Aliases for Your System:\n",
      "   тАв phi-4-mini\n",
      "   тАв phi-3.5-mini\n",
      "   тАв qwen2.5-0.5b\n",
      "   тАв qwen2.5-coder-0.5b\n",
      "\n",
      "ЁЯТб About Model Aliases:\n",
      "   тЬУ Use base alias (e.g., phi-4-mini, not phi-4-mini-cpu)\n",
      "   тЬУ Foundry Local automatically selects CPU variant for your hardware\n",
      "   тЬУ No GPU required - optimized for CPU inference\n",
      "   тЬУ Predictable memory usage and consistent performance\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get system memory information\n",
    "total_memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "\n",
    "print('ЁЯЦея╕П  System Memory Information')\n",
    "print('=' * 70)\n",
    "print(f'Total Memory:     {total_memory_gb:.2f} GB')\n",
    "print(f'Available Memory: {available_memory_gb:.2f} GB')\n",
    "print()\n",
    "\n",
    "# Recommend models based on available memory\n",
    "# Using model aliases - Foundry Local will automatically select CPU variant\n",
    "model_aliases = []\n",
    "\n",
    "if total_memory_gb >= 32:\n",
    "    model_aliases = ['phi-4-mini', 'phi-3.5-mini', 'qwen2.5-0.5b', 'qwen2.5-coder-0.5b']\n",
    "    print('тЬЕ High Memory System (32GB+)')\n",
    "    print('   Can run 3-4 models simultaneously')\n",
    "elif total_memory_gb >= 16:\n",
    "    model_aliases = ['phi-4-mini', 'qwen2.5-0.5b', 'phi-3.5-mini']\n",
    "    print('тЬЕ Medium Memory System (16-32GB)')\n",
    "    print('   Can run 2-3 models simultaneously')\n",
    "elif total_memory_gb >= 8:\n",
    "    model_aliases = ['qwen2.5-0.5b', 'phi-3.5-mini']\n",
    "    print('тЪая╕П  Lower Memory System (8-16GB)')\n",
    "    print('   Recommended: 2 smaller models')\n",
    "else:\n",
    "    model_aliases = ['qwen2.5-0.5b']\n",
    "    print('тЪая╕П  Limited Memory System (<8GB)')\n",
    "    print('   Recommended: Use only smallest model')\n",
    "\n",
    "print()\n",
    "print('ЁЯУЛ Recommended Model Aliases for Your System:')\n",
    "for model in model_aliases:\n",
    "    print(f'   тАв {model}')\n",
    "\n",
    "print()\n",
    "print('ЁЯТб About Model Aliases:')\n",
    "print('   тЬУ Use base alias (e.g., phi-4-mini, not phi-4-mini-cpu)')\n",
    "print('   тЬУ Foundry Local automatically selects CPU variant for your hardware')\n",
    "print('   тЬУ No GPU required - optimized for CPU inference')\n",
    "print('   тЬУ Predictable memory usage and consistent performance')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69590b94",
   "metadata": {},
   "source": [
    "## ЁЯдЦ рдЪрд░рдг рей: рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рдореЛрдбреЗрд▓ рд▓реЛрдб рдЧрд░реНрджреИ\n",
    "\n",
    "рдпреЛ рд╕реЗрд▓рд▓реЗ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдкрдорд╛:\n",
    "1. Foundry Local рд╕реЗрд╡рд╛ рд╕реБрд░реБ рдЧрд░реНрдЫ (рдпрджрд┐ рдЪрд▓рд┐рд░рд╣реЗрдХреЛ рдЫреИрди рднрдиреЗ)\n",
    "2. `--retain true` рдХреЛ рд╕рд╛рде рд╕рд┐рдлрд╛рд░рд┐рд╕ рдЧрд░рд┐рдПрдХрд╛ рдореЛрдбреЗрд▓рд╣рд░реВ рд▓реЛрдб рдЧрд░реНрдЫ (рдзреЗрд░реИ рдореЛрдбреЗрд▓рд╣рд░реВ рдореЗрдореЛрд░реАрдорд╛ рд░рд╛рдЦреНрдЫ)\n",
    "3. SDK рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рд╕рдмреИ рдореЛрдбреЗрд▓рд╣рд░реВ рддрдпрд╛рд░ рдЫрдиреН рдХрд┐ рдЫреИрдирдиреН рднрдиреЗрд░ рдкреБрд╖реНрдЯрд┐ рдЧрд░реНрдЫ\n",
    "\n",
    "тП▒я╕П **рдЕрдкреЗрдХреНрд╖рд┐рдд рд╕рдордп**: рд╕рдмреИ рдореЛрдбреЗрд▓рд╣рд░реВрдХреЛ рд▓рд╛рдЧрд┐ рей-рел рдорд┐рдиреЗрдЯ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "543fd976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯЪА Automatic Model Loading with SDK Verification\n",
      "======================================================================\n",
      "ЁЯУЛ Loading 3 models: ['phi-4-mini', 'phi-3.5-mini', 'qwen2.5-0.5b']\n",
      "ЁЯТб Using model aliases - Foundry will load CPU variants automatically\n",
      "\n",
      "ЁЯУб Step 1: Checking Foundry Local service...\n",
      "   тЬЕ Service is already running\n",
      "\n",
      "ЁЯдЦ Step 2: Loading models with retention...\n",
      "   [1/3] Starting phi-4-mini...\n",
      "       тЬЕ phi-4-mini loading in background\n",
      "   [2/3] Starting phi-3.5-mini...\n",
      "       тЬЕ phi-3.5-mini loading in background\n",
      "   [3/3] Starting qwen2.5-0.5b...\n",
      "       тЬЕ qwen2.5-0.5b loading in background\n",
      "\n",
      "тЬЕ Step 3: Verifying models (this may take 2-3 minutes)...\n",
      "======================================================================\n",
      "\n",
      "   Attempt 1/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 2/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 3/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 4/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 5/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 6/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 7/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 8/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 9/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 10/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 11/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 12/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 13/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 14/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 15/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 16/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 17/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 18/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 19/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 20/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 21/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 22/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 23/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 24/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 25/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 26/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 27/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 28/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 29/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 30/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "======================================================================\n",
      "ЁЯУж Final Status: 0/3 models ready\n",
      "   тЭМ phi-4-mini - NOT READY\n",
      "   тЭМ phi-3.5-mini - NOT READY\n",
      "   тЭМ qwen2.5-0.5b - NOT READY\n",
      "\n",
      "тЪая╕П  Some models not ready. Check: foundry model ls\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add samples directory for workshop_utils (Foundry SDK pattern)\n",
    "sys.path.append(os.path.join('..', 'samples'))\n",
    "\n",
    "print('ЁЯЪА Automatic Model Loading with SDK Verification')\n",
    "print('=' * 70)\n",
    "\n",
    "# Use top 3 recommended models (aliases)\n",
    "# Foundry will automatically load CPU variants\n",
    "REQUIRED_MODELS = model_aliases[:3]\n",
    "print(f'ЁЯУЛ Loading {len(REQUIRED_MODELS)} models: {REQUIRED_MODELS}')\n",
    "print('ЁЯТб Using model aliases - Foundry will load CPU variants automatically')\n",
    "print()\n",
    "\n",
    "# Step 1: Ensure Foundry Local service is running\n",
    "print('ЁЯУб Step 1: Checking Foundry Local service...')\n",
    "try:\n",
    "    result = subprocess.run(['foundry', 'service', 'status'], \n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print('   тЬЕ Service is already running')\n",
    "    else:\n",
    "        print('   тЪЩя╕П  Starting Foundry Local service...')\n",
    "        subprocess.run(['foundry', 'service', 'start'], \n",
    "                      capture_output=True, text=True, timeout=30)\n",
    "        time.sleep(5)\n",
    "        print('   тЬЕ Service started')\n",
    "except Exception as e:\n",
    "    print(f'   тЪая╕П  Could not verify service: {e}')\n",
    "    print('   ЁЯТб Try manually: foundry service start')\n",
    "\n",
    "# Step 2: Load each model with --retain true\n",
    "print(f'\\nЁЯдЦ Step 2: Loading models with retention...')\n",
    "for i, model in enumerate(REQUIRED_MODELS, 1):\n",
    "    print(f'   [{i}/{len(REQUIRED_MODELS)}] Starting {model}...')\n",
    "    try:\n",
    "        subprocess.Popen(['foundry', 'model', 'run', model, '--retain', 'true'],\n",
    "                        stdout=subprocess.DEVNULL,\n",
    "                        stderr=subprocess.DEVNULL)\n",
    "        print(f'       тЬЕ {model} loading in background')\n",
    "    except Exception as e:\n",
    "        print(f'       тЭМ Error starting {model}: {e}')\n",
    "\n",
    "# Step 3: Verify models are ready\n",
    "print(f'\\nтЬЕ Step 3: Verifying models (this may take 2-3 minutes)...')\n",
    "print('=' * 70)\n",
    "\n",
    "try:\n",
    "    from workshop_utils import get_client\n",
    "    \n",
    "    ready_models = []\n",
    "    max_attempts = 30\n",
    "    attempt = 0\n",
    "    \n",
    "    while len(ready_models) < len(REQUIRED_MODELS) and attempt < max_attempts:\n",
    "        attempt += 1\n",
    "        print(f'\\n   Attempt {attempt}/{max_attempts}...')\n",
    "        \n",
    "        for model in REQUIRED_MODELS:\n",
    "            if model in ready_models:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                manager, client, model_id = get_client(model, None)\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "                    max_tokens=5,\n",
    "                    temperature=0\n",
    "                )\n",
    "                \n",
    "                if response and response.choices:\n",
    "                    ready_models.append(model)\n",
    "                    print(f'   тЬЕ {model} is READY')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_msg = str(e).lower()\n",
    "                if 'connection' in error_msg or 'timeout' in error_msg:\n",
    "                    print(f'   тП│ {model} still loading...')\n",
    "                else:\n",
    "                    print(f'   тЪая╕П  {model} error: {str(e)[:60]}...')\n",
    "        \n",
    "        if len(ready_models) == len(REQUIRED_MODELS):\n",
    "            break\n",
    "            \n",
    "        if len(ready_models) < len(REQUIRED_MODELS):\n",
    "            time.sleep(10)\n",
    "    \n",
    "    # Final status\n",
    "    print('\\n' + '=' * 70)\n",
    "    print(f'ЁЯУж Final Status: {len(ready_models)}/{len(REQUIRED_MODELS)} models ready')\n",
    "    \n",
    "    for model in REQUIRED_MODELS:\n",
    "        if model in ready_models:\n",
    "            print(f'   тЬЕ {model} - READY (retained in memory)')\n",
    "        else:\n",
    "            print(f'   тЭМ {model} - NOT READY')\n",
    "    \n",
    "    if len(ready_models) == len(REQUIRED_MODELS):\n",
    "        print('\\nЁЯОЙ All models loaded and verified!')\n",
    "        print('   тЬЕ Ready for intent-based routing')\n",
    "    else:\n",
    "        print(f'\\nтЪая╕П  Some models not ready. Check: foundry model ls')\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f'\\nтЭМ Cannot import workshop_utils: {e}')\n",
    "    print('   ЁЯТб Ensure workshop_utils.py is in ../samples/')\n",
    "except Exception as e:\n",
    "    print(f'\\nтЭМ Verification error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682909b",
   "metadata": {},
   "source": [
    "## ЁЯОп рдЪрд░рдг рек: рдЙрджреНрджреЗрд╢реНрдп рдкрд╣рд┐рдЪрд╛рди рд░ рдореЛрдбреЗрд▓ рдХреНрдпрд╛рдЯрд▓рдЧ рдХрдиреНрдлрд┐рдЧрд░ рдЧрд░реНрдиреБрд╣реЛрд╕реН\n",
    "\n",
    "рд░рд╛рдЙрдЯрд┐рдЩ рдкреНрд░рдгрд╛рд▓реА рд╕реЗрдЯрдЕрдк рдЧрд░реНрдиреБрд╣реЛрд╕реН:\n",
    "- **рдЙрджреНрджреЗрд╢реНрдп рдирд┐рдпрдорд╣рд░реВ**: рдкреНрд░рдореНрдкреНрдЯрд╣рд░реВ рд╡рд░реНрдЧреАрдХреГрдд рдЧрд░реНрди Regex рдврд╛рдБрдЪрд╛рд╣рд░реВ\n",
    "- **рдореЛрдбреЗрд▓ рдХреНрдпрд╛рдЯрд▓рдЧ**: рдореЛрдбреЗрд▓ рдХреНрд╖рдорддрд╛рд╣рд░реВрд▓рд╛рдИ рдЙрджреНрджреЗрд╢реНрдп рд╢реНрд░реЗрдгреАрд╣рд░реВрд╕рдБрдЧ рдорд┐рд▓рд╛рдЙрдБрдЫ\n",
    "- **рдкреНрд░рд╛рдердорд┐рдХрддрд╛ рдкреНрд░рдгрд╛рд▓реА**: рдзреЗрд░реИ рдореЛрдбреЗрд▓рд╣рд░реВ рдореЗрд▓ рдЦрд╛рдБрджрд╛ рдореЛрдбреЗрд▓ рдЪрдпрди рдирд┐рд░реНрдзрд╛рд░рдг рдЧрд░реНрджрдЫ\n",
    "\n",
    "**CPU рдореЛрдбреЗрд▓рдХрд╛ рдлрд╛рдЗрджрд╛рд╣рд░реВ**:\n",
    "- тЬЕ GPU рдЖрд╡рд╢реНрдпрдХ рдЫреИрди\n",
    "- тЬЕ рд╕реНрдерд┐рд░ рдкреНрд░рджрд░реНрд╢рди\n",
    "- тЬЕ рдХрдо рдКрд░реНрдЬрд╛ рдЦрдкрдд\n",
    "- тЬЕ рдЕрдиреБрдорд╛рдирд┐рдд рдореЗрдореЛрд░реА рдкреНрд░рдпреЛрдЧ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3620a4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯУЛ Active Model Catalog (Hardware-Optimized Aliases)\n",
      "======================================================================\n",
      "ЁЯТб Using model aliases - Foundry automatically selects CPU variants\n",
      "\n",
      "   тАв phi-4-mini\n",
      "     Capabilities: general, summarize, reasoning\n",
      "     Priority: 3\n",
      "\n",
      "   тАв qwen2.5-0.5b\n",
      "     Capabilities: classification, fast, general\n",
      "     Priority: 1\n",
      "\n",
      "   тАв phi-3.5-mini\n",
      "     Capabilities: code, refactor, technical\n",
      "     Priority: 2\n",
      "\n",
      "   тАв qwen2.5-coder-0.5b\n",
      "     Capabilities: code, programming, debug\n",
      "     Priority: 1\n",
      "\n",
      "тЬЕ Intent detection and model selection configured\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ЁЯТб Using model aliases - Foundry automatically selects CPU variants\n",
      "\n",
      "   тАв phi-4-mini\n",
      "     Capabilities: general, summarize, reasoning\n",
      "     Priority: 3\n",
      "\n",
      "   тАв qwen2.5-0.5b\n",
      "     Capabilities: classification, fast, general\n",
      "     Priority: 1\n",
      "\n",
      "   тАв phi-3.5-mini\n",
      "     Capabilities: code, refactor, technical\n",
      "     Priority: 2\n",
      "\n",
      "   тАв qwen2.5-coder-0.5b\n",
      "     Capabilities: code, programming, debug\n",
      "     Priority: 1\n",
      "\n",
      "тЬЕ Intent detection and model selection configured\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Model capability catalog (maps model aliases to capabilities)\n",
    "# Use base aliases - Foundry Local will automatically select CPU variants\n",
    "CATALOG = {\n",
    "    'phi-4-mini': {\n",
    "        'capabilities': ['general', 'summarize', 'reasoning'],\n",
    "        'priority': 3\n",
    "    },\n",
    "    'qwen2.5-0.5b': {\n",
    "        'capabilities': ['classification', 'fast', 'general'],\n",
    "        'priority': 1\n",
    "    },\n",
    "    'phi-3.5-mini': {\n",
    "        'capabilities': ['code', 'refactor', 'technical'],\n",
    "        'priority': 2\n",
    "    },\n",
    "    'qwen2.5-coder-0.5b': {\n",
    "        'capabilities': ['code', 'programming', 'debug'],\n",
    "        'priority': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Filter to only include models recommended for this system\n",
    "CATALOG = {k: v for k, v in CATALOG.items() if k in model_aliases}\n",
    "\n",
    "print('ЁЯУЛ Active Model Catalog (Hardware-Optimized Aliases)')\n",
    "print('=' * 70)\n",
    "print('ЁЯТб Using model aliases - Foundry automatically selects CPU variants')\n",
    "print()\n",
    "for model, info in CATALOG.items():\n",
    "    caps = ', '.join(info['capabilities'])\n",
    "    print(f'   тАв {model}')\n",
    "    print(f'     Capabilities: {caps}')\n",
    "    print(f'     Priority: {info[\"priority\"]}')\n",
    "    print()\n",
    "\n",
    "# Intent detection rules (regex pattern -> intent label)\n",
    "INTENT_RULES = [\n",
    "    (re.compile(r'code|refactor|function|debug|program', re.I), 'code'),\n",
    "    (re.compile(r'summar|abstract|tl;?dr|brief', re.I), 'summarize'),\n",
    "    (re.compile(r'classif|categor|label|sentiment', re.I), 'classification'),\n",
    "    (re.compile(r'explain|teach|describe', re.I), 'general'),\n",
    "]\n",
    "\n",
    "def detect_intent(prompt: str) -> str:\n",
    "    \"\"\"Detect intent from prompt using regex patterns.\n",
    "    \n",
    "    Args:\n",
    "        prompt: User input text\n",
    "        \n",
    "    Returns:\n",
    "        Intent label: 'code', 'summarize', 'classification', or 'general'\n",
    "    \"\"\"\n",
    "    for pattern, intent in INTENT_RULES:\n",
    "        if pattern.search(prompt):\n",
    "            return intent\n",
    "    return 'general'\n",
    "\n",
    "def pick_model(intent: str) -> str:\n",
    "    \"\"\"Select best model for intent based on capabilities and priority.\n",
    "    \n",
    "    Args:\n",
    "        intent: Detected intent category\n",
    "        \n",
    "    Returns:\n",
    "        Model alias string, or first available model if no match\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        (alias, info['priority']) \n",
    "        for alias, info in CATALOG.items() \n",
    "        if intent in info['capabilities']\n",
    "    ]\n",
    "    \n",
    "    if candidates:\n",
    "        # Sort by priority (higher = better)\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return candidates[0][0]\n",
    "    \n",
    "    # Fallback to first available model\n",
    "    return list(CATALOG.keys())[0] if CATALOG else None\n",
    "\n",
    "print('тЬЕ Intent detection and model selection configured')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb6d09",
   "metadata": {},
   "source": [
    "## ЁЯзк рдЪрд░рдг рел: рдЙрджреНрджреЗрд╢реНрдп рдкрд╣рд┐рдЪрд╛рди рдкрд░реАрдХреНрд╖рдг рдЧрд░реНрдиреБрд╣реЛрд╕реН\n",
    "\n",
    "рдкрдХреНрдХрд╛ рдЧрд░реНрдиреБрд╣реЛрд╕реН рдХрд┐ рдЙрджреНрджреЗрд╢реНрдп рдкрд╣рд┐рдЪрд╛рди рдкреНрд░рдгрд╛рд▓реАрд▓реЗ рд╡рд┐рднрд┐рдиреНрди рдкреНрд░рдХрд╛рд░рдХрд╛ рд╕рдВрдХреЗрддрд╣рд░реВрд▓рд╛рдИ рд╕рд╣реА рд░реВрдкрдорд╛ рд╡рд░реНрдЧреАрдХреГрдд рдЧрд░реНрджрдЫред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0fd85468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯзк Testing Intent Detection\n",
      "======================================================================\n",
      "\n",
      "Prompt: Refactor this Python function for better readabili...\n",
      "   Intent: code            тЖТ Model: phi-3.5-mini\n",
      "\n",
      "Prompt: Summarize the key points of this article...\n",
      "   Intent: summarize       тЖТ Model: phi-4-mini\n",
      "\n",
      "Prompt: Classify this customer feedback as positive or neg...\n",
      "   Intent: classification  тЖТ Model: qwen2.5-0.5b\n",
      "\n",
      "Prompt: Explain how edge AI differs from cloud AI...\n",
      "   Intent: general         тЖТ Model: phi-4-mini\n",
      "\n",
      "Prompt: Write a function to calculate fibonacci numbers...\n",
      "   Intent: code            тЖТ Model: phi-3.5-mini\n",
      "\n",
      "Prompt: Give me a brief overview of small language models...\n",
      "   Intent: summarize       тЖТ Model: phi-4-mini\n",
      "\n",
      "======================================================================\n",
      "тЬЕ Intent detection working correctly\n"
     ]
    }
   ],
   "source": [
    "# Test intent detection with sample prompts\n",
    "test_prompts = [\n",
    "    'Refactor this Python function for better readability',\n",
    "    'Summarize the key points of this article',\n",
    "    'Classify this customer feedback as positive or negative',\n",
    "    'Explain how edge AI differs from cloud AI',\n",
    "    'Write a function to calculate fibonacci numbers',\n",
    "    'Give me a brief overview of small language models'\n",
    "]\n",
    "\n",
    "print('ЁЯзк Testing Intent Detection')\n",
    "print('=' * 70)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    intent = detect_intent(prompt)\n",
    "    model = pick_model(intent)\n",
    "    print(f'\\nPrompt: {prompt[:50]}...')\n",
    "    print(f'   Intent: {intent:15s} тЖТ Model: {model}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('тЬЕ Intent detection working correctly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6a08b",
   "metadata": {},
   "source": [
    "## ЁЯЪА рдЪрд░рдг рем: рд░рд╛рдЙрдЯрд┐рдЩ рдлрдЩреНрд╕рди рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рдЧрд░реНрдиреБрд╣реЛрд╕реН\n",
    "\n",
    "рдореБрдЦреНрдп рд░рд╛рдЙрдЯрд┐рдЩ рдлрдЩреНрд╕рди рдмрдирд╛рдЙрдиреБрд╣реЛрд╕реН рдЬрд╕рд▓реЗ:\n",
    "1. рдкреНрд░рдореНрдкреНрдЯрдмрд╛рдЯ рдЙрджреНрджреЗрд╢реНрдп рдкрддреНрддрд╛ рд▓рдЧрд╛рдЙрдБрдЫ\n",
    "2. рдЙрдкрдпреБрдХреНрдд рдореЛрдбреЗрд▓ рдЪрдпрди рдЧрд░реНрдЫ\n",
    "3. Foundry Local SDK рдорд╛рд░реНрдлрдд рдЕрдиреБрд░реЛрдз рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рдЧрд░реНрдЫ\n",
    "4. рдЯреЛрдХрди рдкреНрд░рдпреЛрдЧ рд░ рддреНрд░реБрдЯрд┐рд╣рд░реВ рдЯреНрд░реНрдпрд╛рдХ рдЧрд░реНрдЫ\n",
    "\n",
    "**workshop_utils рдврд╛рдБрдЪрд╛ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрджрдЫ**:\n",
    "- рдПрдХреНрд╕реНрдкреЛрдиреЗрдиреНрд╕рд┐рдпрд▓ рдмреНрдпрд╛рдХрдЕрдлрд╕рд╣рд┐рдд рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рдкреБрди: рдкреНрд░рдпрд╛рд╕\n",
    "- OpenAI-рд╕рдБрдЧ рдорд┐рд▓реНрджреЛ API\n",
    "- рдЯреЛрдХрди рдЯреНрд░реНрдпрд╛рдХрд┐рдЩ рд░ рддреНрд░реБрдЯрд┐ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "24cc251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Routing function ready\n",
      "   Using Foundry Local SDK via workshop_utils\n",
      "   Token tracking: Enabled\n",
      "   Retry logic: Automatic with exponential backoff\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from workshop_utils import chat_once\n",
    "\n",
    "# Fix RETRY_BACKOFF environment variable if it has comments\n",
    "if 'RETRY_BACKOFF' in os.environ:\n",
    "    retry_val = os.environ['RETRY_BACKOFF'].strip().split()[0]\n",
    "    try:\n",
    "        float(retry_val)\n",
    "        os.environ['RETRY_BACKOFF'] = retry_val\n",
    "    except ValueError:\n",
    "        os.environ['RETRY_BACKOFF'] = '1.0'\n",
    "\n",
    "def route(prompt: str, max_tokens: int = 200, temperature: float = 0.7):\n",
    "    \"\"\"Route prompt to appropriate model based on intent.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Detect intent using regex patterns\n",
    "    2. Select best model by capability + priority\n",
    "    3. Execute via Foundry Local SDK\n",
    "    \n",
    "    Args:\n",
    "        prompt: User input text\n",
    "        max_tokens: Maximum tokens in response\n",
    "        temperature: Sampling temperature (0-1)\n",
    "        \n",
    "    Returns:\n",
    "        Dict with: intent, model, output, tokens, usage, error\n",
    "    \"\"\"\n",
    "    intent = detect_intent(prompt)\n",
    "    model_alias = pick_model(intent)\n",
    "    \n",
    "    if not model_alias:\n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'model': None,\n",
    "            'output': '',\n",
    "            'tokens': None,\n",
    "            'usage': {},\n",
    "            'error': 'No suitable model found'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Call Foundry Local via workshop_utils\n",
    "        text, usage = chat_once(\n",
    "            model_alias,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        # Extract token information\n",
    "        usage_info = {}\n",
    "        if usage:\n",
    "            usage_info['prompt_tokens'] = getattr(usage, 'prompt_tokens', None)\n",
    "            usage_info['completion_tokens'] = getattr(usage, 'completion_tokens', None)\n",
    "            usage_info['total_tokens'] = getattr(usage, 'total_tokens', None)\n",
    "        \n",
    "        # Estimate if not provided\n",
    "        if not usage_info.get('total_tokens'):\n",
    "            est_prompt = len(prompt) // 4\n",
    "            est_completion = len(text or '') // 4\n",
    "            usage_info['estimated_tokens'] = est_prompt + est_completion\n",
    "        \n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'model': model_alias,\n",
    "            'output': (text or '').strip(),\n",
    "            'tokens': usage_info.get('total_tokens') or usage_info.get('estimated_tokens'),\n",
    "            'usage': usage_info,\n",
    "            'error': None\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'model': model_alias,\n",
    "            'output': '',\n",
    "            'tokens': None,\n",
    "            'usage': {},\n",
    "            'error': f'{type(e).__name__}: {str(e)}'\n",
    "        }\n",
    "\n",
    "print('тЬЕ Routing function ready')\n",
    "print('   Using Foundry Local SDK via workshop_utils')\n",
    "print('   Token tracking: Enabled')\n",
    "print('   Retry logic: Automatic with exponential backoff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5c915",
   "metadata": {},
   "source": [
    "## ЁЯОп рдЪрд░рдг рен: рд░рд╛рдЙрдЯрд┐рдЩ рдкрд░реАрдХреНрд╖рдгрд╣рд░реВ рдЪрд▓рд╛рдЙрдиреБрд╣реЛрд╕реН\n",
    "\n",
    "рд╡рд┐рднрд┐рдиреНрди рдкреНрд░рдореНрдкреНрдЯрд╣рд░реВ рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдкреВрд░реНрдг рд░рд╛рдЙрдЯрд┐рдЩ рдкреНрд░рдгрд╛рд▓реА рдкрд░реАрдХреНрд╖рдг рдЧрд░реНрдиреБрд╣реЛрд╕реН рддрд╛рдХрд┐ рджреЗрдЦрд╛рдЙрди рд╕рдХрд┐рдпреЛрд╕реН:\n",
    "- рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рдЙрджреНрджреЗрд╢реНрдп рдкрд╣рд┐рдЪрд╛рди\n",
    "- рдмреМрджреНрдзрд┐рдХ рдореЛрдбреЗрд▓ рдЪрдпрди\n",
    "- рдмрд╣реБ-рдореЛрдбреЗрд▓ рд░рд╛рдЙрдЯрд┐рдЩрд╕рдБрдЧ рдореЛрдбреЗрд▓рд╣рд░реВ рдХрд╛рдпрдо рд░рд╛рдЦреНрдиреЗ\n",
    "- рдЯреЛрдХрди рдЯреНрд░реНрдпрд╛рдХрд┐рдЩ рд░ рдкреНрд░рджрд░реНрд╢рди рдореЗрдЯреНрд░рд┐рдХреНрд╕\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c46ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯОп Running Intent-Based Routing Tests\n",
      "================================================================================\n",
      "\n",
      "[1/6] Testing prompt...\n",
      "Prompt: Refactor this Python function to make it more efficient and readable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Expected Intent: code\n",
      "   Detected Intent: code тЬЕ\n",
      "   Selected Model:  phi-3.5-mini\n",
      "   тЬЕ Response: To refactor a Python function for efficiency and readability, I would need to see the specific funct...\n",
      "   ЁЯУК Tokens: ~158 (estimated)\n",
      "\n",
      "[2/6] Testing prompt...\n",
      "Prompt: Summarize the key benefits of using small language models at the edge\n",
      "   Expected Intent: summarize\n",
      "   Detected Intent: summarize тЬЕ\n",
      "   Selected Model:  phi-4-mini\n",
      "   тЭМ Error: APIConnectionError: Connection error.\n",
      "\n",
      "[3/6] Testing prompt...\n",
      "Prompt: Classify this user feedback: The app is slow but the UI looks great\n",
      "   Expected Intent: classification\n",
      "   Detected Intent: classification тЬЕ\n",
      "   Selected Model:  qwen2.5-0.5b\n",
      "   тЭМ Error: APIConnectionError: Connection error.\n",
      "\n",
      "[4/6] Testing prompt...\n",
      "Prompt: Explain the difference between local and cloud inference\n",
      "   Expected Intent: general\n",
      "   Detected Intent: general тЬЕ\n",
      "   Selected Model:  phi-4-mini\n",
      "   тЭМ Error: APIConnectionError: Connection error.\n",
      "\n",
      "[5/6] Testing prompt...\n",
      "Prompt: Write a Python function to calculate the Fibonacci sequence\n"
     ]
    }
   ],
   "source": [
    "# Test prompts covering all intent categories\n",
    "test_cases = [\n",
    "    {\n",
    "        'prompt': 'Refactor this Python function to make it more efficient and readable',\n",
    "        'expected_intent': 'code'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Summarize the key benefits of using small language models at the edge',\n",
    "        'expected_intent': 'summarize'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Classify this user feedback: The app is slow but the UI looks great',\n",
    "        'expected_intent': 'classification'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Explain the difference between local and cloud inference',\n",
    "        'expected_intent': 'general'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Write a Python function to calculate the Fibonacci sequence',\n",
    "        'expected_intent': 'code'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Give me a brief overview of the Phi model family',\n",
    "        'expected_intent': 'summarize'\n",
    "    }\n",
    "]\n",
    "\n",
    "print('ЁЯОп Running Intent-Based Routing Tests')\n",
    "print('=' * 80)\n",
    "\n",
    "results = []\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f'\\n[{i}/{len(test_cases)}] Testing prompt...')\n",
    "    print(f'Prompt: {test[\"prompt\"]}')\n",
    "    \n",
    "    result = route(test['prompt'], max_tokens=150)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f'   Expected Intent: {test[\"expected_intent\"]}')\n",
    "    print(f'   Detected Intent: {result[\"intent\"]} {\"тЬЕ\" if result[\"intent\"] == test[\"expected_intent\"] else \"тЪая╕П\"}')\n",
    "    print(f'   Selected Model:  {result[\"model\"]}')\n",
    "    \n",
    "    if result['error']:\n",
    "        print(f'   тЭМ Error: {result[\"error\"]}')\n",
    "    else:\n",
    "        output_preview = result['output'][:100] + '...' if len(result['output']) > 100 else result['output']\n",
    "        print(f'   тЬЕ Response: {output_preview}')\n",
    "        \n",
    "        tokens = result.get('tokens', 0)\n",
    "        if tokens:\n",
    "            usage = result.get('usage', {})\n",
    "            if 'estimated_tokens' in usage:\n",
    "                print(f'   ЁЯУК Tokens: ~{tokens} (estimated)')\n",
    "            else:\n",
    "                print(f'   ЁЯУК Tokens: {tokens}')\n",
    "\n",
    "# Summary statistics\n",
    "print('\\n' + '=' * 80)\n",
    "print('ЁЯУК ROUTING SUMMARY')\n",
    "print('=' * 80)\n",
    "\n",
    "success_count = sum(1 for r in results if not r['error'])\n",
    "total_tokens = sum(r.get('tokens', 0) or 0 for r in results if not r['error'])\n",
    "intent_accuracy = sum(1 for i, r in enumerate(results) if r['intent'] == test_cases[i]['expected_intent'])\n",
    "\n",
    "print(f'Total Prompts:        {len(results)}')\n",
    "print(f'тЬЕ Successful:         {success_count}/{len(results)}')\n",
    "print(f'тЭМ Failed:             {len(results) - success_count}')\n",
    "print(f'ЁЯОп Intent Accuracy:    {intent_accuracy}/{len(results)} ({intent_accuracy/len(results)*100:.1f}%)')\n",
    "print(f'ЁЯУК Total Tokens Used:  {total_tokens}')\n",
    "\n",
    "# Model usage distribution\n",
    "print('\\nЁЯУЛ Model Usage Distribution:')\n",
    "model_counts = {}\n",
    "for r in results:\n",
    "    if r['model']:\n",
    "        model_counts[r['model']] = model_counts.get(r['model'], 0) + 1\n",
    "\n",
    "for model, count in sorted(model_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / len(results)) * 100\n",
    "    print(f'   тАв {model}: {count} requests ({percentage:.1f}%)')\n",
    "\n",
    "if success_count == len(results):\n",
    "    print('\\nЁЯОЙ All routing tests passed successfully!')\n",
    "else:\n",
    "    print(f'\\nтЪая╕П  {len(results) - success_count} test(s) failed')\n",
    "    print('   Check Foundry Local service: foundry service status')\n",
    "    print('   Verify models loaded: foundry model ls')\n",
    "\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764811e",
   "metadata": {},
   "source": [
    "## ЁЯФз рдЪрд░рдг рео: рдЕрдиреНрддрд░рдХреНрд░рд┐рдпрд╛рддреНрдордХ рдкрд░реАрдХреНрд╖рдг\n",
    "\n",
    "рдЖрдлреНрдиреИ рдкреНрд░рдореНрдкреНрдЯрд╣рд░реВ рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рд░рд╛рдЙрдЯрд┐рдЩ рдкреНрд░рдгрд╛рд▓реАрд▓рд╛рдИ рдХрд╛рдордорд╛ рд╣реЗрд░реНрдиреБрд╣реЛрд╕реН!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fdd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯОп Interactive Routing Test\n",
      "================================================================================\n",
      "Your prompt: Explain how model quantization reduces memory usage\n",
      "\n",
      "Detected Intent: general\n",
      "Selected Model:  phi-4-mini\n",
      "\n",
      "тЬЕ Response:\n",
      "--------------------------------------------------------------------------------\n",
      "Model quantization is a technique used to reduce the memory footprint of a machine learning model, particularly deep learning models. It works by converting the high-precision weights of a neural network, typically represented as 32-bit floating-point numbers, into lower-precision representations, such as 8-bit integers or even binary values.\n",
      "\n",
      "\n",
      "The primary reason for quantization is to decrease the amount of memory required to store the model's parameters. Since floating-point numbers take up more space than integers, by quantizing the weights, we can significantly reduce the model's size. This reduction in size not only saves memory but also can lead to faster computation during inference, as integer operations are generally faster than floating-point operations on many hardware platforms.\n",
      "\n",
      "\n",
      "However, quantization can introduce some loss of accuracy because the lower precision representation may not capture the full range of values that the floating-point representation can. To mitigate this, techniques such as quantization-aware training can be used, where the model is trained with quantization in mind,\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ЁЯУК Tokens used: 292\n",
      "\n",
      "ЁЯТб Try different prompts to test routing behavior!\n",
      "Detected Intent: general\n",
      "Selected Model:  phi-4-mini\n",
      "\n",
      "тЬЕ Response:\n",
      "--------------------------------------------------------------------------------\n",
      "Model quantization is a technique used to reduce the memory footprint of a machine learning model, particularly deep learning models. It works by converting the high-precision weights of a neural network, typically represented as 32-bit floating-point numbers, into lower-precision representations, such as 8-bit integers or even binary values.\n",
      "\n",
      "\n",
      "The primary reason for quantization is to decrease the amount of memory required to store the model's parameters. Since floating-point numbers take up more space than integers, by quantizing the weights, we can significantly reduce the model's size. This reduction in size not only saves memory but also can lead to faster computation during inference, as integer operations are generally faster than floating-point operations on many hardware platforms.\n",
      "\n",
      "\n",
      "However, quantization can introduce some loss of accuracy because the lower precision representation may not capture the full range of values that the floating-point representation can. To mitigate this, techniques such as quantization-aware training can be used, where the model is trained with quantization in mind,\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ЁЯУК Tokens used: 292\n",
      "\n",
      "ЁЯТб Try different prompts to test routing behavior!\n"
     ]
    }
   ],
   "source": [
    "# Interactive testing - modify the prompt and run this cell\n",
    "custom_prompt = \"Explain how model quantization reduces memory usage\"\n",
    "\n",
    "print('ЁЯОп Interactive Routing Test')\n",
    "print('=' * 80)\n",
    "print(f'Your prompt: {custom_prompt}')\n",
    "print()\n",
    "\n",
    "result = route(custom_prompt, max_tokens=200)\n",
    "\n",
    "print(f'Detected Intent: {result[\"intent\"]}')\n",
    "print(f'Selected Model:  {result[\"model\"]}')\n",
    "print()\n",
    "\n",
    "if result['error']:\n",
    "    print(f'тЭМ Error: {result[\"error\"]}')\n",
    "else:\n",
    "    print('тЬЕ Response:')\n",
    "    print('-' * 80)\n",
    "    print(result['output'])\n",
    "    print('-' * 80)\n",
    "    \n",
    "    if result['tokens']:\n",
    "        print(f'\\nЁЯУК Tokens used: {result[\"tokens\"]}')\n",
    "\n",
    "print('\\nЁЯТб Try different prompts to test routing behavior!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c17226c",
   "metadata": {},
   "source": [
    "## ЁЯУК рдЪрд░рдг реп: рдкреНрд░рджрд░реНрд╢рди рд╡рд┐рд╢реНрд▓реЗрд╖рдг\n",
    "\n",
    "рд░рд╛рдЙрдЯрд┐рдЩ рдкреНрд░рдгрд╛рд▓реАрдХреЛ рдкреНрд░рджрд░реНрд╢рди рд░ рдореЛрдбреЗрд▓рдХреЛ рдЙрдкрдпреЛрдЧрдХреЛ рд╡рд┐рд╢реНрд▓реЗрд╖рдг рдЧрд░реНрдиреБрд╣реЛрд╕реНред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЪб Performance Benchmark\n",
      "================================================================================\n",
      "\n",
      "Prompt: Write a hello world function...\n",
      "   Model: phi-3.5-mini\n",
      "   Time: 3.31s\n",
      "   Tokens: 60\n",
      "\n",
      "Prompt: Write a hello world function...\n",
      "   Model: phi-3.5-mini\n",
      "   Time: 3.31s\n",
      "   Tokens: 60\n",
      "\n",
      "Prompt: Summarize: AI at the edge is powerful...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 84\n",
      "\n",
      "Prompt: Summarize: AI at the edge is powerful...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 84\n",
      "\n",
      "Prompt: Classify: Good product...\n",
      "   Model: qwen2.5-0.5b\n",
      "   Time: 7.21s\n",
      "   Tokens: 69\n",
      "\n",
      "Prompt: Classify: Good product...\n",
      "   Model: qwen2.5-0.5b\n",
      "   Time: 7.21s\n",
      "   Tokens: 69\n",
      "\n",
      "Prompt: Explain edge computing...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 72\n",
      "\n",
      "================================================================================\n",
      "ЁЯУК Performance Statistics:\n",
      "   Average response time: 27.46s\n",
      "   Fastest response:      3.31s\n",
      "   Slowest response:      49.67s\n",
      "\n",
      "ЁЯТб Note: First request may be slower due to model initialization\n",
      "================================================================================\n",
      "\n",
      "Prompt: Explain edge computing...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 72\n",
      "\n",
      "================================================================================\n",
      "ЁЯУК Performance Statistics:\n",
      "   Average response time: 27.46s\n",
      "   Fastest response:      3.31s\n",
      "   Slowest response:      49.67s\n",
      "\n",
      "ЁЯТб Note: First request may be slower due to model initialization\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Performance benchmark\n",
    "benchmark_prompts = [\n",
    "    'Write a hello world function',\n",
    "    'Summarize: AI at the edge is powerful',\n",
    "    'Classify: Good product',\n",
    "    'Explain edge computing'\n",
    "]\n",
    "\n",
    "print('тЪб Performance Benchmark')\n",
    "print('=' * 80)\n",
    "\n",
    "timings = []\n",
    "for prompt in benchmark_prompts:\n",
    "    start = time.time()\n",
    "    result = route(prompt, max_tokens=50)\n",
    "    duration = time.time() - start\n",
    "    timings.append(duration)\n",
    "    \n",
    "    print(f'\\nPrompt: {prompt[:40]}...')\n",
    "    print(f'   Model: {result[\"model\"]}')\n",
    "    print(f'   Time: {duration:.2f}s')\n",
    "    if result.get('tokens'):\n",
    "        print(f'   Tokens: {result[\"tokens\"]}')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('ЁЯУК Performance Statistics:')\n",
    "print(f'   Average response time: {sum(timings)/len(timings):.2f}s')\n",
    "print(f'   Fastest response:      {min(timings):.2f}s')\n",
    "print(f'   Slowest response:      {max(timings):.2f}s')\n",
    "print('\\nЁЯТб Note: First request may be slower due to model initialization')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db64ff",
   "metadata": {},
   "source": [
    "## ЁЯОУ рдореБрдЦреНрдп рд╕рд┐рдХрд╛рдЗрд╣рд░реВ рд░ рдЖрдЧрд╛рдореА рдХрджрдорд╣рд░реВ\n",
    "\n",
    "### тЬЕ рддрдкрд╛рдИрдВрд▓реЗ рдХреЗ рд╕рд┐рдХреНрдиреБрднрдпреЛ\n",
    "\n",
    "1. **рдЗрдиреНрдЯреЗрдиреНрдЯ-рдЖрдзрд╛рд░рд┐рдд рд░рд╛рдЙрдЯрд┐рдЩ**: рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдкрдорд╛ рдкреНрд░рдореНрдкреНрдЯрд╣рд░реВ рд╡рд░реНрдЧреАрдХрд░рдг рдЧрд░реНрдиреБрд╣реЛрд╕реН рд░ рд╡рд┐рд╢реЗрд╖ рдореЛрдбреЗрд▓рд╣рд░реВрдорд╛ рд░рд╛рдЙрдЯ рдЧрд░реНрдиреБрд╣реЛрд╕реН  \n",
    "2. **рдореЗрдореЛрд░реА-рдкреНрд░рднрд╛рд╡рд┐рдд рдЪрдпрди**: рдЙрдкрд▓рдмреНрдз рдкреНрд░рдгрд╛рд▓реАрдХреЛ RAM рдЕрдиреБрд╕рд╛рд░ CPU рдореЛрдбреЗрд▓рд╣рд░реВ рдЪрдпрди рдЧрд░реНрдиреБрд╣реЛрд╕реН  \n",
    "3. **рдорд▓реНрдЯрд┐-рдореЛрдбреЗрд▓ рд░рд┐рдЯреЗрдиреНрд╕рди**: `--retain true` рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдзреЗрд░реИ рдореЛрдбреЗрд▓рд╣рд░реВ рд▓реЛрдб рд░рд╛рдЦреНрдиреБрд╣реЛрд╕реН  \n",
    "4. **рдЙрддреНрдкрд╛рджрди рдврд╛рдБрдЪрд╛**: рдкреБрди: рдкреНрд░рдпрд╛рд╕ рдЧрд░реНрдиреЗ рддрд░реНрдХ, рддреНрд░реБрдЯрд┐ рд╣реНрдпрд╛рдиреНрдбрд▓рд┐рдЩ, рд░ рдЯреЛрдХрди рдЯреНрд░реНрдпрд╛рдХрд┐рдЩ  \n",
    "5. **CPU рдЕрдиреБрдХреВрд▓рди**: GPU рдмрд┐рдирд╛ рдХреБрд╢рд▓рддрд╛рдкреВрд░реНрд╡рдХ рддреИрдирд╛рдд рдЧрд░реНрдиреБрд╣реЛрд╕реН  \n",
    "\n",
    "### ЁЯЪА рдкреНрд░рдпреЛрдЧрдХрд╛ рд╡рд┐рдЪрд╛рд░рд╣рд░реВ\n",
    "\n",
    "1. **рдХрд╕реНрдЯрдо рдЗрдиреНрдЯреЗрдиреНрдЯрд╣рд░реВ рдердкреНрдиреБрд╣реЛрд╕реН**:  \n",
    "   ```python\n",
    "   INTENT_RULES.append(\n",
    "       (re.compile(r'translate|convert', re.I), 'translation')\n",
    "   )\n",
    "   ```\n",
    "  \n",
    "2. **рдердк рдореЛрдбреЗрд▓рд╣рд░реВ рд▓реЛрдб рдЧрд░реНрдиреБрд╣реЛрд╕реН**:  \n",
    "   ```bash\n",
    "   foundry model run llama-3.2-1b-cpu --retain true\n",
    "   ```\n",
    "  \n",
    "3. **рдореЛрдбреЗрд▓ рдЪрдпрди рдЯреНрдпреБрди рдЧрд░реНрдиреБрд╣реЛрд╕реН**:  \n",
    "   - CATALOG рдорд╛ рдкреНрд░рд╛рдердорд┐рдХрддрд╛ рдорд╛рдирд╣рд░реВ рд╕рдорд╛рдпреЛрдЬрди рдЧрд░реНрдиреБрд╣реЛрд╕реН  \n",
    "   - рдердк рдХреНрд╖рдорддрд╛ рдЯреНрдпрд╛рдЧрд╣рд░реВ рдердкреНрдиреБрд╣реЛрд╕реН  \n",
    "   - рдлрд▓рдмреНрдпрд╛рдХ рд░рдгрдиреАрддрд┐рд╣рд░реВ рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рдЧрд░реНрдиреБрд╣реЛрд╕реН  \n",
    "\n",
    "4. **рдкреНрд░рджрд░реНрд╢рди рдЕрдиреБрдЧрдорди рдЧрд░реНрдиреБрд╣реЛрд╕реН**:  \n",
    "   ```python\n",
    "   import psutil\n",
    "   print(f\"Memory: {psutil.virtual_memory().percent}%\")\n",
    "   ```\n",
    "  \n",
    "\n",
    "### ЁЯУЪ рдердк рд╕реНрд░реЛрддрд╣рд░реВ\n",
    "\n",
    "- **Foundry Local SDK**: https://github.com/microsoft/Foundry-Local  \n",
    "- **рд╡рд░реНрдХрд╢рдк рдирдореВрдирд╛рд╣рд░реВ**: ../samples/  \n",
    "- **рдПрдЬ AI рдХреЛрд░реНрд╕**: ../../Module08/  \n",
    "\n",
    "### ЁЯТб рдЙрддреНрдХреГрд╖реНрдЯ рдЕрднреНрдпрд╛рд╕рд╣рд░реВ\n",
    "\n",
    "тЬЕ CPU рдореЛрдбреЗрд▓рд╣рд░реВ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреБрд╣реЛрд╕реН рддрд╛рдХрд┐ рдХреНрд░рд╕-рдкреНрд▓реНрдпрд╛рдЯрдлрд░реНрдо рд╡реНрдпрд╡рд╣рд╛рд░ рд╕реНрдерд┐рд░ рд░рд╣реЛрд╕реН  \n",
    "тЬЕ рдзреЗрд░реИ рдореЛрдбреЗрд▓рд╣рд░реВ рд▓реЛрдб рдЧрд░реНрдиреБ рдЕрдШрд┐ рд╕рдзреИрдВ рдкреНрд░рдгрд╛рд▓реАрдХреЛ рдореЗрдореЛрд░реА рдЬрд╛рдБрдЪ рдЧрд░реНрдиреБрд╣реЛрд╕реН  \n",
    "тЬЕ рд░рд╛рдЙрдЯрд┐рдЩ рдкрд░рд┐рджреГрд╢реНрдпрд╣рд░реВрдХреЛ рд▓рд╛рдЧрд┐ `--retain true` рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреБрд╣реЛрд╕реН  \n",
    "тЬЕ рдЙрдЪрд┐рдд рддреНрд░реБрдЯрд┐ рд╣реНрдпрд╛рдиреНрдбрд▓рд┐рдЩ рд░ рдкреБрди: рдкреНрд░рдпрд╛рд╕ рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рдЧрд░реНрдиреБрд╣реЛрд╕реН  \n",
    "тЬЕ рд▓рд╛рдЧрдд/рдкреНрд░рджрд░реНрд╢рди рдЕрдиреБрдХреВрд▓рдирдХреЛ рд▓рд╛рдЧрд┐ рдЯреЛрдХрди рдкреНрд░рдпреЛрдЧ рдЯреНрд░реНрдпрд╛рдХ рдЧрд░реНрдиреБрд╣реЛрд╕реН  \n",
    "\n",
    "---\n",
    "\n",
    "**ЁЯОЙ рдмрдзрд╛рдИ рдЫ!** рддрдкрд╛рдИрдВрд▓реЗ Foundry Local SDK рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ CPU-рдЕрдиреБрдХреВрд▓рд┐рдд рдореЛрдбреЗрд▓рд╣рд░реВрдХреЛ рд╕рд╛рде рдЙрддреНрдкрд╛рджрди-рддрдпрд╛рд░ рдЗрдиреНрдЯреЗрдиреНрдЯ-рдЖрдзрд╛рд░рд┐рдд рдореЛрдбреЗрд▓ рд░рд╛рдЙрдЯрд░ рдирд┐рд░реНрдорд╛рдг рдЧрд░реНрдиреБрднрдПрдХреЛ рдЫ!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**рдЕрд╕реНрд╡реАрдХрд░рдг**:  \nрдпреЛ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝ AI рдЕрдиреБрд╡рд╛рдж рд╕реЗрд╡рд╛ [Co-op Translator](https://github.com/Azure/co-op-translator) рдкреНрд░рдпреЛрдЧ рдЧрд░реА рдЕрдиреБрд╡рд╛рдж рдЧрд░рд┐рдПрдХреЛ рд╣реЛред рд╣рд╛рдореА рдпрдерд╛рд╕рдореНрднрд╡ рд╕рдЯреАрдХрддрд╛ рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдЧрд░реНрди рдкреНрд░рдпрд╛рд╕ рдЧрд░реНрдЫреМрдВ, рддрд░ рдХреГрдкрдпрд╛ рдзреНрдпрд╛рди рджрд┐рдиреБрд╣реЛрд╕реН рдХрд┐ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рдЕрдиреБрд╡рд╛рджрд╣рд░реВрдорд╛ рддреНрд░реБрдЯрд┐рд╣рд░реВ рд╡рд╛ рдЕрд╢реБрджреНрдзрддрд╛рд╣рд░реВ рд╣реБрди рд╕рдХреНрдЫрдиреНред рдпрд╕рдХреЛ рдореВрд▓ рднрд╛рд╖рд╛рдорд╛ рд░рд╣реЗрдХреЛ рдореВрд▓ рджрд╕реНрддрд╛рд╡реЗрдЬрд▓рд╛рдИ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ рд╕реНрд░реЛрдд рдорд╛рдирд┐рдиреБрдкрд░реНрдЫред рдорд╣рддреНрддреНрд╡рдкреВрд░реНрдг рдЬрд╛рдирдХрд╛рд░реАрдХрд╛ рд▓рд╛рдЧрд┐, рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рдорд╛рдирд╡ рдЕрдиреБрд╡рд╛рдж рд╕рд┐рдлрд╛рд░рд┐рд╕ рдЧрд░рд┐рдиреНрдЫред рдпрд╕ рдЕрдиреБрд╡рд╛рджрдХреЛ рдкреНрд░рдпреЛрдЧрдмрд╛рдЯ рдЙрддреНрдкрдиреНрди рд╣реБрдиреЗ рдХреБрдиреИ рдкрдирд┐ рдЧрд▓рддрдлрд╣рдореА рд╡рд╛ рдЧрд▓рдд рд╡реНрдпрд╛рдЦреНрдпрд╛рдХреЛ рд▓рд╛рдЧрд┐ рд╣рд╛рдореА рдЬрд┐рдореНрдореЗрд╡рд╛рд░ рд╣реБрдиреЗ рдЫреИрдиреМрдВред\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "149e1ff0f023ecf1f4221663a7928ff5",
   "translation_date": "2025-10-09T09:58:13+00:00",
   "source_file": "Workshop/notebooks/session06_models_router.ipynb",
   "language_code": "ne"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}