<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2f1754a482b6a84e07287a5b775e65b6",
  "translation_date": "2025-09-30T22:58:44+00:00",
  "source_file": "Module08/samples/04/README.md",
  "language_code": "es"
}
-->
# Ejemplo 04: Aplicaciones de Chat de ProducciÃ³n con Chainlit

Un ejemplo completo que demuestra mÃºltiples enfoques para construir aplicaciones de chat listas para producciÃ³n utilizando Microsoft Foundry Local, con interfaces web modernas, respuestas en streaming y tecnologÃ­as avanzadas de navegador.

## QuÃ© Incluye

- **ðŸš€ AplicaciÃ³n de Chat Chainlit** (`app.py`): AplicaciÃ³n de chat lista para producciÃ³n con streaming
- **ðŸŒ Demo WebGPU** (`webgpu-demo/`): Inferencia de IA basada en navegador con aceleraciÃ³n por hardware
- **ðŸŽ¨ IntegraciÃ³n Open WebUI** (`open-webui-guide.md`): Interfaz profesional similar a ChatGPT
- **ðŸ“š Notebook Educativo** (`chainlit_app.ipynb`): Materiales interactivos de aprendizaje

## Inicio RÃ¡pido

### 1. AplicaciÃ³n de Chat Chainlit

```cmd
# Navigate to Module08 directory
cd Module08

# Start your model
foundry model run phi-4-mini

# Run Chainlit app (using port 8080 to avoid conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Se abre en: `http://localhost:8080`

### 2. Demo de Navegador WebGPU

```cmd
# Navigate to WebGPU demo
cd Module08\samples\04\webgpu-demo

# Serve the demo
python -m http.server 5173
```

Se abre en: `http://localhost:5173`

### 3. ConfiguraciÃ³n de Open WebUI

```cmd
# Run Open WebUI with Docker
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

Se abre en: `http://localhost:3000`

## Patrones de Arquitectura

### Matriz de DecisiÃ³n Local vs Nube

| Escenario | RecomendaciÃ³n | RazÃ³n |
|-----------|---------------|-------|
| **Datos Sensibles** | ðŸ  Local (Foundry) | Los datos nunca salen del dispositivo |
| **Razonamiento Complejo** | â˜ï¸ Nube (Azure OpenAI) | Acceso a modelos mÃ¡s grandes |
| **Chat en Tiempo Real** | ðŸ  Local (Foundry) | Menor latencia, respuestas mÃ¡s rÃ¡pidas |
| **AnÃ¡lisis de Documentos** | ðŸ”„ HÃ­brido | Local para extracciÃ³n, nube para anÃ¡lisis |
| **GeneraciÃ³n de CÃ³digo** | ðŸ  Local (Foundry) | Privacidad + modelos especializados |
| **Tareas de InvestigaciÃ³n** | â˜ï¸ Nube (Azure OpenAI) | Se necesita una base de conocimiento amplia |

### ComparaciÃ³n de TecnologÃ­as

| TecnologÃ­a | Caso de Uso | Ventajas | Desventajas |
|------------|-------------|----------|-------------|
| **Chainlit** | Desarrolladores Python, prototipado rÃ¡pido | ConfiguraciÃ³n sencilla, soporte de streaming | Solo Python |
| **WebGPU** | MÃ¡xima privacidad, escenarios offline | Nativo del navegador, no requiere servidor | TamaÃ±o de modelo limitado |
| **Open WebUI** | Despliegue en producciÃ³n, equipos | UI profesional, gestiÃ³n de usuarios | Requiere Docker |

## Requisitos Previos

- **Foundry Local**: Instalado y ejecutÃ¡ndose ([Descargar](https://aka.ms/foundry-local-installer))
- **Python**: 3.10+ con entorno virtual
- **Modelo**: Al menos uno cargado (`foundry model run phi-4-mini`)
- **Navegador**: Chrome/Edge con soporte WebGPU para demos
- **Docker**: Para Open WebUI (opcional)

## InstalaciÃ³n y ConfiguraciÃ³n

### 1. ConfiguraciÃ³n del Entorno Python

```cmd
# Navigate to Module08 directory
cd Module08

# Create and activate virtual environment
py -m venv .venv
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. ConfiguraciÃ³n de Foundry Local

```cmd
# Verify Foundry Local installation
foundry --version

# Start the service
foundry service start

# Load a model
foundry model run phi-4-mini

# Verify model is running
foundry service ps
```

## Aplicaciones de Ejemplo

### AplicaciÃ³n de Chat Chainlit

**CaracterÃ­sticas:**
- ðŸš€ **Streaming en Tiempo Real**: Los tokens aparecen a medida que se generan
- ðŸ›¡ï¸ **Manejo Robusto de Errores**: DegradaciÃ³n y recuperaciÃ³n elegantes
- ðŸŽ¨ **UI Moderna**: Interfaz de chat profesional lista para usar
- ðŸ”§ **ConfiguraciÃ³n Flexible**: Variables de entorno y detecciÃ³n automÃ¡tica
- ðŸ“± **DiseÃ±o Responsivo**: Funciona en dispositivos de escritorio y mÃ³viles

**Inicio RÃ¡pido:**
```cmd
# Run with default settings (recommended)
chainlit run samples\04\app.py -w --port 8080

# Use specific model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Manual endpoint configuration
set BASE_URL=http://localhost:51211
set API_KEY=your-api-key
chainlit run samples\04\app.py -w --port 8080
```

### Demo de Navegador WebGPU

**CaracterÃ­sticas:**
- ðŸŒ **IA Nativa del Navegador**: No requiere servidor, se ejecuta completamente en el navegador
- âš¡ **AceleraciÃ³n WebGPU**: AceleraciÃ³n por hardware cuando estÃ¡ disponible
- ðŸ”’ **MÃ¡xima Privacidad**: Los datos nunca salen de tu dispositivo
- ðŸŽ¯ **InstalaciÃ³n Cero**: Funciona en cualquier navegador compatible
- ðŸ”„ **Fallback Elegante**: Cambia automÃ¡ticamente a CPU si WebGPU no estÃ¡ disponible

**EjecuciÃ³n:**
```cmd
cd samples\04\webgpu-demo
python -m http.server 5173
# Open http://localhost:5173
```

### IntegraciÃ³n Open WebUI

**CaracterÃ­sticas:**
- ðŸŽ¨ **Interfaz Similar a ChatGPT**: UI profesional y familiar
- ðŸ‘¥ **Soporte Multiusuario**: Cuentas de usuario e historial de conversaciones
- ðŸ“ **Procesamiento de Archivos**: Subir y analizar documentos
- ðŸ”„ **Cambio de Modelos**: Cambio fÃ¡cil entre diferentes modelos
- ðŸ³ **Despliegue con Docker**: ConfiguraciÃ³n lista para producciÃ³n en contenedores

**ConfiguraciÃ³n RÃ¡pida:**
```cmd
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

## Referencia de ConfiguraciÃ³n

### Variables de Entorno

| Variable | DescripciÃ³n | Predeterminado | Ejemplo |
|----------|-------------|----------------|---------|
| `MODEL` | Alias del modelo a usar | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Endpoint de Foundry Local | Detectado automÃ¡ticamente | `http://localhost:51211` |
| `API_KEY` | Clave API (opcional para local) | `""` | `your-api-key` |

## SoluciÃ³n de Problemas

### Problemas Comunes

**AplicaciÃ³n Chainlit:**

1. **Servicio no disponible:**
   ```cmd
   # Check Foundry Local status
   foundry service status
   foundry service ps
   
   # Validate API endpoint (note: port 51211)
   curl http://localhost:51211/v1/models
   ```

2. **Conflictos de puertos:**
   ```cmd
   # Check what's using port 8080
   netstat -ano | findstr :8080
   
   # Use different port if needed
   chainlit run samples\04\app.py -w --port 3000
   ```

3. **Problemas con el entorno Python:**
   ```cmd
   # Verify correct interpreter in VS Code
   # Ctrl+Shift+P â†’ Python: Select Interpreter
   # Choose: Module08/.venv/Scripts/python.exe
   
   # Reinstall dependencies
   pip install -r requirements.txt
   ```

**Demo WebGPU:**

1. **WebGPU no soportado:**
   - Actualiza a Chrome/Edge 113+
   - Habilita WebGPU: `chrome://flags/#enable-unsafe-webgpu`
   - Verifica el estado de la GPU: `chrome://gpu`
   - El demo cambiarÃ¡ automÃ¡ticamente a CPU

2. **Errores de carga de modelos:**
   - AsegÃºrate de tener conexiÃ³n a internet para descargar el modelo
   - Revisa la consola del navegador para errores CORS
   - Verifica que estÃ©s sirviendo vÃ­a HTTP (no file://)

**Open WebUI:**

1. **ConexiÃ³n rechazada:**
   ```cmd
   # Check Docker is running
   docker --version
   
   # Check container status
   docker ps | findstr open-webui
   
   # View container logs
   docker logs open-webui
   ```

2. **Modelos no aparecen:**
   ```cmd
   # Verify Foundry Local endpoint
   curl http://localhost:51211/v1/models
   
   # Restart Open WebUI
   docker restart open-webui
   ```

### Lista de VerificaciÃ³n de ValidaciÃ³n

```cmd
# âœ… 1. Foundry Local Setup
foundry --version                    # Should show version
foundry service status               # Should show "running"
foundry model list                   # Should show loaded models
curl http://localhost:51211/v1/models  # Should return JSON

# âœ… 2. Python Environment  
python --version                     # Should be 3.10+
pip list | findstr chainlit         # Should show chainlit package
pip list | findstr openai           # Should show openai package

# âœ… 3. Application Testing
chainlit run samples\04\app.py -w --port 8080  # Should open browser
# Test WebGPU demo at localhost:5173
# Test Open WebUI at localhost:3000
```

## Uso Avanzado

### OptimizaciÃ³n de Rendimiento

**Chainlit:**
- Usa streaming para mejorar la percepciÃ³n de rendimiento
- Implementa agrupaciÃ³n de conexiones para alta concurrencia
- Cachea respuestas de modelos para consultas repetidas
- Monitorea el uso de memoria con historiales de conversaciÃ³n grandes

**WebGPU:**
- Usa WebGPU para mÃ¡xima privacidad y velocidad
- Implementa cuantizaciÃ³n de modelos para modelos mÃ¡s pequeÃ±os
- Usa Web Workers para procesamiento en segundo plano
- Cachea modelos compilados en el almacenamiento del navegador

**Open WebUI:**
- Usa volÃºmenes persistentes para historial de conversaciones
- Configura lÃ­mites de recursos para el contenedor Docker
- Implementa estrategias de respaldo para datos de usuarios
- Configura un proxy inverso para terminaciÃ³n SSL

### Patrones de IntegraciÃ³n

**HÃ­brido Local/Nube:**
```python
# Route based on complexity and privacy requirements
async def intelligent_routing(prompt: str, metadata: dict):
    if metadata.get("contains_pii"):
        return await foundry_local_completion(prompt)  # Privacy-sensitive
    elif len(prompt.split()) > 200:
        return await azure_openai_completion(prompt)   # Complex reasoning
    else:
        return await foundry_local_completion(prompt)  # Default local
```

**Pipeline Multimodal:**
```python
# Combine different AI capabilities
async def analyze_document(file_path: str):
    # 1. OCR with WebGPU (browser-based)
    text = await webgpu_ocr(file_path)
    
    # 2. Analysis with Foundry Local (private)
    summary = await foundry_local_analyze(text)
    
    # 3. Enhancement with cloud (if needed)
    if summary.confidence < 0.8:
        summary = await azure_openai_enhance(summary)
    
    return summary
```

## Despliegue en ProducciÃ³n

### Consideraciones de Seguridad

- **Claves API**: Usa variables de entorno, nunca las codifiques directamente
- **Red**: Usa HTTPS en producciÃ³n, considera VPN para acceso del equipo
- **Control de Acceso**: Implementa autenticaciÃ³n para Open WebUI
- **Privacidad de Datos**: Audita quÃ© datos permanecen locales y cuÃ¡les van a la nube
- **Actualizaciones**: MantÃ©n Foundry Local y los contenedores actualizados

### Monitoreo y Mantenimiento

- **Verificaciones de Salud**: Implementa monitoreo de endpoints
- **Registro**: Centraliza los registros de todos los componentes
- **MÃ©tricas**: Rastrea tiempos de respuesta, tasas de error, uso de recursos
- **Respaldo**: Realiza respaldos regulares de datos de conversaciones y configuraciones

## Referencias y Recursos

### DocumentaciÃ³n
- [DocumentaciÃ³n de Chainlit](https://docs.chainlit.io/) - GuÃ­a completa del framework
- [DocumentaciÃ³n de Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/) - Documentos oficiales de Microsoft
- [ONNX Runtime Web](https://onnxruntime.ai/docs/get-started/with-javascript/web.html) - IntegraciÃ³n WebGPU
- [DocumentaciÃ³n de Open WebUI](https://docs.openwebui.com/) - ConfiguraciÃ³n avanzada

### Archivos de Ejemplo
- [`app.py`](../../../../../Module08/samples/04/app.py) - AplicaciÃ³n Chainlit de producciÃ³n
- [`chainlit_app.ipynb`](./chainlit_app.ipynb) - Notebook educativo
- [`webgpu-demo/`](../../../../../Module08/samples/04/webgpu-demo) - Inferencia de IA basada en navegador
- [`open-webui-guide.md`](./open-webui-guide.md) - ConfiguraciÃ³n completa de Open WebUI

### Ejemplos Relacionados
- [DocumentaciÃ³n de la SesiÃ³n 4](../../04.CuttingEdgeModels.md) - GuÃ­a completa de la sesiÃ³n
- [Ejemplos de Foundry Local](https://github.com/microsoft/foundry-local/tree/main/samples) - Ejemplos oficiales

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducciÃ³n automÃ¡tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por garantizar la precisiÃ³n, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaciÃ³n crÃ­tica, se recomienda una traducciÃ³n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones errÃ³neas que puedan surgir del uso de esta traducciÃ³n.