<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2f1754a482b6a84e07287a5b775e65b6",
  "translation_date": "2025-10-01T01:16:11+00:00",
  "source_file": "Module08/samples/04/README.md",
  "language_code": "hu"
}
-->
# Minta 04: Chatalkalmaz√°sok gy√°rt√°si k√∂rnyezetben Chainlit seg√≠ts√©g√©vel

Egy √°tfog√≥ p√©lda, amely bemutatja a gy√°rt√°sra k√©sz chatalkalmaz√°sok k√ºl√∂nb√∂z≈ë megk√∂zel√≠t√©seit a Microsoft Foundry Local haszn√°lat√°val, modern webes fel√ºletekkel, streaming v√°laszokkal √©s leg√∫jabb b√∂ng√©sz≈ëtechnol√≥gi√°kkal.

## Mi tal√°lhat√≥ benne?

- **üöÄ Chainlit Chatalkalmaz√°s** (`app.py`): Gy√°rt√°sra k√©sz chatalkalmaz√°s streaming funkci√≥val
- **üåê WebGPU Dem√≥** (`webgpu-demo/`): B√∂ng√©sz≈ëalap√∫ AI k√∂vetkeztet√©s hardvergyors√≠t√°ssal
- **üé® Open WebUI Integr√°ci√≥** (`open-webui-guide.md`): Professzion√°lis ChatGPT-szer≈± fel√ºlet
- **üìö Oktat√°si Jegyzetf√ºzet** (`chainlit_app.ipynb`): Interakt√≠v tananyagok

## Gyors kezd√©s

### 1. Chainlit Chatalkalmaz√°s

```cmd
# Navigate to Module08 directory
cd Module08

# Start your model
foundry model run phi-4-mini

# Run Chainlit app (using port 8080 to avoid conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Megny√≠lik: `http://localhost:8080`

### 2. WebGPU B√∂ng√©sz≈ë Dem√≥

```cmd
# Navigate to WebGPU demo
cd Module08\samples\04\webgpu-demo

# Serve the demo
python -m http.server 5173
```

Megny√≠lik: `http://localhost:5173`

### 3. Open WebUI Be√°ll√≠t√°s

```cmd
# Run Open WebUI with Docker
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

Megny√≠lik: `http://localhost:3000`

## Architekt√∫ra mint√°k

### Lok√°lis vs Felh≈ë d√∂nt√©si m√°trix

| Forgat√≥k√∂nyv | Aj√°nl√°s | Indok |
|--------------|---------|-------|
| **Adatv√©delem √©rz√©keny adatok** | üè† Lok√°lis (Foundry) | Az adatok nem hagyj√°k el az eszk√∂zt |
| **Komplex k√∂vetkeztet√©s** | ‚òÅÔ∏è Felh≈ë (Azure OpenAI) | Nagyobb modellek el√©r√©se |
| **Val√≥s idej≈± chat** | üè† Lok√°lis (Foundry) | Alacsonyabb k√©sleltet√©s, gyorsabb v√°laszok |
| **Dokumentumelemz√©s** | üîÑ Hibrid | Lok√°lis az adatkinyer√©shez, felh≈ë az elemz√©shez |
| **K√≥dgener√°l√°s** | üè† Lok√°lis (Foundry) | Adatv√©delem + speci√°lis modellek |
| **Kutat√°si feladatok** | ‚òÅÔ∏è Felh≈ë (Azure OpenAI) | Sz√©lesk√∂r≈± tud√°sb√°zis sz√ºks√©ges |

### Technol√≥giai √∂sszehasonl√≠t√°s

| Technol√≥gia | Felhaszn√°l√°si ter√ºlet | El≈ëny√∂k | H√°tr√°nyok |
|-------------|-----------------------|---------|-----------|
| **Chainlit** | Python fejleszt≈ëk, gyors protot√≠pus k√©sz√≠t√©s | K√∂nny≈± be√°ll√≠t√°s, streaming t√°mogat√°s | Csak Python |
| **WebGPU** | Maxim√°lis adatv√©delem, offline forgat√≥k√∂nyvek | B√∂ng√©sz≈ë-alap√∫, nincs sz√ºks√©g szerverre | Korl√°tozott modellm√©ret |
| **Open WebUI** | Gy√°rt√°si k√∂rnyezet, csapatok | Professzion√°lis UI, felhaszn√°l√≥kezel√©s | Docker sz√ºks√©ges |

## El≈ëfelt√©telek

- **Foundry Local**: Telep√≠tve √©s futtatva ([Let√∂lt√©s](https://aka.ms/foundry-local-installer))
- **Python**: 3.10+ virtu√°lis k√∂rnyezettel
- **Modell**: Legal√°bb egy bet√∂ltve (`foundry model run phi-4-mini`)
- **B√∂ng√©sz≈ë**: Chrome/Edge WebGPU t√°mogat√°ssal a dem√≥khoz
- **Docker**: Open WebUI-hoz (opcion√°lis)

## Telep√≠t√©s √©s be√°ll√≠t√°s

### 1. Python k√∂rnyezet be√°ll√≠t√°sa

```cmd
# Navigate to Module08 directory
cd Module08

# Create and activate virtual environment
py -m venv .venv
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Foundry Local be√°ll√≠t√°sa

```cmd
# Verify Foundry Local installation
foundry --version

# Start the service
foundry service start

# Load a model
foundry model run phi-4-mini

# Verify model is running
foundry service ps
```

## Mintaalkalmaz√°sok

### Chainlit Chatalkalmaz√°s

**Funkci√≥k:**
- üöÄ **Val√≥s idej≈± streaming**: A tokenek gener√°l√°s k√∂zben jelennek meg
- üõ°Ô∏è **Er≈ës hibaelh√°r√≠t√°s**: Z√∂kken≈ëmentes degrad√°ci√≥ √©s helyre√°ll√≠t√°s
- üé® **Modern UI**: Professzion√°lis chatfel√ºlet alapb√≥l
- üîß **Rugalmas konfigur√°ci√≥**: K√∂rnyezeti v√°ltoz√≥k √©s automatikus felismer√©s
- üì± **Reszponz√≠v diz√°jn**: M≈±k√∂dik asztali √©s mobil eszk√∂z√∂k√∂n

**Gyors kezd√©s:**
```cmd
# Run with default settings (recommended)
chainlit run samples\04\app.py -w --port 8080

# Use specific model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Manual endpoint configuration
set BASE_URL=http://localhost:51211
set API_KEY=your-api-key
chainlit run samples\04\app.py -w --port 8080
```

### WebGPU B√∂ng√©sz≈ë Dem√≥

**Funkci√≥k:**
- üåê **B√∂ng√©sz≈ë-alap√∫ AI**: Nincs sz√ºks√©g szerverre, teljesen b√∂ng√©sz≈ëben fut
- ‚ö° **WebGPU gyors√≠t√°s**: Hardvergyors√≠t√°s, ha el√©rhet≈ë
- üîí **Maxim√°lis adatv√©delem**: Az adatok soha nem hagyj√°k el az eszk√∂zt
- üéØ **Nulla telep√≠t√©s**: M≈±k√∂dik b√°rmely kompatibilis b√∂ng√©sz≈ëben
- üîÑ **Z√∂kken≈ëmentes vissza√°ll√°s**: CPU-ra v√°lt, ha WebGPU nem el√©rhet≈ë

**Futtat√°s:**
```cmd
cd samples\04\webgpu-demo
python -m http.server 5173
# Open http://localhost:5173
```

### Open WebUI Integr√°ci√≥

**Funkci√≥k:**
- üé® **ChatGPT-szer≈± fel√ºlet**: Professzion√°lis, ismer≈ës UI
- üë• **T√∂bbfelhaszn√°l√≥s t√°mogat√°s**: Felhaszn√°l√≥i fi√≥kok √©s besz√©lget√©si el≈ëzm√©nyek
- üìÅ **F√°jlkezel√©s**: Dokumentumok felt√∂lt√©se √©s elemz√©se
- üîÑ **Modellv√°lt√°s**: K√∂nny≈± v√°lt√°s k√ºl√∂nb√∂z≈ë modellek k√∂z√∂tt
- üê≥ **Docker telep√≠t√©s**: Gy√°rt√°sra k√©sz kont√©neres be√°ll√≠t√°s

**Gyors be√°ll√≠t√°s:**
```cmd
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

## Konfigur√°ci√≥s referencia

### K√∂rnyezeti v√°ltoz√≥k

| V√°ltoz√≥ | Le√≠r√°s | Alap√©rtelmezett | P√©lda |
|---------|--------|-----------------|-------|
| `MODEL` | Haszn√°land√≥ modell alias | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Foundry Local v√©gpont | Automatikusan felismerve | `http://localhost:51211` |
| `API_KEY` | API kulcs (opcion√°lis lok√°lis haszn√°lathoz) | `""` | `your-api-key` |

## Hibakeres√©s

### Gyakori probl√©m√°k

**Chainlit alkalmaz√°s:**

1. **Szolg√°ltat√°s nem el√©rhet≈ë:**
   ```cmd
   # Check Foundry Local status
   foundry service status
   foundry service ps
   
   # Validate API endpoint (note: port 51211)
   curl http://localhost:51211/v1/models
   ```

2. **Port√ºtk√∂z√©sek:**
   ```cmd
   # Check what's using port 8080
   netstat -ano | findstr :8080
   
   # Use different port if needed
   chainlit run samples\04\app.py -w --port 3000
   ```

3. **Python k√∂rnyezeti probl√©m√°k:**
   ```cmd
   # Verify correct interpreter in VS Code
   # Ctrl+Shift+P ‚Üí Python: Select Interpreter
   # Choose: Module08/.venv/Scripts/python.exe
   
   # Reinstall dependencies
   pip install -r requirements.txt
   ```

**WebGPU Dem√≥:**

1. **WebGPU nem t√°mogatott:**
   - Friss√≠ts Chrome/Edge 113+ verzi√≥ra
   - Enged√©lyezd a WebGPU-t: `chrome://flags/#enable-unsafe-webgpu`
   - Ellen≈ërizd a GPU √°llapot√°t: `chrome://gpu`
   - A dem√≥ automatikusan CPU-ra v√°lt

2. **Modell bet√∂lt√©si hib√°k:**
   - Biztos√≠ts internetkapcsolatot a modell let√∂lt√©s√©hez
   - Ellen≈ërizd a b√∂ng√©sz≈ë konzolt CORS hib√°k√©rt
   - Gy≈ëz≈ëdj meg r√≥la, hogy HTTP-n kereszt√ºl szolg√°ltatsz (nem file://)

**Open WebUI:**

1. **Kapcsolat megtagadva:**
   ```cmd
   # Check Docker is running
   docker --version
   
   # Check container status
   docker ps | findstr open-webui
   
   # View container logs
   docker logs open-webui
   ```

2. **Modellek nem jelennek meg:**
   ```cmd
   # Verify Foundry Local endpoint
   curl http://localhost:51211/v1/models
   
   # Restart Open WebUI
   docker restart open-webui
   ```

### √ârv√©nyes√≠t√©si ellen≈ërz≈ëlista

```cmd
# ‚úÖ 1. Foundry Local Setup
foundry --version                    # Should show version
foundry service status               # Should show "running"
foundry model list                   # Should show loaded models
curl http://localhost:51211/v1/models  # Should return JSON

# ‚úÖ 2. Python Environment  
python --version                     # Should be 3.10+
pip list | findstr chainlit         # Should show chainlit package
pip list | findstr openai           # Should show openai package

# ‚úÖ 3. Application Testing
chainlit run samples\04\app.py -w --port 8080  # Should open browser
# Test WebGPU demo at localhost:5173
# Test Open WebUI at localhost:3000
```

## Halad√≥ haszn√°lat

### Teljes√≠tm√©nyoptimaliz√°l√°s

**Chainlit:**
- Haszn√°lj streaminget a jobb √©rz√©kelt teljes√≠tm√©ny √©rdek√©ben
- Implement√°lj kapcsolat poolingot magas p√°rhuzamoss√°ghoz
- Cache-eld a modell v√°laszait ism√©telt lek√©rdez√©sekhez
- Figyeld a mem√≥riahaszn√°latot nagy besz√©lget√©si el≈ëzm√©nyek eset√©n

**WebGPU:**
- Haszn√°lj WebGPU-t a maxim√°lis adatv√©delem √©s sebess√©g √©rdek√©ben
- Implement√°lj modell kvant√°l√°st kisebb modellekhez
- Haszn√°lj Web Worker-eket h√°tt√©rfeldolgoz√°shoz
- Cache-eld a leford√≠tott modelleket a b√∂ng√©sz≈ë t√°rol√≥j√°ban

**Open WebUI:**
- Haszn√°lj tart√≥s k√∂teteket a besz√©lget√©si el≈ëzm√©nyekhez
- Konfigur√°ld az er≈ëforr√°skorl√°tokat a Docker kont√©nerhez
- Implement√°lj biztons√°gi ment√©si strat√©gi√°kat a felhaszn√°l√≥i adatokhoz
- √Åll√≠ts be ford√≠tott proxy-t SSL termin√°l√°shoz

### Integr√°ci√≥s mint√°k

**Hibrid Lok√°lis/Felh≈ë:**
```python
# Route based on complexity and privacy requirements
async def intelligent_routing(prompt: str, metadata: dict):
    if metadata.get("contains_pii"):
        return await foundry_local_completion(prompt)  # Privacy-sensitive
    elif len(prompt.split()) > 200:
        return await azure_openai_completion(prompt)   # Complex reasoning
    else:
        return await foundry_local_completion(prompt)  # Default local
```

**Multi-Mod√°lis Pipeline:**
```python
# Combine different AI capabilities
async def analyze_document(file_path: str):
    # 1. OCR with WebGPU (browser-based)
    text = await webgpu_ocr(file_path)
    
    # 2. Analysis with Foundry Local (private)
    summary = await foundry_local_analyze(text)
    
    # 3. Enhancement with cloud (if needed)
    if summary.confidence < 0.8:
        summary = await azure_openai_enhance(summary)
    
    return summary
```

## Gy√°rt√°si telep√≠t√©s

### Biztons√°gi szempontok

- **API kulcsok**: Haszn√°lj k√∂rnyezeti v√°ltoz√≥kat, soha ne k√≥dold be
- **H√°l√≥zat**: Haszn√°lj HTTPS-t gy√°rt√°sban, fontold meg VPN-t csapat hozz√°f√©r√©shez
- **Hozz√°f√©r√©s-vez√©rl√©s**: Implement√°lj hiteles√≠t√©st az Open WebUI-hoz
- **Adatv√©delem**: Ellen≈ërizd, hogy mely adatok maradnak lok√°lisan √©s melyek ker√ºlnek a felh≈ëbe
- **Friss√≠t√©sek**: Tartsd naprak√©szen a Foundry Local-t √©s a kont√©nereket

### Fel√ºgyelet √©s karbantart√°s

- **Eg√©szs√©gellen≈ërz√©sek**: Implement√°lj v√©gpont monitoroz√°st
- **Napl√≥z√°s**: Centraliz√°ld az √∂sszes komponens napl√≥it
- **Metrik√°k**: K√∂vesd a v√°laszid≈ëket, hibaar√°nyokat, er≈ëforr√°s-haszn√°latot
- **Biztons√°gi ment√©s**: Rendszeres ment√©s a besz√©lget√©si adatok √©s konfigur√°ci√≥k sz√°m√°ra

## Referenci√°k √©s forr√°sok

### Dokument√°ci√≥
- [Chainlit Dokument√°ci√≥](https://docs.chainlit.io/) - Teljes keretrendszer √∫tmutat√≥
- [Foundry Local Dokument√°ci√≥](https://learn.microsoft.com/azure/ai-foundry/foundry-local/) - Hivatalos Microsoft dokument√°ci√≥
- [ONNX Runtime Web](https://onnxruntime.ai/docs/get-started/with-javascript/web.html) - WebGPU integr√°ci√≥
- [Open WebUI Dokument√°ci√≥](https://docs.openwebui.com/) - Halad√≥ konfigur√°ci√≥

### Mintaf√°jlok
- [`app.py`](../../../../../Module08/samples/04/app.py) - Gy√°rt√°sra k√©sz Chainlit alkalmaz√°s
- [`chainlit_app.ipynb`](./chainlit_app.ipynb) - Oktat√°si jegyzetf√ºzet
- [`webgpu-demo/`](../../../../../Module08/samples/04/webgpu-demo) - B√∂ng√©sz≈ëalap√∫ AI k√∂vetkeztet√©s
- [`open-webui-guide.md`](./open-webui-guide.md) - Teljes Open WebUI be√°ll√≠t√°s

### Kapcsol√≥d√≥ mint√°k
- [4. szekci√≥ dokument√°ci√≥ja](../../04.CuttingEdgeModels.md) - Teljes szekci√≥ √∫tmutat√≥
- [Foundry Local mint√°k](https://github.com/microsoft/foundry-local/tree/main/samples) - Hivatalos mint√°k

---

**Felel≈ëss√©gi nyilatkozat**:  
Ez a dokumentum az [Co-op Translator](https://github.com/Azure/co-op-translator) AI ford√≠t√°si szolg√°ltat√°s seg√≠ts√©g√©vel ker√ºlt leford√≠t√°sra. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.