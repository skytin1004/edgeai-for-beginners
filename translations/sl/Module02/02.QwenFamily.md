<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-18T22:24:41+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "sl"
}
-->
# Poglavje 2: Osnove druÅ¾ine Qwen

Modelna druÅ¾ina Qwen predstavlja celovit pristop Alibaba Clouda k velikim jezikovnim modelom in multimodalni umetni inteligenci, kar dokazuje, da lahko odprtokodni modeli doseÅ¾ejo izjemno zmogljivost, hkrati pa so dostopni v razliÄnih scenarijih uporabe. Pomembno je razumeti, kako druÅ¾ina Qwen omogoÄa zmogljive AI zmoÅ¾nosti z fleksibilnimi moÅ¾nostmi uporabe, hkrati pa ohranja konkurenÄno zmogljivost pri raznolikih nalogah.

## Viri za razvijalce

### Repozitorij modelov Hugging Face
Izbrani modeli druÅ¾ine Qwen so na voljo prek [Hugging Face](https://huggingface.co/models?search=qwen), kar omogoÄa dostop do nekaterih razliÄic teh modelov. Raziskujete lahko razpoloÅ¾ljive razliÄice, jih prilagodite za svoje specifiÄne primere uporabe in jih uporabite prek razliÄnih ogrodij.

### Orodja za lokalni razvoj
Za lokalni razvoj in testiranje lahko uporabite [Microsoft Foundry Local](https://github.com/microsoft/foundry-local), da zaÅ¾enete razpoloÅ¾ljive modele Qwen na svojem razvojnem raÄunalniku z optimizirano zmogljivostjo.

### Dokumentacijski viri
- [Dokumentacija modelov Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Optimizacija modelov Qwen za uporabo na robu](https://github.com/microsoft/olive)

## Uvod

V tem priroÄniku bomo raziskali druÅ¾ino modelov Qwen podjetja Alibaba in njene osnovne koncepte. Pokrili bomo razvoj druÅ¾ine Qwen, inovativne metodologije uÄenja, ki naredijo modele Qwen uÄinkovite, kljuÄne razliÄice v druÅ¾ini ter praktiÄne aplikacije v razliÄnih scenarijih.

## Cilji uÄenja

Do konca tega priroÄnika boste lahko:

- Razumeli filozofijo oblikovanja in razvoj druÅ¾ine modelov Qwen podjetja Alibaba
- Prepoznali kljuÄne inovacije, ki omogoÄajo modelom Qwen doseganje visoke zmogljivosti pri razliÄnih velikostih parametrov
- Prepoznali prednosti in omejitve razliÄnih razliÄic modelov Qwen
- Uporabili znanje o modelih Qwen za izbiro ustreznih razliÄic za resniÄne scenarije

## Razumevanje sodobne pokrajine AI modelov

Pokrajina umetne inteligence se je moÄno razvila, pri Äemer razliÄne organizacije sledijo razliÄnim pristopom k razvoju jezikovnih modelov. Medtem ko se nekateri osredotoÄajo na lastniÅ¡ke zaprte modele, drugi poudarjajo odprtokodno dostopnost in transparentnost. Tradicionalni pristop vkljuÄuje bodisi ogromne lastniÅ¡ke modele, dostopne le prek API-jev, bodisi odprtokodne modele, ki lahko zaostajajo v zmogljivostih.

Ta paradigma ustvarja izzive za organizacije, ki iÅ¡Äejo zmogljive AI zmoÅ¾nosti, hkrati pa Å¾elijo ohraniti nadzor nad svojimi podatki, stroÅ¡ki in fleksibilnostjo uporabe. Konvencionalni pristop pogosto zahteva izbiro med vrhunsko zmogljivostjo in praktiÄnimi vidiki uporabe.

## Izziv dostopne AI odliÄnosti

Potreba po visokokakovostni, dostopni umetni inteligenci postaja vse bolj pomembna v razliÄnih scenarijih. UpoÅ¡tevajte aplikacije, ki zahtevajo fleksibilne moÅ¾nosti uporabe za razliÄne organizacijske potrebe, stroÅ¡kovno uÄinkovite implementacije, kjer lahko stroÅ¡ki API-jev postanejo pomembni, veÄjeziÄne zmoÅ¾nosti za globalne aplikacije ali specializirano strokovno znanje na podroÄjih, kot sta programiranje in matematika.

### KljuÄne zahteve za uporabo

Sodobne AI implementacije se sooÄajo z veÄ temeljnimi zahtevami, ki omejujejo praktiÄno uporabnost:

- **Dostopnost**: Odprtokodna dostopnost za transparentnost in prilagoditev
- **StroÅ¡kovna uÄinkovitost**: Razumne zahteve po raÄunalniÅ¡kih virih za razliÄne proraÄune
- **Fleksibilnost**: VeÄ velikosti modelov za razliÄne scenarije uporabe
- **Globalni doseg**: MoÄne veÄjeziÄne in medkulturne zmoÅ¾nosti
- **Specializacija**: RazliÄice, prilagojene specifiÄnim domenam za posebne primere uporabe

## Filozofija modelov Qwen

DruÅ¾ina modelov Qwen predstavlja celovit pristop k razvoju AI modelov, ki daje prednost odprtokodni dostopnosti, veÄjeziÄnim zmoÅ¾nostim in praktiÄni uporabi, hkrati pa ohranja konkurenÄne zmogljivostne znaÄilnosti. Modeli Qwen to doseÅ¾ejo z raznolikimi velikostmi modelov, visokokakovostnimi metodologijami uÄenja in specializiranimi razliÄicami za razliÄne domene.

DruÅ¾ina Qwen vkljuÄuje razliÄne pristope, zasnovane za zagotavljanje moÅ¾nosti v spektru zmogljivosti in uÄinkovitosti, kar omogoÄa uporabo od mobilnih naprav do streÅ¾nikov v podjetjih, hkrati pa zagotavlja smiselne AI zmoÅ¾nosti. Cilj je demokratizirati dostop do visokokakovostne umetne inteligence, hkrati pa zagotavljati fleksibilnost pri izbiri uporabe.

### Temeljna naÄela oblikovanja Qwen

Modeli Qwen temeljijo na veÄ osnovnih naÄelih, ki jih loÄujejo od drugih druÅ¾in jezikovnih modelov:

- **Najprej odprtokodnost**: Popolna transparentnost in dostopnost za raziskave in komercialno uporabo
- **Celovito uÄenje**: UÄenje na ogromnih, raznolikih podatkovnih nizih, ki pokrivajo veÄ jezikov in domen
- **Prilagodljiva arhitektura**: VeÄ velikosti modelov za prilagoditev razliÄnim raÄunalniÅ¡kim zahtevam
- **Specializirana odliÄnost**: RazliÄice, optimizirane za specifiÄne naloge

## KljuÄne tehnologije, ki omogoÄajo druÅ¾ino Qwen

### UÄenje na ogromnem obsegu

Ena od znaÄilnosti druÅ¾ine Qwen je obseÅ¾nost podatkov za uÄenje in raÄunalniÅ¡kih virov, vloÅ¾enih v razvoj modelov. Modeli Qwen izkoriÅ¡Äajo skrbno izbrane, veÄjeziÄne podatkovne nize, ki obsegajo bilijone tokenov, zasnovane za zagotavljanje celovitega svetovnega znanja in zmoÅ¾nosti sklepanja.

Ta pristop zdruÅ¾uje visokokakovostno spletno vsebino, akademsko literaturo, repozitorije kode in veÄjeziÄne vire. Metodologija uÄenja poudarja tako Å¡irino znanja kot globino razumevanja v razliÄnih domenah in jezikih.

### Napredno sklepanje in razmiÅ¡ljanje

NovejÅ¡i modeli Qwen vkljuÄujejo sofisticirane zmoÅ¾nosti sklepanja, ki omogoÄajo kompleksno veÄstopenjsko reÅ¡evanje problemov:

**NaÄin razmiÅ¡ljanja (Qwen3)**: Modeli se lahko vkljuÄijo v podrobno veÄstopenjsko sklepanje, preden podajo konÄne odgovore, podobno ÄloveÅ¡kim pristopom k reÅ¡evanju problemov.

**Dvojni naÄin delovanja**: ZmoÅ¾nost preklopa med hitrim naÄinom odziva za enostavna vpraÅ¡anja in globokim naÄinom razmiÅ¡ljanja za kompleksne probleme.

**Integracija verige misli**: Naravna vkljuÄitev korakov sklepanja, ki izboljÅ¡ujejo transparentnost in natanÄnost pri kompleksnih nalogah.

### Arhitekturne inovacije

DruÅ¾ina Qwen vkljuÄuje veÄ arhitekturnih optimizacij, zasnovanih za zmogljivost in uÄinkovitost:

**Prilagodljiva zasnova**: Dosledna arhitektura med velikostmi modelov, ki omogoÄa enostavno skaliranje in primerjavo.

**Multimodalna integracija**: Brezhibna integracija zmoÅ¾nosti obdelave besedila, vizualnih in zvoÄnih podatkov znotraj enotnih arhitektur.

**Optimizacija uporabe**: VeÄ moÅ¾nosti kvantizacije in formatov uporabe za razliÄne konfiguracije strojne opreme.

## Velikost modelov in moÅ¾nosti uporabe

Sodobna okolja uporabe koristijo fleksibilnost modelov Qwen pri razliÄnih raÄunalniÅ¡kih zahtevah:

### Majhni modeli (0.5B-3B)

Qwen ponuja uÄinkovite majhne modele, primerne za uporabo na robu, mobilne aplikacije in okolja z omejenimi viri, hkrati pa ohranja impresivne zmoÅ¾nosti.

### Srednji modeli (7B-32B)

Srednje veliki modeli ponujajo izboljÅ¡ane zmoÅ¾nosti za profesionalne aplikacije, kar zagotavlja odliÄno ravnovesje med zmogljivostjo in raÄunalniÅ¡kimi zahtevami.

### Veliki modeli (72B+)

Modeli polnega obsega zagotavljajo vrhunsko zmogljivost za zahtevne aplikacije, raziskave in uporabo v podjetjih, ki zahtevajo najveÄje zmoÅ¾nosti.

## Prednosti druÅ¾ine modelov Qwen

### Dostopnost odprte kode

Modeli Qwen zagotavljajo popolno transparentnost in zmoÅ¾nosti prilagoditve, kar organizacijam omogoÄa razumevanje, spreminjanje in prilagajanje modelov njihovim specifiÄnim potrebam brez odvisnosti od ponudnika.

### Fleksibilnost uporabe

Razpon velikosti modelov omogoÄa uporabo v razliÄnih konfiguracijah strojne opreme, od mobilnih naprav do zmogljivih streÅ¾nikov, kar organizacijam zagotavlja fleksibilnost pri izbiri AI infrastrukture.

### VeÄjeziÄna odliÄnost

Modeli Qwen se odlikujejo v razumevanju in generiranju veÄ jezikov, podpirajo desetine jezikov z moÄnim poudarkom na angleÅ¡Äini in kitajÅ¡Äini, kar jih naredi primerne za globalne aplikacije.

### KonkurenÄna zmogljivost

Modeli Qwen dosledno dosegajo konkurenÄne rezultate na merilih, hkrati pa zagotavljajo odprtokodno dostopnost, kar dokazuje, da lahko odprti modeli konkurirajo lastniÅ¡kim alternativam.

### Specializirane zmoÅ¾nosti

RazliÄice, kot sta Qwen-Coder in Qwen-Math, zagotavljajo specializirano strokovno znanje, hkrati pa ohranjajo sploÅ¡ne zmoÅ¾nosti razumevanja jezika.

## PraktiÄni primeri in uporabe

Preden se poglobimo v tehniÄne podrobnosti, si oglejmo nekaj konkretnih primerov, kaj modeli Qwen lahko doseÅ¾ejo:

### Primer matematiÄnega sklepanja

Qwen-Math se odlikuje pri veÄstopenjskem reÅ¡evanju matematiÄnih problemov. Na primer, ko je vpraÅ¡anje kompleksnega problema iz raÄuna:

```
User: Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = xÂ³ + 2xÂ² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹

Breaking down each term:
- d/dx(xÂ³) = 3xÂ²
- d/dx(2xÂ²) = 2 Â· 2xÂ¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3xÂ² + 4x - 5
```

### Primer veÄjeziÄne podpore

Modeli Qwen kaÅ¾ejo moÄne veÄjeziÄne zmoÅ¾nosti v razliÄnih jezikih:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**ä¸­æ–‡:**
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡æœºå™¨ï¼ˆç‰¹åˆ«æ˜¯è®¡ç®—æœºç³»ç»Ÿï¼‰å¯¹äººç±»æ™ºèƒ½è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚è¿™äº›è¿‡ç¨‹åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€è§£å†³é—®é¢˜ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚äººå·¥æ™ºèƒ½ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»è®¤çŸ¥èƒ½åŠ›çš„ä»»åŠ¡ã€‚
```

### Primer multimodalnih zmoÅ¾nosti

Qwen-VL lahko hkrati obdeluje besedilo in slike:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Primer generiranja kode

Qwen-Coder se odlikuje pri generiranju in razlagi kode v veÄ programskih jezikih:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Izvedi binarno iskanje v urejenem seznamu za iskanje ciljne vrednosti.
    
    Args:
        arr (list): Urejen seznam primerljivih elementov
        target: Vrednost, ki jo iÅ¡Äemo
        
    Returns:
        int: Indeks cilja, Äe je najden, -1, Äe ni najden
        
    ÄŒasovna kompleksnost: O(log n)
    Prostorska kompleksnost: O(1)
    """
    # Inicializiraj leve in desne kazalce
    left, right = 0, len(arr) - 1
    
    # Nadaljuj iskanje, dokler je iskalni prostor veljaven
    while left <= right:
        # IzraÄunaj srednji indeks, da se izogneÅ¡ prelivu celih Å¡tevil
        mid = left + (right - left) // 2
        
        # Preveri, ali smo naÅ¡li cilj
        if arr[mid] == target:
            return mid
        
        # ÄŒe je cilj manjÅ¡i, iÅ¡Äi v levi polovici
        elif arr[mid] > target:
            right = mid - 1
        
        # ÄŒe je cilj veÄji, iÅ¡Äi v desni polovici
        else:
            left = mid + 1
    
    # Cilj ni najden
    return -1

# Primer uporabe:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Indeks 7: {result}")  # Izhod: Indeks 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### Primer uporabe na robu

Modeli Qwen se lahko uporabljajo na razliÄnih robnih napravah z optimiziranimi konfiguracijami:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Razvoj druÅ¾ine Qwen

### Qwen 1.0 in 1.5: Temeljni modeli

Zgodnji modeli Qwen so vzpostavili temeljna naÄela celovitega uÄenja in odprtokodne dostopnosti:

- **Qwen-7B (7B parametrov)**: Prva izdaja, osredotoÄena na razumevanje kitajskega in angleÅ¡kega jezika
- **Qwen-14B (14B parametrov)**: IzboljÅ¡ane zmoÅ¾nosti z izboljÅ¡anim sklepanjem in znanjem
- **Qwen-72B (72B parametrov)**: Model velikega obsega, ki zagotavlja vrhunsko zmogljivost
- **Serija Qwen1.5**: RazÅ¡irjena na veÄ velikosti (0.5B do 110B) z izboljÅ¡anim obravnavanjem dolgega konteksta

### DruÅ¾ina Qwen2: Multimodalna razÅ¡iritev

Serija Qwen2 je prinesla pomemben napredek tako v jezikovnih kot multimodalnih zmoÅ¾nostih:

- **Qwen2-0.5B do 72B**: Celovit razpon jezikovnih modelov za razliÄne potrebe uporabe
- **Qwen2-57B-A14B (MoE)**: Arhitektura meÅ¡anice strokovnjakov za uÄinkovito uporabo parametrov
- **Qwen2-VL**: Napredne zmoÅ¾nosti vizualnega jezika za razumevanje slik
- **Qwen2-Audio**: ZmoÅ¾nosti obdelave in razumevanja zvoka
- **Qwen2-Math**: Specializirano matematiÄno sklepanje in reÅ¡evanje problemov

### DruÅ¾ina Qwen2.5: IzboljÅ¡ana zmogljivost

Serija Qwen2.5 je prinesla pomembne izboljÅ¡ave na vseh podroÄjih:

- **RazÅ¡irjeno uÄenje**: 18 bilijonov tokenov podatkov za uÄenje za izboljÅ¡ane zmoÅ¾nosti
- **RazÅ¡irjen kontekst**: Do dolÅ¾ine konteksta 128K tokenov, z razliÄico Turbo, ki podpira 1M tokenov
- **IzboljÅ¡ana specializacija**: IzboljÅ¡ane razliÄice Qwen2.5-Coder in Qwen2.5-Math
- **BoljÅ¡a veÄjeziÄna podpora**: IzboljÅ¡ana zmogljivost v veÄ kot 27 jezikih

### DruÅ¾ina Qwen3: Napredno sklepanje

NajnovejÅ¡a generacija premika meje zmoÅ¾nosti sklepanja in razmiÅ¡ljanja:

- **Qwen3-235B-A22B**: Vodilni model meÅ¡anice strokovnjakov s skupno 235B parametri
- **Qwen3-30B-A3B**: UÄinkovit MoE model z moÄno zmogljivostjo na aktivni parameter
- **GostejÅ¡i modeli**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B za razliÄne scenarije uporabe
- **NaÄin razmiÅ¡ljanja**: Hibridni pristop k sklepanju, ki podpira tako hitre odzive kot globoko razmiÅ¡ljanje
- **VeÄjeziÄna odliÄnost**: Podpora za 119 jezikov in dialektov
- **IzboljÅ¡ano uÄenje**: 36 bilijonov tokenov raznolikih, visokokakovostnih podatkov za uÄenje

## Uporabe modelov Qwen

### Uporabe v podjetjih

Organizacije uporabljajo modele Qwen za analizo dokumentov, avtomatizacijo storitev za stranke, pomoÄ pri generiranju kode in aplikacije poslovne inteligence. Odprtokodna narava omogoÄa prilagoditev specifiÄnim potrebam podjetja, hkrati pa ohranja zasebnost podatkov in nadzor.

### Mobilno in robno raÄunalniÅ¡tvo

Mobilne aplikacije izkoriÅ¡Äajo modele Qwen za prevajanje v realnem Äasu, inteligentne asistente, generiranje vsebine in personalizirana priporoÄila. Razpon velikosti modelov omogoÄa uporabo od mobilnih naprav do robnih streÅ¾nikov.

### IzobraÅ¾evalna tehnologija

IzobraÅ¾evalne platforme uporabljajo modele Qwen za personalizirano tutorstvo, avtomatizirano generiranje vsebine, pomoÄ pri uÄenju jezikov in
Tukaj je, kako zaÄeti z modeli Qwen z uporabo knjiÅ¾nice Hugging Face Transformers:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Uporaba modelov Qwen2.5

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### Uporaba specializiranih modelov

**Generiranje kode z Qwen-Coder:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**ReÅ¡evanje matematiÄnih problemov:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**Naloge vizualnega jezika:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### NaÄin razmiÅ¡ljanja (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### ğŸ“± Mobilna in robna uporaba

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### Primer uporabe API-ja

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## Merila zmogljivosti in doseÅ¾ki

DruÅ¾ina modelov Qwen je dosegla izjemne rezultate na razliÄnih merilih zmogljivosti, hkrati pa ohranila dostopnost odprte kode:

### KljuÄni poudarki zmogljivosti

**OdliÄnost v razmiÅ¡ljanju:**
- Qwen3-235B-A22B dosega konkurenÄne rezultate pri ocenjevanju zmogljivosti na podroÄju kodiranja, matematike in sploÅ¡nih sposobnosti v primerjavi z drugimi vrhunskimi modeli, kot so DeepSeek-R1, o1, o3-mini, Grok-3 in Gemini-2.5-Pro.
- Qwen3-30B-A3B presega QwQ-32B z 10-krat veÄ aktiviranimi parametri.
- Qwen3-4B lahko dosega zmogljivost Qwen2.5-72B-Instruct.

**DoseÅ¾ki uÄinkovitosti:**
- Osnovni modeli Qwen3-MoE doseÅ¾ejo podobno zmogljivost kot osnovni modeli Qwen2.5, pri Äemer uporabljajo le 10 % aktivnih parametrov.
- Pomembni prihranki pri stroÅ¡kih tako pri usposabljanju kot pri sklepanju v primerjavi z gostimi modeli.

**VeÄjeziÄne zmogljivosti:**
- Modeli Qwen3 podpirajo 119 jezikov in nareÄij.
- MoÄna zmogljivost v razliÄnih jezikovnih in kulturnih kontekstih.

**Obseg usposabljanja:**
- Qwen3 uporablja skoraj dvakrat veÄ podatkov, pribliÅ¾no 36 bilijonov tokenov, ki pokrivajo 119 jezikov in nareÄij, v primerjavi z 18 bilijoni tokenov pri Qwen2.5.

### Primerjalna matrika modelov

| Serija modelov | Obseg parametrov | DolÅ¾ina konteksta | KljuÄne prednosti | NajboljÅ¡e uporabe |
|----------------|------------------|-------------------|-------------------|-------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | UravnoteÅ¾ena zmogljivost, veÄjeziÄnost | SploÅ¡ne aplikacije, produkcijska uporaba |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | Generiranje kode, programiranje | Razvoj programske opreme, pomoÄ pri kodiranju |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | MatematiÄno razmiÅ¡ljanje | IzobraÅ¾evalne platforme, STEM aplikacije |
| **Qwen2.5-VL** | RazliÄno | Spremenljivo | Razumevanje vizualnega jezika | Multimodalne aplikacije, analiza slik |
| **Qwen3** | 0.6B-235B | Spremenljivo | Napredno razmiÅ¡ljanje, naÄin razmiÅ¡ljanja | Kompleksno razmiÅ¡ljanje, raziskovalne aplikacije |
| **Qwen3 MoE** | 30B-235B skupaj | Spremenljivo | UÄinkovita zmogljivost na velikem obsegu | PodjetniÅ¡ke aplikacije, potrebe po visoki zmogljivosti |

## Vodnik za izbiro modela

### Za osnovne aplikacije
- **Qwen2.5-0.5B/1.5B**: Mobilne aplikacije, robne naprave, aplikacije v realnem Äasu
- **Qwen2.5-3B/7B**: SploÅ¡ni klepetalni roboti, generiranje vsebine, sistemi vpraÅ¡anj in odgovorov

### Za matematiÄne in razmiÅ¡ljalne naloge
- **Qwen2.5-Math**: ReÅ¡evanje matematiÄnih problemov in STEM izobraÅ¾evanje
- **Qwen3 z naÄinom razmiÅ¡ljanja**: Kompleksno razmiÅ¡ljanje, ki zahteva analizo korak za korakom

### Za programiranje in razvoj
- **Qwen2.5-Coder**: Generiranje kode, odpravljanje napak, pomoÄ pri programiranju
- **Qwen3**: Napredne naloge programiranja z zmogljivostmi razmiÅ¡ljanja

### Za multimodalne aplikacije
- **Qwen2.5-VL**: Razumevanje slik, vizualno odgovarjanje na vpraÅ¡anja
- **Qwen-Audio**: Obdelava zvoka in razumevanje govora

### Za podjetniÅ¡ko uporabo
- **Qwen2.5-32B/72B**: Visoko zmogljivo razumevanje jezika
- **Qwen3-235B-A22B**: Maksimalne zmogljivosti za zahtevne aplikacije

## Platforme za uporabo in dostopnost
### Oblak
- **Hugging Face Hub**: Celovita zbirka modelov s podporo skupnosti
- **ModelScope**: Alibaba platforma za modele z orodji za optimizacijo
- **RazliÄni ponudniki oblakov**: Podpora prek standardnih platform za strojno uÄenje

### Lokalni razvojni okviri
- **Transformers**: Standardna integracija Hugging Face za enostavno uporabo
- **vLLM**: Visoko zmogljivo streÅ¾nik za produkcijska okolja
- **Ollama**: Poenostavljena lokalna uporaba in upravljanje
- **ONNX Runtime**: Optimizacija za razliÄne strojne opreme
- **llama.cpp**: UÄinkovita implementacija v C++ za razliÄne platforme

### UÄni viri
- **Dokumentacija Qwen**: Uradna dokumentacija in kartice modelov
- **Hugging Face Model Hub**: Interaktivni demoji in primeri skupnosti
- **Raziskovalni Älanki**: TehniÄni Älanki na arxiv za poglobljeno razumevanje
- **Forumi skupnosti**: Aktivna podpora skupnosti in razprave

### ZaÄetek z modeli Qwen

#### Razvojne platforme
1. **Hugging Face Transformers**: ZaÄnite s standardno Python integracijo
2. **ModelScope**: RaziÅ¡Äite optimizirana orodja za uporabo pri Alibabi
3. **Lokalna uporaba**: Uporabite Ollama ali direktne transformers za lokalno testiranje

#### UÄna pot
1. **Razumevanje osnovnih konceptov**: PreuÄite arhitekturo in zmogljivosti druÅ¾ine Qwen
2. **Eksperimentiranje z razliÄicami**: Preizkusite razliÄne velikosti modelov za razumevanje kompromisov zmogljivosti
3. **PraktiÄna implementacija**: Uporabite modele v razvojnih okoljih
4. **Optimizacija uporabe**: Prilagodite za produkcijske primere

#### NajboljÅ¡e prakse
- **ZaÄnite z manjÅ¡imi modeli**: ZaÄetek z manjÅ¡imi modeli (1.5B-7B) za zaÄetni razvoj
- **Uporabite predloge za klepet**: Uporabite pravilno oblikovanje za optimalne rezultate
- **Spremljajte vire**: Spremljajte porabo pomnilnika in hitrost sklepanja
- **Razmislite o specializaciji**: Izberite razliÄice, specifiÄne za domeno, kadar je to primerno

## Napredni vzorci uporabe

### Primeri prilagajanja

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### Specializirano oblikovanje pozivov

**Za kompleksne naloge razmiÅ¡ljanja:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**Za generiranje kode s kontekstom:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### VeÄjeziÄne aplikacije

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ä¸­æ–‡)",
        "es": "Spanish (EspaÃ±ol)",
        "fr": "French (FranÃ§ais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (æ—¥æœ¬èª)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ğŸ”§ Vzorci produkcijske uporabe

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## Strategije optimizacije zmogljivosti

### Optimizacija pomnilnika

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### Optimizacija sklepanja

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## NajboljÅ¡e prakse in smernice

### Varnost in zasebnost

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### Spremljanje in ocenjevanje

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## ZakljuÄek

DruÅ¾ina modelov Qwen predstavlja celovit pristop k demokratizaciji tehnologije umetne inteligence, hkrati pa ohranja konkurenÄno zmogljivost v razliÄnih aplikacijah. S svojo zavezanostjo dostopnosti odprte kode, veÄjeziÄnim zmogljivostim in prilagodljivim moÅ¾nostim uporabe Qwen omogoÄa organizacijam in razvijalcem, da izkoristijo zmogljivosti umetne inteligence ne glede na njihove vire ali specifiÄne zahteve.

### KljuÄne toÄke

**OdliÄnost odprte kode**: Qwen dokazuje, da lahko modeli odprte kode doseÅ¾ejo zmogljivost, konkurenÄno lastniÅ¡kim alternativam, hkrati pa zagotavljajo transparentnost, prilagodljivost in nadzor.

**Prilagodljiva arhitektura**: Razpon od 0.5B do 235B parametrov omogoÄa uporabo v celotnem spektru raÄunalniÅ¡kih okolij, od mobilnih naprav do podjetniÅ¡kih grozdov.

**Specializirane zmogljivosti**: RazliÄice, specifiÄne za domeno, kot so Qwen-Coder, Qwen-Math in Qwen-VL, zagotavljajo specializirano strokovno znanje, hkrati pa ohranjajo sploÅ¡no razumevanje jezika.

**Globalna dostopnost**: MoÄna podpora veÄ kot 119 jezikom omogoÄa Qwenu uporabo v mednarodnih aplikacijah in raznolikih uporabniÅ¡kih bazah.

**Nenehne inovacije**: Evolucija od Qwen 1.0 do Qwen3 kaÅ¾e na dosledno izboljÅ¡anje zmogljivosti, uÄinkovitosti in moÅ¾nosti uporabe.

### Pogled v prihodnost

PriÄakujemo lahko, da se bo druÅ¾ina Qwen Å¡e naprej razvijala z:
- **IzboljÅ¡ano uÄinkovitostjo**: Nadaljnja optimizacija za boljÅ¡e razmerje med zmogljivostjo in parametri
- **RazÅ¡irjenimi multimodalnimi zmogljivostmi**: Integracija bolj sofisticiranega procesiranja vizualnih, zvoÄnih in besedilnih podatkov
- **IzboljÅ¡anim razmiÅ¡ljanjem**: Napredni mehanizmi razmiÅ¡ljanja in sposobnosti reÅ¡evanja problemov v veÄ korakih
- **BoljÅ¡a orodja za uporabo**: IzboljÅ¡ani okviri in orodja za optimizacijo za razliÄne scenarije uporabe
- **Rast skupnosti**: RazÅ¡irjen ekosistem orodij, aplikacij in prispevkov skupnosti

### Naslednji koraki

Ne glede na to, ali gradite klepetalni robot, razvijate izobraÅ¾evalna orodja, ustvarjate asistente za kodiranje ali delate na veÄjeziÄnih aplikacijah, druÅ¾ina Qwen ponuja prilagodljive reÅ¡itve z moÄno podporo skupnosti in celovito dokumentacijo.

Za najnovejÅ¡e posodobitve, izdaje modelov in podrobno tehniÄno dokumentacijo obiÅ¡Äite uradne repozitorije Qwen na Hugging Face in raziÅ¡Äite aktivne razprave skupnosti ter primere.

Prihodnost razvoja umetne inteligence leÅ¾i v dostopnih, transparentnih in zmogljivih orodjih, ki omogoÄajo inovacije v vseh sektorjih in obsegih. DruÅ¾ina Qwen uteleÅ¡a to vizijo, saj organizacijam in razvijalcem zagotavlja temelje za gradnjo naslednje generacije aplikacij, ki jih poganja umetna inteligenca.

## Dodatni viri

- **Uradna dokumentacija**: [Dokumentacija Qwen](https://qwen.readthedocs.io/)
- **Model Hub**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **TehniÄni Älanki**: [Raziskovalne publikacije Qwen](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **Skupnost**: [GitHub razprave in teÅ¾ave](https://github.com/QwenLM/)
- **Platforma ModelScope**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## UÄni cilji

Po zakljuÄku tega modula boste lahko:

1. RazloÅ¾ili arhitekturne prednosti druÅ¾ine modelov Qwen in njen pristop odprte kode
2. Izbrali ustrezno razliÄico Qwen glede na specifiÄne zahteve aplikacije in omejitve virov
3. Implementirali modele Qwen v razliÄnih scenarijih uporabe z optimiziranimi konfiguracijami
4. Uporabili tehnike kvantizacije in optimizacije za izboljÅ¡anje zmogljivosti modelov Qwen
5. Ocenili kompromise med velikostjo modela, zmogljivostjo in sposobnostmi v druÅ¾ini Qwen

## Kaj sledi

- [03: Osnove druÅ¾ine Gemma](03.GemmaFamily.md)

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje z umetno inteligenco [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas prosimo, da upoÅ¡tevate, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za kljuÄne informacije priporoÄamo profesionalni ÄloveÅ¡ki prevod. Ne prevzemamo odgovornosti za morebitna nesporazume ali napaÄne razlage, ki bi nastale zaradi uporabe tega prevoda.