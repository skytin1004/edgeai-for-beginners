<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "38b68a204a9621126d056b0e5b51ab7c",
  "translation_date": "2025-10-09T20:55:00+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "sl"
}
-->
# Poglavje 1: Napredno uƒçenje SLM - Osnove in optimizacija

Majhni jezikovni modeli (SLM) predstavljajo kljuƒçen napredek v EdgeAI, saj omogoƒçajo napredne zmogljivosti obdelave naravnega jezika na napravah z omejenimi viri. Razumevanje uƒçinkovite uporabe, optimizacije in implementacije SLM-jev je bistveno za gradnjo praktiƒçnih re≈°itev umetne inteligence na robu.

## Uvod

V tej lekciji bomo raziskali majhne jezikovne modele (SLM) in njihove napredne strategije implementacije. Pokrili bomo temeljne koncepte SLM-jev, njihove meje parametrov in klasifikacije, tehnike optimizacije ter praktiƒçne strategije implementacije v okolju robnega raƒçunalni≈°tva.

## Cilji uƒçenja

Do konca te lekcije boste sposobni:

- üî¢ Razumeti meje parametrov in klasifikacije majhnih jezikovnih modelov.
- üõ†Ô∏è Prepoznati kljuƒçne tehnike optimizacije za implementacijo SLM-jev na robnih napravah.
- üöÄ Nauƒçiti se izvajati napredne strategije kvantizacije in kompresije za SLM-je.

## Razumevanje meja parametrov in klasifikacije SLM-jev

Majhni jezikovni modeli (SLM) so AI modeli, zasnovani za obdelavo, razumevanje in generiranje vsebine naravnega jezika z bistveno manj parametri kot njihovi veliki kolegi. Medtem ko veliki jezikovni modeli (LLM) vsebujejo stotine milijard do trilijonov parametrov, so SLM-ji posebej zasnovani za uƒçinkovitost in implementacijo na robu.

Okvir klasifikacije parametrov nam pomaga razumeti razliƒçne kategorije SLM-jev in njihove ustrezne primere uporabe. Ta klasifikacija je kljuƒçna za izbiro pravega modela za specifiƒçne scenarije robnega raƒçunalni≈°tva.

### Okvir klasifikacije parametrov

Razumevanje meja parametrov pomaga pri izbiri ustreznih modelov za razliƒçne scenarije robnega raƒçunalni≈°tva:

- **üî¨ Mikro SLM-ji**: 100M - 1,4B parametrov (izjemno lahki za mobilne naprave)
- **üì± Majhni SLM-ji**: 1,5B - 13,9B parametrov (uravnote≈æena zmogljivost in uƒçinkovitost)
- **‚öñÔ∏è Srednji SLM-ji**: 14B - 30B parametrov (pribli≈æevanje zmogljivostim LLM-jev ob ohranjanju uƒçinkovitosti)

Natanƒçna meja ostaja fluidna v raziskovalni skupnosti, vendar veƒçina praktikov modele z manj kot 30 milijardami parametrov ≈°teje za "majhne," pri ƒçemer nekateri viri postavljajo prag ≈°e ni≈æje, na 10 milijard parametrov.

### Kljuƒçne prednosti SLM-jev

SLM-ji ponujajo veƒç temeljnih prednosti, zaradi katerih so idealni za aplikacije robnega raƒçunalni≈°tva:

**Operativna uƒçinkovitost**: SLM-ji omogoƒçajo hitrej≈°e ƒçase sklepanja zaradi manj parametrov za obdelavo, kar jih naredi idealne za aplikacije v realnem ƒçasu. Zahtevajo manj raƒçunalni≈°kih virov, kar omogoƒça implementacijo na napravah z omejenimi viri, hkrati pa porabijo manj energije in ohranjajo zmanj≈°an ogljiƒçni odtis.

**Fleksibilnost implementacije**: Ti modeli omogoƒçajo zmogljivosti AI na napravi brez potrebe po internetni povezavi, izbolj≈°ujejo zasebnost in varnost z lokalno obdelavo, jih je mogoƒçe prilagoditi za aplikacije specifiƒçne za doloƒçeno podroƒçje in so primerni za razliƒçna okolja robnega raƒçunalni≈°tva.

**Stro≈°kovna uƒçinkovitost**: SLM-ji ponujajo stro≈°kovno uƒçinkovito usposabljanje in implementacijo v primerjavi z LLM-ji, z zmanj≈°animi operativnimi stro≈°ki in ni≈æjimi zahtevami po pasovni ≈°irini za aplikacije na robu.

## Napredne strategije pridobivanja modelov

### Ekosistem Hugging Face

Hugging Face slu≈æi kot primarno sredi≈°ƒçe za odkrivanje in dostop do najsodobnej≈°ih SLM-jev. Platforma ponuja obse≈æne vire za odkrivanje in implementacijo modelov:

**Funkcije odkrivanja modelov**: Platforma omogoƒça napredno filtriranje po ≈°tevilu parametrov, vrsti licence in metrikah zmogljivosti. Uporabniki lahko dostopajo do orodij za primerjavo modelov, rezultatov ocenjevanja zmogljivosti v realnem ƒçasu in WebGPU demo razliƒçic za takoj≈°nje testiranje.

**Kurirane zbirke SLM-jev**: Priljubljeni modeli vkljuƒçujejo Phi-4-mini-3.8B za napredne naloge sklepanja, serijo Qwen3 (0.6B/1.7B/4B) za veƒçjeziƒçne aplikacije, Google Gemma3 za uƒçinkovite splo≈°ne naloge in eksperimentalne modele, kot je BitNET za ultra-nizko precizno implementacijo. Platforma vkljuƒçuje tudi zbirke, ki jih vodi skupnost, s specializiranimi modeli za specifiƒçna podroƒçja ter predhodno usposobljene in na navodila optimizirane razliƒçice za razliƒçne primere uporabe.

### Katalog modelov Azure AI Foundry

Katalog modelov Azure AI Foundry omogoƒça dostop do SLM-jev na ravni podjetja z izbolj≈°animi integracijskimi zmogljivostmi:

**Integracija na ravni podjetja**: Katalog vkljuƒçuje modele, ki jih neposredno prodaja Azure z podporo na ravni podjetja in SLA-ji, vkljuƒçno s Phi-4-mini-3.8B za napredne zmogljivosti sklepanja in Llama 3-8B za produkcijsko implementacijo. Vkljuƒçuje tudi modele, kot je Qwen3 8B, ki jih ponujajo zaupanja vredni tretji odprtokodni viri.

**Prednosti za podjetja**: Vgrajena orodja za prilagajanje, opazovanje in odgovorno AI so integrirana z zamenljivim Provisioned Throughput med dru≈æinami modelov. Neposredna podpora Microsofta z SLA-ji na ravni podjetja, integrirane funkcije varnosti in skladnosti ter celoviti delovni tokovi implementacije izbolj≈°ujejo izku≈°njo na ravni podjetja.

## Napredne tehnike kvantizacije in optimizacije

### Okvir optimizacije Llama.cpp

Llama.cpp ponuja najsodobnej≈°e tehnike kvantizacije za maksimalno uƒçinkovitost pri implementaciji na robu:

**Metode kvantizacije**: Okvir podpira razliƒçne ravni kvantizacije, vkljuƒçno z Q4_0 (4-bitna kvantizacija z odliƒçno redukcijo velikosti - idealna za mobilno implementacijo Qwen3-0.6B), Q5_1 (5-bitna kvantizacija, ki uravnote≈æi kakovost in kompresijo - primerna za robno sklepanje Phi-4-mini-3.8B) in Q8_0 (8-bitna kvantizacija za skoraj originalno kakovost - priporoƒçljiva za produkcijsko uporabo Google Gemma3). BitNET predstavlja vrhunec z 1-bitno kvantizacijo za ekstremne scenarije kompresije.

**Prednosti implementacije**: Sklepanje, optimizirano za CPU, s pospe≈°evanjem SIMD omogoƒça uƒçinkovito nalaganje in izvajanje modelov. Zdru≈æljivost med platformami na arhitekturah x86, ARM in Apple Silicon omogoƒça strojno neodvisne zmogljivosti implementacije.

**Praktiƒçen primer implementacije**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Primerjava pomnilni≈°kega odtisa**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Optimizacijski paket Microsoft Olive

Microsoft Olive ponuja celovite delovne tokove optimizacije modelov, zasnovane za produkcijska okolja:

**Tehnike optimizacije**: Paket vkljuƒçuje dinamiƒçno kvantizacijo za samodejno izbiro natanƒçnosti (≈°e posebej uƒçinkovito pri modelih serije Qwen3), optimizacijo grafov in zdru≈æevanje operaterjev (optimizirano za arhitekturo Google Gemma3), optimizacije, specifiƒçne za strojno opremo, za CPU, GPU in NPU (s posebno podporo za Phi-4-mini-3.8B na ARM napravah) ter veƒçstopenjske optimizacijske tokove. Modeli BitNET zahtevajo specializirane delovne tokove za 1-bitno kvantizacijo znotraj okvira Olive.

**Avtomatizacija delovnih tokov**: Avtomatizirano primerjanje med razliƒçicami optimizacije zagotavlja ohranjanje kakovostnih metrik med optimizacijo. Integracija s priljubljenimi okviri ML, kot sta PyTorch in ONNX, omogoƒça optimizacijo za oblak in robno implementacijo.

**Praktiƒçen primer implementacije**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Okvir Apple MLX

Apple MLX ponuja nativno optimizacijo, posebej zasnovano za naprave Apple Silicon:

**Optimizacija za Apple Silicon**: Okvir uporablja arhitekturo enotnega pomnilnika z integracijo Metal Performance Shaders, samodejno me≈°ano natanƒçnost sklepanja (≈°e posebej uƒçinkovito pri Google Gemma3) in optimizirano uporabo pasovne ≈°irine pomnilnika. Phi-4-mini-3.8B ka≈æe izjemno zmogljivost na ƒçipih serije M, medtem ko Qwen3-1.7B zagotavlja optimalno ravnovesje za implementacijo na MacBook Air.

**Razvojne funkcije**: Podpora za Python in Swift API z operacijami, zdru≈æljivimi z NumPy, zmogljivostmi samodejne diferenciacije in brezhibno integracijo z razvojnimi orodji Apple zagotavljajo celovito razvojno okolje.

**Praktiƒçen primer implementacije**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Produkcijska implementacija in strategije sklepanja

### Ollama: Poenostavljena lokalna implementacija

Ollama poenostavi implementacijo SLM-jev z znaƒçilnostmi, pripravljenimi za podjetja, za lokalna in robna okolja:

**Zmogljivosti implementacije**: Namestitev in izvajanje modela z enim ukazom z avtomatskim pridobivanjem in predpomnjenjem modela. Podpora za Phi-4-mini-3.8B, celotno serijo Qwen3 (0.6B/1.7B/4B) in Google Gemma3 z REST API za integracijo aplikacij ter zmogljivosti upravljanja in preklapljanja med modeli. Modeli BitNET zahtevajo eksperimentalne konfiguracije gradnje za podporo 1-bitni kvantizaciji.

**Napredne funkcije**: Podpora za prilagajanje modelov, generiranje Dockerfile za implementacijo v kontejnerjih, pospe≈°evanje GPU z avtomatskim zaznavanjem ter mo≈ænosti kvantizacije in optimizacije modelov zagotavljajo celovito fleksibilnost implementacije.

### VLLM: Sklepanje z visoko zmogljivostjo

VLLM omogoƒça optimizacijo sklepanja na ravni produkcije za scenarije z visokim pretokom:

**Optimizacije zmogljivosti**: PagedAttention za uƒçinkovito raƒçunalni≈°ko obdelavo pozornosti (≈°e posebej koristno za transformacijsko arhitekturo Phi-4-mini-3.8B), dinamiƒçno zdru≈æevanje za optimizacijo pretoka (optimizirano za paralelno obdelavo serije Qwen3), paralelizem tenzorjev za skaliranje na veƒç GPU-jih (podpora za Google Gemma3) in spekulativno dekodiranje za zmanj≈°anje zakasnitve. Modeli BitNET zahtevajo specializirane jedrne funkcije sklepanja za 1-bitne operacije.

**Integracija na ravni podjetja**: API konƒçne toƒçke, zdru≈æljive z OpenAI, podpora za implementacijo Kubernetes, integracija opazovanja in spremljanja ter zmogljivosti samodejnega skaliranja zagotavljajo re≈°itve implementacije na ravni podjetja.

### Foundry Local: Microsoftova re≈°itev za rob

Foundry Local zagotavlja celovite zmogljivosti implementacije na robu za okolja na ravni podjetja:

**Znaƒçilnosti robnega raƒçunalni≈°tva**: Zasnova arhitekture "offline-first" z optimizacijo za omejene vire, upravljanje lokalnega registra modelov in zmogljivosti sinhronizacije med robom in oblakom zagotavljajo zanesljivo implementacijo na robu.

**Varnost in skladnost**: Lokalna obdelava podatkov za ohranjanje zasebnosti, varnostni nadzor na ravni podjetja, bele≈æenje revizij in poroƒçanje o skladnosti ter upravljanje dostopa na podlagi vlog zagotavljajo celovito varnost za implementacije na robu.

## Najbolj≈°e prakse za implementacijo SLM-jev

### Smernice za izbiro modelov

Pri izbiri SLM-jev za implementacijo na robu upo≈°tevajte naslednje dejavnike:

**Premisleki o ≈°tevilu parametrov**: Izberite mikro SLM-je, kot je Qwen3-0.6B, za izjemno lahke mobilne aplikacije, majhne SLM-je, kot sta Qwen3-1.7B ali Google Gemma3, za uravnote≈æene scenarije zmogljivosti, in srednje SLM-je, kot sta Phi-4-mini-3.8B ali Qwen3-4B, ko se pribli≈æujete zmogljivostim LLM-jev ob ohranjanju uƒçinkovitosti. Modeli BitNET ponujajo eksperimentalno ultra-kompresijo za specifiƒçne raziskovalne aplikacije.

**Usklajenost s primerom uporabe**: Ujemajte zmogljivosti modela s specifiƒçnimi zahtevami aplikacije, pri ƒçemer upo≈°tevajte dejavnike, kot so kakovost odziva, hitrost sklepanja, omejitve pomnilnika in zahteve za delovanje brez povezave.

### Izbira strategije optimizacije

**Pristop kvantizacije**: Izberite ustrezne ravni kvantizacije glede na zahteve kakovosti in omejitve strojne opreme. Upo≈°tevajte Q4_0 za maksimalno kompresijo (idealno za mobilno implementacijo Qwen3-0.6B), Q5_1 za uravnote≈æeno razmerje med kakovostjo in kompresijo (primerno za Phi-4-mini-3.8B in Google Gemma3) ter Q8_0 za ohranjanje skoraj originalne kakovosti (priporoƒçljivo za produkcijska okolja Qwen3-4B). BitNET-ova 1-bitna kvantizacija predstavlja skrajno mejo kompresije za specializirane aplikacije.

**Izbira okvira**: Izberite optimizacijske okvire glede na ciljno strojno opremo in zahteve implementacije. Uporabite Llama.cpp za implementacijo, optimizirano za CPU, Microsoft Olive za celovite delovne tokove optimizacije in Apple MLX za naprave Apple Silicon.

## Praktiƒçni primeri modelov in primeri uporabe

### Scenariji implementacije v resniƒçnem svetu

**Mobilne aplikacije**: Qwen3-0.6B se odliƒçno obnese v aplikacijah za pametne telefone z minimalnim pomnilni≈°kim odtisom, medtem ko Google Gemma3 zagotavlja uravnote≈æeno zmogljivost za izobra≈æevalna orodja na tablicah. Phi-4-mini-3.8B ponuja vrhunske zmogljivosti sklepanja za aplikacije mobilne produktivnosti.

**Namizno in robno raƒçunalni≈°tvo**: Qwen3-1.7B zagotavlja optimalno zmogljivost za aplikacije namiznih pomoƒçnikov, Phi-4-mini-3.8B ponuja napredne zmogljivosti generiranja kode za razvojna orodja, medtem ko Qwen3-4B omogoƒça sofisticirano analizo dokumentov v delovnih okoljih.

**Raziskave in eksperimentalno**: Modeli BitNET omogoƒçajo raziskovanje ultra-nizko preciznega sklepanja za akademske raziskave in aplikacije dokazovanja koncepta, ki zahtevajo ekstremne omejitve virov.

### Primerjalne zmogljivosti

**Hitrost sklepanja**: Qwen3-0.6B dosega najhitrej≈°e ƒçase sklepanja na mobilnih CPU-jih, Google Gemma3 zagotavlja uravnote≈æeno razmerje med hitrostjo in kakovostjo za splo≈°ne aplikacije, Phi-4-mini-3.8B ponuja vrhunsko hitrost sklepanja za kompleksne naloge, medtem ko BitNET zagotavlja teoretiƒçno maksimalen pretok s specializirano strojno opremo.

**Zahteve po pomnilniku**: Pomnilni≈°ki odtisi modelov segajo od Qwen3-0.6B (manj kot 1GB kvantiziran) do Phi-4-mini-3.8B (pribli≈æno 3-4GB kvantiziran), medtem ko BitNET dosega odtise pod 500MB v eksperimentalnih konfiguracijah.

## Izzivi in premisleki

### Kompromisi zmogljivosti

Implementacija SLM-jev zahteva skrbno razmislek o kompromisih med velikostjo modela, hitrostjo

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve AI za prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). ƒåeprav si prizadevamo za natanƒçnost, vas prosimo, da upo≈°tevate, da lahko avtomatizirani prevodi vsebujejo napake ali netoƒçnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za kljuƒçne informacije priporoƒçamo profesionalni ƒçlove≈°ki prevod. Ne odgovarjamo za morebitna nesporazumevanja ali napaƒçne razlage, ki izhajajo iz uporabe tega prevoda.