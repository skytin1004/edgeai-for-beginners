<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-01T20:20:44+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "ru"
}
-->
# –†–∞–∑–¥–µ–ª 2: –û—Å–Ω–æ–≤—ã —Å–µ–º–µ–π—Å—Ç–≤–∞ Qwen

–°–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Qwen –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ Alibaba Cloud –∫ –∫—Ä—É–ø–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º—É –ò–ò, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –º–æ–≥—É—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å –≤—ã–¥–∞—é—â–∏—Ö—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –æ—Å—Ç–∞–≤–∞—è—Å—å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è. –í–∞–∂–Ω–æ –ø–æ–Ω—è—Ç—å, –∫–∞–∫ —Å–µ–º–µ–π—Å—Ç–≤–æ Qwen –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –º–æ—â–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ò–ò —Å –≥–∏–±–∫–∏–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á.

## –†–µ—Å—É—Ä—Å—ã –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤

### –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –º–æ–¥–µ–ª–µ–π Hugging Face
–í—ã–±—Ä–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–µ–º–µ–π—Å—Ç–≤–∞ Qwen –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ [Hugging Face](https://huggingface.co/models?search=qwen), –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –¥–æ—Å—Ç—É–ø –∫ –Ω–µ–∫–æ—Ç–æ—Ä—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π. –í—ã –º–æ–∂–µ—Ç–µ –∏–∑—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã, –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∏—Ö –ø–æ–¥ —Å–≤–æ–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å –∏—Ö —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏.

### –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
–î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å [Microsoft Foundry Local](https://github.com/microsoft/foundry-local), —á—Ç–æ–±—ã –∑–∞–ø—É—Å–∫–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ Qwen –Ω–∞ –≤–∞—à–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–µ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é.

### –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π Qwen –¥–ª—è —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –Ω–∞ –ø–µ—Ä–∏—Ñ–µ—Ä–∏–π–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö](https://github.com/microsoft/olive)

## –í–≤–µ–¥–µ–Ω–∏–µ

–í —ç—Ç–æ–º —É—á–µ–±–Ω–æ–º –ø–æ—Å–æ–±–∏–∏ –º—ã –∏–∑—É—á–∏–º —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Qwen –æ—Ç Alibaba –∏ –µ–≥–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏. –ú—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —ç–≤–æ–ª—é—Ü–∏—é —Å–µ–º–µ–π—Å—Ç–≤–∞ Qwen, –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–ª–∞—é—Ç –º–æ–¥–µ–ª–∏ Qwen —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º–∏, –∫–ª—é—á–µ–≤—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –≤ —Å–µ–º–µ–π—Å—Ç–≤–µ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.

## –¶–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è

–ö –∫–æ–Ω—Ü—É —ç—Ç–æ–≥–æ —É—á–µ–±–Ω–æ–≥–æ –ø–æ—Å–æ–±–∏—è –≤—ã —Å–º–æ–∂–µ—Ç–µ:

- –ü–æ–Ω—è—Ç—å —Ñ–∏–ª–æ—Å–æ—Ñ–∏—é –¥–∏–∑–∞–π–Ω–∞ –∏ —ç–≤–æ–ª—é—Ü–∏—é —Å–µ–º–µ–π—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π Qwen –æ—Ç Alibaba
- –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –º–æ–¥–µ–ª—è–º Qwen –¥–æ—Å—Ç–∏–≥–∞—Ç—å –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π Qwen
- –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∑–Ω–∞–Ω–∏—è –æ –º–æ–¥–µ–ª—è—Ö Qwen –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤

## –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ª–∞–Ω–¥—à–∞—Ñ—Ç–∞ –ò–ò

–õ–∞–Ω–¥—à–∞—Ñ—Ç –ò–ò –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏–ª—Å—è, –∏ —Ä–∞–∑–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Å—Ä–µ–¥–æ—Ç–∞—á–∏–≤–∞—é—Ç—Å—è –Ω–∞ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –∑–∞–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º, –¥—Ä—É–≥–∏–µ –¥–µ–ª–∞—é—Ç –∞–∫—Ü–µ–Ω—Ç –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞. –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç –ª–∏–±–æ –æ–≥—Ä–æ–º–Ω—ã–µ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏, –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ API, –ª–∏–±–æ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —É—Å—Ç—É–ø–∞—Ç—å –≤ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö.

–≠—Ç–æ—Ç –ø–∞—Ä–∞–¥–∏–≥–º–∞ —Å–æ–∑–¥–∞–µ—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –¥–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π, —Å—Ç—Ä–µ–º—è—â–∏—Ö—Å—è –∫ –º–æ—â–Ω—ã–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º –ò–ò, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ —Å–≤–æ–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏, –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ –∏ –≥–∏–±–∫–æ—Å—Ç—å—é —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è. –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —á–∞—Å—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –≤—ã–±–æ—Ä–∞ –º–µ–∂–¥—É –ø–µ—Ä–µ–¥–æ–≤–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è.

## –ü—Ä–æ–±–ª–µ–º–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –ò–ò –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞

–ü–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –∏ –¥–æ—Å—Ç—É–ø–Ω–æ–º –ò–ò —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤—Å–µ –±–æ–ª–µ–µ –≤–∞–∂–Ω–æ–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, —Ç—Ä–µ–±—É—é—â–∏–µ –≥–∏–±–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω—É–∂–¥, —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏, –≥–¥–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ API –º–æ–≥—É—Ç —Å—Ç–∞—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º–∏, –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –≤ —Ç–∞–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –∫–∞–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞.

### –û—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é

–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –ò–ò —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å —Ä—è–¥–æ–º —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏—Ö –∏—Ö –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å:

- **–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å**: –ù–∞–ª–∏—á–∏–µ –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞ –¥–ª—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
- **–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å**: –†–∞–∑—É–º–Ω—ã–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±—é–¥–∂–µ—Ç–æ–≤
- **–ì–∏–±–∫–æ—Å—Ç—å**: –ù–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è
- **–ì–ª–æ–±–∞–ª—å–Ω—ã–π –æ—Ö–≤–∞—Ç**: –°–∏–ª—å–Ω—ã–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –∏ –º–µ–∂–∫—É–ª—å—Ç—É—Ä–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
- **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è**: –í–∞—Ä–∏–∞–Ω—Ç—ã, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á

## –§–∏–ª–æ—Å–æ—Ñ–∏—è –º–æ–¥–µ–ª–µ–π Qwen

–°–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Qwen –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –º–æ–¥–µ–ª–µ–π –ò–ò, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –∫–æ—Ç–æ—Ä–æ–≥–æ —è–≤–ª—è—é—Ç—Å—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞, –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ú–æ–¥–µ–ª–∏ Qwen –¥–æ—Å—Ç–∏–≥–∞—é—Ç —ç—Ç–æ–≥–æ –±–ª–∞–≥–æ–¥–∞—Ä—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º —Ä–∞–∑–º–µ—Ä–∞–º –º–æ–¥–µ–ª–µ–π, –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –º–µ—Ç–æ–¥–∞–º –æ–±—É—á–µ–Ω–∏—è –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π.

–°–µ–º–µ–π—Å—Ç–≤–æ Qwen –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ –≤—Å–µ–º—É —Å–ø–µ–∫—Ç—Ä—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, –ø–æ–∑–≤–æ–ª—è—è —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –æ—Ç –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –¥–æ —Å–µ—Ä–≤–µ—Ä–æ–≤ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –∑–Ω–∞—á–∏–º—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ò–ò. –¶–µ–ª—å ‚Äî –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ—Å—Ç—É–ø –∫ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º—É –ò–ò, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –≥–∏–±–∫–æ—Å—Ç—å –≤ –≤—ã–±–æ—Ä–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è.

### –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –¥–∏–∑–∞–π–Ω–∞ Qwen

–ú–æ–¥–µ–ª–∏ Qwen –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–ª–∏—á–∞—é—Ç –∏—Ö –æ—Ç –¥—Ä—É–≥–∏—Ö —Å–µ–º–µ–π—Å—Ç–≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π:

- **–û—Ç–∫—Ä—ã—Ç—ã–π –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å**: –ü–æ–ª–Ω–∞—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- **–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**: –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–≥—Ä–æ–º–Ω—ã—Ö, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö –º–Ω–æ–∂–µ—Å—Ç–≤–æ —è–∑—ã–∫–æ–≤ –∏ –æ–±–ª–∞—Å—Ç–µ–π
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: –ù–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
- **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ**: –í–∞—Ä–∏–∞–Ω—Ç—ã, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á

## –ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–µ —Å–µ–º–µ–π—Å—Ç–≤–æ Qwen

### –ú–∞—Å—à—Ç–∞–±–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

–û–¥–Ω–æ–π –∏–∑ –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Å–µ–º–µ–π—Å—Ç–≤–∞ Qwen —è–≤–ª—è–µ—Ç—Å—è –º–∞—Å—à—Ç–∞–± –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –º–æ–¥–µ–ª–µ–π. –ú–æ–¥–µ–ª–∏ Qwen –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–µ —Ç—Ä–∏–ª–ª–∏–æ–Ω—ã —Ç–æ–∫–µ–Ω–æ–≤, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –æ –º–∏—Ä–µ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.

–≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Å–æ—á–µ—Ç–∞–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç, –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫—É—é –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—É, —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –∫–æ–¥–∞ –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã. –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –∫–∞–∫ —à–∏—Ä–æ—Ç—É –∑–Ω–∞–Ω–∏–π, —Ç–∞–∫ –∏ –≥–ª—É–±–∏–Ω—É –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö –∏ —è–∑—ã–∫–∞—Ö.

### –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è

–ü–æ—Å–ª–µ–¥–Ω–∏–µ –º–æ–¥–µ–ª–∏ Qwen –≤–∫–ª—é—á–∞—é—Ç —Å–ª–æ–∂–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∏–µ —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏:

**–†–µ–∂–∏–º –º—ã—à–ª–µ–Ω–∏—è (Qwen3)**: –ú–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∞–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è.

**–î–≤—É—Ö—Ä–µ–∂–∏–º–Ω–∞—è —Ä–∞–±–æ—Ç–∞**: –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è –º–µ–∂–¥—É —Ä–µ–∂–∏–º–æ–º –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ —Ä–µ–∂–∏–º–æ–º –≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á.

**–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π**: –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–∫–ª—é—á–µ–Ω–∏–µ —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —É–ª—É—á—à–∞—é—â–µ–µ –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å –∏ —Ç–æ—á–Ω–æ—Å—Ç—å –≤ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏

–°–µ–º–µ–π—Å—Ç–≤–æ Qwen –≤–∫–ª—é—á–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏:

**–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –¥–∏–∑–∞–π–Ω**: –ï–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –≤—Å–µ—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∞—è –ª–µ–≥–∫–æ—Å—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.

**–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è**: –ë–µ—Å—à–æ–≤–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∞—É–¥–∏–æ –≤ –µ–¥–∏–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É.

**–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è**: –ù–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –∏ —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è.

## –†–∞–∑–º–µ—Ä—ã –º–æ–¥–µ–ª–µ–π –∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è

–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—Ä–µ–¥—ã —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –≤—ã–∏–≥—Ä—ã–≤–∞—é—Ç –æ—Ç –≥–∏–±–∫–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π Qwen, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º:

### –ú–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ (0.5B-3B)

Qwen –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º–∞–ª—ã–µ –º–æ–¥–µ–ª–∏, –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –¥–ª—è —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –Ω–∞ –ø–µ—Ä–∏—Ñ–µ—Ä–∏–π–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö, –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö –∏ –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.

### –°—Ä–µ–¥–Ω–∏–µ –º–æ–¥–µ–ª–∏ (7B-32B)

–ú–æ–¥–µ–ª–∏ —Å—Ä–µ–¥–Ω–µ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –æ—Ç–ª–∏—á–Ω—ã–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏.

### –ë–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ (72B+)

–ú–æ–¥–µ–ª–∏ –ø–æ–ª–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –ø–µ—Ä–µ–¥–æ–≤—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–π, —Ç—Ä–µ–±—É—é—â–∏—Ö –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π.

## –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Å–µ–º–µ–π—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π Qwen

### –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞

–ú–æ–¥–µ–ª–∏ Qwen –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –ø–æ–ª–Ω—É—é –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, –ø–æ–∑–≤–æ–ª—è—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º –ø–æ–Ω–∏–º–∞—Ç—å, –∏–∑–º–µ–Ω—è—Ç—å –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –ø–æ–¥ —Å–≤–æ–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω—É–∂–¥—ã –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞.

### –ì–∏–±–∫–æ—Å—Ç—å —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è

–î–∏–∞–ø–∞–∑–æ–Ω —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è, –æ—Ç –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –¥–æ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º –≥–∏–±–∫–æ—Å—Ç—å –≤ –≤—ã–±–æ—Ä–µ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ò–ò.

### –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ

–ú–æ–¥–µ–ª–∏ Qwen –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–∞—Ö, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—è –¥–µ—Å—è—Ç–∫–∏ —è–∑—ã–∫–æ–≤, —Å –æ—Å–æ–±–æ–π —Å–∏–ª–æ–π –≤ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –∫–∏—Ç–∞–π—Å–∫–æ–º, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö –ø–æ–¥—Ö–æ–¥—è—â–∏–º–∏ –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π.

### –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

–ú–æ–¥–µ–ª–∏ Qwen —Å—Ç–∞–±–∏–ª—å–Ω–æ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ç–µ—Å—Ç–∞—Ö, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è, —á—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Å–æ–ø–µ—Ä–Ω–∏—á–∞—Ç—å —Å –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–º–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞–º–∏.

### –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ Qwen-Coder –∏ Qwen-Math, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –æ–±—â–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —è–∑—ã–∫–∞.

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏ —Å–ª—É—á–∞–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

–ü—Ä–µ–∂–¥–µ —á–µ–º —É–≥–ª—É–±–ª—è—Ç—å—Å—è –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏, –¥–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–æ–≥–æ, —á—Ç–æ –º–æ–≥—É—Ç –¥–µ–ª–∞—Ç—å –º–æ–¥–µ–ª–∏ Qwen:

### –ü—Ä–∏–º–µ—Ä –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è

Qwen-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤ –ø–æ—à–∞–≥–æ–≤–æ–º —Ä–µ—à–µ–Ω–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–∏ –ø–æ –∏—Å—á–∏—Å–ª–µ–Ω–∏—é:

```
User: Find the derivative of f(x) = x¬≥ + 2x¬≤ - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x¬≥ + 2x¬≤ - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(x‚Åø) = n¬∑x‚Åø‚Åª¬π

Breaking down each term:
- d/dx(x¬≥) = 3x¬≤
- d/dx(2x¬≤) = 2 ¬∑ 2x¬π = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x¬≤ + 4x - 5
```

### –ü—Ä–∏–º–µ—Ä –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏

–ú–æ–¥–µ–ª–∏ Qwen –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Å–∏–ª—å–Ω—ã–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–∞—Ö:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**‰∏≠Êñá:**
‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâÊòØÊåáÊú∫Âô®ÔºàÁâπÂà´ÊòØËÆ°ÁÆóÊú∫Á≥ªÁªüÔºâÂØπ‰∫∫Á±ªÊô∫ËÉΩËøáÁ®ãÁöÑÊ®°Êãü„ÄÇËøô‰∫õËøáÁ®ãÂåÖÊã¨Â≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥ÈóÆÈ¢ò„ÄÅÊÑüÁü•ÂíåËØ≠Ë®ÄÁêÜËß£„ÄÇ‰∫∫Â∑•Êô∫ËÉΩ‰ΩøÊú∫Âô®ËÉΩÂ§üÊâßË°åÈÄöÂ∏∏ÈúÄË¶Å‰∫∫Á±ªËÆ§Áü•ËÉΩÂäõÁöÑ‰ªªÂä°„ÄÇ
```

### –ü—Ä–∏–º–µ—Ä –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π

Qwen-VL –º–æ–∂–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### –ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞

Qwen-Coder –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–∏ –∫–æ–¥–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

–≠—Ç–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–µ–¥—É–µ—Ç –ª—É—á—à–∏–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º —Å —á–µ—Ç–∫–∏–º–∏ –∏–º–µ–Ω–∞–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö, –ø–æ–¥—Ä–æ–±–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ª–æ–≥–∏–∫–æ–π.
```

### Edge Deployment Example

Qwen models can be deployed on various edge devices with optimized configurations:

```
# –ü—Ä–∏–º–µ—Ä —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –Ω–∞ –º–æ–±–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ —Å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –º–æ–±–∏–ª—å–Ω–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è

```
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## –≠–≤–æ–ª—é—Ü–∏—è —Å–µ–º–µ–π—Å—Ç–≤–∞ Qwen

### Qwen 1.0 –∏ 1.5: –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏

–†–∞–Ω–Ω–∏–µ –º–æ–¥–µ–ª–∏ Qwen –∑–∞–ª–æ–∂–∏–ª–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞:

- **Qwen-7B (7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)**: –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã–π –≤—ã–ø—É—Å–∫ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤
- **Qwen-14B (14 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)**: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –∏ –∑–Ω–∞–Ω–∏—è–º–∏
- **Qwen-72B (72 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)**: –ö—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–∞—è –º–æ–¥–µ–ª—å, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∞—è –ø–µ—Ä–µ–¥–æ–≤—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- **–°–µ—Ä–∏—è Qwen1.5**: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ (0.5B –¥–æ 110B) —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

### –°–µ–º–µ–π—Å—Ç–≤–æ Qwen2: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ

–°–µ—Ä–∏—è Qwen2 –æ–∑–Ω–∞–º–µ–Ω–æ–≤–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∫ –≤ —è–∑—ã–∫–æ–≤—ã—Ö, —Ç–∞–∫ –∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö:

- **Qwen2-0.5B –¥–æ 72B**: –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω—É–∂–¥ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è
- **Qwen2-57B-A14B (MoE)**: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **Qwen2-VL**: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
- **Qwen2-Audio**: –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—É–¥–∏–æ
- **Qwen2-Math**: –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á

### –°–µ–º–µ–π—Å—Ç–≤–æ Qwen2.5: —É–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

–°–µ—Ä–∏—è Qwen2.5 –ø—Ä–∏–Ω–µ—Å–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤–æ –≤—Å–µ—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö:

- **–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**: 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
- **–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç**: –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ 128K —Ç–æ–∫–µ–Ω–æ–≤, —Å –≤–∞—Ä–∏–∞–Ω—Ç–æ–º Turbo, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–º 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è**: –£–ª—É—á—à–µ–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã Qwen2.5-Coder –∏ Qwen2.5-Math
- **–õ—É—á—à–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞**: –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ 27+ —è–∑—ã–∫–∞—Ö

### –°–µ–º–µ–π—Å—Ç–≤–æ Qwen3: –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è

–ü–æ—Å–ª–µ–¥–Ω–µ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –º—ã—à–ª–µ–Ω–∏—è:

- **Qwen3-235B-A22B**: –§–ª–∞–≥–º–∞–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ —Å 235 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **Qwen3-30B-A3B**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å MoE —Å –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –Ω–∞ –∞–∫—Ç–∏–≤–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
- **–ü–ª–æ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è
- **–†–µ–∂–∏–º –º—ã—à–ª–µ–Ω–∏—è**: –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π –∫–∞–∫ –±—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã, —Ç–∞–∫ –∏ –≥–ª—É–±–æ–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ
- **–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ**: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ 119 —è–∑—ã–∫–æ–≤ –∏ –¥–∏–∞–ª–µ–∫—Ç–æ–≤
- **–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**: 36 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö, –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è

## –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Qwen

### –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è

–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–æ–¥–µ–ª–∏ Qwen –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤, –ø–æ–º–æ—â–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫–∏. –û—Ç–∫—Ä—ã—Ç—ã–π –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –±–∏–∑–Ω–µ—Å-–ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å.

### –ú–æ–±–∏–ª—å–Ω—ã–µ –∏ –ø–µ—Ä–∏—Ñ–µ—Ä–∏–π–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è

–ú–æ–±–∏–ª—å–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–æ–¥–µ–ª–∏ Qwen –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –ø–æ–º–æ—â–Ω–∏–∫–æ–≤, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π. –î–∏–∞–ø–∞–∑–æ–Ω —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –æ—Ç –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –¥–æ –ø–µ—Ä–∏—Ñ–µ—Ä–∏–π–Ω—ã—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤.

### –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏

–û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–æ–¥–µ–ª–∏ Qwen –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –ø–æ–º–æ—â–∏ –≤ –∏–∑—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤ –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –æ–ø—ã—Ç–æ–≤. –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ Qwen-Math, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.

### –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è

–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤—ã–∏–≥—Ä—ã–≤–∞—é—Ç –æ—Ç —Å–∏–ª—å–Ω—ã—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π Qwen, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –æ–ø—ã—Ç –ò–ò –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –∏ –≤ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö.

## –ü—Ä–æ–±–ª–µ–º—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

### –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

–•–æ—Ç—è Qwen –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤, –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –≤—Å–µ –µ—â–µ —Ç—Ä–µ–±—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —á—Ç–æ –º–æ–∂–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π.

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö

–•–æ—Ç—è –º–æ–¥–µ–ª–∏ Qwen —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç –≤ –æ–±—â–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –≤—ã—Å–æ–∫–æ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –º–æ–≥—É—Ç –≤—ã–∏–≥—Ä–∞—Ç—å –æ—Ç —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.

### –°–ª–æ–∂–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏

–®–∏—Ä–æ–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –º–æ–∂–µ—Ç –∑–∞—Ç—Ä—É–¥–Ω–∏—Ç—å –≤—ã–±–æ—Ä –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –Ω–µ–∑–Ω–∞–∫–æ–º—ã—Ö —Å —ç–∫–æ—Å–∏—Å—Ç–µ–º–æ–π.

### –î–∏—Å–±–∞–ª–∞–Ω—Å —è–∑—ã–∫–æ–≤

–•–æ—Ç—è –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –º–Ω–æ–∂–µ—Å—Ç–≤–æ —è–∑—ã–∫–æ–≤, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–∂–µ—Ç –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —è–∑—ã–∫–∞, —Å –Ω–∞–∏–ª—É—á—à–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –∏ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ.

## –ë—É–¥—É—â–µ–µ —Å–µ–º–µ–π—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π Qwen

–°–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Qwen –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø—Ä–æ–¥–æ–ª–∂–∞—é—â—É—é—Å—è —ç–≤–æ–ª—é—Ü–∏—é –≤ —Å—Ç–æ—Ä–æ–Ω—É –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ò–ò. –ë—É–¥—É—â–∏–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –≤–∫–ª—é—á–∞—é—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, —É–ª—É—á—à–µ–Ω–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ª—É—á—à—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è.

–ü–æ –º–µ—Ä–µ —Ä–∞–∑–≤–∏—Ç–∏—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –º–æ–∂–Ω–æ –æ–∂–∏–¥–∞—Ç—å, —á—Ç–æ –º–æ–¥–µ–ª–∏ Qwen —Å—Ç–∞–Ω—É—Ç –µ—â–µ –±–æ–ª–µ–µ –º–æ—â–Ω—ã–º–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞—Ç—å –ò–ò –≤ —Å–∞–º—ã—Ö —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –∏ —Å–ª—É—á–∞—è—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.

–°–µ–º–µ–π—Å—Ç–≤–æ Qwen –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –±—É–¥—É—â–µ–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ò–ò –º–æ–∂–µ—Ç —Å–æ—á–µ—Ç–∞—Ç—å –ø–µ—Ä–µ–¥–æ–≤—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –æ—Ç–∫—Ä—ã—Ç—É—é –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º –º–æ—â–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ –∏ –∫–æ–Ω—Ç—Ä–æ–ª—è.

## –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

### –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç —Å Transformers

–í–æ—Ç –∫–∞–∫ –Ω–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É —Å –º–æ–¥–µ–ª—è–º–∏ Qwen, –∏—Å–ø–æ–ª—å–∑—É—è –±–∏–±–ª–∏–æ—Ç–µ–∫—É Hugging Face Transformers:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Qwen2.5

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

**–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ —Å Qwen-Coder:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**–†–µ—à–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = x¬≥ + 2x¬≤ - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**–ó–∞–¥–∞—á–∏ —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### –†–µ–∂–∏–º –º—ã—à–ª–µ–Ω–∏—è (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### üì± –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö –∏ –ø–µ—Ä–∏—Ñ–µ—Ä–∏–π–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### –ü—Ä–∏–º–µ—Ä —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ API

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è

–°–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Qwen –¥–æ—Å—Ç–∏–≥–ª–æ –≤—ã–¥–∞—é—â–∏—Ö—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, —Å–æ—Ö—Ä–∞–Ω—è—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞:

### –û—Å–Ω–æ–≤–Ω—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

**–ü—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö:**
- Qwen3-235B-A22B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –æ—Ü–µ–Ω–∫–∞—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é, –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –æ–±—â–∏–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ —Ç–æ–ø–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ DeepSeek-R1, o1, o3-mini, Grok-3 –∏ Gemini-2.5-Pro.
- Qwen3-30B-A3B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç QwQ-32B —Å 10-–∫—Ä–∞—Ç–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
- Qwen3-4B –º–æ–∂–µ—Ç —Å–æ–ø–µ—Ä–Ω–∏—á–∞—Ç—å —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é Qwen2.5-72B-Instruct.

**–î–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏:**
- –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ Qwen3-MoE –¥–æ—Å—Ç–∏–≥–∞—é—Ç –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∫–∞–∫ –∏ –ø–ª–æ—Ç–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ Qwen2.5, –∏—Å–ø–æ–ª—å–∑—É—è –ª–∏—à—å 10% –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
- –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è –∑–∞—Ç—Ä–∞—Ç –∫–∞–∫ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ, —Ç–∞–∫ –∏ –Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–ª–æ—Ç–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.

**–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- –ú–æ–¥–µ–ª–∏ Qwen3 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç 119 —è–∑—ã–∫–æ–≤ –∏ –¥–∏–∞–ª–µ–∫—Ç–æ–≤.
- –í—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –∏ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö.

**–ú–∞—Å—à—Ç–∞–± –æ–±—É—á–µ–Ω–∏—è:**
- Qwen3 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—á—Ç–∏ –≤–¥–≤–æ–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–∏–º–µ—Ä–Ω–æ 36 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 119 —è–∑—ã–∫–æ–≤ –∏ –¥–∏–∞–ª–µ–∫—Ç–æ–≤, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤ —É Qwen2.5.

### –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–æ–¥–µ–ª–µ–π

| –°–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π | –î–∏–∞–ø–∞–∑–æ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ | –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ | –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ | –õ—É—á—à–∏–µ —Å–ª—É—á–∞–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è |
|---------------|---------------------|-----------------|-----------------------|----------------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç—å | –û–±—â–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ | –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ü–û, –ø–æ–º–æ—â—å –≤ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ | –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è STEM |
| **Qwen2.5-VL** | –†–∞–∑–Ω—ã–µ | –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è | –ü–æ–Ω–∏–º–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —è–∑—ã–∫–∞ | –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π |
| **Qwen3** | 0.6B-235B | –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è | –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, —Ä–µ–∂–∏–º –º—ã—à–ª–µ–Ω–∏—è | –°–ª–æ–∂–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è |
| **Qwen3 MoE** | 30B-235B –≤—Å–µ–≥–æ | –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è | –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∫—Ä—É–ø–Ω–æ–º –º–∞—Å—à—Ç–∞–±–µ | –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –∑–∞–¥–∞—á–∏ –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ |

## –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –≤—ã–±–æ—Ä—É –º–æ–¥–µ–ª–∏

### –î–ª—è –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π
- **Qwen2.5-0.5B/1.5B**: –ú–æ–±–∏–ª—å–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –Ω–∞ –ø–µ—Ä–∏—Ñ–µ—Ä–∏–∏, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.
- **Qwen2.5-3B/7B**: –û–±—â–∏–µ —á–∞—Ç-–±–æ—Ç—ã, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞, —Å–∏—Å—Ç–µ–º—ã –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤.

### –î–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
- **Qwen2.5-Math**: –†–µ—à–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ STEM.
- **Qwen3 —Å —Ä–µ–∂–∏–º–æ–º –º—ã—à–ª–µ–Ω–∏—è**: –°–ª–æ–∂–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, —Ç—Ä–µ–±—É—é—â–µ–µ –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.

### –î–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
- **Qwen2.5-Coder**: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞, –æ—Ç–ª–∞–¥–∫–∞, –ø–æ–º–æ—â—å –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏.
- **Qwen3**: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∑–∞–¥–∞—á–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.

### –î–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π
- **Qwen2.5-VL**: –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã.
- **Qwen-Audio**: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–µ—á–∏.

### –î–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è
- **Qwen2.5-32B/72B**: –í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞.
- **Qwen3-235B-A22B**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π.

## –ü–ª–∞—Ç—Ñ–æ—Ä–º—ã —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
### –û–±–ª–∞—á–Ω—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
- **Hugging Face Hub**: –ü–æ–ª–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–æ–æ–±—â–µ—Å—Ç–≤–∞.
- **ModelScope**: –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –º–æ–¥–µ–ª–µ–π Alibaba —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.
- **–†–∞–∑–ª–∏—á–Ω—ã–µ –æ–±–ª–∞—á–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã**: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.

### –õ–æ–∫–∞–ª—å–Ω—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
- **Transformers**: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Hugging Face –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è.
- **vLLM**: –í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥.
- **Ollama**: –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ.
- **ONNX Runtime**: –ö—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω–æ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è.
- **llama.cpp**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ C++ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º.

### –†–µ—Å—É—Ä—Å—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
- **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è Qwen**: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏ –∫–∞—Ä—Ç–æ—á–∫–∏ –º–æ–¥–µ–ª–µ–π.
- **Hugging Face Model Hub**: –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏ –ø—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ—Å—Ç–≤–∞.
- **–ù–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏**: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–∞ arxiv –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è.
- **–§–æ—Ä—É–º—ã —Å–æ–æ–±—â–µ—Å—Ç–≤–∞**: –ê–∫—Ç–∏–≤–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ –∏ –æ–±—Å—É–∂–¥–µ–Ω–∏—è.

### –ù–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏ Qwen

#### –ü–ª–∞—Ç—Ñ–æ—Ä–º—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
1. **Hugging Face Transformers**: –ù–∞—á–Ω–∏—Ç–µ —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –Ω–∞ Python.
2. **ModelScope**: –ò—Å—Å–ª–µ–¥—É–π—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –æ—Ç Alibaba.
3. **–õ–æ–∫–∞–ª—å–Ω–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Ollama –∏–ª–∏ –ø—Ä—è–º—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é Transformers –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.

#### –ü—É—Ç—å –æ–±—É—á–µ–Ω–∏—è
1. **–ü–æ–Ω—è—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏**: –ò–∑—É—á–∏—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–µ–º–µ–π—Å—Ç–≤–∞ Qwen.
2. **–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏**: –ü—Ä–æ–±—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –º–æ–¥–µ–ª–µ–π, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å –∫–æ–º–ø—Ä–æ–º–∏—Å—Å—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
3. **–ü—Ä–∞–∫—Ç–∏–∫—É–π—Ç–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ**: –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–π—Ç–µ –º–æ–¥–µ–ª–∏ –≤ —Å—Ä–µ–¥–∞—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.
4. **–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ**: –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–π—Ç–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.

#### –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏
- **–ù–∞—á–∏–Ω–∞–π—Ç–µ —Å –º–∞–ª–æ–≥–æ**: –ù–∞—á–Ω–∏—Ç–µ —Å –º–µ–Ω—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π (1.5B-7B) –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.
- **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —à–∞–±–ª–æ–Ω—ã —á–∞—Ç–æ–≤**: –ü—Ä–∏–º–µ–Ω—è–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
- **–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–π—Ç–µ —Ä–µ—Å—É—Ä—Å—ã**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.
- **–£—á–∏—Ç—ã–≤–∞–π—Ç–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é**: –í—ã–±–∏—Ä–∞–π—Ç–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏, –∫–æ–≥–¥–∞ —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ.

## –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ü—Ä–∏–º–µ—Ä—ã –¥–æ–æ–±—É—á–µ–Ω–∏—è

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω–∂–µ–Ω–µ—Ä–∏—è –ø–æ–¥—Å–∫–∞–∑–æ–∫

**–î–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**–î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (‰∏≠Êñá)",
        "es": "Spanish (Espa√±ol)",
        "fr": "French (Fran√ßais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (Êó•Êú¨Ë™û)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### üîß –®–∞–±–ª–æ–Ω—ã —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –≤ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ—Ü–µ–Ω–∫–∞

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–°–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Qwen –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –ò–ò, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö. –ë–ª–∞–≥–æ–¥–∞—Ä—è –ø—Ä–∏–≤–µ—Ä–∂–µ–Ω–Ω–æ—Å—Ç–∏ –∫ –æ—Ç–∫—Ä—ã—Ç–æ—Å—Ç–∏, –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º –∏ –≥–∏–±–∫–∏–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è, Qwen –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ—â–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ò–ò –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∏–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π.

### –û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã

**–û—Ç–∫—Ä—ã—Ç–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ**: Qwen –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –∫–æ–Ω–∫—É—Ä–∏—Ä–æ–≤–∞—Ç—å —Å –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–º–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞–º–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å, –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å.

**–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: –î–∏–∞–ø–∞–∑–æ–Ω –æ—Ç 0.5B –¥–æ 235B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω—è—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥ ‚Äî –æ—Ç –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –¥–æ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.

**–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**: –í–∞—Ä–∏–∞–Ω—Ç—ã, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ Qwen-Coder, Qwen-Math –∏ Qwen-VL, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—É –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ–±—â–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —è–∑—ã–∫–∞.

**–ì–ª–æ–±–∞–ª—å–Ω–∞—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å**: –°–∏–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏ –¥–ª—è –±–æ–ª–µ–µ —á–µ–º 119 —è–∑—ã–∫–æ–≤ –¥–µ–ª–∞–µ—Ç Qwen –ø–æ–¥—Ö–æ–¥—è—â–∏–º –¥–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –±–∞–∑.

**–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏**: –≠–≤–æ–ª—é—Ü–∏—è –æ—Ç Qwen 1.0 –¥–æ Qwen3 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è.

### –ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã –Ω–∞ –±—É–¥—É—â–µ–µ

–° —Ä–∞–∑–≤–∏—Ç–∏–µ–º —Å–µ–º–µ–π—Å—Ç–≤–∞ Qwen –º–æ–∂–Ω–æ –æ–∂–∏–¥–∞—Ç—å:

- **–ü–æ–≤—ã—à–µ–Ω–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏**: –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
- **–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π**: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞—É–¥–∏–æ –∏ —Ç–µ–∫—Å—Ç–∞.
- **–£–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è**: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –º—ã—à–ª–µ–Ω–∏—è –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á.
- **–õ—É—á—à–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è**: –£–ª—É—á—à–µ–Ω–Ω—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è.
- **–†–æ—Å—Ç —Å–æ–æ–±—â–µ—Å—Ç–≤–∞**: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –∏ –≤–∫–ª–∞–¥–æ–≤ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞.

### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

–ù–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ç–æ–≥–æ, —Å–æ–∑–¥–∞–µ—Ç–µ –ª–∏ –≤—ã —á–∞—Ç-–±–æ—Ç–∞, —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, —Å–æ–∑–¥–∞–µ—Ç–µ –ø–æ–º–æ—â–Ω–∏–∫–æ–≤ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–ª–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç–µ –Ω–∞–¥ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–º–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏, —Å–µ–º–µ–π—Å—Ç–≤–æ Qwen –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–µ —Ä–µ—à–µ–Ω–∏—è —Å —Å–∏–ª—å–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ –∏ –ø–æ–¥—Ä–æ–±–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π.

–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π, –≤—ã–ø—É—Å–∫–æ–≤ –º–æ–¥–µ–ª–µ–π –∏ –ø–æ–¥—Ä–æ–±–Ω–æ–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ—Å–µ—Ç–∏—Ç–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ Qwen –Ω–∞ Hugging Face –∏ –∏–∑—É—á–∏—Ç–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –æ–±—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ—Å—Ç–≤–∞.

–ë—É–¥—É—â–µ–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ò–ò –ª–µ–∂–∏—Ç –≤ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö, –ø—Ä–æ–∑—Ä–∞—á–Ω—ã—Ö –∏ –º–æ—â–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏—è–º –≤–æ –≤—Å–µ—Ö —Å–µ–∫—Ç–æ—Ä–∞—Ö –∏ –º–∞—Å—à—Ç–∞–±–∞—Ö. –°–µ–º–µ–π—Å—Ç–≤–æ Qwen –≤–æ–ø–ª–æ—â–∞–µ—Ç —ç—Ç–æ –≤–∏–¥–µ–Ω–∏–µ, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º –æ—Å–Ω–æ–≤—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ò–ò.

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- **–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è**: [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è Qwen](https://qwen.readthedocs.io/)
- **–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –º–æ–¥–µ–ª–µ–π**: [–ö–æ–ª–ª–µ–∫—Ü–∏–∏ Qwen –Ω–∞ Hugging Face](https://huggingface.co/collections/Qwen/)
- **–ù–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏**: [–ü—É–±–ª–∏–∫–∞—Ü–∏–∏ Qwen –Ω–∞ arxiv](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **–°–æ–æ–±—â–µ—Å—Ç–≤–æ**: [–û–±—Å—É–∂–¥–µ–Ω–∏—è –∏ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ GitHub](https://github.com/QwenLM/)
- **–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ ModelScope**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è –≤—ã —Å–º–æ–∂–µ—Ç–µ:

1. –û–±—ä—è—Å–Ω–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Å–µ–º–µ–π—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π Qwen –∏ –µ–≥–æ –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ç–∫—Ä—ã—Ç–æ—Å—Ç–∏.
2. –í—ã–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–π –≤–∞—Ä–∏–∞–Ω—Ç Qwen –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Ä–µ—Å—É—Ä—Å–æ–≤.
3. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ Qwen –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏.
4. –ü—Ä–∏–º–µ–Ω—è—Ç—å –º–µ—Ç–æ–¥—ã –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π Qwen.
5. –û—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–æ–º–ø—Ä–æ–º–∏—Å—Å—ã –º–µ–∂–¥—É —Ä–∞–∑–º–µ—Ä–æ–º –º–æ–¥–µ–ª–∏, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –≤ —Ä–∞–º–∫–∞—Ö —Å–µ–º–µ–π—Å—Ç–≤–∞ Qwen.

## –ß—Ç–æ –¥–∞–ª—å—à–µ

- [03: –û—Å–Ω–æ–≤—ã —Å–µ–º–µ–π—Å—Ç–≤–∞ Gemma](03.GemmaFamily.md)

---

**–û—Ç–∫–∞–∑ –æ—Ç –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏**:  
–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –±—ã–ª –ø–µ—Ä–µ–≤–µ–¥–µ–Ω —Å –ø–æ–º–æ—â—å—é —Å–µ—Ä–≤–∏—Å–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ [Co-op Translator](https://github.com/Azure/co-op-translator). –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –Ω–∞—à–∏ —É—Å–∏–ª–∏—è –æ–±–µ—Å–ø–µ—á–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –æ—à–∏–±–∫–∏ –∏–ª–∏ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏. –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –µ–≥–æ —Ä–æ–¥–Ω–æ–º —è–∑—ã–∫–µ —Å–ª–µ–¥—É–µ—Ç —Å—á–∏—Ç–∞—Ç—å –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º. –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —á–µ–ª–æ–≤–µ–∫–æ–º. –ú—ã –Ω–µ –Ω–µ—Å–µ–º –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∑–∞ –ª—é–±—ã–µ –Ω–µ–¥–æ—Ä–∞–∑—É–º–µ–Ω–∏—è –∏–ª–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏, –≤–æ–∑–Ω–∏–∫—à–∏–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞.