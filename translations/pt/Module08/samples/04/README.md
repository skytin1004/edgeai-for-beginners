<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "562ac0eae12d808c9f45fbb77eb5c84f",
  "translation_date": "2025-09-24T11:28:55+00:00",
  "source_file": "Module08/samples/04/README.md",
  "language_code": "pt"
}
-->
# Exemplo 04: AplicaÃ§Ãµes de Chat para ProduÃ§Ã£o com Chainlit

Um exemplo abrangente que demonstra vÃ¡rias abordagens para construir aplicaÃ§Ãµes de chat prontas para produÃ§Ã£o utilizando o Microsoft Foundry Local, com interfaces web modernas, respostas em streaming e tecnologias avanÃ§adas de navegador.

## O que estÃ¡ incluÃ­do

- **ðŸš€ AplicaÃ§Ã£o de Chat Chainlit** (`app.py`): AplicaÃ§Ã£o de chat pronta para produÃ§Ã£o com streaming
- **ðŸŒ DemonstraÃ§Ã£o WebGPU** (`webgpu-demo/`): InferÃªncia de IA baseada no navegador com aceleraÃ§Ã£o de hardware
- **ðŸŽ¨ IntegraÃ§Ã£o Open WebUI** (`open-webui-guide.md`): Interface profissional semelhante ao ChatGPT
- **ðŸ“š Notebook Educacional** (`chainlit_app.ipynb`): Materiais interativos de aprendizagem

## InÃ­cio RÃ¡pido

### 1. AplicaÃ§Ã£o de Chat Chainlit

```cmd
# Navigate to Module08 directory
cd Module08

# Start your model
foundry model run phi-4-mini

# Run Chainlit app (using port 8080 to avoid conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Abre em: `http://localhost:8080`

### 2. DemonstraÃ§Ã£o WebGPU no Navegador

```cmd
# Navigate to WebGPU demo
cd Module08\samples\04\webgpu-demo

# Serve the demo
python -m http.server 5173
```

Abre em: `http://localhost:5173`

### 3. ConfiguraÃ§Ã£o Open WebUI

```cmd
# Run Open WebUI with Docker
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

Abre em: `http://localhost:3000`

## PadrÃµes de Arquitetura

### Matriz de DecisÃ£o Local vs Cloud

| CenÃ¡rio | RecomendaÃ§Ã£o | Motivo |
|---------|--------------|--------|
| **Dados SensÃ­veis** | ðŸ  Local (Foundry) | Os dados nunca saem do dispositivo |
| **RaciocÃ­nio Complexo** | â˜ï¸ Cloud (Azure OpenAI) | Acesso a modelos maiores |
| **Chat em Tempo Real** | ðŸ  Local (Foundry) | Menor latÃªncia, respostas mais rÃ¡pidas |
| **AnÃ¡lise de Documentos** | ðŸ”„ HÃ­brido | Local para extraÃ§Ã£o, cloud para anÃ¡lise |
| **GeraÃ§Ã£o de CÃ³digo** | ðŸ  Local (Foundry) | Privacidade + modelos especializados |
| **Tarefas de Pesquisa** | â˜ï¸ Cloud (Azure OpenAI) | Necessidade de uma base de conhecimento ampla |

### ComparaÃ§Ã£o de Tecnologias

| Tecnologia | Caso de Uso | Vantagens | Desvantagens |
|------------|-------------|-----------|--------------|
| **Chainlit** | Desenvolvedores Python, prototipagem rÃ¡pida | ConfiguraÃ§Ã£o fÃ¡cil, suporte a streaming | Apenas Python |
| **WebGPU** | MÃ¡xima privacidade, cenÃ¡rios offline | Nativo do navegador, sem necessidade de servidor | Tamanho limitado de modelo |
| **Open WebUI** | ImplementaÃ§Ã£o em produÃ§Ã£o, equipas | UI profissional, gestÃ£o de utilizadores | Requer Docker |

## PrÃ©-requisitos

- **Foundry Local**: Instalado e em execuÃ§Ã£o ([Download](https://aka.ms/foundry-local-installer))
- **Python**: 3.10+ com ambiente virtual
- **Modelo**: Pelo menos um carregado (`foundry model run phi-4-mini`)
- **Navegador**: Chrome/Edge com suporte a WebGPU para demonstraÃ§Ãµes
- **Docker**: Para Open WebUI (opcional)

## InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1. ConfiguraÃ§Ã£o do Ambiente Python

```cmd
# Navigate to Module08 directory
cd Module08

# Create and activate virtual environment
py -m venv .venv
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. ConfiguraÃ§Ã£o do Foundry Local

```cmd
# Verify Foundry Local installation
foundry --version

# Start the service
foundry service start

# Load a model
foundry model run phi-4-mini

# Verify model is running
foundry service ps
```

## AplicaÃ§Ãµes de Exemplo

### AplicaÃ§Ã£o de Chat Chainlit

**Funcionalidades:**
- ðŸš€ **Streaming em Tempo Real**: Os tokens aparecem Ã  medida que sÃ£o gerados
- ðŸ›¡ï¸ **GestÃ£o Robusta de Erros**: DegradaÃ§Ã£o e recuperaÃ§Ã£o suaves
- ðŸŽ¨ **UI Moderna**: Interface de chat profissional pronta para uso
- ðŸ”§ **ConfiguraÃ§Ã£o FlexÃ­vel**: VariÃ¡veis de ambiente e deteÃ§Ã£o automÃ¡tica
- ðŸ“± **Design Responsivo**: Funciona em dispositivos desktop e mÃ³veis

**InÃ­cio RÃ¡pido:**
```cmd
# Run with default settings (recommended)
chainlit run samples\04\app.py -w --port 8080

# Use specific model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Manual endpoint configuration
set BASE_URL=http://localhost:51211
set API_KEY=your-api-key
chainlit run samples\04\app.py -w --port 8080
```

### DemonstraÃ§Ã£o WebGPU no Navegador

**Funcionalidades:**
- ðŸŒ **IA Nativa do Navegador**: Sem necessidade de servidor, executa inteiramente no navegador
- âš¡ **AceleraÃ§Ã£o WebGPU**: AceleraÃ§Ã£o de hardware quando disponÃ­vel
- ðŸ”’ **MÃ¡xima Privacidade**: Nenhum dado sai do seu dispositivo
- ðŸŽ¯ **Zero InstalaÃ§Ã£o**: Funciona em qualquer navegador compatÃ­vel
- ðŸ”„ **Fallback Suave**: Reverte para CPU se WebGPU nÃ£o estiver disponÃ­vel

**ExecuÃ§Ã£o:**
```cmd
cd samples\04\webgpu-demo
python -m http.server 5173
# Open http://localhost:5173
```

### IntegraÃ§Ã£o Open WebUI

**Funcionalidades:**
- ðŸŽ¨ **Interface Semelhante ao ChatGPT**: UI profissional e familiar
- ðŸ‘¥ **Suporte Multi-utilizador**: Contas de utilizadores e histÃ³rico de conversas
- ðŸ“ **Processamento de Ficheiros**: Carregar e analisar documentos
- ðŸ”„ **Troca de Modelos**: AlteraÃ§Ã£o fÃ¡cil entre diferentes modelos
- ðŸ³ **ImplementaÃ§Ã£o com Docker**: ConfiguraÃ§Ã£o pronta para produÃ§Ã£o em contÃªineres

**ConfiguraÃ§Ã£o RÃ¡pida:**
```cmd
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

## ReferÃªncia de ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

| VariÃ¡vel | DescriÃ§Ã£o | PadrÃ£o | Exemplo |
|----------|-----------|--------|---------|
| `MODEL` | Alias do modelo a usar | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | Endpoint do Foundry Local | DeteÃ§Ã£o automÃ¡tica | `http://localhost:51211` |
| `API_KEY` | Chave API (opcional para local) | `""` | `your-api-key` |

## ResoluÃ§Ã£o de Problemas

### Problemas Comuns

**AplicaÃ§Ã£o Chainlit:**

1. **ServiÃ§o nÃ£o disponÃ­vel:**
   ```cmd
   # Check Foundry Local status
   foundry service status
   foundry service ps
   
   # Validate API endpoint (note: port 51211)
   curl http://localhost:51211/v1/models
   ```

2. **Conflitos de porta:**
   ```cmd
   # Check what's using port 8080
   netstat -ano | findstr :8080
   
   # Use different port if needed
   chainlit run samples\04\app.py -w --port 3000
   ```

3. **Problemas no ambiente Python:**
   ```cmd
   # Verify correct interpreter in VS Code
   # Ctrl+Shift+P â†’ Python: Select Interpreter
   # Choose: Module08/.venv/Scripts/python.exe
   
   # Reinstall dependencies
   pip install -r requirements.txt
   ```

**DemonstraÃ§Ã£o WebGPU:**

1. **WebGPU nÃ£o suportado:**
   - Atualize para Chrome/Edge 113+
   - Ative WebGPU: `chrome://flags/#enable-unsafe-webgpu`
   - Verifique o estado da GPU: `chrome://gpu`
   - A demonstraÃ§Ã£o reverterÃ¡ automaticamente para CPU

2. **Erros ao carregar modelo:**
   - Certifique-se de que tem conexÃ£o Ã  internet para download do modelo
   - Verifique o console do navegador para erros de CORS
   - Confirme que estÃ¡ a servir via HTTP (nÃ£o file://)

**Open WebUI:**

1. **ConexÃ£o recusada:**
   ```cmd
   # Check Docker is running
   docker --version
   
   # Check container status
   docker ps | findstr open-webui
   
   # View container logs
   docker logs open-webui
   ```

2. **Modelos nÃ£o aparecem:**
   ```cmd
   # Verify Foundry Local endpoint
   curl http://localhost:51211/v1/models
   
   # Restart Open WebUI
   docker restart open-webui
   ```

### Lista de VerificaÃ§Ã£o de ValidaÃ§Ã£o

```cmd
# âœ… 1. Foundry Local Setup
foundry --version                    # Should show version
foundry service status               # Should show "running"
foundry model list                   # Should show loaded models
curl http://localhost:51211/v1/models  # Should return JSON

# âœ… 2. Python Environment  
python --version                     # Should be 3.10+
pip list | findstr chainlit         # Should show chainlit package
pip list | findstr openai           # Should show openai package

# âœ… 3. Application Testing
chainlit run samples\04\app.py -w --port 8080  # Should open browser
# Test WebGPU demo at localhost:5173
# Test Open WebUI at localhost:3000
```

## Uso AvanÃ§ado

### OtimizaÃ§Ã£o de Desempenho

**Chainlit:**
- Use streaming para melhor desempenho percebido
- Implemente pooling de conexÃµes para alta concorrÃªncia
- Cache de respostas de modelos para consultas repetidas
- Monitore o uso de memÃ³ria com histÃ³ricos de conversas grandes

**WebGPU:**
- Use WebGPU para mÃ¡xima privacidade e velocidade
- Implemente quantizaÃ§Ã£o de modelos para modelos menores
- Use Web Workers para processamento em segundo plano
- Cache de modelos compilados no armazenamento do navegador

**Open WebUI:**
- Use volumes persistentes para histÃ³rico de conversas
- Configure limites de recursos para o contÃªiner Docker
- Implemente estratÃ©gias de backup para dados de utilizadores
- Configure proxy reverso para terminaÃ§Ã£o SSL

### PadrÃµes de IntegraÃ§Ã£o

**HÃ­brido Local/Cloud:**
```python
# Route based on complexity and privacy requirements
async def intelligent_routing(prompt: str, metadata: dict):
    if metadata.get("contains_pii"):
        return await foundry_local_completion(prompt)  # Privacy-sensitive
    elif len(prompt.split()) > 200:
        return await azure_openai_completion(prompt)   # Complex reasoning
    else:
        return await foundry_local_completion(prompt)  # Default local
```

**Pipeline Multi-modal:**
```python
# Combine different AI capabilities
async def analyze_document(file_path: str):
    # 1. OCR with WebGPU (browser-based)
    text = await webgpu_ocr(file_path)
    
    # 2. Analysis with Foundry Local (private)
    summary = await foundry_local_analyze(text)
    
    # 3. Enhancement with cloud (if needed)
    if summary.confidence < 0.8:
        summary = await azure_openai_enhance(summary)
    
    return summary
```

## ImplementaÃ§Ã£o em ProduÃ§Ã£o

### ConsideraÃ§Ãµes de SeguranÃ§a

- **Chaves API**: Use variÃ¡veis de ambiente, nunca codifique diretamente
- **Rede**: Use HTTPS em produÃ§Ã£o, considere VPN para acesso da equipa
- **Controlo de Acesso**: Implemente autenticaÃ§Ã£o para Open WebUI
- **Privacidade de Dados**: Audite quais dados permanecem locais vs. enviados para a cloud
- **AtualizaÃ§Ãµes**: Mantenha Foundry Local e contÃªineres atualizados

### MonitorizaÃ§Ã£o e ManutenÃ§Ã£o

- **VerificaÃ§Ãµes de SaÃºde**: Implemente monitorizaÃ§Ã£o de endpoints
- **Registos**: Centralize os registos de todos os componentes
- **MÃ©tricas**: Acompanhe tempos de resposta, taxas de erro, uso de recursos
- **Backup**: Backup regular dos dados de conversas e configuraÃ§Ãµes

## ReferÃªncias e Recursos

### DocumentaÃ§Ã£o
- [DocumentaÃ§Ã£o Chainlit](https://docs.chainlit.io/) - Guia completo do framework
- [DocumentaÃ§Ã£o Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/) - DocumentaÃ§Ã£o oficial da Microsoft
- [ONNX Runtime Web](https://onnxruntime.ai/docs/get-started/with-javascript/web.html) - IntegraÃ§Ã£o WebGPU
- [DocumentaÃ§Ã£o Open WebUI](https://docs.openwebui.com/) - ConfiguraÃ§Ã£o avanÃ§ada

### Ficheiros de Exemplo
- [`app.py`](../../../../../Module08/samples/04/app.py) - AplicaÃ§Ã£o Chainlit para produÃ§Ã£o
- [`chainlit_app.ipynb`](./chainlit_app.ipynb) - Notebook educacional
- [`webgpu-demo/`](../../../../../Module08/samples/04/webgpu-demo) - InferÃªncia de IA baseada no navegador
- [`open-webui-guide.md`](./open-webui-guide.md) - ConfiguraÃ§Ã£o completa do Open WebUI

### Exemplos Relacionados
- [DocumentaÃ§Ã£o da SessÃ£o 4](../../04.CuttingEdgeModels.md) - Guia completo da sessÃ£o
- [Exemplos Foundry Local](https://github.com/microsoft/foundry-local/tree/main/samples) - Exemplos oficiais

---

