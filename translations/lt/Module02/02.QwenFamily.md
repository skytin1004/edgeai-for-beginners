<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-01T22:04:38+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "lt"
}
-->
# 2 skyrius: Qwen Å¡eimos pagrindai

Qwen modeliÅ³ Å¡eima atspindi â€Alibaba Cloudâ€œ visapusiÅ¡kÄ… poÅ¾iÅ«rÄ¯ Ä¯ didelius kalbos modelius ir multimodalinÄ™ AI, Ä¯rodydama, kad atvirojo kodo modeliai gali pasiekti puikiÅ³ rezultatÅ³ ir bÅ«ti prieinami Ä¯vairiuose diegimo scenarijuose. Svarbu suprasti, kaip Qwen Å¡eima suteikia galingas AI galimybes su lanksÄiais diegimo pasirinkimais, iÅ¡laikant konkurencingÄ… naÅ¡umÄ… Ä¯vairiose uÅ¾duotyse.

## IÅ¡tekliai kÅ«rÄ—jams

### Hugging Face modeliÅ³ saugykla
Pasirinkti Qwen Å¡eimos modeliai yra prieinami per [Hugging Face](https://huggingface.co/models?search=qwen), suteikiant galimybÄ™ naudotis kai kuriais Å¡iÅ³ modeliÅ³ variantais. Galite tyrinÄ—ti galimus variantus, pritaikyti juos savo specifiniams poreikiams ir diegti per Ä¯vairias sistemas.

### VietinÄ—s kÅ«rimo priemonÄ—s
Vietiniam kÅ«rimui ir testavimui galite naudoti [Microsoft Foundry Local](https://github.com/microsoft/foundry-local), kad paleistumÄ—te prieinamus Qwen modelius savo kÅ«rimo kompiuteryje su optimizuotu naÅ¡umu.

### Dokumentacijos iÅ¡tekliai
- [Qwen modeliÅ³ dokumentacija](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Qwen modeliÅ³ optimizavimas kraÅ¡tiniam diegimui](https://github.com/microsoft/olive)

## Ä®vadas

Å iame vadove mes nagrinÄ—sime â€Alibabaâ€œ Qwen modeliÅ³ Å¡eimÄ… ir jos pagrindines sÄ…vokas. Aptarsime Qwen Å¡eimos evoliucijÄ…, novatoriÅ¡kus mokymo metodus, kurie daro Qwen modelius efektyviais, pagrindinius Å¡eimos variantus ir praktinius pritaikymus Ä¯vairiuose scenarijuose.

## Mokymosi tikslai

Å io vadovo pabaigoje galÄ—site:

- Suprasti â€Alibabaâ€œ Qwen modeliÅ³ Å¡eimos dizaino filosofijÄ… ir evoliucijÄ…
- Identifikuoti pagrindines inovacijas, leidÅ¾ianÄias Qwen modeliams pasiekti aukÅ¡tÄ… naÅ¡umÄ… Ä¯vairiÅ³ parametrÅ³ dydÅ¾iÅ³ modeliuose
- AtpaÅ¾inti skirtingÅ³ Qwen modeliÅ³ variantÅ³ privalumus ir trÅ«kumus
- Taikyti Å¾inias apie Qwen modelius, kad pasirinktumÄ—te tinkamus variantus realiems scenarijams

## Å iuolaikinio AI modeliÅ³ kraÅ¡tovaizdÅ¾io supratimas

AI kraÅ¡tovaizdis labai pasikeitÄ—, kai skirtingos organizacijos siekia Ä¯vairiÅ³ poÅ¾iÅ«riÅ³ Ä¯ kalbos modeliÅ³ kÅ«rimÄ…. Kai kurios organizacijos orientuojasi Ä¯ nuosavybinius uÅ¾darus modelius, o kitos pabrÄ—Å¾ia atvirojo kodo prieinamumÄ… ir skaidrumÄ…. Tradicinis poÅ¾iÅ«ris apima arba didÅ¾iulius nuosavybinius modelius, prieinamus tik per API, arba atvirojo kodo modelius, kurie gali atsilikti savo galimybÄ—mis.

Å i paradigma sukuria iÅ¡Å¡Å«kius organizacijoms, siekianÄioms galingÅ³ AI galimybiÅ³, iÅ¡laikant kontrolÄ™ savo duomenims, iÅ¡laidoms ir diegimo lankstumui. Tradicinis poÅ¾iÅ«ris daÅ¾nai reikalauja rinktis tarp paÅ¾angiausio naÅ¡umo ir praktiniÅ³ diegimo aspektÅ³.

## AukÅ¡tos kokybÄ—s ir prieinamos AI poreikis

AukÅ¡tos kokybÄ—s ir prieinamos AI poreikis tampa vis svarbesnis Ä¯vairiuose scenarijuose. Apsvarstykite programas, kurioms reikalingos lanksÄios diegimo galimybÄ—s skirtingiems organizaciniams poreikiams, ekonomiÅ¡kos Ä¯gyvendinimo galimybÄ—s, kur API iÅ¡laidos gali tapti reikÅ¡mingos, daugiakalbÄ—s galimybÄ—s globalioms programoms arba specializuotos srities Å¾inios, tokios kaip kodavimas ir matematika.

### Pagrindiniai diegimo reikalavimai

Å iuolaikiniai AI diegimai susiduria su keliais pagrindiniais reikalavimais, kurie riboja praktinÄ¯ pritaikymÄ…:

- **Prieinamumas**: Atvirojo kodo prieinamumas skaidrumui ir pritaikymui
- **EkonomiÅ¡kumas**: Protingi skaiÄiavimo reikalavimai Ä¯vairiems biudÅ¾etams
- **Lankstumas**: Ä®vairÅ«s modeliÅ³ dydÅ¾iai skirtingiems diegimo scenarijams
- **Globalus pasiekiamumas**: Stiprios daugiakalbÄ—s ir tarpkultÅ«rinÄ—s galimybÄ—s
- **Specializacija**: Srities specifiniai variantai tam tikriems naudojimo atvejams

## Qwen modeliÅ³ filosofija

Qwen modeliÅ³ Å¡eima atspindi visapusiÅ¡kÄ… poÅ¾iÅ«rÄ¯ Ä¯ AI modeliÅ³ kÅ«rimÄ…, pirmenybÄ™ teikiant atvirojo kodo prieinamumui, daugiakalbÄ—ms galimybÄ—ms ir praktiniam diegimui, iÅ¡laikant konkurencingas naÅ¡umo charakteristikas. Qwen modeliai tai pasiekia per Ä¯vairius modeliÅ³ dydÅ¾ius, aukÅ¡tos kokybÄ—s mokymo metodikas ir specializuotus variantus skirtingoms sritims.

Qwen Å¡eima apima Ä¯vairius poÅ¾iÅ«rius, skirtus suteikti pasirinkimus naÅ¡umo ir efektyvumo spektre, leidÅ¾iant diegimÄ… nuo mobiliÅ³jÅ³ Ä¯renginiÅ³ iki Ä¯moniÅ³ serveriÅ³, tuo paÄiu uÅ¾tikrinant prasmingas AI galimybes. Tikslas yra demokratizuoti prieigÄ… prie aukÅ¡tos kokybÄ—s AI, suteikiant lankstumÄ… diegimo pasirinkimuose.

### Pagrindiniai Qwen dizaino principai

Qwen modeliai yra sukurti remiantis keliais pagrindiniais principais, kurie juos iÅ¡skiria iÅ¡ kitÅ³ kalbos modeliÅ³ Å¡eimÅ³:

- **Atvirojo kodo prioritetas**: VisiÅ¡kas skaidrumas ir prieinamumas tyrimams ir komerciniam naudojimui
- **VisapusiÅ¡kas mokymas**: Mokymas naudojant didÅ¾iulius, Ä¯vairius duomenÅ³ rinkinius, apimanÄius kelias kalbas ir sritis
- **Mastelio architektÅ«ra**: Ä®vairÅ«s modeliÅ³ dydÅ¾iai, atitinkantys skirtingus skaiÄiavimo reikalavimus
- **Specializuotas meistriÅ¡kumas**: Srities specifiniai variantai, optimizuoti tam tikroms uÅ¾duotims

## PagrindinÄ—s technologijos, leidÅ¾ianÄios Qwen Å¡eimai

### Didelio masto mokymas

Vienas iÅ¡ Qwen Å¡eimos apibrÄ—Å¾ianÄiÅ³ aspektÅ³ yra didelio masto mokymo duomenys ir skaiÄiavimo iÅ¡tekliai, investuoti Ä¯ modeliÅ³ kÅ«rimÄ…. Qwen modeliai naudoja kruopÅ¡Äiai atrinktus, daugiakalbius duomenÅ³ rinkinius, apimanÄius trilijonus Å¾etonÅ³, skirtus suteikti visapusiÅ¡kas pasaulio Å¾inias ir loginio mÄ…stymo galimybes.

Å is poÅ¾iÅ«ris veikia derinant aukÅ¡tos kokybÄ—s interneto turinÄ¯, akademinÄ™ literatÅ«rÄ…, kodÅ³ saugyklas ir daugiakalbius iÅ¡teklius. Mokymo metodika pabrÄ—Å¾ia tiek Å¾iniÅ³ platumÄ…, tiek supratimo gylÄ¯ Ä¯vairiose srityse ir kalbose.

### PaÅ¾angus loginis mÄ…stymas

Naujausi Qwen modeliai Ä¯traukia sudÄ—tingas loginio mÄ…stymo galimybes, leidÅ¾ianÄias sprÄ™sti sudÄ—tingas daugiapakopes problemas:

**MÄ…stymo reÅ¾imas (Qwen3)**: Modeliai gali Ä¯sitraukti Ä¯ detaliÄ… Å¾ingsnis po Å¾ingsnio logikÄ… prieÅ¡ pateikdami galutinius atsakymus, panaÅ¡iai kaip Å¾mogaus problemÅ³ sprendimo metodai.

**DviejÅ³ reÅ¾imÅ³ veikimas**: GalimybÄ— perjungti greito atsako reÅ¾imÄ… paprastiems uÅ¾klausoms ir gilesnio mÄ…stymo reÅ¾imÄ… sudÄ—tingoms problemoms.

**MÄ…stymo grandinÄ—s integracija**: NatÅ«rali logikos Å¾ingsniÅ³ integracija, gerinanti skaidrumÄ… ir tikslumÄ… sudÄ—tingose uÅ¾duotyse.

### ArchitektÅ«rinÄ—s inovacijos

Qwen Å¡eima Ä¯traukia kelias architektÅ«rines optimizacijas, skirtas tiek naÅ¡umui, tiek efektyvumui:

**Mastelio dizainas**: Nuosekli architektÅ«ra visÅ³ modeliÅ³ dydÅ¾iams, leidÅ¾ianti lengvai plÄ—sti ir palyginti.

**MultimodalinÄ— integracija**: Sklandi teksto, vaizdo ir garso apdorojimo galimybiÅ³ integracija vieningose architektÅ«rose.

**Diegimo optimizacija**: Ä®vairios kvantavimo parinktys ir diegimo formatai skirtingoms aparatÅ«ros konfigÅ«racijoms.

## ModeliÅ³ dydÅ¾iai ir diegimo galimybÄ—s

Å iuolaikinÄ—s diegimo aplinkos naudoja Qwen modeliÅ³ lankstumÄ… Ä¯vairiems skaiÄiavimo reikalavimams:

### MaÅ¾i modeliai (0.5B-3B)

Qwen siÅ«lo efektyvius maÅ¾us modelius, tinkamus kraÅ¡tiniam diegimui, mobiliosioms programoms ir ribotÅ³ iÅ¡tekliÅ³ aplinkoms, iÅ¡laikant Ä¯spÅ«dingas galimybes.

### Vidutiniai modeliai (7B-32B)

Vidutinio dydÅ¾io modeliai siÅ«lo patobulintas galimybes profesionalioms programoms, suteikiant puikÅ³ naÅ¡umo ir skaiÄiavimo reikalavimÅ³ balansÄ….

### Dideli modeliai (72B+)

Pilno masto modeliai uÅ¾tikrina paÅ¾angiausiÄ… naÅ¡umÄ… reiklioms programoms, tyrimams ir Ä¯moniÅ³ diegimams, kuriems reikalingos maksimalios galimybÄ—s.

## Qwen modeliÅ³ Å¡eimos privalumai

### Atvirojo kodo prieinamumas

Qwen modeliai suteikia visiÅ¡kÄ… skaidrumÄ… ir pritaikymo galimybes, leidÅ¾iant organizacijoms suprasti, modifikuoti ir pritaikyti modelius savo specifiniams poreikiams be priklausomybÄ—s nuo tiekÄ—jo.

### Diegimo lankstumas

ModeliÅ³ dydÅ¾iÅ³ Ä¯vairovÄ— leidÅ¾ia diegti Ä¯vairiose aparatÅ«ros konfigÅ«racijose, nuo mobiliÅ³jÅ³ Ä¯renginiÅ³ iki aukÅ¡tos klasÄ—s serveriÅ³, suteikiant organizacijoms lankstumÄ… jÅ³ AI infrastruktÅ«ros pasirinkimuose.

### DaugiakalbÄ—s galimybÄ—s

Qwen modeliai pasiÅ¾ymi puikiu daugiakalbiu supratimu ir generavimu, palaikydami deÅ¡imtis kalbÅ³, ypaÄ stipriai anglÅ³ ir kinÅ³ kalbomis, todÄ—l jie tinkami globalioms programoms.

### Konkurencingas naÅ¡umas

Qwen modeliai nuolat pasiekia konkurencingus rezultatus testuose, tuo paÄiu uÅ¾tikrindami atvirojo kodo prieinamumÄ…, Ä¯rodydami, kad atviri modeliai gali prilygti nuosavybiniams alternatyvoms.

### Specializuotos galimybÄ—s

Srities specifiniai variantai, tokie kaip Qwen-Coder ir Qwen-Math, suteikia specializuotÄ… ekspertizÄ™, iÅ¡laikant bendrÄ… kalbos supratimo galimybÄ™.

## Praktiniai pavyzdÅ¾iai ir naudojimo atvejai

PrieÅ¡ gilindamiesi Ä¯ technines detales, paÅ¾velkime Ä¯ keletÄ… konkreÄiÅ³ pavyzdÅ¾iÅ³, kÄ… Qwen modeliai gali pasiekti:

### Matematinio loginio mÄ…stymo pavyzdys

Qwen-Math puikiai sprendÅ¾ia Å¾ingsnis po Å¾ingsnio matematines problemas. PavyzdÅ¾iui, papraÅ¡ius iÅ¡sprÄ™sti sudÄ—tingÄ… integralÅ³ problemÄ…:

```
User: Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = xÂ³ + 2xÂ² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹

Breaking down each term:
- d/dx(xÂ³) = 3xÂ²
- d/dx(2xÂ²) = 2 Â· 2xÂ¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3xÂ² + 4x - 5
```

### DaugiakalbÄ—s palaikymo pavyzdys

Qwen modeliai demonstruoja stiprias daugiakalbes galimybes Ä¯vairiomis kalbomis:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**ä¸­æ–‡:**
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡æœºå™¨ï¼ˆç‰¹åˆ«æ˜¯è®¡ç®—æœºç³»ç»Ÿï¼‰å¯¹äººç±»æ™ºèƒ½è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚è¿™äº›è¿‡ç¨‹åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€è§£å†³é—®é¢˜ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚äººå·¥æ™ºèƒ½ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»è®¤çŸ¥èƒ½åŠ›çš„ä»»åŠ¡ã€‚
```

### MultimodalinÄ—s galimybÄ—s pavyzdys

Qwen-VL gali vienu metu apdoroti tekstÄ… ir vaizdus:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Kodo generavimo pavyzdys

Qwen-Coder puikiai generuoja ir paaiÅ¡kina kodÄ… Ä¯vairiomis programavimo kalbomis:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

Å i implementacija laikosi geriausios praktikos su aiÅ¡kiais kintamÅ³jÅ³ pavadinimais, iÅ¡samia dokumentacija ir efektyvia logika.
```

### Edge Deployment Example

Qwen models can be deployed on various edge devices with optimized configurations:

```
# Pavyzdinis diegimas mobiliajame Ä¯renginyje su kvantavimu
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Ä®kelkite kvantuotÄ… modelÄ¯ mobiliajam diegimui

```
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Qwen Å¡eimos evoliucija

### Qwen 1.0 ir 1.5: Pagrindiniai modeliai

Ankstyvieji Qwen modeliai nustatÄ— pagrindinius visapusiÅ¡ko mokymo ir atvirojo kodo prieinamumo principus:

- **Qwen-7B (7B parametrai)**: Pradinis leidimas, orientuotas Ä¯ kinÅ³ ir anglÅ³ kalbÅ³ supratimÄ…
- **Qwen-14B (14B parametrai)**: Patobulintos galimybÄ—s su geresniu loginiu mÄ…stymu ir Å¾iniomis
- **Qwen-72B (72B parametrai)**: Didelio masto modelis, uÅ¾tikrinantis paÅ¾angiausiÄ… naÅ¡umÄ…
- **Qwen1.5 serija**: IÅ¡plÄ—sta iki keliÅ³ dydÅ¾iÅ³ (0.5B iki 110B) su patobulintu ilgo konteksto apdorojimu

### Qwen2 Å¡eima: MultimodalinÄ— plÄ—tra

Qwen2 serija paÅ¾ymÄ—jo reikÅ¡mingÄ… paÅ¾angÄ… tiek kalbos, tiek multimodalinÄ—se galimybÄ—se:

- **Qwen2-0.5B iki 72B**: VisapusiÅ¡kas kalbos modeliÅ³ spektras Ä¯vairiems diegimo poreikiams
- **Qwen2-57B-A14B (MoE)**: EkspertÅ³ miÅ¡inio architektÅ«ra efektyviam parametrÅ³ naudojimui
- **Qwen2-VL**: PaÅ¾angios vaizdo ir kalbos galimybÄ—s vaizdÅ³ supratimui
- **Qwen2-Audio**: Garso apdorojimo ir supratimo galimybÄ—s
- **Qwen2-Math**: Specializuotas matematinis loginis mÄ…stymas ir problemÅ³ sprendimas

### Qwen2.5 Å¡eima: Patobulintas naÅ¡umas

Qwen2.5 serija atneÅ¡Ä— reikÅ¡mingus patobulinimus visose dimensijose:

- **IÅ¡plÄ—stas mokymas**: 18 trilijonÅ³ Å¾etonÅ³ mokymo duomenÅ³, siekiant pagerinti galimybes
- **IÅ¡plÄ—stas kontekstas**: Iki 128K Å¾etonÅ³ konteksto ilgis, Turbo variantas palaiko 1M Å¾etonÅ³
- **Patobulinta specializacija**: Patobulinti Qwen2.5-Coder ir Qwen2.5-Math variantai
- **Geresnis daugiakalbis palaikymas**: Patobulintas naÅ¡umas 27+ kalbose

### Qwen3 Å¡eima: PaÅ¾angus loginis mÄ…stymas

Naujausia karta stumia loginio mÄ…stymo ir mÄ…stymo galimybiÅ³ ribas:

- **Qwen3-235B-A22B**: Flagmanas ekspertÅ³ miÅ¡inio modelis su 235B bendrÅ³ parametrÅ³
- **Qwen3-30B-A3B**: Efektyvus MoE modelis su stipriu naÅ¡umu per aktyvÅ³ parametrÄ…
- **TankÅ«s modeliai**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B Ä¯vairiems diegimo scenarijams
- **MÄ…stymo reÅ¾imas**: Hibridinis loginio mÄ…stymo poÅ¾iÅ«ris, palaikantis tiek greitus atsakymus, tiek gilÅ³ mÄ…stymÄ…
- **DaugiakalbÄ—s galimybÄ—s**: Palaikymas 119 kalbÅ³ ir dialektÅ³
- **Patobulintas mokymas**: 36 trilijonai Å¾etonÅ³ Ä¯vairiÅ³, aukÅ¡tos kokybÄ—s mokymo duomenÅ³

## Qwen modeliÅ³ pritaikymas

### Ä®moniÅ³ programos

Organizacijos naudoja Qwen modelius dokumentÅ³ analizei, klientÅ³ aptarnavimo automatizavimui, kodo generavimo pagalbai ir verslo intelekto programoms. Atvirojo kodo pobÅ«dis leidÅ¾ia pritaikyti specifiniams verslo poreikiams, iÅ¡laikant duomenÅ³ privatumÄ… ir kontrolÄ™.

### Mobilieji ir kraÅ¡tiniai skaiÄiavimai

Mobiliosios programos naudoja Qwen modelius realaus laiko vertimui, intelektualiems asistentams, turinio generavimui ir personalizuotoms rekomendacijoms. ModeliÅ³ dydÅ¾iÅ³ Ä¯vairovÄ— leidÅ¾ia diegti nuo mobiliÅ³jÅ³ Ä¯renginiÅ³ iki kraÅ¡tiniÅ³ serveriÅ³.

### Å vietimo technolog
- Qwen3-235B-A22B pasiekia konkurencingus rezultatus vertinant kodavimo, matematikos ir bendrÅ³jÅ³ gebÄ—jimÅ³ testus, lyginant su kitais aukÅ¡Äiausios klasÄ—s modeliais, tokiais kaip DeepSeek-R1, o1, o3-mini, Grok-3 ir Gemini-2.5-Pro.
- Qwen3-30B-A3B pranoksta QwQ-32B, turÄ—damas 10 kartÅ³ daugiau aktyvuotÅ³ parametrÅ³.
- Qwen3-4B gali prilygti Qwen2.5-72B-Instruct veikimui.

**Efektyvumo pasiekimai:**
- Qwen3-MoE baziniai modeliai pasiekia panaÅ¡Å³ veikimÄ… kaip Qwen2.5 tankÅ«s baziniai modeliai, naudodami tik 10% aktyviÅ³ parametrÅ³.
- ReikÅ¡mingas kaÅ¡tÅ³ taupymas tiek mokymui, tiek inferencijai, lyginant su tankiais modeliais.

**Daugiakalbiai gebÄ—jimai:**
- Qwen3 modeliai palaiko 119 kalbÅ³ ir dialektÅ³.
- Stiprus veikimas Ä¯vairiuose lingvistiniuose ir kultÅ«riniuose kontekstuose.

**Mokymo mastas:**
- Qwen3 naudoja beveik dvigubai daugiau duomenÅ³ â€“ apie 36 trilijonus Å¾etonÅ³, apimanÄiÅ³ 119 kalbÅ³ ir dialektÅ³, palyginti su Qwen2.5, kuris naudoja 18 trilijonÅ³ Å¾etonÅ³.

### ModeliÅ³ palyginimo matrica

| ModeliÅ³ serija | ParametrÅ³ diapazonas | Konteksto ilgis | Pagrindiniai privalumai | Geriausi naudojimo atvejai |
|----------------|----------------------|-----------------|-------------------------|----------------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | Subalansuotas veikimas, daugiakalbiai gebÄ—jimai | Bendrosios aplikacijos, diegimas gamyboje |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | Kodo generavimas, programavimas | PrograminÄ—s Ä¯rangos kÅ«rimas, pagalba koduojant |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | Matematinis samprotavimas | Å vietimo platformos, STEM aplikacijos |
| **Qwen2.5-VL** | Ä®vairÅ«s | Kintamas | Vaizdo ir kalbos supratimas | MultimodalinÄ—s aplikacijos, vaizdÅ³ analizÄ— |
| **Qwen3** | 0.6B-235B | Kintamas | PaÅ¾angus samprotavimas, mÄ…stymo reÅ¾imas | SudÄ—tingas samprotavimas, moksliniai tyrimai |
| **Qwen3 MoE** | 30B-235B iÅ¡ viso | Kintamas | Efektyvus didelio masto veikimas | Ä®moniÅ³ aplikacijos, aukÅ¡tos kokybÄ—s poreikiai |

## Modelio pasirinkimo vadovas

### PagrindinÄ—ms aplikacijoms
- **Qwen2.5-0.5B/1.5B**: Mobiliosios aplikacijos, kraÅ¡tiniai Ä¯renginiai, realaus laiko aplikacijos.
- **Qwen2.5-3B/7B**: Bendrieji pokalbiÅ³ robotai, turinio generavimas, klausimÅ³-atsakymÅ³ sistemos.

### MatematinÄ—ms ir samprotavimo uÅ¾duotims
- **Qwen2.5-Math**: MatematinÄ—s problemos sprendimas ir STEM Å¡vietimas.
- **Qwen3 su mÄ…stymo reÅ¾imu**: SudÄ—tingas samprotavimas, reikalaujantis Å¾ingsnis po Å¾ingsnio analizÄ—s.

### Programavimui ir kÅ«rimui
- **Qwen2.5-Coder**: Kodo generavimas, klaidÅ³ taisymas, pagalba programavime.
- **Qwen3**: PaÅ¾angios programavimo uÅ¾duotys su samprotavimo gebÄ—jimais.

### MultimodalinÄ—ms aplikacijoms
- **Qwen2.5-VL**: VaizdÅ³ supratimas, vizualiniÅ³ klausimÅ³ atsakymai.
- **Qwen-Audio**: Garso apdorojimas ir kalbos supratimas.

### Ä®moniÅ³ diegimui
- **Qwen2.5-32B/72B**: AukÅ¡tos kokybÄ—s kalbos supratimas.
- **Qwen3-235B-A22B**: MaksimalÅ«s gebÄ—jimai reiklioms aplikacijoms.

## Diegimo platformos ir prieinamumas

### DebesÅ³ platformos
- **Hugging Face Hub**: IÅ¡samus modeliÅ³ saugykla su bendruomenÄ—s palaikymu.
- **ModelScope**: Alibaba modeliÅ³ platforma su optimizavimo Ä¯rankiais.
- **Ä®vairÅ«s debesÅ³ tiekÄ—jai**: Palaikymas per standartines ML platformas.

### VietinÄ—s kÅ«rimo sistemos
- **Transformers**: StandartinÄ— Hugging Face integracija lengvam diegimui.
- **vLLM**: AukÅ¡tos kokybÄ—s aptarnavimas gamybos aplinkoms.
- **Ollama**: Supaprastintas vietinis diegimas ir valdymas.
- **ONNX Runtime**: KryÅ¾minÄ—s platformos optimizacija Ä¯vairiai aparatÅ«rai.
- **llama.cpp**: Efektyvi C++ implementacija Ä¯vairioms platformoms.

### Mokymosi iÅ¡tekliai
- **Qwen dokumentacija**: OficialÅ«s dokumentai ir modeliÅ³ kortelÄ—s.
- **Hugging Face Model Hub**: InteraktyvÅ«s demonstraciniai pavyzdÅ¾iai ir bendruomenÄ—s pavyzdÅ¾iai.
- **Moksliniai straipsniai**: Techniniai straipsniai arxiv platformoje iÅ¡samiam supratimui.
- **BendruomenÄ—s forumai**: Aktyvus bendruomenÄ—s palaikymas ir diskusijos.

### PradÅ¾ia su Qwen modeliais

#### KÅ«rimo platformos
1. **Hugging Face Transformers**: PradÄ—kite nuo standartinÄ—s Python integracijos.
2. **ModelScope**: IÅ¡bandykite Alibaba optimizuotus diegimo Ä¯rankius.
3. **Vietinis diegimas**: Naudokite Ollama arba tiesioginius transformers vietiniam testavimui.

#### Mokymosi kelias
1. **Supraskite pagrindines sÄ…vokas**: IÅ¡studijuokite Qwen Å¡eimos architektÅ«rÄ… ir gebÄ—jimus.
2. **Eksperimentuokite su variantais**: IÅ¡bandykite skirtingus modeliÅ³ dydÅ¾ius, kad suprastumÄ—te veikimo kompromisus.
3. **Praktikuokite Ä¯gyvendinimÄ…**: Diekite modelius kÅ«rimo aplinkose.
4. **Optimizuokite diegimÄ…**: Derinkite gamybos naudojimo atvejams.

#### Geriausios praktikos
- **PradÄ—kite nuo maÅ¾Å³ modeliÅ³**: PradÄ—kite nuo maÅ¾esniÅ³ modeliÅ³ (1.5B-7B) pradiniam kÅ«rimui.
- **Naudokite pokalbiÅ³ Å¡ablonus**: Taikykite tinkamÄ… formatavimÄ… optimaliems rezultatams.
- **StebÄ—kite iÅ¡teklius**: Sekite atminties naudojimÄ… ir inferencijos greitÄ¯.
- **Apsvarstykite specializacijÄ…**: Pasirinkite specifinius variantus pagal sritÄ¯, kai tai tinkama.

## PaÅ¾angÅ«s naudojimo modeliai

### Fine-tuning pavyzdÅ¾iai

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### Specializuotas uÅ¾klausÅ³ kÅ«rimas

**SudÄ—tingoms samprotavimo uÅ¾duotims:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**Kodo generavimui su kontekstu:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### DaugiakalbÄ—s aplikacijos

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ä¸­æ–‡)",
        "es": "Spanish (EspaÃ±ol)",
        "fr": "French (FranÃ§ais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (æ—¥æœ¬èª)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ğŸ”§ Gamybos diegimo modeliai

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## Veikimo optimizavimo strategijos

### Atminties optimizavimas

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### Inferencijos optimizavimas

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## Geriausios praktikos ir gairÄ—s

### Saugumas ir privatumas

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### StebÄ—jimas ir vertinimas

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## IÅ¡vada

Qwen modeliÅ³ Å¡eima atspindi iÅ¡samÅ³ poÅ¾iÅ«rÄ¯ Ä¯ AI technologijÅ³ demokratizavimÄ…, iÅ¡laikant konkurencingÄ… veikimÄ… Ä¯vairiose aplikacijose. Per savo Ä¯sipareigojimÄ… atviram kodui, daugiakalbiams gebÄ—jimams ir lanksÄioms diegimo galimybÄ—ms, Qwen suteikia organizacijoms ir kÅ«rÄ—jams galimybÄ™ pasinaudoti galingais AI gebÄ—jimais, nepriklausomai nuo jÅ³ iÅ¡tekliÅ³ ar specifiniÅ³ poreikiÅ³.

### PagrindinÄ—s iÅ¡vados

**Atviro kodo pranaÅ¡umas**: Qwen parodo, kad atviro kodo modeliai gali pasiekti veikimÄ…, konkurencingÄ… su patentuotais alternatyvomis, tuo paÄiu uÅ¾tikrinant skaidrumÄ…, pritaikymÄ… ir kontrolÄ™.

**Mastelio architektÅ«ra**: ParametrÅ³ diapazonas nuo 0.5B iki 235B leidÅ¾ia diegimÄ… visame kompiuteriniÅ³ aplinkÅ³ spektre â€“ nuo mobiliÅ³jÅ³ Ä¯renginiÅ³ iki Ä¯moniÅ³ klasteriÅ³.

**Specializuoti gebÄ—jimai**: Specifiniai variantai, tokie kaip Qwen-Coder, Qwen-Math ir Qwen-VL, suteikia specializuotÄ… ekspertizÄ™, iÅ¡laikant bendrÄ… kalbos supratimÄ….

**Pasaulinis prieinamumas**: Stiprus daugiakalbis palaikymas daugiau nei 119 kalbÅ³ daro Qwen tinkamÄ… tarptautinÄ—ms aplikacijoms ir Ä¯vairioms vartotojÅ³ grupÄ—ms.

**NuolatinÄ— inovacija**: Qwen evoliucija nuo 1.0 iki 3.0 rodo nuolatinÄ¯ gebÄ—jimÅ³, efektyvumo ir diegimo galimybiÅ³ tobulinimÄ….

### Ateities perspektyvos

Kaip Qwen Å¡eima toliau vystosi, galima tikÄ—tis:

- **Pagerinto efektyvumo**: NuolatinÄ—s optimizacijos geresniems veikimo ir parametrÅ³ santykiams.
- **IÅ¡plÄ—stÅ³ multimodaliniÅ³ gebÄ—jimÅ³**: SudÄ—tingesnÄ—s vaizdo, garso ir teksto apdorojimo integracijos.
- **Patobulinto samprotavimo**: PaÅ¾angiÅ³ mÄ…stymo mechanizmÅ³ ir daugiapakopiÅ³ problemÅ³ sprendimo gebÄ—jimÅ³.
- **GeresniÅ³ diegimo Ä¯rankiÅ³**: PatobulintÅ³ sistemÅ³ ir optimizavimo Ä¯rankiÅ³ Ä¯vairiems diegimo scenarijams.
- **BendruomenÄ—s augimo**: IÅ¡plÄ—stos ekosistemos Ä¯rankiÅ³, aplikacijÅ³ ir bendruomenÄ—s indÄ—liÅ³.

### Kiti Å¾ingsniai

Nesvarbu, ar kuriate pokalbiÅ³ robotÄ…, Å¡vietimo Ä¯rankius, kodavimo asistentus ar daugiakalbes aplikacijas, Qwen Å¡eima siÅ«lo mastelio sprendimus su stipriu bendruomenÄ—s palaikymu ir iÅ¡samia dokumentacija.

NorÄ—dami gauti naujausius atnaujinimus, modeliÅ³ leidimus ir iÅ¡samiÄ… techninÄ™ dokumentacijÄ…, apsilankykite oficialiuose Qwen saugyklose Hugging Face platformoje ir tyrinÄ—kite aktyvias bendruomenÄ—s diskusijas bei pavyzdÅ¾ius.

AI kÅ«rimo ateitis slypi prieinamuose, skaidriuose ir galinguose Ä¯rankiuose, kurie leidÅ¾ia inovacijas visose srityse ir mastuose. Qwen Å¡eima Ä¯kÅ«nija Å¡iÄ… vizijÄ…, suteikdama organizacijoms ir kÅ«rÄ—jams pagrindÄ… kurti naujos kartos AI pagrÄ¯stas aplikacijas.

## Papildomi iÅ¡tekliai

- **Oficiali dokumentacija**: [Qwen dokumentacija](https://qwen.readthedocs.io/)
- **ModeliÅ³ saugykla**: [Hugging Face Qwen kolekcijos](https://huggingface.co/collections/Qwen/)
- **Techniniai straipsniai**: [Qwen moksliniai leidiniai](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **BendruomenÄ—**: [GitHub diskusijos ir problemos](https://github.com/QwenLM/)
- **ModelScope platforma**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## Mokymosi rezultatai

BaigÄ™ Å¡Ä¯ modulÄ¯, galÄ—site:

1. PaaiÅ¡kinti Qwen modeliÅ³ Å¡eimos architektÅ«rinius pranaÅ¡umus ir jos atviro kodo poÅ¾iÅ«rÄ¯.
2. Pasirinkti tinkamÄ… Qwen variantÄ… pagal specifinius aplikacijos reikalavimus ir iÅ¡tekliÅ³ apribojimus.
3. Ä®gyvendinti Qwen modelius Ä¯vairiuose diegimo scenarijuose su optimizuotomis konfigÅ«racijomis.
4. Taikyti kvantavimo ir optimizavimo technikas, siekiant pagerinti Qwen modeliÅ³ veikimÄ….
5. Ä®vertinti kompromisus tarp modelio dydÅ¾io, veikimo ir gebÄ—jimÅ³ visoje Qwen Å¡eimoje.

## Kas toliau

- [03: Gemma Å¡eimos pagrindai](03.GemmaFamily.md)

---

**AtsakomybÄ—s atsisakymas**:  
Å is dokumentas buvo iÅ¡verstas naudojant AI vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors stengiamÄ—s uÅ¾tikrinti tikslumÄ…, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama naudoti profesionalÅ³ Å¾mogaus vertimÄ…. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar klaidingus interpretavimus, atsiradusius naudojant Å¡Ä¯ vertimÄ….