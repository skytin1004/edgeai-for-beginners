<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "38b68a204a9621126d056b0e5b51ab7c",
  "translation_date": "2025-10-09T21:03:32+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "lt"
}
-->
# 1 skyrius: SLM paÅ¾angus mokymasis â€“ pagrindai ir optimizavimas

MaÅ¾i kalbos modeliai (SLM) yra svarbus EdgeAI paÅ¾angos etapas, leidÅ¾iantis sudÄ—tingÄ… natÅ«ralios kalbos apdorojimÄ… Ä¯renginiuose su ribotais iÅ¡tekliais. Suprasti, kaip efektyviai diegti, optimizuoti ir naudoti SLM, yra bÅ«tina kuriant praktiÅ¡kus AI sprendimus kraÅ¡tiniuose Ä¯renginiuose.

## Ä®vadas

Å ioje pamokoje nagrinÄ—sime maÅ¾us kalbos modelius (SLM) ir jÅ³ paÅ¾angias Ä¯gyvendinimo strategijas. Aptarsime pagrindines SLM sÄ…vokas, jÅ³ parametrÅ³ ribas ir klasifikacijas, optimizavimo technikas bei praktines diegimo strategijas kraÅ¡tiniuose kompiuterijos aplinkose.

## Mokymosi tikslai

Pamokos pabaigoje galÄ—site:

- ğŸ”¢ Suprasti maÅ¾Å³ kalbos modeliÅ³ parametrÅ³ ribas ir klasifikacijas.
- ğŸ› ï¸ Nustatyti pagrindines optimizavimo technikas SLM diegimui kraÅ¡tiniuose Ä¯renginiuose.
- ğŸš€ IÅ¡mokti Ä¯gyvendinti paÅ¾angias kvantavimo ir suspaudimo strategijas SLM.

## SLM parametrÅ³ ribÅ³ ir klasifikacijÅ³ supratimas

MaÅ¾i kalbos modeliai (SLM) yra AI modeliai, sukurti apdoroti, suprasti ir generuoti natÅ«ralios kalbos turinÄ¯, turint Å¾ymiai maÅ¾iau parametrÅ³ nei jÅ³ didieji analogai. Nors dideli kalbos modeliai (LLM) turi Å¡imtus milijardÅ³ ar trilijonus parametrÅ³, SLM yra specialiai sukurti efektyvumui ir diegimui kraÅ¡tiniuose Ä¯renginiuose.

ParametrÅ³ klasifikavimo sistema padeda suprasti skirtingas SLM kategorijas ir jÅ³ tinkamus naudojimo atvejus. Å i klasifikacija yra labai svarbi renkantis tinkamÄ… modelÄ¯ konkretiems kraÅ¡tinÄ—s kompiuterijos scenarijams.

### ParametrÅ³ klasifikavimo sistema

ParametrÅ³ ribÅ³ supratimas padeda pasirinkti tinkamus modelius skirtingiems kraÅ¡tinÄ—s kompiuterijos scenarijams:

- **ğŸ”¬ Mikro SLM**: 100M - 1,4B parametrai (ypaÄ lengvi mobiliesiems Ä¯renginiams)
- **ğŸ“± MaÅ¾i SLM**: 1,5B - 13,9B parametrai (subalansuotas naÅ¡umas ir efektyvumas)
- **âš–ï¸ Vidutiniai SLM**: 14B - 30B parametrai (artÄ—jant prie LLM galimybiÅ³, iÅ¡laikant efektyvumÄ…)

TikslÅ«s parametrai lieka lankstÅ«s moksliniÅ³ tyrimÅ³ bendruomenÄ—je, taÄiau dauguma praktikÅ³ modelius, turinÄius maÅ¾iau nei 30 milijardÅ³ parametrÅ³, laiko â€maÅ¾aisâ€œ, kai kurie Å¡altiniai ribÄ… nustato dar Å¾emiau â€“ 10 milijardÅ³ parametrÅ³.

### Pagrindiniai SLM privalumai

SLM turi keletÄ… pagrindiniÅ³ privalumÅ³, dÄ—l kuriÅ³ jie yra idealÅ«s kraÅ¡tinÄ—s kompiuterijos programoms:

**Operacinis efektyvumas**: SLM uÅ¾tikrina greitesnÄ¯ iÅ¡vedimÄ… dÄ—l maÅ¾esnio parametrÅ³ kiekio, todÄ—l jie yra idealÅ«s realaus laiko programoms. Jie reikalauja maÅ¾iau skaiÄiavimo iÅ¡tekliÅ³, leidÅ¾ia diegti Ä¯renginiuose su ribotais iÅ¡tekliais, sunaudoja maÅ¾iau energijos ir maÅ¾ina anglies pÄ—dsakÄ….

**Diegimo lankstumas**: Å ie modeliai leidÅ¾ia AI veikti Ä¯renginiuose be interneto ryÅ¡io, didina privatumÄ… ir saugumÄ… per vietinÄ¯ apdorojimÄ…, gali bÅ«ti pritaikyti specifinÄ—ms sritims ir yra tinkami Ä¯vairioms kraÅ¡tinÄ—s kompiuterijos aplinkoms.

**EkonomiÅ¡kumas**: SLM siÅ«lo ekonomiÅ¡kÄ… mokymÄ… ir diegimÄ…, palyginti su LLM, maÅ¾esnes eksploatavimo iÅ¡laidas ir maÅ¾esnius pralaidumo reikalavimus kraÅ¡tinÄ—ms programoms.

## PaÅ¾angios modeliÅ³ Ä¯sigijimo strategijos

### Hugging Face ekosistema

Hugging Face yra pagrindinis centras, skirtas atrasti ir pasiekti paÅ¾angius SLM. Platforma siÅ«lo iÅ¡samius iÅ¡teklius modeliÅ³ atradimui ir diegimui:

**ModeliÅ³ atradimo funkcijos**: Platforma siÅ«lo paÅ¾angÅ³ filtravimÄ… pagal parametrÅ³ skaiÄiÅ³, licencijos tipÄ… ir naÅ¡umo metrikas. Vartotojai gali naudotis modeliÅ³ palyginimo Ä¯rankiais, realaus laiko naÅ¡umo etalonais ir vertinimo rezultatais bei WebGPU demonstracijomis, skirtomis nedelsiant iÅ¡bandyti.

**Kuruotos SLM kolekcijos**: PopuliarÅ«s modeliai apima Phi-4-mini-3.8B paÅ¾angioms loginÄ—ms uÅ¾duotims, Qwen3 serijÄ… (0.6B/1.7B/4B) daugiakalbÄ—ms programoms, Google Gemma3 efektyvioms bendrojo naudojimo uÅ¾duotims ir eksperimentinius modelius, tokius kaip BitNET, skirtus itin maÅ¾o tikslumo diegimui. Platforma taip pat siÅ«lo bendruomenÄ—s kuruotas kolekcijas su specializuotais modeliais konkreÄioms sritims ir iÅ¡ anksto apmokytus bei instrukcijomis pritaikytus variantus, optimizuotus skirtingiems naudojimo atvejams.

### Azure AI Foundry modeliÅ³ katalogas

Azure AI Foundry modeliÅ³ katalogas siÅ«lo Ä¯monÄ—s lygio prieigÄ… prie SLM su patobulintomis integracijos galimybÄ—mis:

**Ä®monÄ—s integracija**: Kataloge yra modeliai, parduodami tiesiogiai per Azure su Ä¯monÄ—s lygio palaikymu ir SLA, Ä¯skaitant Phi-4-mini-3.8B paÅ¾angioms loginÄ—ms uÅ¾duotims ir Llama 3-8B gamybos diegimui. Taip pat yra modeliai, tokie kaip Qwen3 8B, iÅ¡ patikimÅ³ treÄiÅ³jÅ³ Å¡aliÅ³ atvirojo kodo modeliÅ³.

**Ä®monÄ—s privalumai**: Ä®montuoti Ä¯rankiai pritaikymui, stebÄ—jimui ir atsakingam AI, integruoti su lanksÄiu Provisioned Throughput visose modeliÅ³ Å¡eimose. Tiesioginis Microsoft palaikymas su Ä¯monÄ—s SLA, integruotos saugumo ir atitikties funkcijos bei iÅ¡samÅ«s diegimo darbo srautai pagerina Ä¯monÄ—s patirtÄ¯.

## PaÅ¾angios kvantavimo ir optimizavimo technikos

### Llama.cpp optimizavimo sistema

Llama.cpp siÅ«lo paÅ¾angias kvantavimo technikas, uÅ¾tikrinanÄias maksimalÅ³ efektyvumÄ… kraÅ¡tiniuose diegimuose:

**Kvantavimo metodai**: Sistema palaiko Ä¯vairius kvantavimo lygius, Ä¯skaitant Q4_0 (4 bitÅ³ kvantavimas su puikiu dydÅ¾io sumaÅ¾inimu â€“ idealus Qwen3-0.6B mobiliajam diegimui), Q5_1 (5 bitÅ³ kvantavimas, subalansuojantis kokybÄ™ ir suspaudimÄ… â€“ tinkamas Phi-4-mini-3.8B kraÅ¡tiniam iÅ¡vedimui) ir Q8_0 (8 bitÅ³ kvantavimas, uÅ¾tikrinantis beveik originaliÄ… kokybÄ™ â€“ rekomenduojamas Google Gemma3 gamybos naudojimui). BitNET yra paÅ¾angiausias su 1 bitÅ³ kvantavimu ekstremaliems suspaudimo scenarijams.

**Ä®gyvendinimo privalumai**: CPU optimizuotas iÅ¡vedimas su SIMD pagreitinimu uÅ¾tikrina efektyvÅ³ modelio Ä¯krovimÄ… ir vykdymÄ…. KryÅ¾minÄ— platformÅ³ suderinamumas su x86, ARM ir Apple Silicon architektÅ«romis leidÅ¾ia diegti nepriklausomai nuo aparatÅ«ros.

**Praktinis Ä¯gyvendinimo pavyzdys**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Atminties pÄ—dsako palyginimas**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Microsoft Olive optimizavimo rinkinys

Microsoft Olive siÅ«lo iÅ¡samius modeliÅ³ optimizavimo darbo srautus, skirtus gamybos aplinkoms:

**Optimizavimo technikos**: Rinkinys apima dinaminÄ¯ kvantavimÄ… automatiÅ¡kai parenkant tikslumÄ… (ypaÄ efektyvus su Qwen3 serijos modeliais), grafÅ³ optimizavimÄ… ir operatoriÅ³ sujungimÄ… (optimizuotas Google Gemma3 architektÅ«rai), aparatÅ«ros specifines optimizacijas CPU, GPU ir NPU (su specialiu palaikymu Phi-4-mini-3.8B ARM Ä¯renginiuose) ir daugiapakopius optimizavimo darbo srautus. BitNET modeliams reikalingi specializuoti 1 bitÅ³ kvantavimo darbo srautai Olive sistemoje.

**Darbo srautÅ³ automatizavimas**: Automatinis optimizavimo variantÅ³ etalonÅ³ nustatymas uÅ¾tikrina kokybÄ—s metrikÅ³ iÅ¡saugojimÄ… optimizavimo metu. Integracija su populiariomis ML sistemomis, tokiomis kaip PyTorch ir ONNX, suteikia debesÅ³ ir kraÅ¡tiniÅ³ diegimo optimizavimo galimybes.

**Praktinis Ä¯gyvendinimo pavyzdys**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Apple MLX sistema

Apple MLX siÅ«lo natyviÄ… optimizacijÄ…, specialiai sukurtÄ… Apple Silicon Ä¯renginiams:

**Apple Silicon optimizacija**: Sistema naudoja vieningÄ… atminties architektÅ«rÄ… su Metal Performance Shaders integracija, automatinÄ¯ miÅ¡raus tikslumo iÅ¡vedimÄ… (ypaÄ efektyvus su Google Gemma3) ir optimizuotÄ… atminties pralaidumo panaudojimÄ…. Phi-4-mini-3.8B rodo iÅ¡skirtinÄ¯ naÅ¡umÄ… M serijos lustuose, o Qwen3-1.7B uÅ¾tikrina optimalÅ³ balansÄ… MacBook Air diegimui.

**KÅ«rimo funkcijos**: Python ir Swift API palaikymas su NumPy suderinamomis masyvo operacijomis, automatinio diferencijavimo galimybÄ—mis ir sklandÅ¾ia integracija su Apple kÅ«rimo Ä¯rankiais suteikia iÅ¡samÅ³ kÅ«rimo aplinkÄ….

**Praktinis Ä¯gyvendinimo pavyzdys**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Gamybos diegimas ir iÅ¡vedimo strategijos

### Ollama: supaprastintas vietinis diegimas

Ollama supaprastina SLM diegimÄ… su Ä¯monÄ—s lygio funkcijomis vietinÄ—ms ir kraÅ¡tinÄ—ms aplinkoms:

**Diegimo galimybÄ—s**: Vieno komandos modelio Ä¯diegimas ir vykdymas su automatiniu modelio atsisiuntimu ir talpyklos naudojimu. Palaikymas Phi-4-mini-3.8B, visai Qwen3 serijai (0.6B/1.7B/4B) ir Google Gemma3 su REST API programÅ³ integracijai bei daugiamodeliÅ³ valdymo ir perjungimo galimybÄ—mis. BitNET modeliams reikalingos eksperimentinÄ—s konfigÅ«racijos 1 bitÅ³ kvantavimo palaikymui.

**PaÅ¾angios funkcijos**: PritaikytÅ³ modeliÅ³ pritaikymo palaikymas, Dockerfile generavimas konteinerizuotam diegimui, GPU pagreitinimas su automatiniu aptikimu ir modelio kvantavimo bei optimizavimo galimybÄ—s suteikia iÅ¡samÅ³ diegimo lankstumÄ….

### VLLM: didelio naÅ¡umo iÅ¡vedimas

VLLM uÅ¾tikrina gamybos lygio iÅ¡vedimo optimizacijÄ… didelio pralaidumo scenarijams:

**NaÅ¡umo optimizacijos**: PagedAttention atminties efektyviam dÄ—mesio skaiÄiavimui (ypaÄ naudingas Phi-4-mini-3.8B transformatoriaus architektÅ«rai), dinaminis paketavimas pralaidumo optimizavimui (optimizuotas Qwen3 serijos lygiagreÄiam apdorojimui), tensorinis lygiagretumas daugiagrafiniam masteliui (Google Gemma3 palaikymas) ir spekuliatyvus dekodavimas latencijos maÅ¾inimui. BitNET modeliams reikalingi specializuoti iÅ¡vedimo branduoliai 1 bitÅ³ operacijoms.

**Ä®monÄ—s integracija**: OpenAI suderinami API galiniai taÅ¡kai, Kubernetes diegimo palaikymas, stebÄ—jimo ir stebÄ—jimo integracija bei automatinio mastelio galimybÄ—s uÅ¾tikrina Ä¯monÄ—s lygio diegimo sprendimus.

### Foundry Local: Microsoft kraÅ¡tinis sprendimas

Foundry Local siÅ«lo iÅ¡samias kraÅ¡tinio diegimo galimybes Ä¯monÄ—s aplinkoms:

**KraÅ¡tinÄ—s kompiuterijos funkcijos**: Offline-first architektÅ«ros dizainas su iÅ¡tekliÅ³ apribojimÅ³ optimizavimu, vietinio modelio registro valdymu ir kraÅ¡tinio-debesÅ³ sinchronizavimo galimybÄ—mis uÅ¾tikrina patikimÄ… kraÅ¡tinÄ¯ diegimÄ….

**Saugumas ir atitiktis**: Vietinis duomenÅ³ apdorojimas privatumo iÅ¡saugojimui, Ä¯monÄ—s saugumo kontrolÄ—, audito Å¾urnalÅ³ ir atitikties ataskaitÅ³ teikimas bei prieigos valdymas pagal vaidmenis uÅ¾tikrina iÅ¡samÅ³ saugumÄ… kraÅ¡tiniuose diegimuose.

## Geriausios SLM Ä¯gyvendinimo praktikos

### Modelio pasirinkimo gairÄ—s

Renkantis SLM kraÅ¡tiniam diegimui, atsiÅ¾velkite Ä¯ Å¡iuos veiksnius:

**ParametrÅ³ skaiÄiaus svarstymai**: Pasirinkite mikro SLM, tokius kaip Qwen3-0.6B, itin lengvoms mobiliosioms programoms, maÅ¾us SLM, tokius kaip Qwen3-1.7B ar Google Gemma3, subalansuotoms naÅ¡umo situacijoms, ir vidutinius SLM, tokius kaip Phi-4-mini-3.8B ar Qwen3-4B, kai artÄ—jama prie LLM galimybiÅ³, iÅ¡laikant efektyvumÄ…. BitNET modeliai siÅ«lo eksperimentinÄ¯ itin suspaudimÄ… specifiniams moksliniÅ³ tyrimÅ³ taikymams.

**Naudojimo atvejÅ³ suderinimas**: Suderinkite modelio galimybes su konkreÄiais programÅ³ reikalavimais, atsiÅ¾velgdami Ä¯ tokius veiksnius kaip atsako kokybÄ—, iÅ¡vedimo greitis, atminties apribojimai ir neprisijungus veikimo reikalavimai.

### Optimizavimo strategijos pasirinkimas

**Kvantavimo metodas**: Pasirinkite tinkamus kvantavimo lygius pagal kokybÄ—s reikalavimus ir aparatÅ«ros apribojimus. Apsvarstykite Q4_0 maksimaliai suspaudimui (idealus Qwen3-0.6B mobiliajam diegimui), Q5_1 subalansuotam kokybÄ—s-suspaudimo kompromisui (tinkamas Phi-4-mini-3.8B ir Google Gemma3), ir Q8_0 beveik originalios kokybÄ—s iÅ¡saugojimui (rekomenduojamas Qwen3-4B gamybos aplinkoms). BitNET 1 bitÅ³ kvantavimas yra ekstremalaus suspaudimo riba specializuotoms programoms.

**Sistemos pasirinkimas**: Pasirinkite optimizavimo sistemas pagal tikslinÄ™ aparatÅ«rÄ… ir diegimo reikalavimus. Naudokite Llama.cpp CPU optimizuotam diegimui, Microsoft Olive iÅ¡samiems optimizavimo darbo srautams ir Apple MLX Apple Silicon Ä¯renginiams.

## Praktiniai modeliÅ³ pavyzdÅ¾iai ir naudojimo atvejai

### RealÅ«s diegimo scenarijai

**Mobiliosios programos**: Qwen3-0.6B puikiai tinka iÅ¡maniojo telefono pokalbiÅ³ programoms su minimaliu atminties pÄ—dsaku, o Google Gemma3 uÅ¾tikrina subalansuotÄ… naÅ¡umÄ… planÅ¡etiniÅ³ kompiuteriÅ³ edukaciniams Ä¯rankiams. Phi-4-mini-3.8B siÅ«lo aukÅ¡Äiausio lygio loginio mÄ…stymo galimybes mobiliosiose produktyvumo programose.

**Darbalaukio ir kraÅ¡tinÄ— kompiuterija**: Qwen3-1.7B uÅ¾tikrina optimalÅ³ naÅ¡umÄ… darbalaukio asistento programoms, Phi-4-mini-3.8B siÅ«lo paÅ¾angias kodo generavimo galimybes kÅ«rÄ—jÅ³ Ä¯rankiams, o Qwen3-4B leidÅ¾ia sudÄ—tingÄ… dokumentÅ³ analizÄ™ darbo vietos aplinkose.

**Moksliniai tyrimai ir eksperimentai**: BitNET modeliai leidÅ¾ia tyrinÄ—ti itin maÅ¾o tikslumo iÅ¡vedimÄ… akademiniams tyrimams ir koncepcijos Ä¯rodymo programoms, reikalaujanÄioms ekstremaliÅ³ iÅ¡tekliÅ³ apribojimÅ³.

### NaÅ¡umo etalonai ir palyginimai

**IÅ¡vedimo greitis**: Qwen3-0.6B pasiekia greiÄiausiÄ… iÅ¡vedimo laik

---

**AtsakomybÄ—s atsisakymas**:  
Å is dokumentas buvo iÅ¡verstas naudojant AI vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors stengiamÄ—s uÅ¾tikrinti tikslumÄ…, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama naudoti profesionalÅ³ Å¾mogaus vertimÄ…. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar neteisingus interpretavimus, atsiradusius dÄ—l Å¡io vertimo naudojimo.