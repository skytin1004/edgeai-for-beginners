<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-18T17:14:41+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "cs"
}
-->
# Sekce 4: NasazenÃ­ - Implementace modelu pÅ™ipravenÃ©ho pro produkci

## PÅ™ehled

Tento komplexnÃ­ nÃ¡vod vÃ¡s provede celÃ½m procesem nasazenÃ­ jemnÄ› doladÄ›nÃ½ch kvantovanÃ½ch modelÅ¯ pomocÃ­ Foundry Local. Pokryjeme konverzi modelu, optimalizaci kvantizace a konfiguraci nasazenÃ­ od zaÄÃ¡tku do konce.

## PÅ™edpoklady

NeÅ¾ zaÄnete, ujistÄ›te se, Å¾e mÃ¡te nÃ¡sledujÃ­cÃ­:

- âœ… JemnÄ› doladÄ›nÃ½ ONNX model pÅ™ipravenÃ½ k nasazenÃ­
- âœ… PoÄÃ­taÄ s Windows nebo Mac
- âœ… Python 3.10 nebo novÄ›jÅ¡Ã­
- âœ… MinimÃ¡lnÄ› 8 GB volnÃ© RAM
- âœ… Foundry Local nainstalovanÃ½ na vaÅ¡em systÃ©mu

## ÄŒÃ¡st 1: NastavenÃ­ prostÅ™edÃ­

### Instalace potÅ™ebnÃ½ch nÃ¡strojÅ¯

OtevÅ™ete svÅ¯j terminÃ¡l (Command Prompt na Windows, Terminal na Mac) a postupnÄ› spusÅ¥te nÃ¡sledujÃ­cÃ­ pÅ™Ã­kazy:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

âš ï¸ **DÅ¯leÅ¾itÃ¡ poznÃ¡mka**: Budete takÃ© potÅ™ebovat CMake verze 3.31 nebo novÄ›jÅ¡Ã­, kterÃ½ si mÅ¯Å¾ete stÃ¡hnout z [cmake.org](https://cmake.org/download/).

## ÄŒÃ¡st 2: Konverze modelu a kvantizace

### VÃ½bÄ›r sprÃ¡vnÃ©ho formÃ¡tu

Pro jemnÄ› doladÄ›nÃ© malÃ© jazykovÃ© modely doporuÄujeme pouÅ¾Ã­vat **ONNX formÃ¡t**, protoÅ¾e nabÃ­zÃ­:

- ğŸš€ LepÅ¡Ã­ optimalizaci vÃ½konu
- ğŸ”§ NasazenÃ­ nezÃ¡vislÃ© na hardwaru
- ğŸ­ Schopnosti pÅ™ipravenÃ© pro produkci
- ğŸ“± Kompatibilitu napÅ™Ã­Ä platformami

### Metoda 1: Konverze jednÃ­m pÅ™Ã­kazem (doporuÄeno)

PouÅ¾ijte nÃ¡sledujÃ­cÃ­ pÅ™Ã­kaz pro pÅ™Ã­mou konverzi vaÅ¡eho jemnÄ› doladÄ›nÃ©ho modelu:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**VysvÄ›tlenÃ­ parametrÅ¯:**
- `--model_name_or_path`: Cesta k vaÅ¡emu jemnÄ› doladÄ›nÃ©mu modelu
- `--device cpu`: PouÅ¾itÃ­ CPU pro optimalizaci
- `--precision int4`: PouÅ¾itÃ­ kvantizace INT4 (pÅ™ibliÅ¾nÄ› 75% snÃ­Å¾enÃ­ velikosti)
- `--output_path`: VÃ½stupnÃ­ cesta pro konvertovanÃ½ model

### Metoda 2: PÅ™Ã­stup pomocÃ­ konfiguraÄnÃ­ho souboru (pokroÄilÃ­ uÅ¾ivatelÃ©)

VytvoÅ™te konfiguraÄnÃ­ soubor s nÃ¡zvem `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

PotÃ© spusÅ¥te:

```bash
olive run --config ./finetuned_conversion_config.json
```

### PorovnÃ¡nÃ­ moÅ¾nostÃ­ kvantizace

| PÅ™esnost | Velikost souboru | Rychlost inferencÃ­ | Kvalita modelu | DoporuÄenÃ© pouÅ¾itÃ­ |
|----------|------------------|--------------------|----------------|--------------------|
| FP16     | ZÃ¡kladnÃ­ Ã— 0.5   | RychlÃ¡             | NejlepÅ¡Ã­       | Vysoce vÃ½konnÃ½ hardware |
| INT8     | ZÃ¡kladnÃ­ Ã— 0.25  | Velmi rychlÃ¡       | DobrÃ¡          | VyvÃ¡Å¾enÃ¡ volba |
| INT4     | ZÃ¡kladnÃ­ Ã— 0.125 | NejrychlejÅ¡Ã­       | PÅ™ijatelnÃ¡     | OmezenÃ© zdroje |

ğŸ’¡ **DoporuÄenÃ­**: ZaÄnÄ›te s kvantizacÃ­ INT4 pro vaÅ¡e prvnÃ­ nasazenÃ­. Pokud kvalita nebude uspokojivÃ¡, zkuste INT8 nebo FP16.

## ÄŒÃ¡st 3: Konfigurace nasazenÃ­ Foundry Local

### VytvoÅ™enÃ­ konfigurace modelu

PÅ™ejdÄ›te do adresÃ¡Å™e modelÅ¯ Foundry Local:

```bash
foundry cache cd ./models/
```

VytvoÅ™te strukturu adresÃ¡Å™Å¯ pro vÃ¡Å¡ model:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

VytvoÅ™te konfiguraÄnÃ­ soubor `inference_model.json` ve vaÅ¡em adresÃ¡Å™i modelu:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Å ablony konfiguracÃ­ specifickÃ© pro model

#### Pro modely sÃ©rie Qwen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## ÄŒÃ¡st 4: TestovÃ¡nÃ­ a optimalizace modelu

### OvÄ›Å™enÃ­ instalace modelu

Zkontrolujte, zda Foundry Local rozpoznÃ¡ vÃ¡Å¡ model:

```bash
foundry cache ls
```

MÄ›li byste vidÄ›t `your-finetuned-model-int4` v seznamu.

### ZahÃ¡jenÃ­ testovÃ¡nÃ­ modelu

```bash
foundry model run your-finetuned-model-int4
```

### Benchmarking vÃ½konu

Sledujte klÃ­ÄovÃ© metriky bÄ›hem testovÃ¡nÃ­:

1. **Doba odezvy**: MÄ›Å™enÃ­ prÅ¯mÄ›rnÃ© doby na odpovÄ›Ä
2. **VyuÅ¾itÃ­ pamÄ›ti**: MonitorovÃ¡nÃ­ spotÅ™eby RAM
3. **VyuÅ¾itÃ­ CPU**: Kontrola zatÃ­Å¾enÃ­ procesoru
4. **Kvalita vÃ½stupu**: HodnocenÃ­ relevance a koherence odpovÄ›dÃ­

### KontrolnÃ­ seznam validace kvality

- âœ… Model sprÃ¡vnÄ› reaguje na dotazy z jemnÄ› doladÄ›nÃ© domÃ©ny
- âœ… FormÃ¡t odpovÄ›dÃ­ odpovÃ­dÃ¡ oÄekÃ¡vanÃ© struktuÅ™e vÃ½stupu
- âœ… Å½Ã¡dnÃ© Ãºniky pamÄ›ti pÅ™i dlouhodobÃ©m pouÅ¾Ã­vÃ¡nÃ­
- âœ… KonzistentnÃ­ vÃ½kon napÅ™Ã­Ä rÅ¯znÃ½mi dÃ©lkami vstupÅ¯
- âœ… SprÃ¡vnÃ© zpracovÃ¡nÃ­ hraniÄnÃ­ch pÅ™Ã­padÅ¯ a neplatnÃ½ch vstupÅ¯

## ShrnutÃ­

Gratulujeme! ÃšspÄ›Å¡nÄ› jste dokonÄili:

- âœ… Konverzi formÃ¡tu jemnÄ› doladÄ›nÃ©ho modelu
- âœ… Optimalizaci kvantizace modelu
- âœ… Konfiguraci nasazenÃ­ Foundry Local
- âœ… LadÄ›nÃ­ vÃ½konu a Å™eÅ¡enÃ­ problÃ©mÅ¯

---

**ProhlÃ¡Å¡enÃ­**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by pro automatickÃ½ pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). AÄkoli se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho pÅ¯vodnÃ­m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace doporuÄujeme profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za Å¾Ã¡dnÃ© nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.