<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "94b65d49961cabc07f76062d09a5d09c",
  "translation_date": "2025-10-09T16:55:36+00:00",
  "source_file": "Workshop/Session06-ModelsAsTools.md",
  "language_code": "he"
}
-->
# מפגש 6: Foundry Local – מודלים ככלים

## תקציר

התייחסו למודלים ככלים שניתן להרכיב בתוך שכבת הפעלה מקומית של AI. במפגש זה תלמדו כיצד לשרשר קריאות של SLM/LLM מתמחים, לנתב משימות באופן סלקטיבי, ולחשוף ממשק SDK אחיד ליישומים. תבנו נתב מודלים קל משקל + מתכנן משימות, תשלבו אותו בסקריפט אפליקציה, ותתוו את מסלול ההרחבה ל-Azure AI Foundry עבור עומסי עבודה בייצור.

## מטרות למידה

- **להבין** מודלים ככלים אטומיים עם יכולות מוגדרות
- **לנתב** בקשות על בסיס כוונה / ניקוד היוריסטי
- **לשרשר** פלטים במשימות מרובות שלבים (פירוק → פתרון → שיפור)
- **לשלב** ממשק API אחיד ליישומים במורד הזרם
- **להרחיב** את העיצוב לענן (אותו חוזה תואם OpenAI)

## דרישות מוקדמות

- השלמת מפגשים 1–5
- מספר מודלים מקומיים במטמון (לדוגמה, `phi-4-mini`, `deepseek-coder-1.3b`, `qwen2.5-0.5b`)

### קטע קוד לסביבה חוצה פלטפורמות

Windows PowerShell:
```powershell
py -m venv .venv
 .\.venv\Scripts\Activate.ps1
pip install --upgrade pip
pip install foundry-local-sdk openai
```

macOS / Linux:
```bash
python3 -m venv .venv
source .venv/bin/activate
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

גישה לשירות מרוחק/VM מ-macOS:
```bash
export FOUNDRY_LOCAL_ENDPOINT=http://<windows-host>:5273/v1
```


## זרימת הדגמה (30 דקות)

### 1. הצהרת יכולות כלי (5 דקות)

צרו `samples/06-tools/models_catalog.py`:

```python
CATALOG = {
  "phi-4-mini": {
    "capabilities": ["general", "reasoning", "summarize"],
    "priority": 2
  },
  "deepseek-coder-1.3b": {
    "capabilities": ["code", "refactor", "explain_code"],
    "priority": 1
  },
  "qwen2.5-0.5b": {
    "capabilities": ["fast", "classification", "lightweight"],
    "priority": 3
  }
}
```


### 2. זיהוי כוונה וניתוב (8 דקות)

צרו `samples/06-tools/router.py`:

```python
#!/usr/bin/env python3
"""Model-as-tool router using Foundry Local OpenAI-compatible endpoint."""
from openai import OpenAI
from models_catalog import CATALOG
import re

client = OpenAI(base_url="http://localhost:5273/v1", api_key="not-needed")

INTENT_RULES = [
  (re.compile(r"code|function|refactor|bug|optimi", re.I), "code"),
  (re.compile(r"summari|abstract|tl;dr", re.I), "summarize"),
  (re.compile(r"classif|label|category", re.I), "classification"),
]

def detect_intent(prompt: str) -> str:
    for pat, intent in INTENT_RULES:
        if pat.search(prompt):
            return intent
    return "general"

def select_model(intent: str) -> str:
    # Score catalog: capability match first, then priority
    scored = []
    for name, meta in CATALOG.items():
        caps = meta["capabilities"]
        match = intent in caps
        scored.append((name, match, meta["priority"]))
    # Sort: match True first, then lowest priority value
    scored.sort(key=lambda t: (not t[1], t[2]))
    return scored[0][0]

def run(prompt: str):
    intent = detect_intent(prompt)
    model = select_model(intent)
    resp = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=400,
        temperature=0.5
    )
    return {"intent": intent, "model": model, "output": resp.choices[0].message.content}

if __name__ == "__main__":
    tests = [
        "Refactor this Python function for readability",
        "Summarize the importance of local AI governance",
        "Classify this feedback: 'The UI is slow and confusing'"
    ]
    for t in tests:
        r = run(t)
        print(f"Prompt: {t}\nModel: {r['model']} (intent={r['intent']})\nOutput: {r['output'][:160]}...\n")
```


### 3. שרשור משימות מרובות שלבים (7 דקות)

צרו `samples/06-tools/pipeline.py`:

```python
#!/usr/bin/env python3
"""Multi-step pipeline: plan -> solve -> refine using specialized models."""
from openai import OpenAI
from router import detect_intent, select_model

client = OpenAI(base_url="http://localhost:5273/v1", api_key="not-needed")

def chat(model, content, temp=0.4):
    r = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": content}],
        max_tokens=350,
        temperature=temp
    )
    return r.choices[0].message.content

def pipeline(task: str):
    plan_model = select_model("general")
    plan = chat(plan_model, f"Break the task into 3 ordered steps. Task: {task}")
    steps = [s for s in plan.split('\n') if s.strip()][:3]
    outputs = []
    for step in steps:
        intent = detect_intent(step)
        model = select_model(intent)
        out = chat(model, step)
        outputs.append((step, model, out))
    refine_model = select_model("summarize")
    combined = '\n'.join(o[2] for o in outputs)
    refined = chat(refine_model, f"Condense results into a cohesive answer:\n{combined}")
    return {"plan": plan, "steps": outputs, "final": refined}

if __name__ == '__main__':
    result = pipeline("Generate a refactored version of a slow Python loop and summarize performance gains.")
    print("PLAN:\n", result['plan'])
    print("FINAL:\n", result['final'][:400])
```


### 4. פרויקט התחלתי: התאמת `06-models-as-tools` (5 דקות)

שיפורים:
- הוספת תמיכה בהזרמת טוקנים (עדכון UI מתקדם)
- הוספת ניקוד ביטחון: חפיפה לקסיקלית או מדד הנחיה
- ייצוא JSON מעקב (כוונה → מודל → זמן תגובה → שימוש בטוקנים)
- יישום שימוש חוזר במטמון עבור שלבים חוזרים

### 5. מסלול הרחבה ל-Azure (5 דקות)

| שכבה | מקומי (Foundry) | ענן (Azure AI Foundry) | אסטרטגיית מעבר |
|------|------------------|------------------------|-----------------|
| ניתוב | Python היוריסטי | מיקרו-שירות עמיד | קונטיינריזציה ופריסת API |
| מודלים | SLMs במטמון | פריסות מנוהלות | מיפוי שמות מקומיים ל-IDs פריסה |
| תצפיות | סטטיסטיקות CLI/ידני | רישום מרכזי ומדדים | הוספת אירועי מעקב מובנים |
| אבטחה | מארח מקומי בלבד | אימות Azure / רשתות | הכנסת Key Vault לסודות |
| עלות | משאבי מכשיר | חיוב צריכה | הוספת מגבלות תקציב |

## רשימת בדיקות אימות

```powershell
foundry model run phi-4-mini
foundry model run deepseek-coder-1.3b
python samples/06-tools/router.py
python samples/06-tools/pipeline.py
```

צפו בבחירת מודל מבוססת כוונה ובפלט סופי משופר.

## פתרון בעיות

| בעיה | סיבה | פתרון |
|------|------|-------|
| כל המשימות מנותבות לאותו מודל | כללים חלשים | העשרת סט regex INTENT_RULES |
| הצינור נכשל באמצע שלב | מודל חסר טעון | הרצת `foundry model run <model>` |
| לכידות פלט נמוכה | אין שלב שיפור | הוספת מעבר סיכום/אימות |

## מקורות

- Foundry Local SDK: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python
- Azure AI Foundry Docs: https://learn.microsoft.com/azure/ai-foundry
- דפוסי איכות הנחיה: ראו מפגש 2

---

**משך המפגש**: 30 דקות  
**רמת קושי**: מומחה

## תרחיש לדוגמה ומיפוי סדנה

| סקריפטים / מחברות סדנה | תרחיש | מטרה | מקור קטלוג / מערך נתונים |
|-------------------------|--------|-------|--------------------------|
| `samples/session06/models_router.py` / `notebooks/session06_models_router.ipynb` | עוזר למפתחים המטפל בהנחיות כוונה מעורבות (שכתוב, סיכום, סיווג) | כוונה היוריסטית → ניתוב כינוי מודל עם שימוש בטוקנים | `CATALOG` מוטמע + regex `RULES` |
| `samples/session06/models_pipeline.py` / `notebooks/session06_models_pipeline.ipynb` | תכנון ושיפור מרובי שלבים למשימת עזרה בקידוד מורכבת | פירוק → ביצוע מתמחה → שלב שיפור סיכום | אותו `CATALOG`; שלבים נגזרים מפלט תכנון |

### נרטיב תרחיש
כלי לשיפור פרודוקטיביות הנדסה מקבל משימות הטרוגניות: שכתוב קוד, סיכום הערות ארכיטקטורה, סיווג משוב. כדי למזער זמן תגובה ושימוש במשאבים, מודל כללי קטן מתכנן ומסכם, מודל מתמחה בקוד מטפל בשכתוב, ומודל קל משקל המסוגל לסיווג מתייג משוב. סקריפט הצינור מדגים שרשור + שיפור; סקריפט הנתב מבודד ניתוב הנחיה אדפטיבי יחיד.

### צילום קטלוג
```python
CATALOG = {
    "phi-4-mini": {"capabilities": ["general", "summarize"], "priority": 2},
    "deepseek-coder-1.3b": {"capabilities": ["code", "refactor"], "priority": 1},
    "qwen2.5-0.5b": {"capabilities": ["classification", "fast"], "priority": 3}
}
```


### הנחיות בדיקה לדוגמה
```json
[
    "Refactor this Python function for readability",
    "Summarize the importance of small language models",
    "Classify this feedback: The UI is slow but pretty",
    "Generate a refactored version of a slow Python loop and summarize performance gains."
]
```


### הרחבת מעקב (אופציונלי)
הוספת שורות JSON מעקב לכל שלב עבור `models_pipeline.py`:
```python
trace.append({
    "step": step_idx,
    "intent": intent,
    "alias": alias,
    "latency_ms": round((end-start)*1000,2),
    "tokens": getattr(usage,'total_tokens',None)
})
```


### היוריסטיקת הסלמה (רעיון)
אם התכנון מכיל מילות מפתח כמו "אופטימיזציה", "אבטחה", או אורך שלב > 280 תווים → הסלמה למודל גדול יותר (לדוגמה, `gpt-oss-20b`) עבור אותו שלב בלבד.

### שיפורים אופציונליים

| תחום | שיפור | ערך | רמז |
|------|-------|-----|-----|
| מטמון | שימוש חוזר במנהלי אובייקטים + לקוחות | זמן תגובה נמוך יותר, פחות עומס | שימוש ב-`workshop_utils.get_client` |
| מדדי שימוש | לכידת טוקנים וזמן תגובה לכל שלב | פרופיל ואופטימיזציה | מדידת זמן לכל קריאה מנותבת; אחסון ברשימת מעקב |
| ניתוב אדפטיבי | מודעות לביטחון / עלות | איזון איכות-עלות טוב יותר | הוספת ניקוד: אם הנחיה > N תווים או regex תואם תחום → הסלמה למודל גדול יותר |
| רישום יכולות דינמי | טעינת קטלוג בזמן אמת | ללא הפעלה מחדש | טעינת `catalog.json` בזמן ריצה; מעקב אחר חותמת זמן קובץ |
| אסטרטגיית גיבוי | עמידות תחת כשלונות | זמינות גבוהה יותר | ניסיון ראשוני → במקרה של חריגה מעבר לכינוי גיבוי |
| צינור הזרמה | משוב מוקדם | שיפור חוויית משתמש | הזרמת כל שלב ואגירת קלט שיפור סופי |
| הטמעות כוונה וקטוריות | ניתוב מעודן יותר | דיוק כוונה גבוה יותר | הטמעת הנחיה, קיבוץ ומיפוי מרכז → יכולת |
| ייצוא מעקב | שרשרת ניתנת לביקורת | תאימות/דיווח | פליטת שורות JSON: שלב, כוונה, מודל, זמן תגובה_ms, טוקנים |
| סימולציית עלות | הערכה לפני ענן | תכנון תקציב | הקצאת עלות נומינלית/טוקן לכל מודל וסיכום לכל משימה |
| מצב דטרמיניסטי | שחזור רפרודוקציה | מדדי ביצוע יציבים | סביבה: `temperature=0`, מספר שלבים קבוע |

#### דוגמת מבנה מעקב

```python
trace.append({
  "step": idx,
  "intent": intent,
  "alias": alias,
  "latency_ms": round((end-start)*1000,2),
  "tokens": getattr(usage,'total_tokens',None)
})
```


#### סקיצה להסלמה אדפטיבית

```python
if len(prompt) > 280 or 'compliance' in prompt.lower():
    # escalate to larger reasoning model if available
    alias = 'gpt-oss-20b'
```


#### טעינת קטלוג חם

```python
import json, time, os
CATALOG_PATH = 'catalog.json'
last_mtime = 0
def get_catalog():
    global last_mtime, CATALOG
    m = os.path.getmtime(CATALOG_PATH)
    if m != last_mtime:
        CATALOG = json.load(open(CATALOG_PATH))
        last_mtime = m
    return CATALOG
```


התקדמו בהדרגה—הימנעו מהנדסת יתר בפרוטוטיפים מוקדמים.

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי-דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור הסמכותי. למידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי בני אדם. איננו נושאים באחריות לאי-הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.