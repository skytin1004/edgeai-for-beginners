<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "949045c54206448d55d987e8b23a82cf",
  "translation_date": "2025-09-25T00:00:26+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "he"
}
-->
# ××¤×’×© 4: ×‘× ×™×™×ª ××¤×œ×™×§×¦×™×•×ª ×¦'××˜ ××•×›× ×•×ª ×œ×™×™×¦×•×¨ ×¢× Chainlit

## ×¡×§×™×¨×” ×›×œ×œ×™×ª

×”××¤×’×© ×”×–×” ××ª××§×“ ×‘×‘× ×™×™×ª ××¤×œ×™×§×¦×™×•×ª ×¦'××˜ ××•×›× ×•×ª ×œ×™×™×¦×•×¨ ×‘×××¦×¢×•×ª Chainlit ×•-Microsoft Foundry Local. ×ª×œ××“×• ×œ×™×¦×•×¨ ×××©×§×™ ××™× ×˜×¨× ×˜ ××•×“×¨× ×™×™× ×œ×©×™×—×•×ª AI, ×œ×™×™×©× ×ª×’×•×‘×•×ª ×–×•×¨××•×ª, ×•×œ×¤×¨×•×¡ ××¤×œ×™×§×¦×™×•×ª ×¦'××˜ ×™×¦×™×‘×•×ª ×¢× ×˜×™×¤×•×œ × ×›×•×Ÿ ×‘×©×’×™××•×ª ×•×¢×™×¦×•×‘ ×—×•×•×™×™×ª ××©×ª××©.

**××” ×ª×‘× ×•:**
- **××¤×œ×™×§×¦×™×™×ª ×¦'××˜ Chainlit**: ×××©×§ ××™× ×˜×¨× ×˜ ××•×“×¨× ×™ ×¢× ×ª×’×•×‘×•×ª ×–×•×¨××•×ª
- **×“××• WebGPU**: ×”×¡×§×ª ××¡×§× ×•×ª ×‘×“×¤×“×¤×Ÿ ×œ××¤×œ×™×§×¦×™×•×ª ×¤×¨×˜×™×•×ª  
- **××™× ×˜×’×¨×¦×™×” ×¢× Open WebUI**: ×××©×§ ×¦'××˜ ××§×¦×•×¢×™ ×¢× Foundry Local
- **×ª×‘× ×™×•×ª ×™×™×¦×•×¨**: ×˜×™×¤×•×œ ×‘×©×’×™××•×ª, × ×™×˜×•×¨ ×•××¡×˜×¨×˜×’×™×•×ª ×¤×¨×™×¡×”

## ××˜×¨×•×ª ×œ××™×“×”

- ×‘× ×™×™×ª ××¤×œ×™×§×¦×™×•×ª ×¦'××˜ ××•×›× ×•×ª ×œ×™×™×¦×•×¨ ×¢× Chainlit
- ×™×™×©×•× ×ª×’×•×‘×•×ª ×–×•×¨××•×ª ×œ×©×™×¤×•×¨ ×—×•×•×™×™×ª ×”××©×ª××©
- ×©×œ×™×˜×” ×‘×ª×‘× ×™×•×ª ××™× ×˜×’×¨×¦×™×” ×©×œ Foundry Local SDK
- ×™×™×©×•× ×˜×™×¤×•×œ × ×›×•×Ÿ ×‘×©×’×™××•×ª ×•×”×ª××•×“×“×•×ª ×¢× ×ª×§×œ×•×ª
- ×¤×¨×™×¡×” ×•×§×•× ×¤×™×’×•×¨×¦×™×” ×©×œ ××¤×œ×™×§×¦×™×•×ª ×¦'××˜ ×œ×¡×‘×™×‘×•×ª ×©×•× ×•×ª
- ×”×‘× ×ª ×ª×‘× ×™×•×ª ×××©×§ ××™× ×˜×¨× ×˜ ××•×“×¨× ×™×•×ª ×œ-AI ×©×™×—×ª×™

## ×“×¨×™×©×•×ª ××§×“×™××•×ª

- **Foundry Local**: ××•×ª×§×Ÿ ×•×¤×•×¢×œ ([××“×¨×™×š ×”×ª×§× ×”](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: ×’×¨×¡×” 3.10 ××• ×××•×—×¨×ª ×™×•×ª×¨ ×¢× ×™×›×•×œ×ª ×¡×‘×™×‘×” ×•×™×¨×˜×•××œ×™×ª
- **××•×“×œ**: ×œ×¤×—×•×ª ××•×“×œ ××—×“ ×˜×¢×•×Ÿ (`foundry model run phi-4-mini`)
- **×“×¤×“×¤×Ÿ**: ×“×¤×“×¤×Ÿ ××™× ×˜×¨× ×˜ ××•×“×¨× ×™ ×¢× ×ª××™×›×” ×‘-WebGPU (Chrome/Edge)
- **Docker**: ×œ××™× ×˜×’×¨×¦×™×” ×¢× Open WebUI (××•×¤×¦×™×•× ×œ×™)

## ×—×œ×§ 1: ×”×‘× ×ª ××¤×œ×™×§×¦×™×•×ª ×¦'××˜ ××•×“×¨× ×™×•×ª

### ×¡×§×™×¨×ª ××¨×›×™×˜×§×˜×•×¨×”

```
User Browser â†â†’ Chainlit UI â†â†’ Python Backend â†â†’ Foundry Local â†â†’ AI Model
      â†“              â†“              â†“              â†“            â†“
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### ×˜×›× ×•×œ×•×’×™×•×ª ××¨×›×–×™×•×ª

**×ª×‘× ×™×•×ª Foundry Local SDK:**
- `FoundryLocalManager(alias)`: × ×™×”×•×œ ×©×™×¨×•×ª ××•×˜×•××˜×™
- `manager.endpoint` ×•-`manager.api_key`: ×¤×¨×˜×™ ×—×™×‘×•×¨
- `manager.get_model_info(alias).id`: ×–×™×”×•×™ ××•×“×œ

**××¡×’×¨×ª Chainlit:**
- `@cl.on_chat_start`: ××ª×—×•×œ ×¡×©×Ÿ ×¦'××˜
- `@cl.on_message`: ×˜×™×¤×•×œ ×‘×”×•×“×¢×•×ª ××©×ª××© × ×›× ×¡×•×ª  
- `cl.Message().stream_token()`: ×–×¨×™××” ×‘×–××Ÿ ×××ª
- ×™×¦×™×¨×ª ×××©×§ ××©×ª××© ××•×˜×•××˜×™×ª ×•× ×™×”×•×œ WebSocket

## ×—×œ×§ 2: ××˜×¨×™×¦×ª ×”×—×œ×˜×•×ª ××§×•××™ ××•×œ ×¢× ×Ÿ

### ×××¤×™×™× ×™ ×‘×™×¦×•×¢×™×

| ×”×™×‘×˜ | ××§×•××™ (Foundry) | ×¢× ×Ÿ (Azure OpenAI) |
|------|-----------------|--------------------|
| **×–××Ÿ ×ª×’×•×‘×”** | ğŸš€ 50-200ms (×œ×œ× ×¨×©×ª) | â±ï¸ 200-2000ms (×ª×œ×•×™ ×‘×¨×©×ª) |
| **×¤×¨×˜×™×•×ª** | ğŸ”’ × ×ª×•× ×™× ×œ× ×¢×•×–×‘×™× ××ª ×”××›×©×™×¨ | âš ï¸ × ×ª×•× ×™× × ×©×œ×—×™× ×œ×¢× ×Ÿ |
| **×¢×œ×•×ª** | ğŸ’° ×—×™× × ×œ××—×¨ ×¨×›×™×©×ª ×—×•××¨×” | ğŸ’¸ ×ª×©×œ×•× ×œ×¤×™ ×˜×•×§×Ÿ |
| **××•×¤×œ×™×™×Ÿ** | âœ… ×¢×•×‘×“ ×œ×œ× ××™× ×˜×¨× ×˜ | âŒ ×“×•×¨×© ××™× ×˜×¨× ×˜ |
| **×’×•×“×œ ××•×“×œ** | âš ï¸ ××•×’×‘×œ ×œ×¤×™ ×—×•××¨×” | âœ… ×’×™×©×” ×œ××•×“×œ×™× ×”×’×“×•×œ×™× ×‘×™×•×ª×¨ |
| **×¡×§×™×™×œ×™× ×’** | âš ï¸ ×ª×œ×•×™ ×‘×—×•××¨×” | âœ… ×¡×§×™×™×œ×™× ×’ ×‘×œ×ª×™ ××•×’×‘×œ |

### ×ª×‘× ×™×•×ª ××¡×˜×¨×˜×’×™×” ×”×™×‘×¨×™×“×™×ª

**××§×•××™ ×ª×—×™×œ×” ×¢× ×’×™×‘×•×™:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**× ×™×ª×•×‘ ××‘×•×¡×¡ ××©×™××”:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## ×—×œ×§ 3: ×“×•×’××” 04 - ××¤×œ×™×§×¦×™×™×ª ×¦'××˜ Chainlit

### ×”×ª×—×œ×” ××”×™×¨×”

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

×”××¤×œ×™×§×¦×™×” × ×¤×ª×—×ª ××•×˜×•××˜×™×ª ×‘×›×ª×•×‘×ª `http://localhost:8080` ×¢× ×××©×§ ×¦'××˜ ××•×“×¨× ×™.

### ×™×™×©×•× ××¨×›×–×™

××¤×œ×™×§×¦×™×™×ª ×“×•×’××” 04 ××“×’×™××” ×ª×‘× ×™×•×ª ××•×›× ×•×ª ×œ×™×™×¦×•×¨:

**×’×™×œ×•×™ ×©×™×¨×•×ª ××•×˜×•××˜×™:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**×× ×”×œ ×¦'××˜ ×–×•×¨×:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### ××¤×©×¨×•×™×•×ª ×§×•× ×¤×™×’×•×¨×¦×™×”

**××©×ª× ×™ ×¡×‘×™×‘×”:**

| ××©×ª× ×” | ×ª×™××•×¨ | ×‘×¨×™×¨×ª ××—×“×œ | ×“×•×’××” |
|-------|-------|------------|-------|
| `MODEL` | ×›×™× ×•×™ ××•×“×œ ×œ×©×™××•×© | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | × ×§×•×“×ª ×§×¦×” ×©×œ Foundry Local | ××–×•×”×” ××•×˜×•××˜×™×ª | `http://localhost:51211` |
| `API_KEY` | ××¤×ª×— API (××•×¤×¦×™×•× ×œ×™ ×œ××§×•××™) | `""` | `your-api-key` |

**×©×™××•×© ××ª×§×“×:**
```cmd
# Use different model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## ×—×œ×§ 4: ×™×¦×™×¨×” ×•×©×™××•×© ×‘××—×‘×¨×•×ª Jupyter

### ×¡×§×™×¨×” ×›×œ×œ×™×ª ×©×œ ×ª××™×›×ª ××—×‘×¨×•×ª

×“×•×’××” 04 ×›×•×œ×œ×ª ××—×‘×¨×ª Jupyter ××§×™×¤×” (`chainlit_app.ipynb`) ×©××¡×¤×§×ª:

- **ğŸ“š ×ª×•×›×Ÿ ×œ×™××•×“×™**: ×—×•××¨×™ ×œ×™××•×“ ×©×œ×‘ ××—×¨ ×©×œ×‘
- **ğŸ”¬ ×—×§×¨ ××™× ×˜×¨××§×˜×™×‘×™**: ×”×¨×¦×ª ×ª××™× ×•× ×™×¡×•×™ ×‘×§×•×“
- **ğŸ“Š ×”×“×’××•×ª ×—×–×•×ª×™×•×ª**: ×’×¨×¤×™×, ×“×™××’×¨××•×ª ×•×”×“××™×™×ª ×¤×œ×˜
- **ğŸ› ï¸ ×›×œ×™ ×¤×™×ª×•×—**: ×‘×“×™×§×•×ª ×•×™×›×•×œ×•×ª ×“×™×‘×•×’

### ×™×¦×™×¨×ª ××—×‘×¨×•×ª ××©×œ×›×

#### ×©×œ×‘ 1: ×”×’×“×¨×ª ×¡×‘×™×‘×ª Jupyter

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### ×©×œ×‘ 2: ×™×¦×™×¨×ª ××—×‘×¨×ª ×—×“×©×”

**×©×™××•×© ×‘-VS Code:**
1. ×¤×ª×—×• ××ª VS Code ×‘×ª×™×§×™×™×ª Module08
2. ×¦×¨×• ×§×•×‘×¥ ×—×“×© ×¢× ×¡×™×•××ª `.ipynb`
3. ×‘×—×¨×• ××ª "Foundry Local" ×›×§×¨× ×œ ×›××©×¨ ×ª×ª×‘×§×©×•
4. ×”×ª×—×™×œ×• ×œ×”×•×¡×™×£ ×ª××™× ×¢× ×”×ª×•×›×Ÿ ×©×œ×›×

**×©×™××•×© ×‘-Jupyter Lab:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### ×ª×‘× ×™×•×ª ××‘× ×” ××—×‘×¨×ª

#### ××¨×’×•×Ÿ ×ª××™×

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("âœ… Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### ×“×•×’×××•×ª ××™× ×˜×¨××§×˜×™×‘×™×•×ª ×•×ª×¨×’×™×œ×™×

#### ×ª×¨×’×™×œ 1: ×‘×“×™×§×ª ×§×•× ×¤×™×’×•×¨×¦×™×™×ª ×œ×§×•×—

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b-instruct"},
]

for config in configurations:
    print(f"\nğŸ§ª Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'âœ… Success' if result['status'] == 'ok' else 'âŒ Failed'}")
```

#### ×ª×¨×’×™×œ 2: ×¡×™××•×œ×¦×™×™×ª ×ª×’×•×‘×” ×–×•×¨××ª

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("ğŸŒŠ Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\nâœ… Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## ×—×œ×§ 5: ×“××• ×”×¡×§×ª ××¡×§× ×•×ª ×‘×“×¤×“×¤×Ÿ ×¢× WebGPU

### ×¡×§×™×¨×” ×›×œ×œ×™×ª

WebGPU ×××¤×©×¨ ×”×¨×¦×ª ××•×“×œ×™× ×©×œ AI ×™×©×™×¨×•×ª ×‘×“×¤×“×¤×Ÿ ×œ××§×¡×™××•× ×¤×¨×˜×™×•×ª ×•×—×•×•×™×™×ª ××¤×¡ ×”×ª×§× ×”. ×”×“×•×’××” ×”×–×• ××“×’×™××” ×©×™××•×© ×‘-ONNX Runtime Web ×¢× ×‘×™×¦×•×¢ WebGPU.

### ×©×œ×‘ 1: ×‘×“×™×§×ª ×ª××™×›×ª WebGPU

**×“×¨×™×©×•×ª ×“×¤×“×¤×Ÿ:**
- Chrome/Edge ×’×¨×¡×” 113+ ×¢× WebGPU ××•×¤×¢×œ
- ×‘×“×™×§×”: `chrome://gpu` â†’ ××™×©×•×¨ ×¡×˜×˜×•×¡ "WebGPU"
- ×‘×“×™×§×” ×ª×›× ×•×ª×™×ª: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### ×©×œ×‘ 2: ×™×¦×™×¨×ª ×“××• WebGPU

×¦×¨×• ×ª×™×§×™×™×”: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>ğŸš€ WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = 'âŒ WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'ğŸ” WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('âœ… ONNX Runtime session created with WebGPU');
        log(`ğŸ“Š Input names: ${session.inputNames.join(', ')}`);
        log(`ğŸ“Š Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = 'âœ… WebGPU inference complete!';
        log(`ğŸ¯ Predicted class: ${maxIdx}`);
        log(`ğŸ“ˆ Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `âŒ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### ×©×œ×‘ 3: ×”×¨×¦×ª ×”×“××•

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## ×—×œ×§ 6: ××™× ×˜×’×¨×¦×™×” ×¢× Open WebUI

### ×¡×§×™×¨×” ×›×œ×œ×™×ª

Open WebUI ××¡×¤×§ ×××©×§ ××§×¦×•×¢×™ ×‘×¡×’× ×•×Ÿ ChatGPT ×©××ª×—×‘×¨ ×œ-Foundry Local ×“×¨×š API ×ª×•×× OpenAI.

### ×©×œ×‘ 1: ×“×¨×™×©×•×ª ××§×“×™××•×ª

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### ×©×œ×‘ 2: ×”×’×“×¨×ª Docker (××•××œ×¥)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**×”×¢×¨×”:** `host.docker.internal` ×××¤×©×¨ ×œ××™×›×œ×™ Docker ×œ×’×©×ª ×œ××—×©×‘ ×”×××¨×— ×‘-Windows.

### ×©×œ×‘ 3: ×§×•× ×¤×™×’×•×¨×¦×™×”

1. **×¤×ª×—×• ×“×¤×“×¤×Ÿ:** × ×•×•×˜×• ×œ×›×ª×•×‘×ª `http://localhost:3000`
2. **×”×’×“×¨×” ×¨××©×•× ×™×ª:** ×¦×¨×• ×—×©×‘×•×Ÿ ×× ×”×œ
3. **×§×•× ×¤×™×’×•×¨×¦×™×™×ª ××•×“×œ:**
   - ×”×’×“×¨×•×ª â†’ ××•×“×œ×™× â†’ OpenAI API  
   - ×›×ª×•×‘×ª ×‘×¡×™×¡: `http://host.docker.internal:51211/v1`
   - ××¤×ª×— API: `foundry-local-key` (×›×œ ×¢×¨×š ×™×¢×‘×•×“)
4. **×‘×“×™×§×ª ×—×™×‘×•×¨:** ×”××•×“×œ×™× ×××•×¨×™× ×œ×”×•×¤×™×¢ ×‘×ª×¤×¨×™×˜ ×”× ×¤×ª×—

### ×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª

**×‘×¢×™×•×ª × ×¤×•×¦×•×ª:**

1. **×—×™×‘×•×¨ × ×“×—×”:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **××•×“×œ×™× ×œ× ××•×¤×™×¢×™×:**
   - ×•×“××• ×©×”××•×“×œ ×˜×¢×•×Ÿ: `foundry model list`
   - ×‘×“×§×• ×ª×’×•×‘×ª API: `curl http://localhost:51211/v1/models`
   - ××ª×—×œ×• ××ª ××™×›×œ Open WebUI

## ×—×œ×§ 7: ×©×™×§×•×œ×™ ×¤×¨×™×¡×” ×œ×™×™×¦×•×¨

### ×§×•× ×¤×™×’×•×¨×¦×™×™×ª ×¡×‘×™×‘×”

**×”×’×“×¨×ª ×¤×™×ª×•×—:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**×¤×¨×™×¡×” ×œ×™×™×¦×•×¨:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### ×‘×¢×™×•×ª × ×¤×•×¦×•×ª ×¢× ×¤×•×¨×˜×™× ×•×¤×ª×¨×•× ×•×ª

**×× ×™×¢×ª ×§×•× ×¤×œ×™×§×˜ ×¤×•×¨×˜ 51211:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### × ×™×˜×•×¨ ×‘×™×¦×•×¢×™×

**×™×™×©×•× ×‘×“×™×§×ª ×‘×¨×™××•×ª:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## ×¡×™×›×•×

××¤×’×© 4 ×¢×¡×§ ×‘×‘× ×™×™×ª ××¤×œ×™×§×¦×™×•×ª Chainlit ××•×›× ×•×ª ×œ×™×™×¦×•×¨ ×¢×‘×•×¨ AI ×©×™×—×ª×™. ×œ××“×ª× ×¢×œ:

- âœ… **××¡×’×¨×ª Chainlit**: ×××©×§ ××•×“×¨× ×™ ×•×ª××™×›×” ×‘×–×¨×™××” ×œ××¤×œ×™×§×¦×™×•×ª ×¦'××˜
- âœ… **××™× ×˜×’×¨×¦×™×” ×¢× Foundry Local**: ×©×™××•×© ×‘-SDK ×•×ª×‘× ×™×•×ª ×§×•× ×¤×™×’×•×¨×¦×™×”  
- âœ… **×”×¡×§×ª ××¡×§× ×•×ª ×¢× WebGPU**: AI ×‘×“×¤×“×¤×Ÿ ×œ××§×¡×™××•× ×¤×¨×˜×™×•×ª
- âœ… **×”×’×“×¨×ª Open WebUI**: ×¤×¨×™×¡×ª ×××©×§ ×¦'××˜ ××§×¦×•×¢×™
- âœ… **×ª×‘× ×™×•×ª ×™×™×¦×•×¨**: ×˜×™×¤×•×œ ×‘×©×’×™××•×ª, × ×™×˜×•×¨ ×•×¡×§×™×™×œ×™× ×’

××¤×œ×™×§×¦×™×™×ª ×“×•×’××” 04 ××“×’×™××” ××ª ×”×©×™×˜×•×ª ×”×˜×•×‘×•×ª ×‘×™×•×ª×¨ ×œ×‘× ×™×™×ª ×××©×§×™ ×¦'××˜ ×™×¦×™×‘×™× ×©×× ×¦×œ×™× ××•×“×œ×™× AI ××§×•××™×™× ×“×¨×š Microsoft Foundry Local ×ª×•×š ××ª×Ÿ ×—×•×•×™×™×ª ××©×ª××© ××¦×•×™× ×ª.

## ××§×•×¨×•×ª

- **[×“×•×’××” 04: ××¤×œ×™×§×¦×™×™×ª Chainlit](samples/04/README.md)**: ××¤×œ×™×§×¦×™×” ××œ××” ×¢× ×ª×™×¢×•×“
- **[××—×‘×¨×ª ×œ×™××•×“×™×ª ×©×œ Chainlit](samples/04/chainlit_app.ipynb)**: ×—×•××¨×™ ×œ×™××•×“ ××™× ×˜×¨××§×˜×™×‘×™×™×
- **[×ª×™×¢×•×“ Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: ×ª×™×¢×•×“ ××œ× ×©×œ ×”×¤×œ×˜×¤×•×¨××”
- **[×ª×™×¢×•×“ Chainlit](https://docs.chainlit.io/)**: ×ª×™×¢×•×“ ×¨×©××™ ×©×œ ×”××¡×’×¨×ª
- **[××“×¨×™×š ××™× ×˜×’×¨×¦×™×” ×¢× Open WebUI](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: ××“×¨×™×š ×¨×©××™

---

