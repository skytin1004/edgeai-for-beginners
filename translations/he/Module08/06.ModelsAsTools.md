<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "33ecd8ecf0e9347a2b4839a9916e49fb",
  "translation_date": "2025-10-01T00:52:16+00:00",
  "source_file": "Module08/06.ModelsAsTools.md",
  "language_code": "he"
}
-->
## סקירה כללית

התייחסו למודלים של AI ככלים מודולריים הניתנים להתאמה אישית שפועלים ישירות על המכשיר באמצעות Foundry Local. המפגש הזה מדגיש תהליכי עבודה מעשיים לשמירה על פרטיות, חיזוי בעל זמן תגובה נמוך וכיצד לשלב את הכלים הללו באמצעות SDKs, APIs או CLI. תלמדו גם כיצד להתרחב ל-Azure AI Foundry בעת הצורך.

> **🔄 מעודכן ל-SDK מודרני**: המודול הזה הותאם לדפוסי המאגר העדכניים ביותר של Microsoft Foundry-Local ומתאים ליישום הניתוב החכם ב-`samples/06/`. הדוגמאות משתמשות כעת ב-`foundry-local-sdk` המודרני ובאסטרטגיות מתקדמות לבחירת מודלים.

**🏗️ עיקרי הארכיטקטורה:**
- **ניתוב מודלים חכם**: בחירה מבוססת מילות מפתח בין מודלים כלליים, הסקת מסקנות, קוד ויצירתיות
- **שילוב SDK מודרני**: שימוש ב-`FoundryLocalManager` עם גילוי שירות אוטומטי
- **הגדרת סביבה**: הקצאת מודלים גמישה באמצעות משתני סביבה
- **מעקב בריאות**: אימות שירות ובדיקת זמינות מודלים
- **מוכן לייצור**: טיפול מקיף בשגיאות ומנגנוני גיבוי

**📁 יישום מקומי:**
- `samples/06/router.py` - נתב מודלים חכם עם בחירה מבוססת מילות מפתח
- `samples/06/model_router.ipynb` - דוגמאות אינטראקטיביות ומבחני ביצועים
- `samples/06/README.md` - הוראות הגדרה ושימוש

קישורים:
- תיעוד Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- שילוב עם SDKs לחיזוי: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- הידור מודלים של Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## סקירה כללית

התייחסו למודלים של AI ככלים מודולריים הניתנים להתאמה אישית שפועלים ישירות על המכשיר באמצעות Foundry Local. המפגש הזה מדגיש תהליכי עבודה מעשיים לשמירה על פרטיות, חיזוי בעל זמן תגובה נמוך וכיצד לשלב את הכלים הללו באמצעות SDKs, APIs או CLI. תלמדו גם כיצד להתרחב ל-Azure AI Foundry בעת הצורך.

קישורים:
- תיעוד Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- שילוב עם SDKs לחיזוי: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- הידור מודלים של Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## מטרות למידה
- עיצוב דפוסים של מודלים ככלים על המכשיר
- שילוב באמצעות REST API תואם OpenAI או SDKs
- התאמת מודלים למקרי שימוש ספציפיים לתחום
- תכנון להתרחבות היברידית ל-Azure AI Foundry

## חלק 1: נתב מודלים חכם (יישום מודרני)

מטרה: יישום בחירה חכמה של מודלים עם ניתוב אוטומטי המבוסס על תוכן השאילתה.

> **📋 הערה**: יישום זה מתאים לדפוסים המשמשים ב-`samples/06/router.py` עם בחירה מתקדמת מבוססת מילות מפתח.

שלב 1) הגדרת נתב מודלים מודרני עם FoundryLocalManager  
```python
# router/intelligent_router.py
from foundry_local import FoundryLocalManager
from openai import OpenAI
from typing import Dict, Any, Optional
import os
import json

class ModelRouter:
    """Intelligent model router that selects appropriate models for different task types."""
    
    def __init__(self):
        self.client = None
        self.base_url = None
        self.tools = self._load_tool_registry()
        self._initialize_client()
    
    def _load_tool_registry(self) -> Dict[str, Dict[str, Any]]:
        """Load tool registry from environment or use defaults."""
        default_tools = {
            "general": {
                "model": os.environ.get("GENERAL_MODEL", "phi-4-mini"),
                "notes": "Fast general-purpose chat and Q&A",
                "temperature": 0.7
            },
            "reasoning": {
                "model": os.environ.get("REASONING_MODEL", "deepseek-r1-7b"),
                "notes": "Step-by-step analysis and logical reasoning",
                "temperature": 0.3
            },
            "code": {
                "model": os.environ.get("CODE_MODEL", "qwen2.5-7b"),
                "notes": "Code generation, debugging, and technical tasks",
                "temperature": 0.2
            },
            "creative": {
                "model": os.environ.get("CREATIVE_MODEL", "phi-4-mini"),
                "notes": "Creative writing and storytelling",
                "temperature": 0.9
            }
        }
        
        # Check for environment override
        tools_env = os.environ.get("TOOL_REGISTRY")
        if tools_env:
            try:
                return json.loads(tools_env)
            except json.JSONDecodeError:
                print("Warning: Invalid TOOL_REGISTRY JSON, using defaults")
        
        return default_tools
```
  
שלב 2) אתחול לקוח עם SDK מודרני וגילוי שירות  
```python
    def _initialize_client(self):
        """Initialize OpenAI client with Foundry Local or fallback configuration."""
        try:
            from foundry_local import FoundryLocalManager
            # Try to use any available model for client initialization
            first_model = next(iter(self.tools.values()))["model"]
            manager = FoundryLocalManager(first_model)
            
            self.client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            self.base_url = manager.endpoint
            print(f"✅ Foundry Local SDK initialized")
        except Exception as e:
            print(f"Warning: Could not use Foundry SDK ({e}), falling back to manual configuration")
            # Fallback to manual configuration
            self.base_url = os.environ.get("BASE_URL", "http://localhost:8000")
            api_key = os.environ.get("API_KEY", "")
            
            self.client = OpenAI(
                base_url=f"{self.base_url}/v1",
                api_key=api_key
            )
            print(f"Initialized manual configuration at {self.base_url}")
    
    def select_tool(self, user_query: str) -> str:
        """Select the most appropriate tool based on the user query."""
        query_lower = user_query.lower()
        
        # Code-related keywords
        code_keywords = ["code", "python", "function", "class", "method", "bug", "debug", 
                        "programming", "script", "algorithm", "implementation", "refactor"]
        if any(keyword in query_lower for keyword in code_keywords):
            return "code"
        
        # Reasoning keywords
        reasoning_keywords = ["why", "how", "explain", "step-by-step", "reason", "analyze", 
                             "think", "logic", "because", "cause", "compare", "evaluate"]
        if any(keyword in query_lower for keyword in reasoning_keywords):
            return "reasoning"
        
        # Creative keywords
        creative_keywords = ["story", "poem", "creative", "imagine", "write", "tale", 
                           "narrative", "fiction", "character", "plot"]
        if any(keyword in query_lower for keyword in creative_keywords):
            return "creative"
        
        # Default to general
        return "general"
    
    def chat(self, model: str, content: str, max_tokens: int = 300, temperature: Optional[float] = None) -> str:
        """Send chat completion request to the specified model."""
        try:
            params = {
                "model": model,
                "messages": [{"role": "user", "content": content}],
                "max_tokens": max_tokens
            }
            
            if temperature is not None:
                params["temperature"] = temperature
            
            response = self.client.chat.completions.create(**params)
            return response.choices[0].message.content
        except Exception as e:
            return f"Error generating response with model {model}: {str(e)}"
```
  
שלב 3) יישום ניתוב חכם וביצוע (ראו `samples/06/router.py`)  
```python
    def route_and_run(self, prompt: str) -> Dict[str, Any]:
        """Route the prompt to the appropriate model and generate response."""
        tool_key = self.select_tool(prompt)
        tool_config = self.tools[tool_key]
        model = tool_config["model"]
        temperature = tool_config.get("temperature", 0.7)
        
        print(f"🎯 Selected tool: {tool_key} (model: {model})")
        
        answer = self.chat(
            model=model, 
            content=prompt, 
            max_tokens=400, 
            temperature=temperature
        )
        
        return {
            "tool": tool_key,
            "model": model,
            "tool_description": tool_config["notes"],
            "temperature": temperature,
            "answer": answer
        }
    
    def check_service_health(self) -> Dict[str, Any]:
        """Check Foundry Local service health and available models."""
        try:
            models_response = self.client.models.list()
            available_models = [model.id for model in models_response.data]
            
            return {
                "status": "healthy",
                "base_url": self.base_url,
                "available_models": available_models,
                "tools_configured": list(self.tools.keys())
            }
        except Exception as e:
            return {
                "status": "error",
                "base_url": self.base_url,
                "error": str(e)
            }

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    router = ModelRouter()
    
    # Check health
    health = router.check_service_health()
    print(f"Service Health: {json.dumps(health, indent=2)}")
    
    # Test different query types
    queries = [
        "Write a Python function to calculate fibonacci numbers",  # -> code
        "Explain step-by-step why the sky is blue",  # -> reasoning
        "Tell me a creative story about AI",  # -> creative
        "What's the weather like today?"  # -> general
    ]
    
    for query in queries:
        result = router.route_and_run(query)
        print(f"\nQuery: {query}")
        print(f"Selected: {result['tool']} -> {result['model']}")
        print(f"Answer: {result['answer'][:100]}...")
```
  

## חלק 2: שילוב SDK מודרני (שלב אחר שלב)

מטרה: שימוש ב-SDK של Foundry Local עם OpenAI Python SDK לשילוב חלק.

שלב 1) התקנת תלות  
```cmd
cd Module08
.\.venv\Scripts\activate
pip install foundry-local-sdk openai
```
  
שלב 2) הגדרת סביבה (אופציונלי - ראו `samples/06/README.md`)  
```cmd
REM Override default models per tool
set GENERAL_MODEL=phi-4-mini
set REASONING_MODEL=deepseek-r1-7b
set CODE_MODEL=qwen2.5-7b
REM Or provide a full JSON registry
set TOOL_REGISTRY={"general":{"model":"phi-4-mini"},"reasoning":{"model":"deepseek-r1-7b"}}
```
  
שלב 3) שילוב SDK מודרני  
```python
# modern_sdk_demo.py
from foundry_local import FoundryLocalManager
from openai import OpenAI
import sys

def main():
    """Demonstrate modern SDK integration."""
    try:
        # Initialize with FoundryLocalManager
        alias = "phi-4-mini"
        manager = FoundryLocalManager(alias)
        
        # Create OpenAI client using Foundry Local endpoint
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Get model info
        model_info = manager.get_model_info(alias)
        print(f"Using model: {model_info.id}")
        
        # Make request with streaming
        stream = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Explain edge AI benefits in one paragraph."}],
            stream=True,
            max_tokens=200
        )
        
        print("Response: ", end="")
        for chunk in stream:
            if chunk.choices[0].delta.content:
                print(chunk.choices[0].delta.content, end="", flush=True)
        print()
        
    except Exception as e:
        print(f"Error: {e}")
        print("Ensure Foundry Local is running with: foundry model run phi-4-mini")
        sys.exit(1)

if __name__ == "__main__":
    main()
```
  

## חלק 3: התאמה לתחום (שלב אחר שלב)

מטרה: התאמת הפלטים לתחום באמצעות תבניות הנחיה וסכמת JSON.

שלב 1) יצירת תבנית הנחיה לתחום  
```python
# domain/templates.py
BUSINESS_ANALYST_SYSTEM = """
You are a senior business analyst. Provide:
1) Key insights
2) Risks
3) Next steps
Respond in valid JSON with fields: insights, risks, next_steps.
"""
```
  
שלב 2) אכיפת פלט JSON  
```python
# domain/analyst.py
import requests, os, json

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}

from domain.templates import BUSINESS_ANALYST_SYSTEM

def analyze(text: str) -> dict:
    messages = [
        {"role":"system","content": BUSINESS_ANALYST_SYSTEM},
        {"role":"user","content": f"Analyze this business text:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
    "model":"phi-4-mini",
        "messages": messages,
        "response_format": {"type":"json_object"},
        "temperature": 0.3
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    # Parse JSON content
    content = r.json()["choices"][0]["message"]["content"]
    return json.loads(content)

if __name__ == "__main__":
    print(analyze("Sales dipped 12% in Q3 due to supply constraints and marketing cuts."))
```
  

## חלק 4: עבודה לא מקוונת ועמדת אבטחה (שלב אחר שלב)

מטרה: הבטחת פרטיות ועמידות בעת הפעלת מודלים ככלים באופן מקומי.

שלב 1) חימום מוקדם ואימות נקודת קצה מקומית  
```cmd
foundry model run phi-4-mini
curl http://localhost:8000/v1/models
```
  
שלב 2) ניקוי קלטים  
```python
# security/sanitize.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```
  
שלב 3) דגל מקומי בלבד ורישום  
```python
# security/local_only.py
import os, json, time
LOG = os.getenv("MODELS_AS_TOOLS_LOG", "./tools_logs.jsonl")

def record(event: dict):
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(json.dumps(event) + "\n")

# Usage before each call
def before_call(tool_name, payload):
    record({"ts": time.time(), "tool": tool_name, "event": "before_call"})

# After each call
def after_call(tool_name, result):
    record({"ts": time.time(), "tool": tool_name, "event": "after_call"})
```
  

## חלק 5: פריסה לייצור והתרחבות

מטרה: פריסת הנתב החכם עם מעקב והשתלבות ב-Azure AI Foundry.

> **📋 הערה**: היישום המקומי ב-`samples/06/model_router.ipynb` כולל דוגמאות מקיפות לדפוסי פריסה לייצור.

שלב 1) נתב לייצור עם מעקב (ראו `samples/06/router.py`)  
```python
# production/router.py
from router.intelligent_router import ModelRouter
import json
import time
import sys

class ProductionModelRouter(ModelRouter):
    """Production-ready model router with monitoring and logging."""
    
    def __init__(self):
        super().__init__()
        self.request_count = 0
        self.error_count = 0
        self.start_time = time.time()
    
    def route_and_run_with_monitoring(self, prompt: str) -> Dict[str, Any]:
        """Route with comprehensive monitoring and error handling."""
        start_time = time.time()
        self.request_count += 1
        
        try:
            result = self.route_and_run(prompt)
            processing_time = time.time() - start_time
            
            # Log successful request
            self._log_request({
                "status": "success",
                "tool": result["tool"],
                "model": result["model"],
                "processing_time": processing_time,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })
            
            result["processing_time"] = processing_time
            return result
            
        except Exception as e:
            self.error_count += 1
            error_result = {
                "status": "error",
                "error": str(e),
                "processing_time": time.time() - start_time,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            self._log_request(error_result)
            return error_result
    
    def _log_request(self, data: Dict[str, Any]):
        """Log request data for monitoring."""
        print(f"📊 {json.dumps(data)}")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get router statistics."""
        uptime = time.time() - self.start_time
        return {
            "uptime_seconds": uptime,
            "total_requests": self.request_count,
            "error_count": self.error_count,
            "success_rate": (self.request_count - self.error_count) / max(1, self.request_count),
            "requests_per_minute": self.request_count / max(1, uptime / 60)
        }

def main():
    """Production router demo."""
    router = ProductionModelRouter()
    
    # Health check
    health = router.check_service_health()
    if health["status"] == "error":
        print(f"❌ Service health check failed: {health['error']}")
        sys.exit(1)
    
    print(f"✅ Service healthy with {len(health['available_models'])} models")
    
    # Process user query
    user_prompt = " ".join(sys.argv[1:]) or "Write three benefits of on-device AI in JSON format."
    print(f"\n🎯 Processing: {user_prompt}")
    
    result = router.route_and_run_with_monitoring(user_prompt)
    
    if result.get("status") == "error":
        print(f"❌ Error: {result['error']}")
    else:
        print(f"\n📋 Result:")
        print(f"Tool: {result['tool']} -> Model: {result['model']}")
        print(f"Processing Time: {result['processing_time']:.2f}s")
        print(f"Answer: {result['answer']}")
    
    # Show stats
    stats = router.get_stats()
    print(f"\n📊 Statistics: {json.dumps(stats, indent=2)}")

if __name__ == "__main__":
    main()
```
  

## רשימת בדיקות מעשית
- [ ] יישום נתב מודלים חכם עם בחירה מבוססת מילות מפתח (`samples/06/router.py`)
- [ ] הגדרת מספר מודלים מתמחים (כללי, הסקת מסקנות, קוד, יצירתיות)
- [ ] בדיקת מחברת Jupyter אינטראקטיבית (`samples/06/model_router.ipynb`)
- [ ] הגדרת תצורת מודלים מבוססת סביבה
- [ ] יישום מעקב בריאות שירות וטיפול בשגיאות
- [ ] פריסת נתב לייצור עם רישום מקיף

## שילוב דוגמאות מקומיות

הרצת היישום המלא:  
```cmd
cd Module08
.\.venv\Scripts\activate

REM Start required models
foundry model run phi-4-mini
foundry model run qwen2.5-7b
foundry model run deepseek-r1-7b

REM Test the intelligent router
python samples\06\router.py "Write a Python function to sort a list"
python samples\06\router.py "Explain step-by-step how bubble sort works"
python samples\06\router.py "Tell me a creative story about robots"

REM Explore the interactive notebook
jupyter notebook samples/06/model_router.ipynb
```
  

## קישורים והשלבים הבאים
- **יישום מקומי**: `samples/06/` - נתב חכם מלא עם תמיכה במספר מודלים
- **דוגמאות של Microsoft**: [Hello Foundry Local](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/hello-foundry-local)
- **תיעוד שילוב**: [שילוב עם SDKs לחיזוי](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks)
- **דפוסים מתקדמים**: חקר קריאת פונקציות ותזמור רב-סוכנים במודול 5

## סיכום

Foundry Local מאפשר AI חזק על המכשיר שבו מודלים הופכים לכלים חכמים ומתמחים. עם בחירה אוטומטית של מודלים, מעקב מקיף ודפוסים מוכנים לייצור, צוותים יכולים להשיק יישומי AI מתקדמים שמתאימים לסוגי משימות שונים תוך שמירה על פרטיות וביצועים. דפוס הנתב החכם שהוצג כאן מספק בסיס לבניית מערכות AI מורכבות שיכולות להתרחב מפיתוח מקומי לפריסה לייצור.

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. אנו לא נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.