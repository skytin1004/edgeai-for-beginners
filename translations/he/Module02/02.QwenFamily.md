<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-18T12:01:51+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "he"
}
-->
# ×¡×¢×™×£ 2: ×™×¡×•×“×•×ª ××©×¤×—×ª Qwen

××©×¤×—×ª ×”××•×“×œ×™× Qwen ××™×™×¦×’×ª ××ª ×”×’×™×©×” ×”××§×™×¤×” ×©×œ Alibaba Cloud ×œ××•×“×œ×™× ×©×¤×ª×™×™× ×’×“×•×œ×™× ×•××™× ×˜×œ×™×’× ×¦×™×” ××œ××›×•×ª×™×ª ××•×œ×˜×™××•×“×œ×™×ª, ×•××“×’×™××” ×©××•×“×œ×™× ×‘×§×•×“ ×¤×ª×•×— ×™×›×•×œ×™× ×œ×”×©×™×’ ×‘×™×¦×•×¢×™× ××¨×©×™××™× ×ª×•×š ×©×”× × ×’×™×©×™× ×‘××’×•×•×Ÿ ×ª×¨×—×™×©×™ ×¤×¨×™×¡×”. ×—×©×•×‘ ×œ×”×‘×™×Ÿ ×›×™×¦×“ ××©×¤×—×ª Qwen ×××¤×©×¨×ª ×™×›×•×œ×•×ª AI ×¢×•×¦××ª×™×•×ª ×¢× ××¤×©×¨×•×™×•×ª ×¤×¨×™×¡×” ×’××™×©×•×ª ×ª×•×š ×©××™×¨×” ×¢×œ ×‘×™×¦×•×¢×™× ×ª×—×¨×•×ª×™×™× ×‘××©×™××•×ª ××’×•×•× ×•×ª.

## ××©××‘×™× ×œ××¤×ª×—×™×

### ×××’×¨ ××•×“×œ×™× ×‘-Hugging Face
××•×“×œ×™× × ×‘×—×¨×™× ×××©×¤×—×ª Qwen ×–××™× ×™× ×“×¨×š [Hugging Face](https://huggingface.co/models?search=qwen), ×•××¡×¤×§×™× ×’×™×©×” ×œ×›××” ×•×¨×™××¦×™×•×ª ×©×œ ××•×“×œ×™× ××œ×•. × ×™×ª×Ÿ ×œ×—×§×•×¨ ××ª ×”×•×•×¨×™××¦×™×•×ª ×”×–××™× ×•×ª, ×œ×”×ª××™× ××•×ª×Ÿ ×œ×¦×¨×›×™× ×”×¡×¤×¦×™×¤×™×™× ×©×œ×›× ×•×œ×¤×¨×•×¡ ××•×ª×Ÿ ×‘×××¦×¢×•×ª ××¡×’×¨×•×ª ×©×•× ×•×ª.

### ×›×œ×™× ×œ×¤×™×ª×•×— ××§×•××™
×œ×¦×•×¨×š ×¤×™×ª×•×— ×•×‘×“×™×§×” ××§×•××™×ª, × ×™×ª×Ÿ ×œ×”×©×ª××© ×‘-[Microsoft Foundry Local](https://github.com/microsoft/foundry-local) ×›×“×™ ×œ×”×¤×¢×™×œ ××ª ××•×“×œ×™ Qwen ×”×–××™× ×™× ×¢×œ ××›×•× ×ª ×”×¤×™×ª×•×— ×©×œ×›× ×¢× ×‘×™×¦×•×¢×™× ××™×˜×‘×™×™×.

### ××©××‘×™ ×ª×™×¢×•×“
- [×ª×™×¢×•×“ ××•×“×œ Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [××•×¤×˜×™××™×–×¦×™×” ×©×œ ××•×“×œ×™ Qwen ×œ×¤×¨×™×¡×” ×‘×§×¦×”](https://github.com/microsoft/olive)

## ××‘×•×

×‘××“×¨×™×š ×–×” × ×—×§×•×¨ ××ª ××©×¤×—×ª ×”××•×“×œ×™× Qwen ×©×œ Alibaba ×•××ª ×”×¢×§×¨×•× ×•×ª ×”×‘×¡×™×¡×™×™× ×©×œ×”. × ×¡×§×•×¨ ××ª ×”×”×ª×¤×ª×—×•×ª ×©×œ ××©×¤×—×ª Qwen, ××ª ×©×™×˜×•×ª ×”××™××•×Ÿ ×”×—×“×©× ×™×•×ª ×©×”×•×¤×›×•×ª ××ª ××•×“×œ×™ Qwen ×œ×™×¢×™×œ×™×, ××ª ×”×•×•×¨×™××¦×™×•×ª ×”××¨×›×–×™×•×ª ×‘××©×¤×—×” ×•××ª ×”×™×™×©×•××™× ×”××¢×©×™×™× ×‘×ª×¨×—×™×©×™× ×©×•× ×™×.

## ××˜×¨×•×ª ×œ××™×“×”

×‘×¡×™×•× ×”××“×¨×™×š, ×ª×•×›×œ×•:

- ×œ×”×‘×™×Ÿ ××ª ×¤×™×œ×•×¡×•×¤×™×™×ª ×”×¢×™×¦×•×‘ ×•×”×”×ª×¤×ª×—×•×ª ×©×œ ××©×¤×—×ª ×”××•×“×œ×™× Qwen ×©×œ Alibaba
- ×œ×–×”×•×ª ××ª ×”×—×“×©× ×•×™×•×ª ×”××¨×›×–×™×•×ª ×©×××¤×©×¨×•×ª ×œ××•×“×œ×™ Qwen ×œ×”×©×™×’ ×‘×™×¦×•×¢×™× ×’×‘×•×”×™× ×‘××’×•×•×Ÿ ×’×“×œ×™ ×¤×¨××˜×¨×™×
- ×œ×”×›×™×¨ ××ª ×”×™×ª×¨×•× ×•×ª ×•×”××’×‘×œ×•×ª ×©×œ ×•×¨×™××¦×™×•×ª ×©×•× ×•×ª ×©×œ ××•×“×œ×™ Qwen
- ×œ×™×™×©× ×™×“×¢ ×¢×œ ××•×“×œ×™ Qwen ×›×“×™ ×œ×‘×—×•×¨ ××ª ×”×•×•×¨×™××¦×™×•×ª ×”××ª××™××•×ª ×œ×ª×¨×—×™×©×™× ×‘×¢×•×œ× ×”×××™×ª×™

## ×”×‘× ×ª × ×•×£ ×”××•×“×œ×™× ×”××•×“×¨× ×™×™× ×©×œ AI

× ×•×£ ×”-AI ×”×ª×¤×ª×— ×‘××•×¤×Ÿ ××©××¢×•×ª×™, ×›××©×¨ ××¨×’×•× ×™× ×©×•× ×™× × ×•×§×˜×™× ×‘×’×™×©×•×ª ××’×•×•× ×•×ª ×œ×¤×™×ª×•×— ××•×“×œ×™× ×©×¤×ª×™×™×. ×‘×¢×•×“ ×©×—×œ×§× ××ª××§×“×™× ×‘××•×“×œ×™× ×¡×’×•×¨×™× ×•×§× ×™×™× ×™×™×, ××—×¨×™× ××“×’×™×©×™× × ×’×™×©×•×ª ×•×©×§×™×¤×•×ª ×‘×§×•×“ ×¤×ª×•×—. ×”×’×™×©×” ×”××¡×•×¨×ª×™×ª ×›×•×œ×œ×ª ××•×“×œ×™× ×§× ×™×™× ×™×™× ×¢×¦×•××™× ×”× ×’×™×©×™× ×¨×§ ×“×¨×š APIs ××• ××•×“×œ×™× ×‘×§×•×“ ×¤×ª×•×— ×©×¢×©×•×™×™× ×œ×¤×’×¨ ×××—×•×¨ ×‘×™×›×•×œ×•×ª.

×¤×¨×“×™×’××” ×–×• ×™×•×¦×¨×ª ××ª×’×¨×™× ×¢×‘×•×¨ ××¨×’×•× ×™× ×”××—×¤×©×™× ×™×›×•×œ×•×ª AI ×¢×•×¦××ª×™×•×ª ×ª×•×š ×©××™×¨×” ×¢×œ ×©×œ×™×˜×” ×‘× ×ª×•× ×™×, ×‘×¢×œ×•×™×•×ª ×•×‘×’××™×©×•×ª ×”×¤×¨×™×¡×”. ×”×’×™×©×” ×”××¡×•×¨×ª×™×ª ×œ×¢×™×ª×™× ×§×¨×•×‘×•×ª ×“×•×¨×©×ª ×‘×—×™×¨×” ×‘×™×Ÿ ×‘×™×¦×•×¢×™× ××ª×§×“××™× ×œ×‘×™×Ÿ ×©×™×§×•×œ×™ ×¤×¨×™×¡×” ××¢×©×™×™×.

## ×”××ª×’×¨ ×©×œ ××¦×•×™× ×•×ª AI × ×’×™×©×”

×”×¦×•×¨×š ×‘-AI ××™×›×•×ª×™ ×•× ×’×™×© ×”×¤×š ×œ×—×©×•×‘ ×™×•×ª×¨ ×•×™×•×ª×¨ ×‘×ª×¨×—×™×©×™× ×©×•× ×™×. ×©×§×œ×• ×™×™×©×•××™× ×”×“×•×¨×©×™× ××¤×©×¨×•×™×•×ª ×¤×¨×™×¡×” ×’××™×©×•×ª ×œ×¦×¨×›×™× ××¨×’×•× ×™×™× ×©×•× ×™×, ×™×™×©×•××™× ×—×¡×›×•× ×™×™× ×©×‘×”× ×¢×œ×•×™×•×ª API ×¢×©×•×™×•×ª ×œ×”×™×•×ª ××©××¢×•×ª×™×•×ª, ×™×›×•×œ×•×ª ×¨×‘-×œ×©×•× ×™×•×ª ×œ×™×™×©×•××™× ×’×œ×•×‘×œ×™×™×, ××• ××•××—×™×•×ª ×ª×—×•××™×ª ××™×•×—×“×ª ×‘×ª×—×•××™× ×›××• ×§×™×“×•×“ ×•××ª××˜×™×§×”.

### ×“×¨×™×©×•×ª ×¤×¨×™×¡×” ××¨×›×–×™×•×ª

×¤×¨×™×¡×•×ª AI ××•×“×¨× ×™×•×ª ××ª××•×“×“×•×ª ×¢× ××¡×¤×¨ ×“×¨×™×©×•×ª ×‘×¡×™×¡×™×•×ª ×©××’×‘×™×œ×•×ª ××ª ×”×™×™×©×•× ×”××¢×©×™:

- **× ×’×™×©×•×ª**: ×–××™× ×•×ª ×‘×§×•×“ ×¤×ª×•×— ×œ×©×§×™×¤×•×ª ×•×”×ª×××” ××™×©×™×ª
- **×—×¡×›×•× ×™×•×ª**: ×“×¨×™×©×•×ª ×—×™×©×•×‘ ×¡×‘×™×¨×•×ª ×¢×‘×•×¨ ×ª×§×¦×™×‘×™× ×©×•× ×™×
- **×’××™×©×•×ª**: ×’×“×œ×™ ××•×“×œ×™× ×©×•× ×™× ×œ×ª×¨×—×™×©×™ ×¤×¨×™×¡×” ××’×•×•× ×™×
- **×”×’×¢×” ×’×œ×•×‘×œ×™×ª**: ×™×›×•×œ×•×ª ×¨×‘-×œ×©×•× ×™×•×ª ×•×ª×¨×‘×•×ª×™×•×ª ×—×–×§×•×ª
- **×”×ª××—×•×ª**: ×•×¨×™××¦×™×•×ª ×ª×—×•××™×•×ª ×¡×¤×¦×™×¤×™×•×ª ×œ×©×™××•×©×™× ××¡×•×™××™×

## ×¤×™×œ×•×¡×•×¤×™×™×ª ××•×“×œ Qwen

××©×¤×—×ª ×”××•×“×œ×™× Qwen ××™×™×¦×’×ª ×’×™×©×” ××§×™×¤×” ×œ×¤×™×ª×•×— ××•×“×œ×™× AI, ×ª×•×š ××ª×Ÿ ×¢×“×™×¤×•×ª ×œ× ×’×™×©×•×ª ×‘×§×•×“ ×¤×ª×•×—, ×™×›×•×œ×•×ª ×¨×‘-×œ×©×•× ×™×•×ª ×•×¤×¨×™×¡×” ××¢×©×™×ª, ×ª×•×š ×©××™×¨×” ×¢×œ ×××¤×™×™× ×™ ×‘×™×¦×•×¢×™× ×ª×—×¨×•×ª×™×™×. ××•×“×œ×™ Qwen ××©×™×’×™× ×–××ª ×‘×××¦×¢×•×ª ×’×“×œ×™ ××•×“×œ×™× ××’×•×•× ×™×, ×©×™×˜×•×ª ××™××•×Ÿ ××™×›×•×ª×™×•×ª ×•×•×¨×™××¦×™×•×ª ××™×•×—×“×•×ª ×œ×ª×—×•××™× ×©×•× ×™×.

××©×¤×—×ª Qwen ×›×•×œ×œ×ª ×’×™×©×•×ª ×©×•× ×•×ª ×©× ×•×¢×“×• ×œ×¡×¤×§ ××¤×©×¨×•×™×•×ª ×œ××•×¨×š ×¡×¤×§×˜×¨×•× ×”×‘×™×¦×•×¢×™×-×™×¢×™×œ×•×ª, ×•×××¤×©×¨×•×ª ×¤×¨×™×¡×” ×××›×©×™×¨×™× × ×™×™×“×™× ×•×¢×“ ×©×¨×ª×™ ××¨×’×•× ×™× ×ª×•×š ××ª×Ÿ ×™×›×•×œ×•×ª AI ××©××¢×•×ª×™×•×ª. ×”××˜×¨×” ×”×™× ×œ×”× ×’×™×© AI ××™×›×•×ª×™ ×ª×•×š ××ª×Ÿ ×’××™×©×•×ª ×‘×‘×—×™×¨×•×ª ×”×¤×¨×™×¡×”.

### ×¢×§×¨×•× ×•×ª ×¢×™×¦×•×‘ ××¨×›×–×™×™× ×©×œ Qwen

××•×“×œ×™ Qwen ×‘× ×•×™×™× ×¢×œ ××¡×¤×¨ ×¢×§×¨×•× ×•×ª ×™×¡×•×“ ×©××‘×“×™×œ×™× ××•×ª× ×××©×¤×—×•×ª ××•×“×œ×™× ×©×¤×ª×™×™× ××—×¨×•×ª:

- **×§×•×“ ×¤×ª×•×— ×ª×—×™×œ×”**: ×©×§×™×¤×•×ª ××œ××” ×•× ×’×™×©×•×ª ×œ××—×§×¨ ×•×©×™××•×© ××¡×—×¨×™
- **××™××•×Ÿ ××§×™×£**: ××™××•×Ÿ ×¢×œ ××¢×¨×›×™ × ×ª×•× ×™× ×¢×¦×•××™× ×•××’×•×•× ×™× ×”××›×¡×™× ×©×¤×•×ª ×•×ª×—×•××™× ×¨×‘×™×
- **××¨×›×™×˜×§×˜×•×¨×” × ×™×ª× ×ª ×œ×”×¨×—×‘×”**: ×’×“×œ×™ ××•×“×œ×™× ×©×•× ×™× ×œ×”×ª×××” ×œ×“×¨×™×©×•×ª ×—×™×©×•×‘×™×•×ª ××’×•×•× ×•×ª
- **××¦×•×™× ×•×ª ××™×•×—×“×ª**: ×•×¨×™××¦×™×•×ª ×ª×—×•××™×•×ª ×¡×¤×¦×™×¤×™×•×ª ×”××•×ª×××•×ª ×œ××©×™××•×ª ××¡×•×™××•×ª

## ×˜×›× ×•×œ×•×’×™×•×ª ××¨×›×–×™×•×ª ×”×××¤×©×¨×•×ª ××ª ××©×¤×—×ª Qwen

### ××™××•×Ÿ ×‘×§× ×” ××™×“×” ×¢×¦×•×

××—×“ ×”×××¤×™×™× ×™× ×”××’×“×™×¨×™× ×©×œ ××©×¤×—×ª Qwen ×”×•× ×”×™×§×£ ×”××™××•×Ÿ ×”×¢×¦×•× ×©×œ × ×ª×•× ×™× ×•××©××‘×™× ×—×™×©×•×‘×™×™× ×©×”×•×©×§×¢×• ×‘×¤×™×ª×•×— ×”××•×“×œ×™×. ××•×“×œ×™ Qwen ×× ×¦×œ×™× ××¢×¨×›×™ × ×ª×•× ×™× ×¨×‘-×œ×©×•× ×™×™× ×©× ×‘×—×¨×• ×‘×§×¤×™×“×”, ×”×›×•×œ×œ×™× ×˜×¨×™×œ×™×•× ×™ ×˜×•×§× ×™×, ×©× ×•×¢×“×• ×œ×¡×¤×§ ×™×“×¢ ×¢×•×œ××™ ××§×™×£ ×•×™×›×•×œ×•×ª ×”×¡×§×”.

×’×™×©×” ×–×• ××©×œ×‘×ª ×ª×•×›×Ÿ ××™× ×˜×¨× ×˜ ××™×›×•×ª×™, ×¡×¤×¨×•×ª ××§×“××™×ª, ×××’×¨×™ ×§×•×“ ×•××©××‘×™× ×¨×‘-×œ×©×•× ×™×™×. ×©×™×˜×ª ×”××™××•×Ÿ ××“×’×™×©×” ×”×Ÿ ××ª ×¨×•×—×‘ ×”×™×“×¢ ×•×”×Ÿ ××ª ×¢×•××§ ×”×”×‘× ×” ×‘×ª×—×•××™× ×•×©×¤×•×ª ×©×•× ×™×.

### ×”×¡×§×” ×•×—×©×™×‘×” ××ª×§×“××•×ª

××•×“×œ×™ Qwen ×”××—×¨×•× ×™× ××©×œ×‘×™× ×™×›×•×œ×•×ª ×”×¡×§×” ××ª×•×—×›××•×ª ×©×××¤×©×¨×•×ª ×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª ××•×¨×›×‘×•×ª ×‘××¡×¤×¨ ×©×œ×‘×™×:

**××¦×‘ ×—×©×™×‘×” (Qwen3)**: ×”××•×“×œ×™× ×™×›×•×œ×™× ×œ×¢×¡×•×§ ×‘×”×¡×§×” ××¤×•×¨×˜×ª ×©×œ×‘ ××—×¨ ×©×œ×‘ ×œ×¤× ×™ ××ª×Ÿ ×ª×©×•×‘×•×ª ×¡×•×¤×™×•×ª, ×‘×“×•××” ×œ×’×™×©×•×ª ×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª ×× ×•×©×™×•×ª.

**×¤×¢×•×œ×” ×“×•-××¦×‘×™×ª**: ×™×›×•×œ×ª ×œ×¢×‘×•×¨ ×‘×™×Ÿ ××¦×‘ ×ª×’×•×‘×” ××”×™×¨×” ×œ×©××™×œ×ª×•×ª ×¤×©×•×˜×•×ª ×œ×‘×™×Ÿ ××¦×‘ ×—×©×™×‘×” ××¢××™×§×” ×œ×‘×¢×™×•×ª ××•×¨×›×‘×•×ª.

**×©×™×œ×•×‘ ×©×¨×©×¨×ª ××—×©×‘×”**: ×©×™×œ×•×‘ ×˜×‘×¢×™ ×©×œ ×©×œ×‘×™ ×”×¡×§×” ×©××©×¤×¨×™× ××ª ×”×©×§×™×¤×•×ª ×•×”×“×™×•×§ ×‘××©×™××•×ª ××•×¨×›×‘×•×ª.

### ×—×“×©× ×•×™×•×ª ××¨×›×™×˜×§×˜×•× ×™×•×ª

××©×¤×—×ª Qwen ××©×œ×‘×ª ××¡×¤×¨ ××•×¤×˜×™××™×–×¦×™×•×ª ××¨×›×™×˜×§×˜×•× ×™×•×ª ×©× ×•×¢×“×• ×”×Ÿ ×œ×‘×™×¦×•×¢×™× ×•×”×Ÿ ×œ×™×¢×™×œ×•×ª:

**×¢×™×¦×•×‘ × ×™×ª×Ÿ ×œ×”×¨×—×‘×”**: ××¨×›×™×˜×§×˜×•×¨×” ×¢×§×‘×™×ª ×œ××•×¨×š ×’×“×œ×™ ××•×“×œ×™× ×©×××¤×©×¨×ª ×”×¨×—×‘×” ×•×”×©×•×•××” ×§×œ×”.

**××™× ×˜×’×¨×¦×™×” ××•×œ×˜×™××•×“×œ×™×ª**: ×©×™×œ×•×‘ ×—×œ×§ ×©×œ ×¢×™×‘×•×“ ×˜×§×¡×˜, ×ª××•× ×” ×•××•×“×™×• ×‘×ª×•×š ××¨×›×™×˜×§×˜×•×¨×•×ª ×××•×—×“×•×ª.

**××•×¤×˜×™××™×–×¦×™×™×ª ×¤×¨×™×¡×”**: ××¤×©×¨×•×™×•×ª ×›×™××•×ª ××¨×•×‘×•×ª ×•×ª×‘× ×™×•×ª ×¤×¨×™×¡×” ×©×•× ×•×ª ×¢×‘×•×¨ ×ª×¦×•×¨×•×ª ×—×•××¨×” ××’×•×•× ×•×ª.

## ×’×“×œ×™ ××•×“×œ×™× ×•××¤×©×¨×•×™×•×ª ×¤×¨×™×¡×”

×¡×‘×™×‘×•×ª ×¤×¨×™×¡×” ××•×“×¨× ×™×•×ª × ×”× ×•×ª ××”×’××™×©×•×ª ×©×œ ××•×“×œ×™ Qwen ×‘××’×•×•×Ÿ ×“×¨×™×©×•×ª ×—×™×©×•×‘×™×•×ª:

### ××•×“×œ×™× ×§×˜× ×™× (0.5B-3B)

Qwen ××¡×¤×§ ××•×“×œ×™× ×§×˜× ×™× ×™×¢×™×œ×™× ×”××ª××™××™× ×œ×¤×¨×™×¡×” ×‘×§×¦×”, ×™×™×©×•××™× × ×™×™×“×™× ×•×¡×‘×™×‘×•×ª ××•×’×‘×œ×•×ª ××©××‘×™× ×ª×•×š ×©××™×¨×” ×¢×œ ×™×›×•×œ×•×ª ××¨×©×™××•×ª.

### ××•×“×œ×™× ×‘×™× ×•× ×™×™× (7B-32B)

××•×“×œ×™× ×‘×˜×•×•×— ×”×‘×™× ×™×™× ××¦×™×¢×™× ×™×›×•×œ×•×ª ××©×•×¤×¨×•×ª ×œ×™×™×©×•××™× ××§×¦×•×¢×™×™×, ×•××¡×¤×§×™× ××™×–×•×Ÿ ××¦×•×™×Ÿ ×‘×™×Ÿ ×‘×™×¦×•×¢×™× ×œ×“×¨×™×©×•×ª ×—×™×©×•×‘×™×•×ª.

### ××•×“×œ×™× ×’×“×•×œ×™× (72B+)

××•×“×œ×™× ×‘×§× ×” ××™×“×” ××œ× ××¡×¤×§×™× ×‘×™×¦×•×¢×™× ××ª×§×“××™× ×œ×™×™×©×•××™× ×ª×•×‘×¢× ×™×™×, ××—×§×¨ ×•×¤×¨×™×¡×•×ª ××¨×’×•× ×™×•×ª ×”×“×•×¨×©×•×ª ×™×›×•×œ×ª ××¨×‘×™×ª.

## ×™×ª×¨×•× ×•×ª ××©×¤×—×ª ××•×“×œ×™ Qwen

### × ×’×™×©×•×ª ×‘×§×•×“ ×¤×ª×•×—

××•×“×œ×™ Qwen ××¡×¤×§×™× ×©×§×™×¤×•×ª ××œ××” ×•×™×›×•×œ×•×ª ×”×ª×××” ××™×©×™×ª, ×•×××¤×©×¨×™× ×œ××¨×’×•× ×™× ×œ×”×‘×™×Ÿ, ×œ×©× ×•×ª ×•×œ×”×ª××™× ××ª ×”××•×“×œ×™× ×œ×¦×¨×›×™× ×”×¡×¤×¦×™×¤×™×™× ×©×œ×”× ×œ×œ× ×ª×œ×•×ª ×‘×¡×¤×§.

### ×’××™×©×•×ª ×‘×¤×¨×™×¡×”

××’×•×•×Ÿ ×’×“×œ×™ ×”××•×“×œ×™× ×××¤×©×¨ ×¤×¨×™×¡×” ×‘×ª×¦×•×¨×•×ª ×—×•××¨×” ××’×•×•× ×•×ª, ×××›×©×™×¨×™× × ×™×™×“×™× ×•×¢×“ ×©×¨×ª×™× ××ª×§×“××™×, ×•××¡×¤×§ ×œ××¨×’×•× ×™× ×’××™×©×•×ª ×‘×‘×—×™×¨×•×ª ×ª×©×ª×™×ª ×”-AI ×©×œ×”×.

### ××¦×•×™× ×•×ª ×¨×‘-×œ×©×•× ×™×ª

××•×“×œ×™ Qwen ××¦×˜×™×™× ×™× ×‘×”×‘× ×” ×•×™×¦×™×¨×” ×¨×‘-×œ×©×•× ×™×ª, ×•×ª×•××›×™× ×‘×¢×©×¨×•×ª ×©×¤×•×ª ×¢× ×—×•×–×§ ××™×•×—×“ ×‘×× ×’×œ×™×ª ×•×‘×¡×™× ×™×ª, ××” ×©×”×•×¤×š ××•×ª× ×œ××ª××™××™× ×œ×™×™×©×•××™× ×’×œ×•×‘×œ×™×™×.

### ×‘×™×¦×•×¢×™× ×ª×—×¨×•×ª×™×™×

××•×“×œ×™ Qwen ××©×™×’×™× ×‘××•×¤×Ÿ ×¢×§×‘×™ ×ª×•×¦××•×ª ×ª×—×¨×•×ª×™×•×ª ×‘××“×“×™× ×ª×•×š ×©×”× ××¡×¤×§×™× × ×’×™×©×•×ª ×‘×§×•×“ ×¤×ª×•×—, ×•××“×’×™××™× ×©××•×“×œ×™× ×¤×ª×•×—×™× ×™×›×•×œ×™× ×œ×”×ª×—×¨×•×ª ×‘××œ×˜×¨× ×˜×™×‘×•×ª ×§× ×™×™× ×™×•×ª.

### ×™×›×•×œ×•×ª ××™×•×—×“×•×ª

×•×¨×™××¦×™×•×ª ×ª×—×•××™×•×ª ×›××• Qwen-Coder ×•-Qwen-Math ××¡×¤×§×•×ª ××•××—×™×•×ª ××™×•×—×“×ª ×ª×•×š ×©××™×¨×” ×¢×œ ×™×›×•×œ×•×ª ×”×‘× ×” ×©×¤×ª×™×ª ×›×œ×œ×™×•×ª.

## ×“×•×’×××•×ª ××¢×©×™×•×ª ×•×ª×¨×—×™×©×™ ×©×™××•×©

×œ×¤× ×™ ×©× ×¦×œ×•×œ ×œ×¤×¨×˜×™× ×”×˜×›× ×™×™×, ×‘×•××• × ×—×§×•×¨ ×›××” ×“×•×’×××•×ª ×§×•× ×§×¨×˜×™×•×ª ×œ××” ×©××•×“×œ×™ Qwen ×™×›×•×œ×™× ×œ×”×©×™×’:

### ×“×•×’××” ×œ×”×¡×§×” ××ª××˜×™×ª

Qwen-Math ××¦×˜×™×™×Ÿ ×‘×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª ××ª××˜×™×•×ª ×©×œ×‘ ××—×¨ ×©×œ×‘. ×œ×“×•×’××”, ×›××©×¨ ××ª×‘×§×©×™× ×œ×¤×ª×•×¨ ×‘×¢×™×™×ª ×—×©×‘×•×Ÿ ××•×¨×›×‘×ª:

```
User: Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = xÂ³ + 2xÂ² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹

Breaking down each term:
- d/dx(xÂ³) = 3xÂ²
- d/dx(2xÂ²) = 2 Â· 2xÂ¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3xÂ² + 4x - 5
```

### ×“×•×’××” ×œ×ª××™×›×” ×¨×‘-×œ×©×•× ×™×ª

××•×“×œ×™ Qwen ××“×’×™××™× ×™×›×•×œ×•×ª ×¨×‘-×œ×©×•× ×™×•×ª ×—×–×§×•×ª ×‘××’×•×•×Ÿ ×©×¤×•×ª:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**ä¸­æ–‡:**
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡æœºå™¨ï¼ˆç‰¹åˆ«æ˜¯è®¡ç®—æœºç³»ç»Ÿï¼‰å¯¹äººç±»æ™ºèƒ½è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚è¿™äº›è¿‡ç¨‹åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€è§£å†³é—®é¢˜ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚äººå·¥æ™ºèƒ½ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»è®¤çŸ¥èƒ½åŠ›çš„ä»»åŠ¡ã€‚
```

### ×“×•×’××” ×œ×™×›×•×œ×•×ª ××•×œ×˜×™××•×“×œ×™×•×ª

Qwen-VL ×™×›×•×œ ×œ×¢×‘×“ ×˜×§×¡×˜ ×•×ª××•× ×•×ª ×‘×•-×–×× ×™×ª:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### ×“×•×’××” ×œ×™×¦×™×¨×ª ×§×•×“

Qwen-Coder ××¦×˜×™×™×Ÿ ×‘×™×¦×™×¨×” ×•×”×¡×‘×¨ ×©×œ ×§×•×“ ×‘××’×•×•×Ÿ ×©×¤×•×ª ×ª×›× ×•×ª:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    ×‘×™×¦×•×¢ ×—×™×¤×•×© ×‘×™× ××¨×™ ×‘××¢×¨×š ×××•×™×™×Ÿ ×œ××¦×™××ª ×¢×¨×š ×”×™×¢×“.
    
    Args:
        arr (list): ××¢×¨×š ×××•×™×™×Ÿ ×©×œ ××œ×× ×˜×™× ×œ×”×©×•×•××”
        target: ×”×¢×¨×š ×œ×—×™×¤×•×©
        
    Returns:
        int: ××™× ×“×§×¡ ×©×œ ×”×™×¢×“ ×× × ××¦×, -1 ×× ×œ× × ××¦×
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # ××ª×—×•×œ ××¦×‘×™×¢×™× ×©×××œ×™×™× ×•×™×× ×™×™×
    left, right = 0, len(arr) - 1
    
    # ×”××©×š ×—×™×¤×•×© ×›×œ ×¢×•×“ ××¨×—×‘ ×”×—×™×¤×•×© ×ª×§×£
    while left <= right:
        # ×—×™×©×•×‘ ××™× ×“×§×¡ ×××¦×¢×™ ×›×“×™ ×œ×”×™×× ×¢ ××”×¦×¤×ª ××¡×¤×¨×™× ×©×œ××™×
        mid = left + (right - left) // 2
        
        # ×‘×“×™×§×” ×× ××¦×× ×• ××ª ×”×™×¢×“
        if arr[mid] == target:
            return mid
        
        # ×× ×”×™×¢×“ ×§×˜×Ÿ ×™×•×ª×¨, ×—×™×¤×•×© ×‘×—×¦×™ ×”×©×××œ×™
        elif arr[mid] > target:
            right = mid - 1
        
        # ×× ×”×™×¢×“ ×’×“×•×œ ×™×•×ª×¨, ×—×™×¤×•×© ×‘×—×¦×™ ×”×™×× ×™
        else:
            left = mid + 1
    
    # ×”×™×¢×“ ×œ× × ××¦×
    return -1

# ×“×•×’××ª ×©×™××•×©:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"××™× ×“×§×¡ ×©×œ 7: {result}")  # ×¤×œ×˜: ××™× ×“×§×¡ ×©×œ 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### ×“×•×’××” ×œ×¤×¨×™×¡×” ×‘×§×¦×”

××•×“×œ×™ Qwen ×™×›×•×œ×™× ×œ×”×™×•×ª ×¤×¨×•×¡×™× ×¢×œ ××’×•×•×Ÿ ××›×©×™×¨×™ ×§×¦×” ×¢× ×ª×¦×•×¨×•×ª ××•×¤×˜×™××œ×™×•×ª:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## ×”×ª×¤×ª×—×•×ª ××©×¤×—×ª Qwen

### Qwen 1.0 ×•-1.5: ××•×“×œ×™× ×‘×¡×™×¡×™×™×

×”××•×“×œ×™× ×”×¨××©×•× ×™× ×©×œ Qwen ×‘×™×¡×¡×• ××ª ×¢×§×¨×•× ×•×ª ×”×™×¡×•×“ ×©×œ ××™××•×Ÿ ××§×™×£ ×•× ×’×™×©×•×ª ×‘×§×•×“ ×¤×ª×•×—:

- **Qwen-7B (7B ×¤×¨××˜×¨×™×)**: ×©×—×¨×•×¨ ×¨××©×•× ×™ ×”××ª××§×“ ×‘×”×‘× ×ª ×©×¤×” ×¡×™× ×™×ª ×•×× ×’×œ×™×ª
- **Qwen-14B (14B ×¤×¨××˜×¨×™×)**: ×™×›×•×œ×•×ª ××©×•×¤×¨×•×ª ×¢× ×”×¡×§×” ×•×™×“×¢ ××©×•×¤×¨
- **Qwen-72B (72B ×¤×¨××˜×¨×™×)**: ××•×“×œ ×‘×§× ×” ××™×“×” ×’×“×•×œ ×”××¡×¤×§ ×‘×™×¦×•×¢×™× ××ª×§×“××™×
- **×¡×“×¨×ª Qwen1.5**: ×”×ª×¨×—×‘×•×ª ×œ×’×“×œ×™× ××¨×•×‘×™× (0.5B ×¢×“ 110B) ×¢× ×©×™×¤×•×¨ ×‘×˜×™×¤×•×œ ×‘×”×§×©×¨ ××¨×•×š

### ××©×¤×—×ª Qwen2: ×”×¨×—×‘×” ××•×œ×˜×™××•×“×œ×™×ª

×¡×“×¨×ª Qwen2 ×¡×™×× ×” ×”×ª×§×“××•×ª ××©××¢×•×ª×™×ª ×”×Ÿ ×‘×™×›×•×œ×•×ª ×©×¤×” ×•×”×Ÿ ×‘×™×›×•×œ×•×ª ××•×œ×˜×™××•×“×œ×™×•×ª:

- **Qwen2-0.5B ×¢×“ 72B**: ××’×•×•×Ÿ ×¨×—×‘ ×©×œ ××•×“×œ×™× ×©×¤×ª×™×™× ×œ×¦×¨×›×™ ×¤×¨×™×¡×” ×©×•× ×™×
- **Qwen2-57B-A14B (MoE)**: ××¨×›×™×˜×§×˜×•×¨×ª ×ª×¢×¨×•×‘×ª ××•××—×™× ×œ×©×™××•×© ×™×¢×™×œ ×‘×¤×¨××˜×¨×™×
- **Qwen2-VL**: ×™×›×•×œ×•×ª ××ª×§×“××•×ª ×©×œ ×©×¤×”-×ª××•× ×” ×œ×”×‘× ×ª ×ª××•× ×•×ª
- **Qwen2-Audio**: ×™×›×•×œ×•×ª ×¢×™×‘×•×“ ×•×”×‘× ×ª ××•×“×™×•
- **Qwen2-Math**: ×”×¡×§×” ××ª××˜×™×ª ×•×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª ××ª××—×•×ª

### ××©×¤×—×ª Qwen2.5: ×‘×™×¦×•×¢×™× ××©×•×¤×¨×™×

×¡×“×¨×ª Qwen2.5 ×”×‘×™××” ×©×™×¤×•×¨×™× ××©××¢×•×ª×™×™× ×‘×›×œ ×”×××“×™×:

- **××™××•×Ÿ ××•×¨×—×‘**: 18 ×˜×¨×™×œ×™×•×Ÿ ×˜×•×§× ×™× ×©×œ × ×ª×•× ×™ ××™××•×Ÿ ×œ×©×™×¤×•×¨ ×™×›×•×œ×•×ª
- **×”×§×©×¨ ××•×¨×—×‘**: ×¢×“ 128K ×˜×•×§× ×™× ××•×¨×š ×”×§×©×¨, ×¢× ×’×¨×¡×ª Turbo ×”×ª×•××›×ª ×‘-1M ×˜×•×§× ×™×
- **×”×ª××—×•×ª ××©×•×¤×¨×ª**: ×©×™×¤×•×¨×™× ×‘-Qwen2.5-Coder ×•×‘-Qwen2.5-Math
- **×ª××™×›×” ×¨×‘-×œ×©×•× ×™×ª ×˜×•×‘×” ×™×•×ª×¨**: ×‘×™×¦×•×¢×™× ××©×•×¤×¨×™× ×‘-27+ ×©×¤×•×ª

### ××©×¤×—×ª Qwen3: ×”×¡×§×” ××ª×§×“××ª

×”×“×•×¨ ×”××—×¨×•×Ÿ ×“×•×—×£ ××ª ×’×‘×•×œ×•×ª ×”×”×¡×§×” ×•×™×›×•×œ×•×ª ×”×—×©×™×‘×”:

- **Qwen3-235B-A22B**: ××•×“×œ ×ª×¢×¨×•×‘×ª ××•××—×™× ××•×‘×™×œ ×¢× 235B ×¤×¨××˜×¨×™× ×›×•×œ×œ×™×
- **Qwen3-30B-A3B**: ××•×“×œ MoE ×™×¢×™×œ ×¢× ×‘×™×¦×•×¢×™× ×—×–×§×™× ×œ×›×œ ×¤×¨××˜×¨ ×¤×¢×™×œ
- **××•×“×œ×™× ×¦×¤×•×¤×™×**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B ×œ×ª×¨×—×™×©×™ ×¤×¨×™×¡×” ×©×•× ×™×
- **××¦×‘ ×—×©×™×‘×”**: ×’×™×©×ª ×”×¡×§×” ×”×™×‘×¨×™×“×™×ª ×”×ª×•××›×ª ×”×Ÿ ×‘×ª×’×•×‘×•×ª ××”×™×¨×•×ª ×•×”×Ÿ ×‘×—×©×™×‘×” ××¢××™×§×”
- **××¦×•×™× ×•×ª ×¨×‘-×œ×©×•× ×™×ª**: ×ª××™×›×” ×‘-119 ×©×¤×•×ª ×•× ×™×‘×™×
- **××™××•×Ÿ ××©×•×¤×¨**: 36 ×˜×¨×™×œ×™×•×Ÿ ×˜×•×§× ×™× ×©×œ × ×ª×•× ×™ ××™××•×Ÿ ××’×•×•× ×™× ×•××™×›×•×ª×™×™×

## ×™×™×©×•××™× ×©×œ ××•×“×œ×™ Qwen

### ×™×™×©×•××™× ××¨×’×•× ×™×™×

××¨×’×•× ×™× ××©×ª××©×™× ×‘××•×“×œ×™ Qwen ×œ× ×™×ª×•×— ××¡××›×™×, ××•×˜×•××¦×™×” ×©×œ ×©×™×¨×•×ª ×œ×§×•×—×•×ª, ×¡×™×•×¢ ×‘×™×¦×™×¨×ª ×§×•×“ ×•×™×™×©×•××™ ××•×“×™×¢×™×Ÿ ×¢×¡×§×™. ×”××•×¤×™ ×‘×§×•×“ ×¤×ª×•×— ×××¤×©×¨ ×”×ª×××” ××™×©×™×ª ×œ×¦×¨×›×™× ×¢×¡×§×™×™× ×¡×¤×¦×™×¤×™×™× ×ª×•×š ×©××™×¨×” ×¢×œ ×¤×¨×˜×™×•×ª ×•×©×œ×™×˜×” ×‘× ×ª×•× ×™×.

### ××—×©×•×‘ × ×™×™×“ ×•×§×¦×”

×™×™×©×•××™× × ×™×™×“×™× ×× ×¦×œ×™× ××ª ××•×“×œ×™ Qwen ×œ×ª×¨×’×•× ×‘×–××Ÿ ×××ª, ×¢×•×–×¨×™× ×—×›××™×, ×™×¦×™×¨×ª ×ª×•×›×Ÿ ×•×”××œ×¦×•×ª ××•×ª×××•×ª ××™×©×™×ª. ××’×•×•×Ÿ ×’×“×œ×™ ×”××•×“×œ×™× ×××¤×©×¨ ×¤×¨×™×¡×” ×××›×©×™×¨×™× × ×™×™×“×™× ×•×¢×“ ×©×¨×ª×™ ×§×¦×”.

### ×˜×›× ×•×œ×•×’×™×” ×—×™× ×•×›×™×ª

×¤×œ×˜×¤×•×¨××•×ª ×—×™× ×•×›×™×•×ª ××©×ª××©×•×ª ×‘××•×“×œ×™ Qwen ×œ×”×“×¨×›×” ××•×ª×××ª ××™×©×™×ª, ×™×¦×™×¨×ª ×ª×•×›×Ÿ ××•×˜×•××˜×™×ª, ×¡×™×•×¢ ×‘×œ×™××•×“ ×©×¤×•×ª ×•×—×•×•×™×•×ª ×—×™× ×•×›×™×•×ª ××™× ×˜×¨××§×˜×™×‘×™×•×ª. ××•×“×œ×™× ××™×•×—×“×™× ×›××• Qwen-Math ××¡×¤×§×™× ××•××—×™×•×ª ×ª×—×•××™×ª.

### ×™×™×©×•××™× ×’×œ×•×‘×œ×™×™×

×™×™×©×•××™× ×‘×™× ×œ××•××™×™× × ×”× ×™× ××”×™×›×•×œ×•×ª ×”×¨×‘-×œ×©×•× ×™×•×ª ×”×—×–×§×•×ª ×©×œ ××•×“×œ×™ Qwen, ×”×××¤×©×¨×™× ×—×•×•×™×•×ª AI ×¢×§×‘×™×•×ª ×‘×©×¤×•×ª ×•×ª×¨×‘×•×™×•×ª ×©×•× ×•×ª.

## ××ª×’×¨×™× ×•××’×‘×œ×•×ª

### ×“×¨×™×©×•×ª ×—×™×©×•×‘×™×•×ª

×œ××¨×•×ª ×©-Qwen ××¡×¤×§ ××•×“×œ×™× ×‘×’×“×œ×™× ×©×•× ×™×, ×•×¨×™××¦×™×•×ª ×’×“×•×œ×•×ª ×¢×“×™×™×Ÿ ×“×•×¨×©×•×ª ××©××‘×™× ×—×™×©×•×‘×™×™× ××©××¢×•×ª×™×™× ×œ×‘×™×¦×•×¢×™× ××™×˜×‘×™×™×, ××” ×©×¢×©×•×™ ×œ×”×’×‘×™×œ ××¤×©×¨×•×™×•×ª ×¤×¨×™×¡×” ×¢×‘×•×¨ ×—×œ×§ ××”××¨×’×•× ×™×.

### ×‘×™×¦×•×¢×™× ×ª×—×•××™×™× ××™×•×—×“×™×

×œ××¨×•×ª ×©××•×“×œ×™ Qwen ××ª×¤×§×“×™×
×”× ×” ×›×™×¦×“ ×œ×”×ª×—×™×œ ×œ×¢×‘×•×“ ×¢× ××•×“×œ×™× ×©×œ Qwen ×‘×××¦×¢×•×ª ×¡×¤×¨×™×™×ª Hugging Face Transformers:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### ×©×™××•×© ×‘××•×“×œ×™× Qwen2.5

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### ×©×™××•×© ×‘××•×“×œ×™× ××ª××—×™×

**×™×¦×™×¨×ª ×§×•×“ ×¢× Qwen-Coder:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª ××ª××˜×™×•×ª:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**××©×™××•×ª ×—×–×•×ª×™×•×ª-×œ×©×•× ×™×•×ª:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### ××¦×‘ ×—×©×™×‘×” (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### ğŸ“± ×¤×¨×™×¡×” ×œ××•×‘×™×™×œ ×•×œ××›×©×™×¨×™ ×§×¦×”

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### ×“×•×’××” ×œ×¤×¨×™×¡×ª API

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## ××“×“×™ ×‘×™×¦×•×¢×™× ×•×”×™×©×’×™×

××©×¤×—×ª ×”××•×“×œ×™× Qwen ×”×©×™×’×” ×‘×™×¦×•×¢×™× ××¨×©×™××™× ×‘××’×•×•×Ÿ ××“×“×™× ×ª×•×š ×©××™×¨×” ×¢×œ × ×’×™×©×•×ª ×‘×§×•×“ ×¤×ª×•×—:

### × ×§×•×“×•×ª ×‘×™×¦×•×¢ ××¨×›×–×™×•×ª

**××¦×•×™× ×•×ª ×‘×”×¡×§×”:**
- Qwen3-235B-A22B ××©×™×’ ×ª×•×¦××•×ª ×ª×—×¨×•×ª×™×•×ª ×‘×”×¢×¨×›×•×ª ××“×“×™× ×©×œ ×§×™×“×•×“, ××ª××˜×™×§×” ×•×™×›×•×œ×•×ª ×›×œ×œ×™×•×ª ×‘×”×©×•×•××” ×œ××•×“×œ×™× ××•×‘×™×œ×™× ×›××• DeepSeek-R1, o1, o3-mini, Grok-3, ×•-Gemini-2.5-Pro
- Qwen3-30B-A3B ××ª×¢×œ×” ×¢×œ QwQ-32B ×¢× ×¤×™ 10 ×™×•×ª×¨ ×¤×¨××˜×¨×™× ×¤×¢×™×œ×™×
- Qwen3-4B ×™×›×•×œ ×œ×”×ª×—×¨×•×ª ×‘×‘×™×¦×•×¢×™× ×©×œ Qwen2.5-72B-Instruct

**×”×™×©×’×™× ×‘×™×¢×™×œ×•×ª:**
- ××•×“×œ×™ ×”×‘×¡×™×¡ ×©×œ Qwen3-MoE ××©×™×’×™× ×‘×™×¦×•×¢×™× ×“×•××™× ×œ××•×“×œ×™ ×”×‘×¡×™×¡ ×”×¦×¤×•×¤×™× ×©×œ Qwen2.5 ×ª×•×š ×©×™××•×© ×‘-10% ×‘×œ×‘×“ ××”×¤×¨××˜×¨×™× ×”×¤×¢×™×œ×™×
- ×—×™×¡×›×•×Ÿ ××©××¢×•×ª×™ ×‘×¢×œ×•×™×•×ª ×”×Ÿ ×‘××™××•×Ÿ ×•×”×Ÿ ×‘×”×¡×§×” ×‘×”×©×•×•××” ×œ××•×“×œ×™× ×¦×¤×•×¤×™×

**×™×›×•×œ×•×ª ×¨×‘-×œ×©×•× ×™×•×ª:**
- ××•×“×œ×™ Qwen3 ×ª×•××›×™× ×‘-119 ×©×¤×•×ª ×•× ×™×‘×™×
- ×‘×™×¦×•×¢×™× ×—×–×§×™× ×‘×”×§×©×¨×™× ×œ×©×•× ×™×™× ×•×ª×¨×‘×•×ª×™×™× ××’×•×•× ×™×

**×”×™×§×£ ×”××™××•×Ÿ:**
- Qwen3 ××©×ª××© ×‘×›××¢×˜ ×¤×™ ×©× ×™×™×, ×¢× ×›-36 ×˜×¨×™×œ×™×•×Ÿ ×˜×•×§× ×™× ×”××›×¡×™× 119 ×©×¤×•×ª ×•× ×™×‘×™× ×‘×”×©×•×•××” ×œ-18 ×˜×¨×™×œ×™×•×Ÿ ×˜×•×§× ×™× ×©×œ Qwen2.5

### ×˜×‘×œ×ª ×”×©×•×•××ª ××•×“×œ×™×

| ×¡×“×¨×ª ××•×“×œ×™× | ×˜×•×•×— ×¤×¨××˜×¨×™× | ××•×¨×š ×”×§×©×¨ | ×—×•×–×§×•×ª ××¨×›×–×™×•×ª | ×©×™××•×©×™× ××•××œ×¦×™× |
|--------------|------------------|----------------|---------------|----------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | ×‘×™×¦×•×¢×™× ×××•×–× ×™×, ×¨×‘-×œ×©×•× ×™×•×ª | ×™×™×©×•××™× ×›×œ×œ×™×™×, ×¤×¨×™×¡×” ×œ×™×™×¦×•×¨ |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | ×™×¦×™×¨×ª ×§×•×“, ×ª×›× ×•×ª | ×¤×™×ª×•×— ×ª×•×›× ×”, ×¢×–×¨×” ×‘×§×™×“×•×“ |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | ×”×¡×§×” ××ª××˜×™×ª | ×¤×œ×˜×¤×•×¨××•×ª ×—×™× ×•×›×™×•×ª, ×™×™×©×•××™ STEM |
| **Qwen2.5-VL** | ××©×ª× ×” | ××©×ª× ×” | ×”×‘× ×ª ×—×–×•×ª×™×ª-×œ×©×•× ×™×ª | ×™×™×©×•××™× ××•×œ×˜×™××•×“××œ×™×™×, × ×™×ª×•×— ×ª××•× ×•×ª |
| **Qwen3** | 0.6B-235B | ××©×ª× ×” | ×”×¡×§×” ××ª×§×“××ª, ××¦×‘ ×—×©×™×‘×” | ×”×¡×§×” ××•×¨×›×‘×ª, ×™×™×©×•××™ ××—×§×¨ |
| **Qwen3 MoE** | 30B-235B ×¡×”"×› | ××©×ª× ×” | ×‘×™×¦×•×¢×™× ×™×¢×™×œ×™× ×‘×§× ×” ××™×“×” ×’×“×•×œ | ×™×™×©×•××™× ××¨×’×•× ×™×™×, ×¦×¨×›×™× ×‘×¢×œ×™ ×‘×™×¦×•×¢×™× ×’×‘×•×”×™× |

## ××“×¨×™×š ×œ×‘×—×™×¨×ª ××•×“×œ

### ×œ×™×™×©×•××™× ×‘×¡×™×¡×™×™×
- **Qwen2.5-0.5B/1.5B**: ××¤×œ×™×§×¦×™×•×ª ××•×‘×™×™×œ, ××›×©×™×¨×™ ×§×¦×”, ×™×™×©×•××™× ×‘×–××Ÿ ×××ª
- **Qwen2.5-3B/7B**: ×¦'××˜×‘×•×˜×™× ×›×œ×œ×™×™×, ×™×¦×™×¨×ª ×ª×•×›×Ÿ, ××¢×¨×›×•×ª ×©××œ×•×ª ×•×ª×©×•×‘×•×ª

### ×œ××©×™××•×ª ××ª××˜×™×•×ª ×•×”×¡×§×”
- **Qwen2.5-Math**: ×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª ××ª××˜×™×•×ª ×•×—×™× ×•×š STEM
- **Qwen3 ×¢× ××¦×‘ ×—×©×™×‘×”**: ×”×¡×§×” ××•×¨×›×‘×ª ×”×“×•×¨×©×ª × ×™×ª×•×— ×©×œ×‘-××—×¨-×©×œ×‘

### ×œ×ª×›× ×•×ª ×•×¤×™×ª×•×—
- **Qwen2.5-Coder**: ×™×¦×™×¨×ª ×§×•×“, ××™×ª×•×¨ ×‘××’×™×, ×¢×–×¨×” ×‘×ª×›× ×•×ª
- **Qwen3**: ××©×™××•×ª ×ª×›× ×•×ª ××ª×§×“××•×ª ×¢× ×™×›×•×œ×•×ª ×”×¡×§×”

### ×œ×™×™×©×•××™× ××•×œ×˜×™××•×“××œ×™×™×
- **Qwen2.5-VL**: ×”×‘× ×ª ×ª××•× ×•×ª, ×©××œ×•×ª ×—×–×•×ª×™×•×ª
- **Qwen-Audio**: ×¢×™×‘×•×“ ×©××¢ ×•×”×‘× ×ª ×“×™×‘×•×¨

### ×œ×¤×¨×™×¡×” ××¨×’×•× ×™×ª
- **Qwen2.5-32B/72B**: ×”×‘× ×ª ×©×¤×” ×‘×¢×œ×ª ×‘×™×¦×•×¢×™× ×’×‘×•×”×™×
- **Qwen3-235B-A22B**: ×™×›×•×œ×ª ××§×¡×™××œ×™×ª ×œ×™×™×©×•××™× ×ª×•×‘×¢× ×™×™×

## ×¤×œ×˜×¤×•×¨××•×ª ×¤×¨×™×¡×” ×•× ×’×™×©×•×ª
### ×¤×œ×˜×¤×•×¨××•×ª ×¢× ×Ÿ
- **Hugging Face Hub**: ×××’×¨ ××•×“×œ×™× ××§×™×£ ×¢× ×ª××™×›×” ×§×”×™×œ×ª×™×ª
- **ModelScope**: ×¤×œ×˜×¤×•×¨××ª ××•×“×œ×™× ×©×œ Alibaba ×¢× ×›×œ×™ ××•×¤×˜×™××™×–×¦×™×”
- **×¡×¤×§×™ ×¢× ×Ÿ ×©×•× ×™×**: ×ª××™×›×” ×“×¨×š ×¤×œ×˜×¤×•×¨××•×ª ML ×¡×˜× ×“×¨×˜×™×•×ª

### ××¡×’×¨×•×ª ×¤×™×ª×•×— ××§×•××™×•×ª
- **Transformers**: ××™× ×˜×’×¨×¦×™×” ×¡×˜× ×“×¨×˜×™×ª ×©×œ Hugging Face ×œ×¤×¨×™×¡×” ×§×œ×”
- **vLLM**: ×©×¨×ª ×‘×™×¦×•×¢×™× ×’×‘×•×”×™× ×œ×¡×‘×™×‘×•×ª ×™×™×¦×•×¨
- **Ollama**: ×¤×¨×™×¡×” ×•× ×™×”×•×œ ××§×•××™ ×¤×©×•×˜×™×
- **ONNX Runtime**: ××•×¤×˜×™××™×–×¦×™×” ×—×•×¦×ª ×¤×œ×˜×¤×•×¨××•×ª ×œ×—×•××¨×” ××’×•×•× ×ª
- **llama.cpp**: ×™×™×©×•× ×™×¢×™×œ ×‘-C++ ×œ×¤×œ×˜×¤×•×¨××•×ª ××’×•×•× ×•×ª

### ××©××‘×™ ×œ××™×“×”
- **×ª×™×¢×•×“ Qwen**: ×ª×™×¢×•×“ ×¨×©××™ ×•×›×¨×˜×™×¡×™ ××•×“×œ×™×
- **Hugging Face Model Hub**: ×”×“×’××•×ª ××™× ×˜×¨××§×˜×™×‘×™×•×ª ×•×“×•×’×××•×ª ×§×”×™×œ×ª×™×•×ª
- **××××¨×™ ××—×§×¨**: ××××¨×™× ×˜×›× ×™×™× ×‘-arxiv ×œ×”×‘× ×” ××¢××™×§×”
- **×¤×•×¨×•××™× ×§×”×™×œ×ª×™×™×**: ×ª××™×›×” ×§×”×™×œ×ª×™×ª ×¤×¢×™×œ×” ×•×“×™×•× ×™×

### ×›×™×¦×“ ×œ×”×ª×—×™×œ ×¢× ××•×“×œ×™× ×©×œ Qwen

#### ×¤×œ×˜×¤×•×¨××•×ª ×¤×™×ª×•×—
1. **Hugging Face Transformers**: ×”×ª×—×™×œ×• ×¢× ××™× ×˜×’×¨×¦×™×” ×¡×˜× ×“×¨×˜×™×ª ×‘-Python
2. **ModelScope**: ×—×§×¨×• ××ª ×›×œ×™ ×”×¤×¨×™×¡×” ×”××•×ª×××™× ×©×œ Alibaba
3. **×¤×¨×™×¡×” ××§×•××™×ª**: ×”×©×ª××©×• ×‘-Ollama ××• ×‘-Transformers ×™×©×™×¨×•×ª ×œ×‘×“×™×§×•×ª ××§×•××™×•×ª

#### ××¡×œ×•×œ ×œ××™×“×”
1. **×”×‘× ×ª ××•×©×’×™× ××¨×›×–×™×™×**: ×œ××“×• ××ª ×”××¨×›×™×˜×§×˜×•×¨×” ×•×”×™×›×•×œ×•×ª ×©×œ ××©×¤×—×ª Qwen
2. **× ×™×¡×•×™ ×¢× ×•×¨×™×× ×˜×™×**: × ×¡×• ×’×“×œ×™ ××•×“×œ×™× ×©×•× ×™× ×œ×”×‘× ×ª ×¤×©×¨×•×ª ×‘×™×¦×•×¢×™×
3. **×ª×¨×’×•×œ ×™×™×©×•×**: ×¤×¨×¡×• ××•×“×œ×™× ×‘×¡×‘×™×‘×•×ª ×¤×™×ª×•×—
4. **××•×¤×˜×™××™×–×¦×™×” ×œ×¤×¨×™×¡×”**: ×›×•×•× × ×• ×œ×©×™××•×©×™× ×‘×™×™×¦×•×¨

#### ×©×™×˜×•×ª ×¢×‘×•×“×” ××•××œ×¦×•×ª
- **×”×ª×—×™×œ×• ×‘×§×˜×Ÿ**: ×”×ª×—×™×œ×• ×¢× ××•×“×œ×™× ×§×˜× ×™× (1.5B-7B) ×œ×¤×™×ª×•×— ×¨××©×•× ×™
- **×”×©×ª××©×• ×‘×ª×‘× ×™×•×ª ×¦'××˜**: ×™×™×©××• ×¢×™×¦×•×‘ × ×›×•×Ÿ ×œ×ª×•×¦××•×ª ××™×˜×‘×™×•×ª
- **×¢×§×‘×• ××—×¨ ××©××‘×™×**: ×¢×§×‘×• ××—×¨ ×©×™××•×© ×‘×–×™×›×¨×•×Ÿ ×•××”×™×¨×•×ª ×”×¡×§×”
- **×©×§×œ×• ×”×ª××—×•×ª**: ×‘×—×¨×• ×•×¨×™×× ×˜×™× ×¡×¤×¦×™×¤×™×™× ×œ×ª×—×•× ×›×©××ª××™×

## ×“×¤×•×¡×™ ×©×™××•×© ××ª×§×“××™×

### ×“×•×’×××•×ª ×œ×›×•×•× ×•×Ÿ ×¢×“×™×Ÿ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### ×”× ×“×¡×ª ×”× ×—×™×•×ª ××ª××—×•×ª

**×œ××©×™××•×ª ×”×¡×§×” ××•×¨×›×‘×•×ª:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**×œ×™×¦×™×¨×ª ×§×•×“ ×¢× ×”×§×©×¨:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### ×™×™×©×•××™× ×¨×‘-×œ×©×•× ×™×™×

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ä¸­æ–‡)",
        "es": "Spanish (EspaÃ±ol)",
        "fr": "French (FranÃ§ais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (æ—¥æœ¬èª)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ğŸ”§ ×“×¤×•×¡×™ ×¤×¨×™×¡×” ×œ×™×™×¦×•×¨

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## ××¡×˜×¨×˜×’×™×•×ª ××•×¤×˜×™××™×–×¦×™×” ×œ×‘×™×¦×•×¢×™×

### ××•×¤×˜×™××™×–×¦×™×™×ª ×–×™×›×¨×•×Ÿ

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### ××•×¤×˜×™××™×–×¦×™×™×ª ×”×¡×§×”

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## ×©×™×˜×•×ª ×¢×‘×•×“×” ××•××œ×¦×•×ª ×•×”× ×—×™×•×ª

### ××‘×˜×—×” ×•×¤×¨×˜×™×•×ª

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### × ×™×˜×•×¨ ×•×”×¢×¨×›×”

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## ×¡×™×›×•×

××©×¤×—×ª ×”××•×“×œ×™× Qwen ××™×™×¦×’×ª ×’×™×©×” ××§×™×¤×” ×œ×“××•×§×¨×˜×™×–×¦×™×” ×©×œ ×˜×›× ×•×œ×•×’×™×™×ª AI ×ª×•×š ×©××™×¨×” ×¢×œ ×‘×™×¦×•×¢×™× ×ª×—×¨×•×ª×™×™× ×‘××’×•×•×Ÿ ×™×™×©×•××™×. ×‘×××¦×¢×•×ª ××—×•×™×‘×•×ª×” ×œ× ×’×™×©×•×ª ×‘×§×•×“ ×¤×ª×•×—, ×™×›×•×œ×•×ª ×¨×‘-×œ×©×•× ×™×•×ª ×•××¤×©×¨×•×™×•×ª ×¤×¨×™×¡×” ×’××™×©×•×ª, Qwen ×××¤×©×¨×ª ×œ××¨×’×•× ×™× ×•×œ××¤×ª×—×™× ×œ× ×¦×œ ×™×›×•×œ×•×ª AI ×¢×•×¦××ª×™×•×ª ×œ×œ× ×§×©×¨ ×œ××©××‘×™× ××• ×œ×“×¨×™×©×•×ª ×”×¡×¤×¦×™×¤×™×•×ª ×©×œ×”×.

### × ×§×•×“×•×ª ××¤×ª×—

**××¦×•×™× ×•×ª ×‘×§×•×“ ×¤×ª×•×—**: Qwen ××“×’×™××” ×©××•×“×œ×™× ×‘×§×•×“ ×¤×ª×•×— ×™×›×•×œ×™× ×œ×”×©×™×’ ×‘×™×¦×•×¢×™× ×ª×—×¨×•×ª×™×™× ×¢× ×—×œ×•×¤×•×ª ×§× ×™×™× ×™×•×ª ×ª×•×š ××ª×Ÿ ×©×§×™×¤×•×ª, ×”×ª×××” ××™×©×™×ª ×•×©×œ×™×˜×”.

**××¨×›×™×˜×§×˜×•×¨×” × ×™×ª× ×ª ×œ×”×¨×—×‘×”**: ×”×˜×•×•×— ×-0.5B ×œ-235B ×¤×¨××˜×¨×™× ×××¤×©×¨ ×¤×¨×™×¡×” ×‘×›×œ ×”×¡×¤×§×˜×¨×•× ×©×œ ×¡×‘×™×‘×•×ª ×—×™×©×•×‘, ×××›×©×™×¨×™ ××•×‘×™×™×œ ×•×¢×“ ××©×›×•×œ×•×ª ××¨×’×•× ×™×™×.

**×™×›×•×œ×•×ª ××ª××—×•×ª**: ×•×¨×™×× ×˜×™× ×¡×¤×¦×™×¤×™×™× ×œ×ª×—×•× ×›××• Qwen-Coder, Qwen-Math ×•-Qwen-VL ××¡×¤×§×™× ××•××—×™×•×ª ××ª××—×” ×ª×•×š ×©××™×¨×” ×¢×œ ×”×‘× ×ª ×©×¤×” ×›×œ×œ×™×ª.

**× ×’×™×©×•×ª ×’×œ×•×‘×œ×™×ª**: ×ª××™×›×” ×¨×‘-×œ×©×•× ×™×ª ×—×–×§×” ×‘-119+ ×©×¤×•×ª ×”×•×¤×›×ª ××ª Qwen ×œ××ª××™××” ×œ×™×™×©×•××™× ×‘×™× ×œ××•××™×™× ×•×‘×¡×™×¡×™ ××©×ª××©×™× ××’×•×•× ×™×.

**×—×“×©× ×•×ª ××ª××©×›×ª**: ×”×”×ª×¤×ª×—×•×ª ×-Qwen 1.0 ×œ-Qwen3 ××¨××” ×©×™×¤×•×¨ ×¢×§×‘×™ ×‘×™×›×•×œ×•×ª, ×™×¢×™×œ×•×ª ×•××¤×©×¨×•×™×•×ª ×¤×¨×™×¡×”.

### ××‘×˜ ×œ×¢×ª×™×“

×›×›×œ ×©××©×¤×—×ª Qwen ×××©×™×›×” ×œ×”×ª×¤×ª×—, × ×™×ª×Ÿ ×œ×¦×¤×•×ª:

- **×™×¢×™×œ×•×ª ××©×•×¤×¨×ª**: ××•×¤×˜×™××™×–×¦×™×” ××ª××©×›×ª ×œ×™×—×¡×™ ×‘×™×¦×•×¢×™×-×¤×¨××˜×¨×™× ×˜×•×‘×™× ×™×•×ª×¨
- **×™×›×•×œ×•×ª ××•×œ×˜×™××•×“××œ×™×•×ª ××•×¨×—×‘×•×ª**: ×©×™×œ×•×‘ ×¢×™×‘×•×“ ×—×–×•×ª×™, ×©××¢ ×•×˜×§×¡×˜ ××ª×•×—×›××™× ×™×•×ª×¨
- **×”×¡×§×” ××©×•×¤×¨×ª**: ×× ×’× ×•× ×™ ×—×©×™×‘×” ××ª×§×“××™× ×•×™×›×•×œ×•×ª ×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª ×¨×‘-×©×œ×‘×™×•×ª
- **×›×œ×™ ×¤×¨×™×¡×” ×˜×•×‘×™× ×™×•×ª×¨**: ××¡×’×¨×•×ª ×•×›×œ×™ ××•×¤×˜×™××™×–×¦×™×” ××©×•×¤×¨×™× ×œ×ª×¨×—×™×©×™ ×¤×¨×™×¡×” ××’×•×•× ×™×
- **×¦××™×—×ª ×§×”×™×œ×”**: ×”×¨×—×‘×ª ××§×•×¡×™×¡×˜× ×©×œ ×›×œ×™×, ×™×™×©×•××™× ×•×ª×¨×•××•×ª ×§×”×™×œ×ª×™×•×ª

### ×¦×¢×“×™× ×”×‘××™×

×‘×™×Ÿ ×× ××ª× ×‘×•× ×™× ×¦'××˜×‘×•×˜, ××¤×ª×—×™× ×›×œ×™ ×—×™× ×•×›×™, ×™×•×¦×¨×™× ×¢×•×–×¨×™ ×§×™×“×•×“ ××• ×¢×•×‘×“×™× ×¢×œ ×™×™×©×•××™× ×¨×‘-×œ×©×•× ×™×™×, ××©×¤×—×ª Qwen ××¡×¤×§×ª ×¤×ª×¨×•× ×•×ª × ×™×ª× ×™× ×œ×”×¨×—×‘×” ×¢× ×ª××™×›×” ×§×”×™×œ×ª×™×ª ×—×–×§×” ×•×ª×™×¢×•×“ ××§×™×£.

×œ×¢×“×›×•× ×™× ×”××—×¨×•× ×™×, ×©×—×¨×•×¨×™ ××•×“×œ×™× ×•×ª×™×¢×•×“ ×˜×›× ×™ ××¤×•×¨×˜, ×‘×§×¨×• ×‘×××’×¨×™ Qwen ×”×¨×©××™×™× ×‘-Hugging Face ×•×—×§×¨×• ××ª ×“×™×•× ×™ ×”×§×”×™×œ×” ×”×¤×¢×™×œ×™× ×•×“×•×’×××•×ª.

×¢×ª×™×“ ×¤×™×ª×•×— ×”-AI ×˜××•×Ÿ ×‘×›×œ×™× × ×’×™×©×™×, ×©×§×•×¤×™× ×•×¢×•×¦××ª×™×™× ×©×××¤×©×¨×™× ×—×“×©× ×•×ª ×‘×›×œ ×”××’×–×¨×™× ×•×”×§× ×” ×”××™×“×”. ××©×¤×—×ª Qwen ×××—×™×©×” ×—×–×•×Ÿ ×–×”, ×•××¡×¤×§×ª ×œ××¨×’×•× ×™× ×•×œ××¤×ª×—×™× ××ª ×”×‘×¡×™×¡ ×œ×‘× ×™×™×ª ×”×“×•×¨ ×”×‘× ×©×œ ×™×™×©×•××™× ××‘×•×¡×¡×™ AI.

## ××©××‘×™× × ×•×¡×¤×™×

- **×ª×™×¢×•×“ ×¨×©××™**: [×ª×™×¢×•×“ Qwen](https://qwen.readthedocs.io/)
- **×××’×¨ ××•×“×œ×™×**: [××•×¡×£ Qwen ×‘-Hugging Face](https://huggingface.co/collections/Qwen/)
- **××××¨×™× ×˜×›× ×™×™×**: [×¤×¨×¡×•××™ ××—×§×¨ Qwen](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **×§×”×™×œ×”**: [×“×™×•× ×™× ×•× ×•×©××™× ×‘-GitHub](https://github.com/QwenLM/)
- **×¤×œ×˜×¤×•×¨××ª ModelScope**: [ModelScope ×©×œ Alibaba](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## ×ª×•×¦××•×ª ×œ××™×“×”

×œ××—×¨ ×”×©×œ××ª ××•×“×•×œ ×–×”, ×ª×•×›×œ×•:

1. ×œ×”×¡×‘×™×¨ ××ª ×”×™×ª×¨×•× ×•×ª ×”××¨×›×™×˜×§×˜×•× ×™×™× ×©×œ ××©×¤×—×ª ×”××•×“×œ×™× Qwen ×•××ª ×’×™×©×ª×” ×‘×§×•×“ ×¤×ª×•×—
2. ×œ×‘×—×•×¨ ××ª ×”×•×•×¨×™×× ×˜ ×”××ª××™× ×©×œ Qwen ×‘×”×ª×‘×¡×¡ ×¢×œ ×“×¨×™×©×•×ª ×™×™×©×•× ×•××©××‘×™×
3. ×œ×™×™×©× ××•×“×œ×™× ×©×œ Qwen ×‘×ª×¨×—×™×©×™ ×¤×¨×™×¡×” ×©×•× ×™× ×¢× ×ª×¦×•×¨×•×ª ××•×¤×˜×™××œ×™×•×ª
4. ×œ×™×™×©× ×˜×›× ×™×§×•×ª ×›×™××•×ª ×•××•×¤×˜×™××™×–×¦×™×” ×œ×©×™×¤×•×¨ ×‘×™×¦×•×¢×™ ××•×“×œ×™× ×©×œ Qwen
5. ×œ×”×¢×¨×™×š ××ª ×”×¤×©×¨×•×ª ×‘×™×Ÿ ×’×•×“×œ ××•×“×œ, ×‘×™×¦×•×¢×™× ×•×™×›×•×œ×•×ª ×‘××©×¤×—×ª Qwen

## ××” ×”×œ××”

- [03: ×™×¡×•×“×•×ª ××©×¤×—×ª Gemma](03.GemmaFamily.md)

---

**×›×ª×‘ ×•×™×ª×•×¨**:  
××¡××š ×–×” ×ª×•×¨×’× ×‘×××¦×¢×•×ª ×©×™×¨×•×ª ×ª×¨×’×•× ××‘×•×¡×¡ ×‘×™× ×” ××œ××›×•×ª×™×ª [Co-op Translator](https://github.com/Azure/co-op-translator). ×‘×¢×•×“ ×©×× ×• ×©×•××¤×™× ×œ×“×™×•×§, ×™×© ×œ×”×™×•×ª ××•×“×¢×™× ×œ×›×š ×©×ª×¨×’×•××™× ××•×˜×•××˜×™×™× ×¢×©×•×™×™× ×œ×”×›×™×œ ×©×’×™××•×ª ××• ××™ ×“×™×•×§×™×. ×”××¡××š ×”××§×•×¨×™ ×‘×©×¤×ª×• ×”××§×•×¨×™×ª ×¦×¨×™×š ×œ×”×™×—×©×‘ ×›××§×•×¨ ×¡××›×•×ª×™. ×¢×‘×•×¨ ××™×“×¢ ×§×¨×™×˜×™, ××•××œ×¥ ×œ×”×©×ª××© ×‘×ª×¨×’×•× ××§×¦×•×¢×™ ×¢×œ ×™×“×™ ××“×. ××™× × ×• × ×•×©××™× ×‘××—×¨×™×•×ª ×œ××™ ×”×‘× ×•×ª ××• ×œ×¤×¨×©× ×•×™×•×ª ×©×’×•×™×•×ª ×”× ×•×‘×¢×•×ª ××©×™××•×© ×‘×ª×¨×’×•× ×–×”.