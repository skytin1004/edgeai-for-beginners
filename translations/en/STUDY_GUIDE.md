<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3f8ec059920a41b354c806f312b6ee24",
  "translation_date": "2025-09-26T07:19:29+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "en"
}
-->
# EdgeAI for Beginners: Learning Paths and Study Schedule

### Intensive Learning Path (1 week)

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 0 | Module 0: Introduction to EdgeAI | 1-2 hours |
| Day 1 | Module 1: EdgeAI Fundamentals | 3 hours |
| Day 2 | Module 2: SLM Foundations | 3 hours |
| Day 3 | Module 3: SLM Deployment | 2 hours |
| Day 4-5 | Module 4: Model Optimization (6 frameworks) | 4 hours |
| Day 6 | Module 5: SLMOps | 3 hours |
| Day 7 | Module 6-7: AI Agents & Development Tools | 4 hours |
| Day 8 | Module 8: Foundry Local Toolkit (Modern Implementation) | 1 hour |

### Intensive Learning Path (2 weeks)

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 1-2 | Module 1: EdgeAI Fundamentals | 3 hours |
| Day 3-4 | Module 2: SLM Foundations | 3 hours |
| Day 5-6 | Module 3: SLM Deployment | 2 hours |
| Day 7-8 | Module 4: Model Optimization | 4 hours |
| Day 9-10 | Module 5: SLMOps | 3 hours |
| Day 11-12 | Module 6: AI Agents | 2 hours |
| Day 13-14 | Module 7: Development Tools | 3 hours |

### Part-time Study (4 weeks)

| Week | Focus | Estimated Hours |
|------|-------|------------------|
| Week 1 | Module 1-2: Fundamentals & SLM Foundations | 6 hours |
| Week 2 | Module 3-4: Deployment & Optimization | 6 hours |
| Week 3 | Module 5-6: SLMOps & AI Agents | 5 hours |
| Week 4 | Module 7: Development Tools & Integration | 3 hours |

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 0 | Module 0: Introduction to EdgeAI | 1-2 hours |
| Day 1-2 | Module 1: EdgeAI Fundamentals | 3 hours |
| Day 3-4 | Module 2: SLM Foundations | 3 hours |
| Day 5-6 | Module 3: SLM Deployment | 2 hours |
| Day 7-8 | Module 4: Model Optimization | 4 hours |
| Day 9-10 | Module 5: SLMOps | 3 hours |
| Day 11-12 | Module 6: SLM Agentic Systems | 2 hours |
| Day 13-14 | Module 7: EdgeAI Implementation Samples | 2 hours |

| Module | Completion Date | Hours Spent | Key Takeaways |
|--------|----------------|-------------|--------------|
| Module 0: Introduction to EdgeAI | | | |
| Module 1: EdgeAI Fundamentals | | | |
| Module 2: SLM Foundations | | | |
| Module 3: SLM Deployment | | | |
| Module 4: Model Optimization (6 frameworks) | | | |
| Module 5: SLMOps | | | |
| Module 6: SLM Agentic Systems | | | |
| Module 7: EdgeAI Implementation Samples | | | |
| Hands-on Exercises | | | |
| Mini-Project | | | |

### Part-time Study (4 weeks)

| Week | Focus | Estimated Hours |
|------|-------|------------------|
| Week 1 | Module 1-2: Fundamentals & SLM Foundations | 6 hours |
| Week 2 | Module 3-4: Deployment & Optimization | 6 hours |
| Week 3 | Module 5-6: SLMOps & AI Agents | 5 hours |
| Week 4 | Module 7: Development Tools & Integration | 3 hours |

## Introduction

Welcome to the EdgeAI for Beginners study guide! This document is designed to help you navigate the course materials effectively and maximize your learning experience. It provides structured learning paths, suggested study schedules, key concept summaries, and supplementary resources to deepen your understanding of Edge AI technologies.

This is a concise 20-hour course that delivers essential knowledge about EdgeAI in a time-efficient format, making it perfect for busy professionals and students who want to quickly gain practical skills in this emerging field.

## Course Overview

This course is organized into eight comprehensive modules:

0. **Introduction to EdgeAI** - Foundation and context setting with industry applications and learning objectives  
1. **EdgeAI Fundamentals and Transformation** - Understanding the core concepts and technology shift  
2. **Small Language Model Foundations** - Exploring various SLM families and their architectures  
3. **Small Language Model Deployment** - Implementing practical deployment strategies  
4. **Model Format Conversion and Quantization** - Advanced optimization with 6 frameworks including OpenVINO  
5. **SLMOps - Small Language Model Operations** - Production lifecycle management and deployment  
6. **SLM Agentic Systems** - AI agents, function calling, and Model Context Protocol  
7. **EdgeAI Implementation Samples** - AI Toolkit, Windows development, and platform-specific implementations  
8. **Microsoft Foundry Local – Complete Developer Toolkit** - Local-first development with hybrid Azure integration (Module 08)  

## How to Use This Study Guide

- **Progressive Learning**: Follow the modules in order for the most coherent learning experience  
- **Knowledge Checkpoints**: Use the self-assessment questions after each section  
- **Hands-on Practice**: Complete the suggested exercises to reinforce theoretical concepts  
- **Supplementary Resources**: Explore additional materials for topics that interest you most  

## Study Schedule Recommendations

### Intensive Learning Path (1 week)

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 0 | Module 0: Introduction to EdgeAI | 1-2 hours |
| Day 1-2 | Module 1: EdgeAI Fundamentals | 6 hours |
| Day 3-4 | Module 2: SLM Foundations | 8 hours |
| Day 5 | Module 3: SLM Deployment | 3 hours |
| Day 6 | Module 8: Foundry Local Toolkit | 3 hours |

### Part-time Study (3 weeks)

| Week | Focus | Estimated Hours |
|------|-------|------------------|
| Week 1 | Module 0: Introduction + Module 1: EdgeAI Fundamentals | 7-9 hours |
| Week 2 | Module 2: SLM Foundations | 7-8 hours |
| Week 3 | Module 3: SLM Deployment (3h) + Module 8: Foundry Local Toolkit (2-3h) | 5-6 hours |

## Module 0: Introduction to EdgeAI

### Key Learning Objectives

- Understand what Edge AI is and why it matters in today's technology landscape  
- Identify major industries transformed by Edge AI and their specific use cases  
- Comprehend the advantages of Small Language Models (SLMs) for edge deployment  
- Establish clear learning expectations and outcomes for the complete course  
- Recognize career opportunities and skill requirements in the Edge AI field  

### Study Focus Areas

#### Section 1: Edge AI Paradigm and Definition
- **Priority Concepts**:  
  - Edge AI vs. traditional cloud AI processing  
  - The convergence of hardware, model optimization, and business demands  
  - Real-time, privacy-preserving, and cost-efficient AI deployment  

#### Section 2: Industry Applications
- **Priority Concepts**:  
  - Manufacturing & Industry 4.0: Predictive maintenance and quality control  
  - Healthcare: Diagnostic imaging and patient monitoring  
  - Autonomous Systems: Self-driving vehicles and transportation  
  - Smart Cities: Traffic management and public safety  
  - Consumer Technology: Smartphones, wearables, and smart homes  

#### Section 3: Small Language Models Foundation
- **Priority Concepts**:  
  - SLM characteristics and performance comparisons  
  - Parameter efficiency vs. capability trade-offs  
  - Edge deployment constraints and optimization strategies  

#### Section 4: Learning Framework and Career Path
- **Priority Concepts**:  
  - Course architecture and progressive mastery approach  
  - Technical skills and practical implementation goals  
  - Career advancement opportunities and industry applications  

### Self-Assessment Questions

1. What are the three main technological trends that have enabled Edge AI?  
2. Compare the advantages and challenges of Edge AI vs. cloud-based AI.  
3. Name three industries where Edge AI provides critical business value and explain why.  
4. How do Small Language Models make Edge AI practical for real-world deployment?  
5. What are the key technical skills you'll develop throughout this course?  
6. Describe the four-phase learning approach used in this course.  

### Hands-on Exercises

1. **Industry Research**: Choose one industry application and research a real-world Edge AI implementation (30 minutes)  
2. **Model Exploration**: Browse available Small Language Models on Hugging Face and compare their parameter counts and capabilities (30 minutes)  
3. **Learning Planning**: Review the complete course structure and create your personal study schedule (15 minutes)  

### Supplementary Materials

- [Edge AI Market Overview - McKinsey](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-age-of-ai)  
- [Small Language Models Overview - Hugging Face](https://huggingface.co/blog/small-language-models)  
- [Edge Computing Foundation](https://www.edgecomputing.org/)  

## Module 1: EdgeAI Fundamentals and Transformation

### Key Learning Objectives

- Understand the differences between cloud-based and edge-based AI  
- Master core optimization techniques for resource-constrained environments  
- Analyze real-world applications of EdgeAI technologies  
- Set up a development environment for EdgeAI projects  

### Study Focus Areas

#### Section 1: EdgeAI Fundamentals
- **Priority Concepts**:  
  - Edge vs. Cloud computing paradigms  
  - Model quantization techniques  
  - Hardware acceleration options (NPUs, GPUs, CPUs)  
  - Privacy and security advantages  

- **Supplementary Materials**:  
  - [TensorFlow Lite Documentation](https://www.tensorflow.org/lite)  
  - [ONNX Runtime GitHub](https://github.com/microsoft/onnxruntime)  
  - [Edge Impulse Documentation](https://docs.edgeimpulse.com)  

#### Section 2: Real-World Case Studies
- **Priority Concepts**:  
  - Microsoft Phi & Mu model ecosystem  
  - Practical implementations across industries  
  - Deployment considerations  

#### Section 3: Practical Implementation Guide
- **Priority Concepts**:  
  - Development environment setup  
  - Quantization and optimization tools  
  - Assessment methods for EdgeAI implementations  

#### Section 4: Edge Deployment Hardware
- **Priority Concepts**:  
  - Hardware platform comparisons  
  - Optimization strategies for specific hardware  
  - Deployment considerations  

### Self-Assessment Questions

1. Compare and contrast cloud-based AI with edge-based AI implementations.  
2. Explain three key techniques for optimizing models for edge deployment.  
3. What are the primary advantages of running AI models at the edge?  
4. Describe the process of quantizing a model and how it affects performance.  
5. Explain how different hardware accelerators (NPUs, GPUs, CPUs) influence EdgeAI deployment.  

### Hands-on Exercises

1. **Quick Environment Setup**: Configure a minimal development environment with the essential packages (30 minutes)  
2. **Model Exploration**: Download and examine a pre-trained small language model (1 hour)  
3. **Basic Quantization**: Try simple quantization on a small model (1 hour)  

## Module 2: Small Language Model Foundations

### Key Learning Objectives

- Understand the architectural principles of different SLM families  
- Compare model capabilities across different parameter scales  
- Evaluate models based on efficiency, capability, and deployment requirements  
- Recognize appropriate use cases for different model families  

### Study Focus Areas

#### Section 1: Microsoft Phi Model Family
- **Priority Concepts**:  
  - Design philosophy evolution  
  - Efficiency-first architecture  
  - Specialized capabilities  

#### Section 2: Qwen Family
- **Priority Concepts**:  
  - Open source contributions  
  - Scalable deployment options  
  - Advanced reasoning architecture  

#### Section 3: Gemma Family
- **Priority Concepts**:  
  - Research-driven innovation  
  - Multimodal capabilities  
  - Mobile optimization  

#### Section 4: BitNET Family
- **Priority Concepts**:  
  - 1-bit quantization technology  
  - Inference optimization framework  
  - Sustainability considerations  

#### Section 5: Microsoft Mu Model
- **Priority Concepts**:  
  - Device-first architecture  
  - System integration with Windows  
  - Privacy-preserving operation  

#### Section 6: Phi-Silica
- **Priority Concepts**:  
  - NPU-optimized architecture  
  - Performance metrics  
  - Developer integration  

### Self-Assessment Questions

1. Compare the architectural approaches of the Phi and Qwen model families.  
2. Explain how BitNET's quantization technology differs from traditional quantization.  
3. What are the unique advantages of the Mu model for Windows integration?  
4. Describe how Phi-Silica utilizes NPU hardware to optimize performance.  
5. For a mobile application with limited connectivity, which model family would be the most suitable and why?  

### Hands-on Exercises  

1. **Model Comparison**: Conduct a quick benchmark of two different SLM models (1 hour)  
2. **Simple Text Generation**: Implement basic text generation using a small model (1 hour)  
3. **Fast Optimization**: Apply an optimization technique to improve inference speed (1 hour)  

## Module 3: Small Language Model Deployment  

### Key Learning Objectives  

- Choose the right models based on deployment constraints  
- Learn optimization techniques for different deployment scenarios  
- Implement SLMs in both local and cloud environments  
- Design production-ready configurations for EdgeAI applications  

### Study Focus Areas  

#### Section 1: SLM Advanced Learning  
- **Priority Concepts**:  
  - Parameter classification framework  
  - Advanced optimization techniques  
  - Strategies for acquiring models  

#### Section 2: Local Environment Deployment  
- **Priority Concepts**:  
  - Deployment on the Ollama platform  
  - Microsoft Foundry local solutions  
  - Comparative analysis of frameworks  

#### Section 3: Containerized Cloud Deployment  
- **Priority Concepts**:  
  - High-performance inference with vLLM  
  - Container orchestration  
  - Implementation using ONNX Runtime  

### Self-Assessment Questions  

1. What factors should be considered when deciding between local and cloud deployment?  
2. Compare Ollama and Microsoft Foundry Local as deployment options.  
3. What are the benefits of containerization for SLM deployment?  
4. What performance metrics should be monitored for an edge-deployed SLM?  
5. Outline a complete deployment workflow from model selection to production implementation.  

### Hands-on Exercises  

1. **Basic Local Deployment**: Deploy a simple SLM using Ollama (1 hour)  
2. **Performance Check**: Benchmark your deployed model (30 minutes)  
3. **Simple Integration**: Build a minimal application that uses your deployed model (1 hour)  

## Module 4: Model Format Conversion and Quantization  

### Key Learning Objectives  

- Master advanced quantization techniques ranging from 1-bit to 8-bit precision  
- Learn strategies for format conversion (GGUF, ONNX)  
- Optimize models across six frameworks (Llama.cpp, Olive, OpenVINO, MLX, workflow synthesis)  
- Deploy optimized models for production edge environments on Intel, Apple, and cross-platform hardware  

### Study Focus Areas  

#### Section 1: Quantization Foundations  
- **Priority Concepts**:  
  - Framework for precision classification  
  - Trade-offs between performance and accuracy  
  - Memory footprint optimization  

#### Section 2: Llama.cpp Implementation  
- **Priority Concepts**:  
  - Cross-platform deployment  
  - Optimization using GGUF format  
  - Techniques for hardware acceleration  

#### Section 3: Microsoft Olive Suite  
- **Priority Concepts**:  
  - Hardware-aware optimization  
  - Enterprise-grade deployment  
  - Automated optimization workflows  

#### Section 4: OpenVINO Toolkit  
- **Priority Concepts**:  
  - Optimization for Intel hardware  
  - Neural Network Compression Framework (NNCF)  
  - Cross-platform inference deployment  
  - OpenVINO GenAI for LLM deployment  

#### Section 5: Apple MLX Framework  
- **Priority Concepts**:  
  - Optimization for Apple Silicon  
  - Unified memory architecture  
  - LoRA fine-tuning capabilities  

#### Section 6: Edge AI Development Workflow Synthesis  
- **Priority Concepts**:  
  - Unified workflow architecture  
  - Decision trees for framework selection  
  - Validation for production readiness  
  - Strategies for future-proofing  

### Self-Assessment Questions  

1. Compare quantization strategies across different precision levels (1-bit to 8-bit).  
2. What are the advantages of GGUF format for edge deployment?  
3. How does hardware-aware optimization in Microsoft Olive improve deployment efficiency?  
4. What are the benefits of OpenVINO's NNCF for model compression?  
5. How does Apple MLX utilize unified memory architecture for optimization?  
6. How does workflow synthesis aid in selecting optimal optimization frameworks?  

### Hands-on Exercises  

1. **Model Quantization**: Apply various quantization levels to a model and compare results (1 hour)  
2. **OpenVINO Optimization**: Use NNCF to compress a model for Intel hardware (1 hour)  
3. **Framework Comparison**: Test the same model across three optimization frameworks (1 hour)  
4. **Performance Benchmarking**: Measure the impact of optimization on inference speed and memory usage (1 hour)  

## Module 5: SLMOps - Small Language Model Operations  

### Key Learning Objectives  

- Understand lifecycle management principles for SLMOps  
- Master distillation and fine-tuning techniques for edge deployment  
- Implement production deployment strategies with monitoring  
- Develop enterprise-grade workflows for SLM operations and maintenance  

### Study Focus Areas  

#### Section 1: Introduction to SLMOps  
- **Priority Concepts**:  
  - Paradigm shift in AI operations with SLMOps  
  - Cost-efficient and privacy-first architecture  
  - Strategic business impact and competitive advantages  

#### Section 2: Model Distillation  
- **Priority Concepts**:  
  - Techniques for knowledge transfer  
  - Implementation of a two-stage distillation process  
  - Distillation workflows using Azure ML  

#### Section 3: Fine-tuning Strategies  
- **Priority Concepts**:  
  - Parameter-efficient fine-tuning (PEFT)  
  - Advanced methods like LoRA and QLoRA  
  - Multi-adapter training and hyperparameter optimization  

#### Section 4: Production Deployment  
- **Priority Concepts**:  
  - Model conversion and quantization for production  
  - Configuration for Foundry Local deployment  
  - Performance benchmarking and quality validation  

### Self-Assessment Questions  

1. How does SLMOps differ from traditional MLOps?  
2. What are the benefits of model distillation for edge deployment?  
3. What considerations are important for fine-tuning SLMs in resource-constrained environments?  
4. Outline a complete production deployment pipeline for edge AI applications.  

### Hands-on Exercises  

1. **Basic Distillation**: Create a smaller model from a larger teacher model (1 hour)  
2. **Fine-tuning Experiment**: Fine-tune a model for a specific domain (1 hour)  
3. **Deployment Pipeline**: Set up a basic CI/CD pipeline for model deployment (1 hour)  

## Module 6: SLM Agentic Systems - AI Agents and Function Calling  

### Key Learning Objectives  

- Develop intelligent AI agents for edge environments using Small Language Models  
- Implement function calling capabilities with systematic workflows  
- Master integration of Model Context Protocol (MCP) for standardized tool interaction  
- Build advanced agentic systems with minimal human intervention  

### Study Focus Areas  

#### Section 1: AI Agents and SLM Foundations  
- **Priority Concepts**:  
  - Framework for agent classification (reflex, model-based, goal-based, learning agents)  
  - Analysis of trade-offs between SLMs and LLMs  
  - Design patterns for edge-specific agents  
  - Resource optimization for agents  

#### Section 2: Function Calling in Small Language Models  
- **Priority Concepts**:  
  - Systematic workflow implementation (intent detection, JSON output, external execution)  
  - Platform-specific implementations (Phi-4-mini, selected Qwen models, Microsoft Foundry Local)  
  - Advanced examples (multi-agent collaboration, dynamic tool selection)  
  - Production considerations (rate limiting, audit logging, security measures)  

#### Section 3: Model Context Protocol (MCP) Integration  
- **Priority Concepts**:  
  - Protocol architecture and layered system design  
  - Multi-backend support (Ollama for development, vLLM for production)  
  - Connection protocols (STDIO and SSE modes)  
  - Real-world applications (web automation, data processing, API integration)  

### Self-Assessment Questions  

1. What are the key architectural considerations for edge AI agents?  
2. How does function calling enhance agent capabilities?  
3. What role does Model Context Protocol play in agent communication?  

### Hands-on Exercises  

1. **Simple Agent**: Build a basic AI agent with function calling (1 hour)  
2. **MCP Integration**: Implement MCP in an agent application (30 minutes)  

## Module 7: EdgeAI Implementation Samples  

### Key Learning Objectives  

- Utilize AI Toolkit for Visual Studio Code for comprehensive EdgeAI development workflows  
- Gain expertise in Windows AI Foundry platform and NPU optimization strategies  
- Implement EdgeAI across various hardware platforms and deployment scenarios  
- Build production-ready EdgeAI applications with platform-specific optimizations  

### Study Focus Areas  

#### Section 1: AI Toolkit for Visual Studio Code  
- **Priority Concepts**:  
  - Comprehensive Edge AI development environment within VS Code  
  - Model catalog and discovery for edge deployment  
  - Local testing, optimization, and agent development workflows  
  - Performance monitoring and evaluation for edge scenarios  

#### Section 2: Windows EdgeAI Development Guide  
- **Priority Concepts**:  
  - Overview of Windows AI Foundry platform  
  - Efficient NPU inference using Phi Silica API  
  - Computer Vision APIs for image processing and OCR  
  - Foundry Local CLI for local development and testing  

#### Section 3: Platform-Specific Implementations  
- **Priority Concepts**:  
  - Deployment on NVIDIA Jetson Orin Nano (67 TOPS AI performance)  
  - Mobile applications using .NET MAUI and ONNX Runtime GenAI  
  - Azure EdgeAI solutions with cloud-edge hybrid architecture  
  - Optimization for Windows ML with universal hardware support  
  - Privacy-focused RAG implementation with Foundry Local  

### Self-Assessment Questions  

1. How does AI Toolkit streamline the EdgeAI development workflow?  
2. Compare deployment strategies across different hardware platforms.  
3. What are the advantages of Windows AI Foundry for edge development?  
4. Explain the role of NPU optimization in modern edge AI applications.  
5. How does the Phi Silica API utilize NPU hardware for performance optimization?  
6. Compare the benefits of local vs. cloud deployment for privacy-sensitive applications.  

### Hands-on Exercises  

1. **AI Toolkit Setup**: Configure AI Toolkit and optimize a model (1 hour)  
2. **Windows AI Foundry**: Build a simple Windows AI application using Phi Silica API (1 hour)  
3. **Cross-Platform Deployment**: Deploy the same model on two different platforms (1 hour)  
4. **NPU Optimization**: Test NPU performance with Windows AI Foundry tools (30 minutes)  

## Module 8: Microsoft Foundry Local – Complete Developer Toolkit (Modernized)  

### Key Learning Objectives  

- Install and configure Foundry Local with modern SDK integration  
- Develop advanced multi-agent systems using coordinator patterns  
- Build intelligent model routers for automatic task-based selection  
- Deploy production-ready AI solutions with comprehensive monitoring  
- Integrate with Azure AI Foundry for hybrid deployment scenarios  
- Master modern SDK patterns with FoundryLocalManager and OpenAI client  

### Study Focus Areas  

#### Section 1: Modern Installation and Configuration  
- **Priority Concepts**:  
  - SDK integration with FoundryLocalManager  
  - Automatic service discovery and health monitoring  
  - Configuration patterns based on environment  
  - Considerations for production deployment  

#### Section 2: Advanced Multi-Agent Systems  
- **Priority Concepts**:  
  - Coordinator pattern with specialist agents  
  - Specialization in retrieval, reasoning, and execution agents  
  - Feedback loop mechanisms for refinement  
  - Performance monitoring and statistics tracking  

#### Section 3: Intelligent Model Routing  
- **Priority Concepts**:  
  - Algorithms for keyword-based model selection  
  - Support for multiple models (general, reasoning, code, creative)  
  - Flexible configuration using environment variables  
  - Service health checks and error handling  

#### Section 4: Production-Ready Implementation  
- **Priority Concepts**:  
  - Comprehensive error handling and fallback mechanisms  
  - Monitoring requests and tracking performance  
  - Interactive Jupyter notebook examples with benchmarks  
  - Integration patterns with existing applications  

### Self-Assessment Questions  

1. How does the modern FoundryLocalManager approach differ from manual REST calls?  
2. Explain the coordinator pattern and its role in orchestrating specialist agents.  
3. How does the intelligent router select models based on query content?  
4. What are the key components of a production-ready AI agent system?  
5. How do you implement comprehensive health monitoring for Foundry Local services?  
6. Compare the benefits of the modernized approach versus traditional implementation patterns.  

### Hands-on Exercises  

1. **Modern SDK Setup**: Configure FoundryLocalManager with automatic service discovery (30 minutes)  
2. **Multi-Agent System**: Run the advanced coordinator with specialist agents (30 minutes)  
3. **Intelligent Routing**: Test the model router with different query types (30 minutes)  
4. **Interactive Exploration**: Use Jupyter notebooks to explore advanced features (45 minutes)  
5. **Production Deployment**: Implement monitoring and error handling patterns (30 minutes)  
6. **Hybrid Integration**: Configure Azure AI Foundry fallback scenarios (30 minutes)  

## Time Allocation Guide  

To maximize the 20-hour course timeline, here’s a suggested breakdown of time allocation:  

| Activity | Time Allocation | Description |  
|----------|----------------|-------------|  
| Reading Core Materials | 9 hours | Focus on essential concepts in each module |  
| Hands-on Exercises | 6 hours | Practical application of key techniques |
| Self-Assessment | 2 hours | Test your understanding through questions and reflection |
| Mini-Project | 3 hours | Apply knowledge to a small practical implementation |

### Key Focus Areas Based on Time Availability

**If you only have 10 hours:**
- Complete Module 0 (Introduction) and Modules 1, 2, and 3 (core EdgeAI concepts)
- Do at least one hands-on exercise per module
- Focus on understanding the core concepts rather than implementation details

**If you can dedicate the full 20 hours:**
- Complete all eight modules (including Introduction)
- Perform key hands-on exercises from each module
- Complete one mini-project from Module 7
- Explore at least 2-3 supplementary resources

**If you have more than 20 hours:**
- Complete all modules (including Introduction) with detailed exercises
- Build multiple mini-projects
- Explore advanced optimization techniques in Module 4
- Implement production deployment from Module 5

## Essential Resources

These carefully selected resources provide the most value for your limited study time:

### Must-Read Documentation
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - The most efficient model optimization tool
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Fastest way to deploy SLMs locally
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Reference for a leading edge-optimized model
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Intel's comprehensive optimization toolkit
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Integrated EdgeAI development environment
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Windows-specific EdgeAI development platform

### Time-Saving Tools
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Quick model access and deployment
- [Gradio](https://www.gradio.app/docs/interface) - Rapid UI development for AI demos
- [Microsoft Olive](https://github.com/microsoft/Olive) - Simplified model optimization
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Efficient CPU inference
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Neural network compression framework
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Large language model deployment toolkit

## Progress Tracking Template

Use this simplified template to track your learning progress through the 20-hour course:

| Module | Completion Date | Hours Spent | Key Takeaways |
|--------|----------------|-------------|---------------|
| Module 0: Introduction to EdgeAI | | | |
| Module 1: EdgeAI Fundamentals | | | |
| Module 2: SLM Foundations | | | |
| Module 3: SLM Deployment | | | |
| Module 4: Model Optimization | | | |
| Module 5: SLMOps | | | |
| Module 6: AI Agents | | | |
| Module 7: Development Tools | | | |
| Module 8: Foundry Local Toolkit | | | |
| Hands-on Exercises | | | |
| Mini-Project | | | |

## Mini Project Ideas

Consider completing one of these projects to practice EdgeAI concepts (each designed to take 2-4 hours):

### Beginner Projects (2-3 hours each)
1. **Edge Text Assistant**: Create a simple offline text completion tool using a small language model
2. **Model Comparison Dashboard**: Build a basic visualization of performance metrics across different SLMs
3. **Optimization Experiment**: Measure the impact of different quantization levels on the same base model

### Intermediate Projects (3-4 hours each)
4. **AI Toolkit Workflow**: Use VS Code AI Toolkit to optimize and deploy a model from start to finish
5. **Windows AI Foundry Application**: Create a Windows app using Phi Silica API and NPU optimization
6. **Cross-Platform Deployment**: Deploy the same optimized model on Windows (OpenVINO) and mobile (.NET MAUI)
7. **Function Calling Agent**: Build an AI agent with function calling capabilities for edge scenarios

### Advanced Integration Projects (4-5 hours each)
8. **OpenVINO Optimization Pipeline**: Implement complete model optimization using NNCF and GenAI toolkit
9. **SLMOps Pipeline**: Implement a complete model lifecycle from training to edge deployment
10. **Multi-Model Edge System**: Deploy multiple specialized models working together on edge hardware
11. **MCP Integration System**: Build an agentic system using Model Context Protocol for tool interaction

## References

- Microsoft Learn (Foundry Local)
  - Overview: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
  - Get started: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
  - CLI reference: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
  - Integrate with inference SDKs: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
  - Open WebUI how-to: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
  - Compile Hugging Face models: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Azure AI Foundry
  - Overview: https://learn.microsoft.com/en-us/azure/ai-foundry/
  - Agents (overview): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- Optimization and Inference Tooling
  - Microsoft Olive (docs): https://microsoft.github.io/Olive/
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive
  - ONNX Runtime (getting started): https://onnxruntime.ai/docs/get-started/with-python.html
  - ONNX Runtime Olive integration: https://onnxruntime.ai/docs/performance/olive.html
  - OpenVINO (docs): https://docs.openvino.ai/2025/index.html
  - Apple MLX (docs): https://ml-explore.github.io/mlx/build/html/index.html
- Deployment Frameworks and Models
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index
  - vLLM (docs): https://docs.vllm.ai/
  - Ollama (quick start): https://github.com/ollama/ollama#get-started
- Developer Tools (Windows and VS Code)
  - AI Toolkit for VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview
  - Windows ML (overview): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview

## Learning Community

Join the discussion and connect with fellow learners:
- GitHub Discussions on the [EdgeAI for Beginners repository](https://github.com/microsoft/edgeai-for-beginners/discussions)
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)

## Conclusion

EdgeAI represents the cutting edge of artificial intelligence, bringing powerful capabilities directly to devices while addressing critical concerns like privacy, latency, and connectivity. This 20-hour course equips you with the essential knowledge and practical skills to start working with EdgeAI technologies right away.

The course is designed to be concise and focused on the most important concepts, enabling you to quickly gain valuable expertise without an overwhelming time commitment. Remember, hands-on practice—even with simple examples—is key to solidifying your understanding.

Happy learning!

---

