<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "308873206bc5ec5d87e34ca2907d2c9c",
  "translation_date": "2025-09-18T19:05:20+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "sk"
}
-->
# Sekcia 2: Nasadenie v lokálnom prostredí - Riešenia s dôrazom na súkromie

Lokálne nasadenie malých jazykových modelov (SLM) predstavuje posun smerom k riešeniam AI, ktoré chránia súkromie a sú nákladovo efektívne. Tento komplexný sprievodca skúma dva výkonné rámce—Ollama a Microsoft Foundry Local—ktoré umožňujú vývojárom využiť plný potenciál SLM pri zachovaní úplnej kontroly nad prostredím nasadenia.

## Úvod

V tejto lekcii preskúmame pokročilé stratégie nasadenia malých jazykových modelov v lokálnych prostrediach. Pokryjeme základné koncepty lokálneho nasadenia AI, preskúmame dve popredné platformy (Ollama a Microsoft Foundry Local) a poskytneme praktické pokyny na implementáciu riešení pripravených na produkciu.

## Ciele učenia

Na konci tejto lekcie budete schopní:

- Pochopiť architektúru a výhody rámcov lokálneho nasadenia SLM.
- Implementovať nasadenia pripravené na produkciu pomocou Ollama a Microsoft Foundry Local.
- Porovnať a vybrať vhodnú platformu na základe konkrétnych požiadaviek a obmedzení.
- Optimalizovať lokálne nasadenia z hľadiska výkonu, bezpečnosti a škálovateľnosti.

## Pochopenie architektúr lokálneho nasadenia SLM

Lokálne nasadenie SLM predstavuje zásadný posun od služieb AI závislých na cloude k riešeniam na mieste, ktoré chránia súkromie. Tento prístup umožňuje organizáciám udržiavať úplnú kontrolu nad ich AI infraštruktúrou a zároveň zabezpečiť suverenitu dát a operačnú nezávislosť.

### Klasifikácia rámcov nasadenia

Pochopenie rôznych prístupov k nasadeniu pomáha pri výbere správnej stratégie pre konkrétne použitia:

- **Zamerané na vývoj**: Jednoduché nastavenie na experimentovanie a prototypovanie
- **Podniková úroveň**: Riešenia pripravené na produkciu s možnosťami integrácie do podnikového prostredia  
- **Cross-platformové**: Univerzálna kompatibilita naprieč rôznymi operačnými systémami a hardvérom

### Kľúčové výhody lokálneho nasadenia SLM

Lokálne nasadenie SLM ponúka niekoľko zásadných výhod, ktoré ho robia ideálnym pre podnikové a aplikácie citlivé na súkromie:

**Súkromie a bezpečnosť**: Lokálne spracovanie zabezpečuje, že citlivé údaje nikdy neopustia infraštruktúru organizácie, čo umožňuje dodržiavanie GDPR, HIPAA a ďalších regulačných požiadaviek. Nasadenia v izolovaných prostrediach sú možné pre klasifikované prostredia, zatiaľ čo kompletné záznamy auditu udržiavajú bezpečnostný dohľad.

**Nákladová efektívnosť**: Eliminácia modelov cenotvorby na základe tokenov výrazne znižuje prevádzkové náklady. Nižšie požiadavky na šírku pásma a znížená závislosť na cloude poskytujú predvídateľné nákladové štruktúry pre podnikové rozpočtovanie.

**Výkon a spoľahlivosť**: Rýchlejšie časy inferencie bez latencie siete umožňujú aplikácie v reálnom čase. Offline funkčnosť zabezpečuje nepretržitú prevádzku bez ohľadu na internetové pripojenie, zatiaľ čo optimalizácia lokálnych zdrojov poskytuje konzistentný výkon.

## Ollama: Univerzálna platforma pre lokálne nasadenie

### Základná architektúra a filozofia

Ollama je navrhnutá ako univerzálna, vývojársky priateľská platforma, ktorá demokratizuje lokálne nasadenie LLM naprieč rôznymi hardvérovými konfiguráciami a operačnými systémami.

**Technický základ**: Postavená na robustnom frameworku llama.cpp, Ollama využíva efektívny modelový formát GGUF pre optimálny výkon. Kompatibilita naprieč platformami zabezpečuje konzistentné správanie na Windows, macOS a Linux prostrediach, zatiaľ čo inteligentné riadenie zdrojov optimalizuje využitie CPU, GPU a pamäte.

**Filozofia dizajnu**: Ollama uprednostňuje jednoduchosť bez obetovania funkčnosti, ponúka nasadenie bez konfigurácie pre okamžitú produktivitu. Platforma udržiava širokú kompatibilitu modelov a poskytuje konzistentné API naprieč rôznymi architektúrami modelov.

### Pokročilé funkcie a schopnosti

**Excelentné riadenie modelov**: Ollama poskytuje komplexné riadenie životného cyklu modelov s automatickým sťahovaním, cachovaním a verzovaním. Platforma podporuje rozsiahly ekosystém modelov vrátane Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral a špecializované modely na embedding.

**Prispôsobenie pomocou Modelfiles**: Pokročilí používatelia môžu vytvárať vlastné konfigurácie modelov so špecifickými parametrami, systémovými promptami a úpravami správania. To umožňuje optimalizácie pre konkrétne oblasti a špecializované požiadavky aplikácií.

**Optimalizácia výkonu**: Ollama automaticky detekuje a využíva dostupné hardvérové akcelerácie vrátane NVIDIA CUDA, Apple Metal a OpenCL. Inteligentné riadenie pamäte zabezpečuje optimálne využitie zdrojov naprieč rôznymi hardvérovými konfiguráciami.

### Stratégie implementácie v produkcii

**Inštalácia a nastavenie**: Ollama poskytuje jednoduchú inštaláciu naprieč platformami prostredníctvom natívnych inštalátorov, správcu balíkov (WinGet, Homebrew, APT) a Docker kontajnerov pre kontajnerizované nasadenia.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Základné príkazy a operácie**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Pokročilá konfigurácia**: Modelfiles umožňujú sofistikované prispôsobenie pre podnikové požiadavky:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Príklady integrácie pre vývojárov

**Integrácia API v Pythone**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**Integrácia v JavaScripte/TypeScripte (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**Použitie RESTful API s cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Ladenie výkonu a optimalizácia

**Konfigurácia pamäte a vlákien**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Výber kvantizácie pre rôzny hardvér**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Podniková platforma Edge AI

### Architektúra na podnikovej úrovni

Microsoft Foundry Local predstavuje komplexné podnikové riešenie navrhnuté špeciálne pre produkčné nasadenia Edge AI s hlbokou integráciou do ekosystému Microsoft.

**Základ na ONNX**: Postavený na priemyselnom štandarde ONNX Runtime, Foundry Local poskytuje optimalizovaný výkon naprieč rôznymi hardvérovými architektúrami. Platforma využíva integráciu Windows ML pre natívnu optimalizáciu na Windows, pričom si zachováva kompatibilitu naprieč platformami.

**Excelentná akcelerácia hardvéru**: Foundry Local obsahuje inteligentnú detekciu hardvéru a optimalizáciu naprieč CPU, GPU a NPU. Hlboká spolupráca s výrobcami hardvéru (AMD, Intel, NVIDIA, Qualcomm) zabezpečuje optimálny výkon na podnikových hardvérových konfiguráciách.

### Pokročilé skúsenosti vývojárov

**Prístup cez viacero rozhraní**: Foundry Local poskytuje komplexné rozhrania pre vývoj vrátane výkonného CLI na riadenie modelov a nasadenie, SDK pre viacero jazykov (Python, NodeJS) na natívnu integráciu a RESTful API s kompatibilitou OpenAI na bezproblémovú migráciu.

**Integrácia s Visual Studio**: Platforma sa bezproblémovo integruje s AI Toolkit pre VS Code, poskytujúc nástroje na konverziu modelov, kvantizáciu a optimalizáciu priamo v prostredí vývoja. Táto integrácia urýchľuje pracovné postupy vývoja a znižuje zložitosť nasadenia.

**Pipeline optimalizácie modelov**: Integrácia Microsoft Olive umožňuje sofistikované pracovné postupy optimalizácie modelov vrátane dynamickej kvantizácie, optimalizácie grafov a ladenia špecifického pre hardvér. Možnosti konverzie cez Azure ML poskytujú škálovateľnú optimalizáciu pre veľké modely.

### Stratégie implementácie v produkcii

**Inštalácia a konfigurácia**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Operácie riadenia modelov**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Pokročilá konfigurácia nasadenia**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Integrácia do podnikového ekosystému

**Bezpečnosť a súlad**: Foundry Local poskytuje bezpečnostné funkcie na podnikovej úrovni vrátane kontroly prístupu na základe rolí, záznamov auditu, reportovania súladu a šifrovaného úložiska modelov. Integrácia s bezpečnostnou infraštruktúrou Microsoft zabezpečuje dodržiavanie podnikových bezpečnostných politík.

**Vstavané AI služby**: Platforma ponúka pripravené AI schopnosti vrátane Phi Silica na lokálne spracovanie jazyka, AI Imaging na vylepšenie a analýzu obrázkov a špecializované API na bežné podnikové AI úlohy.

## Porovnávacia analýza: Ollama vs Foundry Local

### Porovnanie technickej architektúry

| **Aspekt** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **Formát modelu** | GGUF (cez llama.cpp) | ONNX (cez ONNX Runtime) |
| **Zameranie platformy** | Univerzálna cross-platformová | Optimalizácia pre Windows/podniky |
| **Integrácia hardvéru** | Podpora GPU/CPU | Hlboká podpora Windows ML, NPU |
| **Optimalizácia** | Kvantizácia llama.cpp | Microsoft Olive + ONNX Runtime |
| **Podnikové funkcie** | Komunitne riadené | Podniková úroveň so SLA |

### Charakteristiky výkonu

**Silné stránky výkonu Ollama**:
- Výnimočný výkon CPU cez optimalizáciu llama.cpp
- Konzistentné správanie naprieč rôznymi platformami a hardvérom
- Efektívne využitie pamäte s inteligentným načítaním modelov
- Rýchle časy studeného štartu pre vývoj a testovanie

**Výhody výkonu Foundry Local**:
- Vynikajúce využitie NPU na modernom hardvéri Windows
- Optimalizovaná akcelerácia GPU cez partnerstvá s výrobcami
- Monitorovanie výkonu na podnikovej úrovni a optimalizácia
- Škálovateľné možnosti nasadenia pre produkčné prostredia

### Analýza skúseností vývojárov

**Skúsenosti vývojárov s Ollama**:
- Minimálne požiadavky na nastavenie s okamžitou produktivitou
- Intuitívne rozhranie príkazového riadku pre všetky operácie
- Rozsiahla podpora komunity a dokumentácia
- Flexibilné prispôsobenie cez Modelfiles

**Skúsenosti vývojárov s Foundry Local**:
- Komplexná integrácia IDE s ekosystémom Visual Studio
- Podnikové pracovné postupy vývoja s funkciami tímovej spolupráce
- Profesionálne podporné kanály s podporou Microsoftu
- Pokročilé nástroje na ladenie a optimalizáciu

### Optimalizácia použitia

**Vyberte Ollama, keď**:
- Vyvíjate cross-platformové aplikácie vyžadujúce konzistentné správanie
- Uprednostňujete transparentnosť open-source a príspevky komunity
- Pracujete s obmedzenými zdrojmi alebo rozpočtovými obmedzeniami
- Budujete experimentálne alebo výskumné aplikácie
- Potrebujete širokú kompatibilitu modelov naprieč rôznymi architektúrami

**Vyberte Foundry Local, keď**:
- Nasadzujete podnikové aplikácie s prísnymi požiadavkami na výkon
- Využívate optimalizácie hardvéru špecifické pre Windows (NPU, Windows ML)
- Potrebujete podnikovú podporu, SLA a funkcie súladu
- Budujete produkčné aplikácie s integráciou do ekosystému Microsoft
- Potrebujete pokročilé nástroje na optimalizáciu a profesionálne pracovné postupy vývoja

## Pokročilé stratégie nasadenia

### Vzory kontajnerizovaného nasadenia

**Kontajnerizácia Ollama**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Podnikové nasadenie Foundry Local**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Techniky optimalizácie výkonu

**Stratégie optimalizácie Ollama**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Optimalizácia Foundry Local**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Bezpečnostné a súladové úvahy

### Implementácia bezpečnosti na podnikovej úrovni

**Najlepšie praktiky bezpečnosti Ollama**:
- Izolácia siete pomocou pravidiel firewallu a prístupu cez VPN
- Autentifikácia cez integráciu reverzného proxy
- Overenie integrity modelu a bezpečná distribúcia modelov
- Záznamy auditu pre prístup k API a operácie modelov

**Podniková bezpečnosť Foundry Local**:
- Zabudovaná kontrola prístupu na základe rolí s integráciou Active Directory
- Komplexné záznamy auditu s reportovaním súladu
- Šifrované úložisko modelov a bezpečné nasadenie modelov
- Integrácia s bezpečnostnou infraštruktúrou Microsoft

### Požiadavky na súlad a reguláciu

Obe platformy podporujú regulačný súlad prostredníctvom:
- Kontroly rezidencie dát zabezpečujúce lokálne spracovanie
- Záznamy auditu pre požiadavky na regulačné reportovanie
- Kontroly prístupu na manipuláciu s citlivými dátami
- Šifrovanie v pokoji a počas prenosu na ochranu dát

## Najlepšie praktiky pre produkčné nasadenie

### Monitorovanie a pozorovateľnosť

**Kľúčové metriky na monitorovanie**:
- Latencia a priepustnosť inferencie modelu
- Využitie zdrojov (CPU, GPU, pamäť)
- Časy odozvy API a miery chýb
- Presnosť modelu a drift výkonu

**Implementácia monitorovania**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### Kontinuálna integrácia a nasadenie

**Integrácia CI/CD pipeline**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## Budúce trendy a úvahy

### Nové technológie

Krajina lokálneho nasadenia SLM sa neustále vyvíja s niekoľkými kľúčovými trendmi:

**Pokročilé architektúry modelov**: Objavujú sa modely novej generácie s lepšou efektivitou a pomerom schopností, vrátane modelov mixture-of-experts na dynamické škálovanie a špecializovaných architektúr na nasadenie na okraji.

**Integrácia hardvéru**: Hlbšia integrácia so špecializovaným AI hardvérom vrátane NPU, vlastného silikónu a akcelerátorov edge computingu poskytne vylepšené výkonové schopnosti.

**Evolúcia ekosystému**: Snaha o štandardizáciu naprieč platformami nasadenia a zlepšená interoperabilita medzi rôznymi rámcami zjednoduší nasadenia na viacerých platformách.

### Vzory prijatia v priemysle

**Podnikové prijatie**: Rastúce prijatie v podnikoch poháňané požiadavkami na súkromie, optimalizáciu nákladov a potreby regulačného súladu. Vládne a obranné sektory sa obzvlášť zameriavajú na nasadenia v izolovaných prostrediach.

**Globálne úvahy**: Medzinárodné požiadavky na suverenitu dát poháňajú prijatie lokálneho nasadenia, najmä v regiónoch s prísnymi reguláciami ochrany dát.

## Výzvy a úvahy

### Technické výzvy

**Požiadavky na infraš

---

**Upozornenie**:  
Tento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho pôvodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nie sme zodpovední za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.