<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "76b134be93f45283a2df8f8a93717d06",
  "translation_date": "2025-10-17T10:06:36+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "sk"
}
-->
# Sekcia 1: ZÃ¡klady EdgeAI

EdgeAI predstavuje zÃ¡sadnÃº zmenu v nasadzovanÃ­ umelej inteligencie, prinÃ¡Å¡ajÃºc AI schopnosti priamo na koncovÃ© zariadenia namiesto vÃ½luÄnÃ©ho spoliehania sa na spracovanie v cloude. Je dÃ´leÅ¾itÃ© pochopiÅ¥, ako EdgeAI umoÅ¾Åˆuje lokÃ¡lne spracovanie AI na zariadeniach s obmedzenÃ½mi zdrojmi, priÄom si zachovÃ¡va primeranÃ½ vÃ½kon a rieÅ¡i vÃ½zvy ako sÃºkromie, latencia a offline schopnosti.

## Ãšvod

V tejto lekcii preskÃºmame EdgeAI a jeho zÃ¡kladnÃ© koncepty. Pokryjeme tradiÄnÃ½ vÃ½poÄtovÃ½ model AI, vÃ½zvy edge computingu, kÄ¾ÃºÄovÃ© technolÃ³gie umoÅ¾ÅˆujÃºce EdgeAI a praktickÃ© aplikÃ¡cie naprieÄ rÃ´znymi odvetviami.

## Ciele uÄenia

Na konci tejto lekcie budete schopnÃ­:

- PochopiÅ¥ rozdiel medzi tradiÄnÃ½m cloudovÃ½m AI prÃ­stupom a EdgeAI prÃ­stupom.
- IdentifikovaÅ¥ kÄ¾ÃºÄovÃ© technolÃ³gie umoÅ¾ÅˆujÃºce spracovanie AI na koncovÃ½ch zariadeniach.
- RozpoznaÅ¥ vÃ½hody a obmedzenia implementÃ¡ciÃ­ EdgeAI.
- AplikovaÅ¥ znalosti EdgeAI na reÃ¡lne scenÃ¡re a prÃ­pady pouÅ¾itia.

## Pochopenie tradiÄnÃ©ho vÃ½poÄtovÃ©ho modelu AI

TradiÄne sa generatÃ­vne AI aplikÃ¡cie spoliehajÃº na infraÅ¡truktÃºru vysokovÃ½konnÃ©ho vÃ½poÄtovÃ©ho spracovania na efektÃ­vne prevÃ¡dzkovanie veÄ¾kÃ½ch jazykovÃ½ch modelov (LLMs). OrganizÃ¡cie zvyÄajne nasadzujÃº tieto modely na GPU klastroch v cloudovÃ½ch prostrediach, priÄom ich schopnosti vyuÅ¾Ã­vajÃº prostrednÃ­ctvom API rozhranÃ­.

Tento centralizovanÃ½ model funguje dobre pre mnohÃ© aplikÃ¡cie, ale mÃ¡ inherentnÃ© obmedzenia v scenÃ¡roch edge computingu. KonvenÄnÃ½ prÃ­stup zahÅ•Åˆa odosielanie pouÅ¾Ã­vateÄ¾skÃ½ch dotazov na vzdialenÃ© servery, ich spracovanie pomocou vÃ½konnÃ©ho hardvÃ©ru a nÃ¡vrat vÃ½sledkov cez internet. Hoci tÃ¡to metÃ³da poskytuje prÃ­stup k najmodernejÅ¡Ã­m modelom, vytvÃ¡ra zÃ¡vislosti na internetovom pripojenÃ­, prinÃ¡Å¡a obavy z latencie a vyvolÃ¡va otÃ¡zky o sÃºkromÃ­, keÄ sa citlivÃ© Ãºdaje musia prenÃ¡Å¡aÅ¥ na externÃ© servery.

Existuje niekoÄ¾ko zÃ¡kladnÃ½ch konceptov, ktorÃ© musÃ­me pochopiÅ¥ pri prÃ¡ci s tradiÄnÃ½mi vÃ½poÄtovÃ½mi modelmi AI, konkrÃ©tne:

- **â˜ï¸ Spracovanie v cloude**: AI modely beÅ¾ia na vÃ½konnÃ½ch serverovÃ½ch infraÅ¡truktÃºrach s vysokÃ½mi vÃ½poÄtovÃ½mi zdrojmi.
- **ğŸ”Œ PrÃ­stup cez API**: AplikÃ¡cie pristupujÃº k AI schopnostiam prostrednÃ­ctvom vzdialenÃ½ch API volanÃ­ namiesto lokÃ¡lneho spracovania.
- **ğŸ›ï¸ CentralizovanÃ¡ sprÃ¡va modelov**: Modely sÃº udrÅ¾iavanÃ© a aktualizovanÃ© centrÃ¡lne, Äo zaruÄuje konzistenciu, ale vyÅ¾aduje sieÅ¥ovÃ© pripojenie.
- **ğŸ“ˆ Å kÃ¡lovateÄ¾nosÅ¥ zdrojov**: CloudovÃ¡ infraÅ¡truktÃºra sa mÃ´Å¾e dynamicky Å¡kÃ¡lovaÅ¥ na zvlÃ¡dnutie rÃ´znych vÃ½poÄtovÃ½ch poÅ¾iadaviek.

## VÃ½zvy edge computingu

KoncovÃ© zariadenia, ako sÃº notebooky, mobilnÃ© telefÃ³ny a zariadenia Internetu vecÃ­ (IoT), ako Raspberry Pi a NVIDIA Orin Nano, predstavujÃº jedineÄnÃ© vÃ½poÄtovÃ© obmedzenia. Tieto zariadenia majÃº zvyÄajne obmedzenÃ½ vÃ½poÄtovÃ½ vÃ½kon, pamÃ¤Å¥ a energetickÃ© zdroje v porovnanÃ­ s infraÅ¡truktÃºrou dÃ¡tovÃ½ch centier.

PrevÃ¡dzkovanie tradiÄnÃ½ch LLMs na takÃ½chto zariadeniach bolo historicky nÃ¡roÄnÃ© kvÃ´li tÃ½mto hardvÃ©rovÃ½m obmedzeniam. AvÅ¡ak potreba spracovania AI na koncovÃ½ch zariadeniach sa stÃ¡va Äoraz dÃ´leÅ¾itejÅ¡ou v rÃ´znych scenÃ¡roch. ZvÃ¡Å¾te situÃ¡cie, kde je internetovÃ© pripojenie nespoÄ¾ahlivÃ© alebo nedostupnÃ©, ako naprÃ­klad vzdialenÃ© priemyselnÃ© lokality, vozidlÃ¡ v pohybe alebo oblasti so slabÃ½m pokrytÃ­m siete. Okrem toho aplikÃ¡cie vyÅ¾adujÃºce vysokÃ© bezpeÄnostnÃ© Å¡tandardy, ako sÃº medicÃ­nske zariadenia, finanÄnÃ© systÃ©my alebo vlÃ¡dne aplikÃ¡cie, mÃ´Å¾u potrebovaÅ¥ spracovaÅ¥ citlivÃ© Ãºdaje lokÃ¡lne, aby si zachovali sÃºkromie a splnili poÅ¾iadavky na sÃºlad.

### KÄ¾ÃºÄovÃ© obmedzenia edge computingu

Prostredia edge computingu Äelia niekoÄ¾kÃ½m zÃ¡kladnÃ½m obmedzeniam, ktorÃ© tradiÄnÃ© cloudovÃ© AI rieÅ¡enia nepoznajÃº:

- **ObmedzenÃ½ vÃ½poÄtovÃ½ vÃ½kon**: KoncovÃ© zariadenia majÃº zvyÄajne menej CPU jadier a niÅ¾Å¡ie taktovacie frekvencie v porovnanÃ­ s hardvÃ©rom serverovej triedy.
- **PamÃ¤Å¥ovÃ© obmedzenia**: DostupnÃ¡ RAM a kapacita ÃºloÅ¾iska sÃº na koncovÃ½ch zariadeniach vÃ½razne znÃ­Å¾enÃ©.
- **EnergetickÃ© obmedzenia**: Zariadenia napÃ¡janÃ© batÃ©riou musia vyvÃ¡Å¾iÅ¥ vÃ½kon s energetickou spotrebou pre dlhÅ¡iu prevÃ¡dzku.
- **TepelnÃ© riadenie**: KompaktnÃ© formÃ¡ty obmedzujÃº schopnosti chladenia, Äo ovplyvÅˆuje udrÅ¾ateÄ¾nÃ½ vÃ½kon pri zaÅ¥aÅ¾enÃ­.

## ÄŒo je EdgeAI?

### Koncept: DefinÃ­cia Edge AI

Edge AI sa tÃ½ka nasadzovania a vykonÃ¡vania algoritmov umelej inteligencie priamo na koncovÃ½ch zariadeniachâ€”fyzickom hardvÃ©ri, ktorÃ½ existuje na "okraji" siete, blÃ­zko miesta, kde sa generujÃº a zhromaÅ¾ÄujÃº Ãºdaje. Tieto zariadenia zahÅ•ÅˆajÃº smartfÃ³ny, IoT senzory, inteligentnÃ© kamery, autonÃ³mne vozidlÃ¡, nositeÄ¾nÃ© zariadenia a priemyselnÃ© vybavenie. Na rozdiel od tradiÄnÃ½ch AI systÃ©mov, ktorÃ© sa spoliehajÃº na cloudovÃ© servery na spracovanie, Edge AI prinÃ¡Å¡a inteligenciu priamo k zdroju Ãºdajov.

V jadre Edge AI ide o decentralizÃ¡ciu spracovania AI, presun od centralizovanÃ½ch dÃ¡tovÃ½ch centier a distribÃºciu naprieÄ rozsiahlym sieÅ¥ovÃ½m ekosystÃ©mom zariadenÃ­. To predstavuje zÃ¡sadnÃº architektonickÃº zmenu v tom, ako sÃº AI systÃ©my navrhnutÃ© a nasadzovanÃ©.

KÄ¾ÃºÄovÃ© konceptuÃ¡lne piliere Edge AI zahÅ•ÅˆajÃº:

- **Spracovanie v blÃ­zkosti**: VÃ½poÄty prebiehajÃº fyzicky blÃ­zko miesta, kde Ãºdaje vznikajÃº.
- **DecentralizovanÃ¡ inteligencia**: Schopnosti rozhodovania sÃº distribuovanÃ© naprieÄ viacerÃ½mi zariadeniami.
- **SvrchovanosÅ¥ Ãºdajov**: InformÃ¡cie zostÃ¡vajÃº pod lokÃ¡lnou kontrolou, Äasto nikdy neopÃºÅ¡Å¥ajÃº zariadenie.
- **AutonÃ³mna prevÃ¡dzka**: Zariadenia mÃ´Å¾u fungovaÅ¥ inteligentne bez potreby neustÃ¡leho pripojenia.
- **VstavanÃ¡ AI**: Inteligencia sa stÃ¡va neoddeliteÄ¾nou schopnosÅ¥ou kaÅ¾dodennÃ½ch zariadenÃ­.

### VizualizÃ¡cia architektÃºry Edge AI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TRADITIONAL AI ARCHITECTURE                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Data Transfer  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   API Response   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Edge Devices â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Cloud Servers â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ End Users â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            EDGE AI ARCHITECTURE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Direct Response  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Edge Devices with Embedded AI         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ End Users â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â”‚ Sensors â”‚â”€>â”‚ SLM Inference â”‚â”€>â”‚ Local Action â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI predstavuje zÃ¡sadnÃº zmenu v nasadzovanÃ­ umelej inteligencie, prinÃ¡Å¡ajÃºc AI schopnosti priamo na koncovÃ© zariadenia namiesto vÃ½luÄnÃ©ho spoliehania sa na spracovanie v cloude. Tento prÃ­stup umoÅ¾Åˆuje AI modelom beÅ¾aÅ¥ lokÃ¡lne na zariadeniach s obmedzenÃ½mi vÃ½poÄtovÃ½mi zdrojmi, poskytujÃºc schopnosti inferencie v reÃ¡lnom Äase bez potreby neustÃ¡leho internetovÃ©ho pripojenia.

EdgeAI zahÅ•Åˆa rÃ´zne technolÃ³gie a techniky navrhnutÃ© na to, aby AI modely boli efektÃ­vnejÅ¡ie a vhodnÃ© na nasadenie na zariadeniach s obmedzenÃ½mi zdrojmi. CieÄ¾om je zachovaÅ¥ primeranÃ½ vÃ½kon pri vÃ½raznom znÃ­Å¾enÃ­ vÃ½poÄtovÃ½ch a pamÃ¤Å¥ovÃ½ch poÅ¾iadaviek AI modelov.

Pozrime sa na zÃ¡kladnÃ© prÃ­stupy, ktorÃ© umoÅ¾ÅˆujÃº implementÃ¡cie EdgeAI naprieÄ rÃ´znymi typmi zariadenÃ­ a prÃ­padmi pouÅ¾itia.

### ZÃ¡kladnÃ© princÃ­py EdgeAI

EdgeAI je postavenÃ© na niekoÄ¾kÃ½ch zÃ¡kladnÃ½ch princÃ­poch, ktorÃ© ho odliÅ¡ujÃº od tradiÄnÃ©ho cloudovÃ©ho AI:

- **LokÃ¡lne spracovanie**: Inferencia AI prebieha priamo na koncovom zariadenÃ­ bez potreby externÃ©ho pripojenia.
- **OptimalizÃ¡cia zdrojov**: Modely sÃº Å¡pecificky optimalizovanÃ© pre hardvÃ©rovÃ© obmedzenia cieÄ¾ovÃ½ch zariadenÃ­.
- **VÃ½kon v reÃ¡lnom Äase**: Spracovanie prebieha s minimÃ¡lnou latenciou pre Äasovo citlivÃ© aplikÃ¡cie.
- **SÃºkromie ako zÃ¡klad**: CitlivÃ© Ãºdaje zostÃ¡vajÃº na zariadenÃ­, ÄÃ­m sa zvyÅ¡uje bezpeÄnosÅ¥ a sÃºlad.

## KÄ¾ÃºÄovÃ© technolÃ³gie umoÅ¾ÅˆujÃºce EdgeAI

### KvantizÃ¡cia modelov

Jednou z najdÃ´leÅ¾itejÅ¡Ã­ch technÃ­k v EdgeAI je kvantizÃ¡cia modelov. Tento proces zahÅ•Åˆa znÃ­Å¾enie presnosti parametrov modelu, zvyÄajne z 32-bitovÃ½ch ÄÃ­sel s pohyblivou desatinnou Äiarkou na 8-bitovÃ© celÃ© ÄÃ­sla alebo dokonca formÃ¡ty s niÅ¾Å¡ou presnosÅ¥ou. Hoci sa toto znÃ­Å¾enie presnosti mÃ´Å¾e zdaÅ¥ znepokojujÃºce, vÃ½skum ukÃ¡zal, Å¾e mnohÃ© AI modely si dokÃ¡Å¾u zachovaÅ¥ svoj vÃ½kon aj pri vÃ½razne znÃ­Å¾enej presnosti.

KvantizÃ¡cia funguje tak, Å¾e mapuje rozsah hodnÃ´t s pohyblivou desatinnou Äiarkou na menÅ¡iu mnoÅ¾inu diskrÃ©tnych hodnÃ´t. NaprÃ­klad namiesto pouÅ¾itia 32 bitov na reprezentÃ¡ciu kaÅ¾dÃ©ho parametra mÃ´Å¾e kvantizÃ¡cia pouÅ¾iÅ¥ iba 8 bitov, Äo vedie k 4-nÃ¡sobnÃ©mu znÃ­Å¾eniu pamÃ¤Å¥ovÃ½ch poÅ¾iadaviek a Äasto k rÃ½chlejÅ¡Ã­m Äasom inferencie.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

RÃ´zne techniky kvantizÃ¡cie zahÅ•ÅˆajÃº:

- **Post-Training Quantization (PTQ)**: AplikovanÃ¡ po trÃ©ningu modelu bez potreby opÃ¤tovnÃ©ho trÃ©ningu.
- **Quantization-Aware Training (QAT)**: ZahÅ•Åˆa ÃºÄinky kvantizÃ¡cie poÄas trÃ©ningu pre lepÅ¡iu presnosÅ¥.
- **DynamickÃ¡ kvantizÃ¡cia**: Kvantizuje vÃ¡hy na int8, ale aktivÃ¡cie poÄÃ­ta dynamicky.
- **StatickÃ¡ kvantizÃ¡cia**: PredpoÄÃ­tava vÅ¡etky kvantizaÄnÃ© parametre pre vÃ¡hy aj aktivÃ¡cie.

Pre nasadenia EdgeAI vÃ½ber vhodnej stratÃ©gie kvantizÃ¡cie zÃ¡visÃ­ od konkrÃ©tnej architektÃºry modelu, poÅ¾iadaviek na vÃ½kon a hardvÃ©rovÃ½ch schopnostÃ­ cieÄ¾ovÃ©ho zariadenia.

### Kompresia a optimalizÃ¡cia modelov

Okrem kvantizÃ¡cie pomÃ¡hajÃº rÃ´zne techniky kompresie zniÅ¾ovaÅ¥ veÄ¾kosÅ¥ modelu a vÃ½poÄtovÃ© poÅ¾iadavky. Patria sem:

**Pruning**: TÃ¡to technika odstraÅˆuje nepotrebnÃ© spojenia alebo neurÃ³ny z neurÃ³novÃ½ch sietÃ­. IdentifikÃ¡ciou a eliminÃ¡ciou parametrov, ktorÃ© mÃ¡lo prispievajÃº k vÃ½konu modelu, mÃ´Å¾e pruning vÃ½razne znÃ­Å¾iÅ¥ veÄ¾kosÅ¥ modelu pri zachovanÃ­ presnosti.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Knowledge Distillation**: Tento prÃ­stup zahÅ•Åˆa trÃ©ning menÅ¡ieho "Å¡tudentskÃ©ho" modelu na napodobnenie sprÃ¡vania vÃ¤ÄÅ¡ieho "uÄiteÄ¾skÃ©ho" modelu. Å tudentskÃ½ model sa uÄÃ­ pribliÅ¾ovaÅ¥ vÃ½stupy uÄiteÄ¾a, Äasto dosahujÃºc podobnÃ½ vÃ½kon s vÃ½razne menÅ¡Ã­m poÄtom parametrov.

**OptimalizÃ¡cia architektÃºry modelu**: VÃ½skumnÃ­ci vyvinuli Å¡pecializovanÃ© architektÃºry navrhnutÃ© Å¡pecificky pre nasadenie na koncovÃ½ch zariadeniach, ako MobileNets, EfficientNets a ÄalÅ¡ie Ä¾ahkÃ© architektÃºry, ktorÃ© vyvaÅ¾ujÃº vÃ½kon s vÃ½poÄtovou efektÃ­vnosÅ¥ou.

### MalÃ© jazykovÃ© modely (SLMs)

VznikajÃºcim trendom v EdgeAI je vÃ½voj malÃ½ch jazykovÃ½ch modelov (SLMs). Tieto modely sÃº od zÃ¡kladu navrhnutÃ© tak, aby boli kompaktnÃ© a efektÃ­vne, priÄom stÃ¡le poskytujÃº zmysluplnÃ© schopnosti spracovania prirodzenÃ©ho jazyka. SLMs to dosahujÃº prostrednÃ­ctvom starostlivÃ½ch architektonickÃ½ch rozhodnutÃ­, efektÃ­vnych trÃ©ningovÃ½ch technÃ­k a zameranÃ©ho trÃ©ningu na konkrÃ©tne domÃ©ny alebo Ãºlohy.

Na rozdiel od tradiÄnÃ½ch prÃ­stupov, ktorÃ© zahÅ•ÅˆajÃº kompresiu veÄ¾kÃ½ch modelov, SLMs sÃº Äasto trÃ©novanÃ© na menÅ¡Ã­ch datasetoch a optimalizovanÃ½ch architektÃºrach Å¡pecificky navrhnutÃ½ch pre nasadenie na koncovÃ½ch zariadeniach. Tento prÃ­stup mÃ´Å¾e viesÅ¥ k modelom, ktorÃ© sÃº nielen menÅ¡ie, ale aj efektÃ­vnejÅ¡ie pre konkrÃ©tne prÃ­pady pouÅ¾itia.

## HardvÃ©rovÃ¡ akcelerÃ¡cia pre EdgeAI

ModernÃ© koncovÃ© zariadenia Äoraz viac zahÅ•ÅˆajÃº Å¡pecializovanÃ½ hardvÃ©r navrhnutÃ½ na akcelerÃ¡ciu AI Ãºloh:

### NeurÃ³novÃ© procesorovÃ© jednotky (NPUs)

NPUs sÃº Å¡pecializovanÃ© procesory navrhnutÃ© Å¡pecificky pre vÃ½poÄty neurÃ³novÃ½ch sietÃ­. Tieto Äipy dokÃ¡Å¾u vykonÃ¡vaÅ¥ AI inferenÄnÃ© Ãºlohy oveÄ¾a efektÃ­vnejÅ¡ie ako tradiÄnÃ© CPU, Äasto s niÅ¾Å¡ou spotrebou energie. MnohÃ© modernÃ© smartfÃ³ny, notebooky a IoT zariadenia teraz zahÅ•ÅˆajÃº NPUs na umoÅ¾nenie AI spracovania priamo na zariadenÃ­.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

Zariadenia s NPUs zahÅ•ÅˆajÃº:

- **Apple**: ÄŒipy sÃ©rie A a M s Neural Engine
- **Qualcomm**: Procesory Snapdragon s Hexagon DSP/NPU
- **Samsung**: Procesory Exynos s NPU
- **Intel**: Movidius VPUs a akcelerÃ¡tory Habana Labs
- **Microsoft**: Windows Copilot+ PC s NPUs

### ğŸ® GPU akcelerÃ¡cia

Hoci koncovÃ© zariadenia nemusia maÅ¥ vÃ½konnÃ© GPU ako v dÃ¡tovÃ½ch centrÃ¡ch, mnohÃ© stÃ¡le zahÅ•ÅˆajÃº integrovanÃ© alebo diskrÃ©tne GPU, ktorÃ© mÃ´Å¾u akcelerovaÅ¥ AI Ãºlohy. ModernÃ© mobilnÃ© GPU a integrovanÃ© grafickÃ© procesory mÃ´Å¾u poskytnÃºÅ¥ vÃ½znamnÃ© zlepÅ¡enia vÃ½konu pre AI inferenÄnÃ© Ãºlohy.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### OptimalizÃ¡cia CPU

Aj zariadenia s iba CPU mÃ´Å¾u profitovaÅ¥ z EdgeAI prostrednÃ­ctvom optimalizovanÃ½ch implementÃ¡ciÃ­. ModernÃ© CPU zahÅ•ÅˆajÃº Å¡pecializovanÃ© inÅ¡trukcie pre AI Ãºlohy a boli vyvinutÃ© softvÃ©rovÃ© rÃ¡mce na maximalizÃ¡ciu vÃ½konu CPU pre AI inferenciu.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

Pre softvÃ©rovÃ½ch inÅ¾inierov pracujÃºcich s EdgeAI je pochopenie, ako vyuÅ¾iÅ¥ tieto moÅ¾nosti hardvÃ©rovej akcelerÃ¡cie, kÄ¾ÃºÄovÃ© pre optimalizÃ¡ciu vÃ½konu inferencie a energetickej efektÃ­vnosti na cieÄ¾ovÃ½ch zariadeniach.

## VÃ½hody EdgeAI

### SÃºkromie a bezpeÄnosÅ¥

Jednou z najvÃ½znamnejÅ¡Ã­ch vÃ½hod EdgeAI je zvÃ½Å¡enÃ© sÃºkromie a bezpeÄnosÅ¥. SpracovanÃ­m Ãºdajov lokÃ¡lne na zariadenÃ­ citlivÃ© informÃ¡cie nikdy neopustia kontrolu pouÅ¾Ã­vateÄ¾a. To je obzvlÃ¡Å¡Å¥ dÃ´leÅ¾itÃ© pre aplikÃ¡cie, ktorÃ© pracujÃº s osobnÃ½mi Ãºdajmi, medicÃ­nskymi informÃ¡ciami alebo dÃ´vernÃ½mi obchodnÃ½mi Ãºdajmi.

### ZnÃ­Å¾enÃ¡ latencia

EdgeAI eliminuje potrebu odosielaÅ¥ Ãºdaje na vzdialenÃ© servery na spracovanie, ÄÃ­m vÃ½razne zniÅ¾uje latenciu. To je rozhodujÃºce pre aplikÃ¡cie v reÃ¡lnom Äase, ako sÃº autonÃ³mne vozidlÃ¡, priemyselnÃ¡ automatizÃ¡cia alebo interaktÃ­vne aplikÃ¡cie, kde sÃº potrebnÃ© okamÅ¾itÃ© reakcie.

### Offline schopnosÅ¥

EdgeAI umoÅ¾Åˆuje funkÄnosÅ¥ AI aj vtedy, keÄ internetovÃ© pripojenie nie je dostupnÃ©. To je cennÃ© pre aplikÃ¡cie v odÄ¾ahlÃ½ch lokalitÃ¡ch, poÄas cestovania alebo v situÃ¡ciÃ¡ch, kde je spoÄ¾ahlivosÅ¥ siete otÃ¡zna.

### NÃ¡kladovÃ¡ efektÃ­vnosÅ¥

ZnÃ­Å¾enÃ­m zÃ¡vislosti na cloudovÃ½ch AI sluÅ¾bÃ¡ch mÃ´Å¾e EdgeAI pomÃ´cÅ¥ znÃ­Å¾iÅ¥ prevÃ¡dzkovÃ© nÃ¡klady, najmÃ¤ pre aplikÃ¡cie s vysokÃ½m objemom pouÅ¾Ã­vania. OrganizÃ¡cie mÃ´Å¾u vyhnÃºÅ¥ sa neustÃ¡lym nÃ¡kladom na API a znÃ­Å¾iÅ¥ poÅ¾iadavky na Å¡Ã­rku pÃ¡sma.

### Å kÃ¡lovateÄ¾nosÅ¥

EdgeAI rozdeÄ¾uje vÃ½poÄtovÃº zÃ¡Å¥aÅ¾ naprieÄ koncovÃ½mi zariadeniami namiesto jej centralizÃ¡cie v dÃ¡tovÃ½ch centrÃ¡ch. To mÃ´Å¾e
- [02: EdgeAI AplikÃ¡cie](02.RealWorldCaseStudies.md)

---

**Zrieknutie sa zodpovednosti**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, berte na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho rodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nenesieme zodpovednosÅ¥ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.