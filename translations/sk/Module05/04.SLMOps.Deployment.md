<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-18T19:02:40+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "sk"
}
-->
# Sekcia 4: Nasadenie - ImplementÃ¡cia modelu pripravenÃ©ho na produkciu

## PrehÄ¾ad

Tento komplexnÃ½ nÃ¡vod vÃ¡s prevedie celÃ½m procesom nasadenia jemne doladenÃ½ch kvantizovanÃ½ch modelov pomocou Foundry Local. Pokryjeme konverziu modelu, optimalizÃ¡ciu kvantizÃ¡cie a konfigurÃ¡ciu nasadenia od zaÄiatku aÅ¾ do konca.

## Predpoklady

Pred zaÄiatkom sa uistite, Å¾e mÃ¡te nasledovnÃ©:

- âœ… Jemne doladenÃ½ onnx model pripravenÃ½ na nasadenie
- âœ… PoÄÃ­taÄ s Windows alebo Mac
- âœ… Python 3.10 alebo novÅ¡Ã­
- âœ… MinimÃ¡lne 8GB voÄ¾nej RAM
- âœ… Foundry Local nainÅ¡talovanÃ½ vo vaÅ¡om systÃ©me

## ÄŒasÅ¥ 1: Nastavenie prostredia

### InÅ¡talÃ¡cia potrebnÃ½ch nÃ¡strojov

Otvorte svoj terminÃ¡l (Command Prompt na Windows, Terminal na Mac) a spustite nasledujÃºce prÃ­kazy v poradÃ­:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

âš ï¸ **DÃ´leÅ¾itÃ¡ poznÃ¡mka**: Budete potrebovaÅ¥ aj CMake verziu 3.31 alebo novÅ¡iu, ktorÃº si mÃ´Å¾ete stiahnuÅ¥ z [cmake.org](https://cmake.org/download/).

## ÄŒasÅ¥ 2: Konverzia a kvantizÃ¡cia modelu

### VÃ½ber sprÃ¡vneho formÃ¡tu

Pre jemne doladenÃ© malÃ© jazykovÃ© modely odporÃºÄame pouÅ¾Ã­vaÅ¥ **ONNX formÃ¡t**, pretoÅ¾e ponÃºka:

- ğŸš€ LepÅ¡iu optimalizÃ¡ciu vÃ½konu
- ğŸ”§ HardvÃ©rovo nezÃ¡vislÃ© nasadenie
- ğŸ­ Schopnosti pripravenÃ© na produkciu
- ğŸ“± Kompatibilitu naprieÄ platformami

### MetÃ³da 1: Konverzia jednÃ½m prÃ­kazom (OdporÃºÄanÃ©)

PouÅ¾ite nasledujÃºci prÃ­kaz na priamu konverziu vÃ¡Å¡ho jemne doladenÃ©ho modelu:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Vysvetlenie parametrov:**
- `--model_name_or_path`: Cesta k vÃ¡Å¡mu jemne doladenÃ©mu modelu
- `--device cpu`: PouÅ¾itie CPU na optimalizÃ¡ciu
- `--precision int4`: PouÅ¾itie INT4 kvantizÃ¡cie (pribliÅ¾ne 75% znÃ­Å¾enie veÄ¾kosti)
- `--output_path`: VÃ½stupnÃ¡ cesta pre konvertovanÃ½ model

### MetÃ³da 2: PrÃ­stup pomocou konfiguraÄnÃ©ho sÃºboru (PokroÄilÃ­ pouÅ¾Ã­vatelia)

Vytvorte konfiguraÄnÃ½ sÃºbor s nÃ¡zvom `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Potom spustite:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Porovnanie moÅ¾nostÃ­ kvantizÃ¡cie

| PresnosÅ¥ | VeÄ¾kosÅ¥ sÃºboru | RÃ½chlosÅ¥ inferencie | Kvalita modelu | OdporÃºÄanÃ© pouÅ¾itie |
|----------|----------------|---------------------|----------------|---------------------|
| FP16     | ZÃ¡klad Ã— 0.5   | RÃ½chla             | NajlepÅ¡ia      | High-end hardvÃ©r    |
| INT8     | ZÃ¡klad Ã— 0.25  | VeÄ¾mi rÃ½chla       | DobrÃ¡          | VyvÃ¡Å¾enÃ¡ voÄ¾ba      |
| INT4     | ZÃ¡klad Ã— 0.125 | NajrÃ½chlejÅ¡ia      | PrijateÄ¾nÃ¡     | ObmedzenÃ© zdroje    |

ğŸ’¡ **OdporÃºÄanie**: ZaÄnite s kvantizÃ¡ciou INT4 pre vaÅ¡e prvÃ© nasadenie. Ak kvalita nebude uspokojivÃ¡, skÃºste INT8 alebo FP16.

## ÄŒasÅ¥ 3: KonfigurÃ¡cia nasadenia vo Foundry Local

### Vytvorenie konfigurÃ¡cie modelu

Prejdite do adresÃ¡ra modelov Foundry Local:

```bash
foundry cache cd ./models/
```

Vytvorte Å¡truktÃºru adresÃ¡ra modelu:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Vytvorte konfiguraÄnÃ½ sÃºbor `inference_model.json` vo vaÅ¡om adresÃ¡ri modelu:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Å ablÃ³ny konfigurÃ¡ciÃ­ Å¡pecifickÃ© pre model

#### Pre modely sÃ©rie Qwen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## ÄŒasÅ¥ 4: Testovanie a optimalizÃ¡cia modelu

### Overenie inÅ¡talÃ¡cie modelu

Skontrolujte, Äi Foundry Local rozpoznÃ¡ vÃ¡Å¡ model:

```bash
foundry cache ls
```

Mali by ste vidieÅ¥ `your-finetuned-model-int4` v zozname.

### Spustenie testovania modelu

```bash
foundry model run your-finetuned-model-int4
```

### Benchmarking vÃ½konu

Monitorujte kÄ¾ÃºÄovÃ© metriky poÄas testovania:

1. **ÄŒas odozvy**: Merajte priemernÃ½ Äas na odpoveÄ
2. **VyuÅ¾itie pamÃ¤te**: Sledujte spotrebu RAM
3. **VyuÅ¾itie CPU**: Kontrolujte zaÅ¥aÅ¾enie procesora
4. **Kvalita vÃ½stupu**: HodnoÅ¥te relevantnosÅ¥ a sÃºdrÅ¾nosÅ¥ odpovedÃ­

### KontrolnÃ½ zoznam validÃ¡cie kvality

- âœ… Model sprÃ¡vne reaguje na dotazy z jemne doladenÃ©ho domÃ©novÃ©ho prostredia
- âœ… FormÃ¡t odpovede zodpovedÃ¡ oÄakÃ¡vanej Å¡truktÃºre vÃ½stupu
- âœ… PoÄas dlhodobÃ©ho pouÅ¾Ã­vania nedochÃ¡dza k Ãºnikom pamÃ¤te
- âœ… KonzistentnÃ½ vÃ½kon pri rÃ´znych dÄºÅ¾kach vstupov
- âœ… SprÃ¡vne spracovanie hraniÄnÃ½ch prÃ­padov a neplatnÃ½ch vstupov

## Zhrnutie

Gratulujeme! ÃšspeÅ¡ne ste dokonÄili:

- âœ… Konverziu formÃ¡tu jemne doladenÃ©ho modelu
- âœ… OptimalizÃ¡ciu kvantizÃ¡cie modelu
- âœ… KonfigurÃ¡ciu nasadenia vo Foundry Local
- âœ… Ladenie vÃ½konu a rieÅ¡enie problÃ©mov

---

**Upozornenie**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, berte na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho pÃ´vodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za Å¾iadne nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.