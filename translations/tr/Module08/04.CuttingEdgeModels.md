<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-10-01T00:19:44+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "tr"
}
-->
# Oturum 4: Chainlit ile Ãœretim Chat UygulamalarÄ± OluÅŸturma

## Genel BakÄ±ÅŸ

Bu oturum, Chainlit ve Microsoft Foundry Local kullanarak Ã¼retime hazÄ±r chat uygulamalarÄ± oluÅŸturmayÄ± ele alÄ±yor. AI sohbetleri iÃ§in modern web arayÃ¼zleri oluÅŸturmayÄ±, akÄ±ÅŸ yanÄ±tlarÄ±nÄ± uygulamayÄ± ve hata yÃ¶netimi ile kullanÄ±cÄ± deneyimi tasarÄ±mÄ±yla saÄŸlam chat uygulamalarÄ± daÄŸÄ±tmayÄ± Ã¶ÄŸreneceksiniz.

**Neler YapacaksÄ±nÄ±z:**
- **Chainlit Chat UygulamasÄ±**: AkÄ±ÅŸ yanÄ±tlarÄ±yla modern web arayÃ¼zÃ¼
- **WebGPU Demo**: Gizlilik odaklÄ± uygulamalar iÃ§in tarayÄ±cÄ± tabanlÄ± Ã§Ä±karÄ±m  
- **Open WebUI Entegrasyonu**: Foundry Local ile profesyonel chat arayÃ¼zÃ¼
- **Ãœretim KalÄ±plarÄ±**: Hata yÃ¶netimi, izleme ve daÄŸÄ±tÄ±m stratejileri

## Ã–ÄŸrenme Hedefleri

- Chainlit ile Ã¼retime hazÄ±r chat uygulamalarÄ± oluÅŸturma
- GeliÅŸmiÅŸ kullanÄ±cÄ± deneyimi iÃ§in akÄ±ÅŸ yanÄ±tlarÄ±nÄ± uygulama
- Foundry Local SDK entegrasyon kalÄ±plarÄ±nÄ± Ã¶ÄŸrenme
- DoÄŸru hata yÃ¶netimi ve zarif bozulma uygulama
- FarklÄ± ortamlar iÃ§in chat uygulamalarÄ±nÄ± daÄŸÄ±tma ve yapÄ±landÄ±rma
- KonuÅŸma AI iÃ§in modern web arayÃ¼zÃ¼ kalÄ±plarÄ±nÄ± anlama

## Ã–n KoÅŸullar

- **Foundry Local**: Kurulu ve Ã§alÄ±ÅŸÄ±r durumda ([Kurulum KÄ±lavuzu](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: 3.10 veya Ã¼stÃ¼, sanal ortam desteÄŸiyle
- **Model**: En az bir model yÃ¼klenmiÅŸ (`foundry model run phi-4-mini`)
- **TarayÄ±cÄ±**: WebGPU destekli modern web tarayÄ±cÄ± (Chrome/Edge)
- **Docker**: Open WebUI entegrasyonu iÃ§in (isteÄŸe baÄŸlÄ±)

## BÃ¶lÃ¼m 1: Modern Chat UygulamalarÄ±nÄ± Anlama

### Mimari Genel BakÄ±ÅŸ

```
User Browser â†â†’ Chainlit UI â†â†’ Python Backend â†â†’ Foundry Local â†â†’ AI Model
      â†“              â†“              â†“              â†“            â†“
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### Temel Teknolojiler

**Foundry Local SDK KalÄ±plarÄ±:**
- `FoundryLocalManager(alias)`: Otomatik servis yÃ¶netimi
- `manager.endpoint` ve `manager.api_key`: BaÄŸlantÄ± detaylarÄ±
- `manager.get_model_info(alias).id`: Model tanÄ±mlama

**Chainlit Framework:**
- `@cl.on_chat_start`: Sohbet oturumlarÄ±nÄ± baÅŸlatma
- `@cl.on_message`: Gelen kullanÄ±cÄ± mesajlarÄ±nÄ± iÅŸleme  
- `cl.Message().stream_token()`: GerÃ§ek zamanlÄ± akÄ±ÅŸ
- Otomatik UI oluÅŸturma ve WebSocket yÃ¶netimi

## BÃ¶lÃ¼m 2: Yerel ve Bulut Karar Matrisi

### Performans Ã–zellikleri

| Ã–zellik | Yerel (Foundry) | Bulut (Azure OpenAI) |
|---------|-----------------|---------------------|
| **Gecikme** | ğŸš€ 50-200ms (aÄŸ yok) | â±ï¸ 200-2000ms (aÄŸ baÄŸÄ±mlÄ±) |
| **Gizlilik** | ğŸ”’ Veri cihazdan Ã§Ä±kmaz | âš ï¸ Veri buluta gÃ¶nderilir |
| **Maliyet** | ğŸ’° DonanÄ±m sonrasÄ± Ã¼cretsiz | ğŸ’¸ Token baÅŸÄ±na Ã¶deme |
| **Ã‡evrimdÄ±ÅŸÄ±** | âœ… Ä°nternet olmadan Ã§alÄ±ÅŸÄ±r | âŒ Ä°nternet gerektirir |
| **Model Boyutu** | âš ï¸ DonanÄ±mla sÄ±nÄ±rlÄ± | âœ… En bÃ¼yÃ¼k modellere eriÅŸim |
| **Ã–lÃ§eklenebilirlik** | âš ï¸ DonanÄ±m baÄŸÄ±mlÄ± | âœ… SÄ±nÄ±rsÄ±z Ã¶lÃ§eklenebilirlik |

### Hibrit Strateji KalÄ±plarÄ±

**Yerel-Ã–ncelikli ve Yedekleme:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**GÃ¶rev TabanlÄ± YÃ¶nlendirme:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## BÃ¶lÃ¼m 3: Ã–rnek 04 - Chainlit Chat UygulamasÄ±

### HÄ±zlÄ± BaÅŸlangÄ±Ã§

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Uygulama otomatik olarak `http://localhost:8080` adresinde modern bir chat arayÃ¼zÃ¼yle aÃ§Ä±lÄ±r.

### Temel Uygulama

Ã–rnek 04 uygulamasÄ± Ã¼retime hazÄ±r kalÄ±plarÄ± gÃ¶sterir:

**Otomatik Servis KeÅŸfi:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**AkÄ±ÅŸ Chat Ä°ÅŸleyicisi:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### YapÄ±landÄ±rma SeÃ§enekleri

**Ortam DeÄŸiÅŸkenleri:**

| DeÄŸiÅŸken | AÃ§Ä±klama | VarsayÄ±lan | Ã–rnek |
|----------|----------|------------|-------|
| `MODEL` | KullanÄ±lacak model takma adÄ± | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Foundry Local endpoint | Otomatik algÄ±lanÄ±r | `http://localhost:51211` |
| `API_KEY` | API anahtarÄ± (yerel iÃ§in isteÄŸe baÄŸlÄ±) | `""` | `your-api-key` |

**GeliÅŸmiÅŸ KullanÄ±m:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## BÃ¶lÃ¼m 4: Jupyter Notebooks OluÅŸturma ve Kullanma

### Notebook DesteÄŸi Genel BakÄ±ÅŸ

Ã–rnek 04, aÅŸaÄŸÄ±daki Ã¶zellikleri sunan kapsamlÄ± bir Jupyter notebook (`chainlit_app.ipynb`) iÃ§erir:

- **ğŸ“š EÄŸitim Ä°Ã§eriÄŸi**: AdÄ±m adÄ±m Ã¶ÄŸrenme materyalleri
- **ğŸ”¬ EtkileÅŸimli KeÅŸif**: Kod hÃ¼crelerini Ã§alÄ±ÅŸtÄ±rma ve deneme
- **ğŸ“Š GÃ¶rsel GÃ¶sterimler**: Grafikler, diyagramlar ve Ã§Ä±ktÄ± gÃ¶rselleÅŸtirme
- **ğŸ› ï¸ GeliÅŸtirme AraÃ§larÄ±**: Test ve hata ayÄ±klama yetenekleri

### Kendi Notebook'larÄ±nÄ±zÄ± OluÅŸturma

#### AdÄ±m 1: Jupyter OrtamÄ±nÄ± Ayarlama

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### AdÄ±m 2: Yeni Bir Notebook OluÅŸturma

**VS Code Kullanarak:**
1. VS Code'u Module08 dizininde aÃ§Ä±n
2. `.ipynb` uzantÄ±lÄ± yeni bir dosya oluÅŸturun
3. Ä°stendiÄŸinde "Foundry Local" kernelini seÃ§in
4. Ä°Ã§eriÄŸinizi eklemeye baÅŸlayÄ±n

**Jupyter Lab Kullanarak:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Notebook YapÄ±sÄ± iÃ§in En Ä°yi Uygulamalar

#### HÃ¼cre Organizasyonu

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("âœ… Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### EtkileÅŸimli Ã–rnekler ve Egzersizler

#### Egzersiz 1: Ä°stemci YapÄ±landÄ±rma Testi

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\nğŸ§ª Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'âœ… Success' if result['status'] == 'ok' else 'âŒ Failed'}")
```

#### Egzersiz 2: AkÄ±ÅŸ YanÄ±tÄ± SimÃ¼lasyonu

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("ğŸŒŠ Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\nâœ… Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## BÃ¶lÃ¼m 5: WebGPU TarayÄ±cÄ± Ã‡Ä±karÄ±m Demo

### Genel BakÄ±ÅŸ

WebGPU, maksimum gizlilik ve sÄ±fÄ±r kurulum deneyimi iÃ§in AI modellerini doÄŸrudan tarayÄ±cÄ±da Ã§alÄ±ÅŸtÄ±rmayÄ± saÄŸlar. Bu Ã¶rnek, ONNX Runtime Web ile WebGPU Ã§alÄ±ÅŸtÄ±rmasÄ±nÄ± gÃ¶sterir.

### AdÄ±m 1: WebGPU DesteÄŸini Kontrol Etme

**TarayÄ±cÄ± Gereksinimleri:**
- WebGPU etkin Chrome/Edge 113+
- Kontrol: `chrome://gpu` â†’ "WebGPU" durumunu doÄŸrulayÄ±n
- Programatik kontrol: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### AdÄ±m 2: WebGPU Demo OluÅŸturma

Dizin oluÅŸturun: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>ğŸš€ WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = 'âŒ WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'ğŸ” WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('âœ… ONNX Runtime session created with WebGPU');
        log(`ğŸ“Š Input names: ${session.inputNames.join(', ')}`);
        log(`ğŸ“Š Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = 'âœ… WebGPU inference complete!';
        log(`ğŸ¯ Predicted class: ${maxIdx}`);
        log(`ğŸ“ˆ Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `âŒ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### AdÄ±m 3: Demoyu Ã‡alÄ±ÅŸtÄ±rma

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## BÃ¶lÃ¼m 6: Open WebUI Entegrasyonu

### Genel BakÄ±ÅŸ

Open WebUI, Foundry Local'Ä±n OpenAI uyumlu API'sine baÄŸlanan profesyonel bir ChatGPT benzeri arayÃ¼z saÄŸlar.

### AdÄ±m 1: Ã–n KoÅŸullar

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### AdÄ±m 2: Docker Kurulumu (Ã–nerilir)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**Not:** `host.docker.internal`, Docker konteynerlerinin Windows'ta ana makineye eriÅŸmesini saÄŸlar.

### AdÄ±m 3: YapÄ±landÄ±rma

1. **TarayÄ±cÄ±yÄ± AÃ§Ä±n:** `http://localhost:3000` adresine gidin
2. **Ä°lk Kurulum:** YÃ¶netici hesabÄ± oluÅŸturun
3. **Model YapÄ±landÄ±rmasÄ±:**
   - Ayarlar â†’ Modeller â†’ OpenAI API  
   - Temel URL: `http://host.docker.internal:51211/v1`
   - API AnahtarÄ±: `foundry-local-key` (herhangi bir deÄŸer kullanÄ±labilir)
4. **BaÄŸlantÄ±yÄ± Test Etme:** Modeller aÃ§Ä±lÄ±r listede gÃ¶rÃ¼nmelidir

### Sorun Giderme

**YaygÄ±n Sorunlar:**

1. **BaÄŸlantÄ± Reddedildi:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Modeller GÃ¶rÃ¼nmÃ¼yor:**
   - Modelin yÃ¼klÃ¼ olduÄŸunu doÄŸrulayÄ±n: `foundry model list`
   - API yanÄ±tÄ±nÄ± kontrol edin: `curl http://localhost:51211/v1/models`
   - Open WebUI konteynerini yeniden baÅŸlatÄ±n

## BÃ¶lÃ¼m 7: Ãœretim DaÄŸÄ±tÄ±m DÃ¼ÅŸÃ¼nceleri

### Ortam YapÄ±landÄ±rmasÄ±

**GeliÅŸtirme Kurulumu:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Ãœretim DaÄŸÄ±tÄ±mÄ±:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### YaygÄ±n Port SorunlarÄ± ve Ã‡Ã¶zÃ¼mleri

**Port 51211 Ã‡akÄ±ÅŸma Ã–nleme:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Performans Ä°zleme

**SaÄŸlÄ±k KontrolÃ¼ UygulamasÄ±:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## Ã–zet

Oturum 4, konuÅŸma AI iÃ§in Ã¼retime hazÄ±r Chainlit uygulamalarÄ± oluÅŸturmayÄ± ele aldÄ±. ÅunlarÄ± Ã¶ÄŸrendiniz:

- âœ… **Chainlit Framework**: Chat uygulamalarÄ± iÃ§in modern UI ve akÄ±ÅŸ desteÄŸi
- âœ… **Foundry Local Entegrasyonu**: SDK kullanÄ±mÄ± ve yapÄ±landÄ±rma kalÄ±plarÄ±  
- âœ… **WebGPU Ã‡Ä±karÄ±mÄ±**: Maksimum gizlilik iÃ§in tarayÄ±cÄ± tabanlÄ± AI
- âœ… **Open WebUI Kurulumu**: Profesyonel chat arayÃ¼zÃ¼ daÄŸÄ±tÄ±mÄ±
- âœ… **Ãœretim KalÄ±plarÄ±**: Hata yÃ¶netimi, izleme ve Ã¶lÃ§eklendirme

Ã–rnek 04 uygulamasÄ±, Microsoft Foundry Local aracÄ±lÄ±ÄŸÄ±yla yerel AI modellerini kullanan saÄŸlam chat arayÃ¼zleri oluÅŸturmak iÃ§in en iyi uygulamalarÄ± gÃ¶sterir ve mÃ¼kemmel kullanÄ±cÄ± deneyimleri sunar.

## Referanslar

- **[Ã–rnek 04: Chainlit UygulamasÄ±](samples/04/README.md)**: Belgelerle tam uygulama
- **[Chainlit EÄŸitim Not Defteri](samples/04/chainlit_app.ipynb)**: EtkileÅŸimli Ã¶ÄŸrenme materyalleri
- **[Foundry Local Belgeleri](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Tam platform belgeleri
- **[Chainlit Belgeleri](https://docs.chainlit.io/)**: Resmi framework belgeleri
- **[Open WebUI Entegrasyon KÄ±lavuzu](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: Resmi Ã¶ÄŸretici

---

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluÄŸu saÄŸlamak iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§eviriler hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebilir. Belgenin orijinal dilindeki hali, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul edilmez.