<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "38b68a204a9621126d056b0e5b51ab7c",
  "translation_date": "2025-10-09T10:33:31+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "tr"
}
-->
# BÃ¶lÃ¼m 1: SLM Ä°leri DÃ¼zey Ã–ÄŸrenme - Temeller ve Optimizasyon

KÃ¼Ã§Ã¼k Dil Modelleri (SLM'ler), EdgeAI'de Ã¶nemli bir ilerlemeyi temsil eder ve kaynak kÄ±sÄ±tlÄ± cihazlarda geliÅŸmiÅŸ doÄŸal dil iÅŸleme yeteneklerini mÃ¼mkÃ¼n kÄ±lar. SLM'leri etkili bir ÅŸekilde daÄŸÄ±tmayÄ±, optimize etmeyi ve kullanmayÄ± Ã¶ÄŸrenmek, pratik edge tabanlÄ± yapay zeka Ã§Ã¶zÃ¼mleri oluÅŸturmak iÃ§in gereklidir.

## GiriÅŸ

Bu derste KÃ¼Ã§Ã¼k Dil Modelleri (SLM'ler) ve bunlarÄ±n ileri dÃ¼zey uygulama stratejilerini inceleyeceÄŸiz. SLM'lerin temel kavramlarÄ±nÄ±, parametre sÄ±nÄ±rlarÄ±nÄ± ve sÄ±nÄ±flandÄ±rmalarÄ±nÄ±, optimizasyon tekniklerini ve edge computing ortamlarÄ±nda pratik daÄŸÄ±tÄ±m stratejilerini ele alacaÄŸÄ±z.

## Ã–ÄŸrenme Hedefleri

Bu dersin sonunda ÅŸunlarÄ± yapabileceksiniz:

- ğŸ”¢ KÃ¼Ã§Ã¼k Dil Modellerinin parametre sÄ±nÄ±rlarÄ±nÄ± ve sÄ±nÄ±flandÄ±rmalarÄ±nÄ± anlayÄ±n.
- ğŸ› ï¸ Edge cihazlarda SLM daÄŸÄ±tÄ±mÄ± iÃ§in temel optimizasyon tekniklerini belirleyin.
- ğŸš€ SLM'ler iÃ§in ileri dÃ¼zey kuantizasyon ve sÄ±kÄ±ÅŸtÄ±rma stratejilerini Ã¶ÄŸrenin.

## SLM Parametre SÄ±nÄ±rlarÄ±nÄ± ve SÄ±nÄ±flandÄ±rmalarÄ±nÄ± Anlama

KÃ¼Ã§Ã¼k Dil Modelleri (SLM'ler), bÃ¼yÃ¼k modellerine kÄ±yasla Ã§ok daha az parametreyle doÄŸal dil iÃ§eriÄŸini iÅŸlemek, anlamak ve Ã¼retmek iÃ§in tasarlanmÄ±ÅŸ yapay zeka modelleridir. BÃ¼yÃ¼k Dil Modelleri (LLM'ler) yÃ¼z milyarlarca ila trilyonlarca parametre iÃ§erirken, SLM'ler verimlilik ve edge daÄŸÄ±tÄ±mÄ± iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸtÄ±r.

Parametre sÄ±nÄ±flandÄ±rma Ã§erÃ§evesi, SLM'lerin farklÄ± kategorilerini ve uygun kullanÄ±m alanlarÄ±nÄ± anlamamÄ±za yardÄ±mcÄ± olur. Bu sÄ±nÄ±flandÄ±rma, belirli edge computing senaryolarÄ± iÃ§in doÄŸru modeli seÃ§mek aÃ§Ä±sÄ±ndan kritik Ã¶neme sahiptir.

### Parametre SÄ±nÄ±flandÄ±rma Ã‡erÃ§evesi

Parametre sÄ±nÄ±rlarÄ±nÄ± anlamak, farklÄ± edge computing senaryolarÄ± iÃ§in uygun modelleri seÃ§meye yardÄ±mcÄ± olur:

- **ğŸ”¬ Mikro SLM'ler**: 100M - 1.4B parametre (mobil cihazlar iÃ§in ultra hafif)
- **ğŸ“± KÃ¼Ã§Ã¼k SLM'ler**: 1.5B - 13.9B parametre (performans ve verimlilik dengesi)
- **âš–ï¸ Orta SLM'ler**: 14B - 30B parametre (LLM yeteneklerine yaklaÅŸÄ±rken verimliliÄŸi koruma)

AraÅŸtÄ±rma topluluÄŸunda kesin sÄ±nÄ±r deÄŸiÅŸkenlik gÃ¶sterse de, Ã§oÄŸu uygulayÄ±cÄ± 30 milyar parametreden az modele "kÃ¼Ã§Ã¼k" olarak bakar; bazÄ± kaynaklar bu eÅŸiÄŸi 10 milyar parametreye kadar dÃ¼ÅŸÃ¼rÃ¼r.

### SLM'lerin Temel AvantajlarÄ±

SLM'ler, edge computing uygulamalarÄ± iÃ§in ideal olmalarÄ±nÄ± saÄŸlayan birkaÃ§ temel avantaj sunar:

**Operasyonel Verimlilik**: Daha az parametre iÅŸlemeyi gerektirdiÄŸinden SLM'ler daha hÄ±zlÄ± Ã§Ä±karÄ±m sÃ¼releri saÄŸlar, bu da onlarÄ± gerÃ§ek zamanlÄ± uygulamalar iÃ§in ideal kÄ±lar. Daha dÃ¼ÅŸÃ¼k hesaplama kaynaklarÄ± gerektirir, bu da kaynak kÄ±sÄ±tlÄ± cihazlarda daÄŸÄ±tÄ±mÄ± mÃ¼mkÃ¼n kÄ±lar ve daha az enerji tÃ¼ketimi ile karbon ayak izini azaltÄ±r.

**DaÄŸÄ±tÄ±m EsnekliÄŸi**: Bu modeller, internet baÄŸlantÄ±sÄ± gerektirmeden cihaz Ã¼zerinde yapay zeka yetenekleri saÄŸlar, yerel iÅŸlemle gizlilik ve gÃ¼venliÄŸi artÄ±rÄ±r, alanlara Ã¶zel uygulamalar iÃ§in Ã¶zelleÅŸtirilebilir ve Ã§eÅŸitli edge computing ortamlarÄ±na uygundur.

**Maliyet EtkinliÄŸi**: SLM'ler, LLM'lere kÄ±yasla daha dÃ¼ÅŸÃ¼k operasyonel maliyetler ve edge uygulamalarÄ± iÃ§in daha az bant geniÅŸliÄŸi gereksinimi ile uygun maliyetli eÄŸitim ve daÄŸÄ±tÄ±m sunar.

## Ä°leri DÃ¼zey Model Edinme Stratejileri

### Hugging Face Ekosistemi

Hugging Face, en son teknoloji SLM'leri keÅŸfetmek ve eriÅŸmek iÃ§in birincil merkezdir. Platform, model keÅŸfi ve daÄŸÄ±tÄ±mÄ± iÃ§in kapsamlÄ± kaynaklar saÄŸlar:

**Model KeÅŸif Ã–zellikleri**: Platform, parametre sayÄ±sÄ±, lisans tÃ¼rÃ¼ ve performans metriklerine gÃ¶re geliÅŸmiÅŸ filtreleme sunar. KullanÄ±cÄ±lar, yan yana model karÅŸÄ±laÅŸtÄ±rma araÃ§larÄ±na, gerÃ§ek zamanlÄ± performans Ã¶lÃ§Ã¼tlerine ve deÄŸerlendirme sonuÃ§larÄ±na, ayrÄ±ca anÄ±nda test iÃ§in WebGPU demolarÄ±na eriÅŸebilir.

**Ã–zenle SeÃ§ilmiÅŸ SLM KoleksiyonlarÄ±**: PopÃ¼ler modeller arasÄ±nda geliÅŸmiÅŸ akÄ±l yÃ¼rÃ¼tme gÃ¶revleri iÃ§in Phi-4-mini-3.8B, Ã§ok dilli uygulamalar iÃ§in Qwen3 serisi (0.6B/1.7B/4B), genel amaÃ§lÄ± gÃ¶revler iÃ§in Google Gemma3 ve ultra dÃ¼ÅŸÃ¼k hassasiyetli daÄŸÄ±tÄ±m iÃ§in BitNET gibi deneysel modeller bulunur. Platform ayrÄ±ca belirli alanlar iÃ§in Ã¶zel modeller iÃ§eren topluluk odaklÄ± koleksiyonlar ve farklÄ± kullanÄ±m alanlarÄ± iÃ§in optimize edilmiÅŸ Ã¶nceden eÄŸitilmiÅŸ ve talimatla ayarlanmÄ±ÅŸ varyantlar sunar.

### Azure AI Foundry Model KataloÄŸu

Azure AI Foundry Model KataloÄŸu, geliÅŸmiÅŸ entegrasyon yetenekleriyle SLM'lere kurumsal dÃ¼zeyde eriÅŸim saÄŸlar:

**Kurumsal Entegrasyon**: Katalog, doÄŸrudan Azure tarafÄ±ndan satÄ±lan ve kurumsal dÃ¼zeyde destek ve SLA'lar iÃ§eren modelleri iÃ§erir. GeliÅŸmiÅŸ akÄ±l yÃ¼rÃ¼tme yetenekleri iÃ§in Phi-4-mini-3.8B ve Ã¼retim daÄŸÄ±tÄ±mÄ± iÃ§in Llama 3-8B gibi modelleri iÃ§erir. AyrÄ±ca, gÃ¼venilir Ã¼Ã§Ã¼ncÃ¼ taraf aÃ§Ä±k kaynak modellerinden Qwen3 8B gibi modelleri de iÃ§erir.

**Kurumsal Avantajlar**: Model aileleri arasÄ±nda deÄŸiÅŸtirilebilir Tahsis EdilmiÅŸ Verimlilik ile ince ayar, gÃ¶zlemlenebilirlik ve sorumlu yapay zeka iÃ§in yerleÅŸik araÃ§lar. Microsoft'un doÄŸrudan desteÄŸi, kurumsal SLA'lar, entegre gÃ¼venlik ve uyumluluk Ã¶zellikleri ve kapsamlÄ± daÄŸÄ±tÄ±m iÅŸ akÄ±ÅŸlarÄ± kurumsal deneyimi geliÅŸtirir.

## Ä°leri DÃ¼zey Kuantizasyon ve Optimizasyon Teknikleri

### Llama.cpp Optimizasyon Ã‡erÃ§evesi

Llama.cpp, edge daÄŸÄ±tÄ±mÄ± iÃ§in maksimum verimlilik saÄŸlayan en son kuantizasyon tekniklerini sunar:

**Kuantizasyon YÃ¶ntemleri**: Ã‡erÃ§eve, Q4_0 (4-bit kuantizasyon ile mÃ¼kemmel boyut azaltma - Qwen3-0.6B mobil daÄŸÄ±tÄ±mÄ± iÃ§in ideal), Q5_1 (kalite ve sÄ±kÄ±ÅŸtÄ±rmayÄ± dengeleyen 5-bit kuantizasyon - Phi-4-mini-3.8B edge Ã§Ä±karÄ±mÄ± iÃ§in uygun) ve Q8_0 (orijinal kaliteye yakÄ±n 8-bit kuantizasyon - Google Gemma3 Ã¼retim kullanÄ±mÄ± iÃ§in Ã¶nerilir) gibi Ã§eÅŸitli kuantizasyon seviyelerini destekler. BitNET, aÅŸÄ±rÄ± sÄ±kÄ±ÅŸtÄ±rma senaryolarÄ± iÃ§in 1-bit kuantizasyon ile en son teknolojiyi temsil eder.

**Uygulama FaydalarÄ±**: SIMD hÄ±zlandÄ±rma ile CPU'ya optimize edilmiÅŸ Ã§Ä±karÄ±m, bellek verimli model yÃ¼kleme ve yÃ¼rÃ¼tme saÄŸlar. x86, ARM ve Apple Silicon mimarileri arasÄ±nda Ã§apraz platform uyumluluÄŸu, donanÄ±m baÄŸÄ±msÄ±z daÄŸÄ±tÄ±m yeteneklerini mÃ¼mkÃ¼n kÄ±lar.

**Pratik Uygulama Ã–rneÄŸi**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Bellek Ayak Ä°zi KarÅŸÄ±laÅŸtÄ±rmasÄ±**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Microsoft Olive Optimizasyon Paketi

Microsoft Olive, Ã¼retim ortamlarÄ± iÃ§in tasarlanmÄ±ÅŸ kapsamlÄ± model optimizasyon iÅŸ akÄ±ÅŸlarÄ± sunar:

**Optimizasyon Teknikleri**: Paket, Qwen3 serisi modellerle Ã¶zellikle etkili olan otomatik hassasiyet seÃ§imi iÃ§in dinamik kuantizasyon, Google Gemma3 mimarisi iÃ§in optimize edilmiÅŸ grafik optimizasyonu ve operatÃ¶r birleÅŸtirme, CPU, GPU ve NPU iÃ§in donanÄ±m Ã¶zel optimizasyonlarÄ± (ARM cihazlarÄ±nda Phi-4-mini-3.8B iÃ§in Ã¶zel destek) ve Ã§ok aÅŸamalÄ± optimizasyon iÅŸ akÄ±ÅŸlarÄ±nÄ± iÃ§erir. BitNET modelleri, Olive Ã§erÃ§evesi iÃ§inde Ã¶zel 1-bit kuantizasyon iÅŸ akÄ±ÅŸlarÄ± gerektirir.

**Ä°ÅŸ AkÄ±ÅŸÄ± Otomasyonu**: Optimizasyon varyantlarÄ± arasÄ±nda otomatik karÅŸÄ±laÅŸtÄ±rma, optimizasyon sÄ±rasÄ±nda kalite metriklerinin korunmasÄ±nÄ± saÄŸlar. PyTorch ve ONNX gibi popÃ¼ler ML Ã§erÃ§eveleriyle entegrasyon, bulut ve edge daÄŸÄ±tÄ±m optimizasyon yetenekleri sunar.

**Pratik Uygulama Ã–rneÄŸi**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Apple MLX Ã‡erÃ§evesi

Apple MLX, Apple Silicon cihazlarÄ± iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸ yerel optimizasyon saÄŸlar:

**Apple Silicon Optimizasyonu**: Ã‡erÃ§eve, Metal Performance Shaders entegrasyonu ile birleÅŸik bellek mimarisi, otomatik karÄ±ÅŸÄ±k hassasiyet Ã§Ä±karÄ±mÄ± (Ã¶zellikle Google Gemma3 ile etkili) ve optimize edilmiÅŸ bellek bant geniÅŸliÄŸi kullanÄ±mÄ± saÄŸlar. Phi-4-mini-3.8B, M serisi Ã§iplerde olaÄŸanÃ¼stÃ¼ performans gÃ¶sterirken, Qwen3-1.7B MacBook Air daÄŸÄ±tÄ±mlarÄ± iÃ§in optimal denge saÄŸlar.

**GeliÅŸtirme Ã–zellikleri**: NumPy uyumlu dizi iÅŸlemleri, otomatik farklÄ±laÅŸma yetenekleri ve Apple geliÅŸtirme araÃ§larÄ±yla sorunsuz entegrasyon saÄŸlayan Python ve Swift API desteÄŸi, kapsamlÄ± bir geliÅŸtirme ortamÄ± sunar.

**Pratik Uygulama Ã–rneÄŸi**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Ãœretim DaÄŸÄ±tÄ±mÄ± ve Ã‡Ä±karÄ±m Stratejileri

### Ollama: BasitleÅŸtirilmiÅŸ Yerel DaÄŸÄ±tÄ±m

Ollama, edge ve yerel ortamlar iÃ§in kurumsal dÃ¼zeyde Ã¶zelliklerle SLM daÄŸÄ±tÄ±mÄ±nÄ± kolaylaÅŸtÄ±rÄ±r:

**DaÄŸÄ±tÄ±m Yetenekleri**: Otomatik model Ã§ekme ve Ã¶nbellekleme ile tek komutla model yÃ¼kleme ve Ã§alÄ±ÅŸtÄ±rma. Phi-4-mini-3.8B, tÃ¼m Qwen3 serisi (0.6B/1.7B/4B) ve Google Gemma3 iÃ§in destek, REST API ile uygulama entegrasyonu ve Ã§oklu model yÃ¶netimi ve geÃ§iÅŸ yetenekleri. BitNET modelleri, 1-bit kuantizasyon desteÄŸi iÃ§in deneysel yapÄ±landÄ±rmalar gerektirir.

**Ä°leri DÃ¼zey Ã–zellikler**: Ã–zel model ince ayar desteÄŸi, konteynerize daÄŸÄ±tÄ±m iÃ§in Dockerfile oluÅŸturma, otomatik algÄ±lama ile GPU hÄ±zlandÄ±rma ve model kuantizasyon ve optimizasyon seÃ§enekleri, kapsamlÄ± daÄŸÄ±tÄ±m esnekliÄŸi saÄŸlar.

### VLLM: YÃ¼ksek PerformanslÄ± Ã‡Ä±karÄ±m

VLLM, yÃ¼ksek verimlilik senaryolarÄ± iÃ§in Ã¼retim dÃ¼zeyinde Ã§Ä±karÄ±m optimizasyonu saÄŸlar:

**Performans OptimizasyonlarÄ±**: Bellek verimli dikkat hesaplamasÄ± iÃ§in PagedAttention (Ã¶zellikle Phi-4-mini-3.8B'nin transformer mimarisi iÃ§in faydalÄ±), verimlilik optimizasyonu iÃ§in dinamik toplama (Qwen3 serisi paralel iÅŸlem iÃ§in optimize edilmiÅŸ), Ã§oklu GPU Ã¶lÃ§eklendirme iÃ§in tensÃ¶r paralelliÄŸi (Google Gemma3 desteÄŸi) ve gecikme azaltÄ±mÄ± iÃ§in spekÃ¼latif kod Ã§Ã¶zme. BitNET modelleri, 1-bit iÅŸlemler iÃ§in Ã¶zel Ã§Ä±karÄ±m Ã§ekirdekleri gerektirir.

**Kurumsal Entegrasyon**: OpenAI uyumlu API uÃ§ noktalarÄ±, Kubernetes daÄŸÄ±tÄ±m desteÄŸi, izleme ve gÃ¶zlemlenebilirlik entegrasyonu ve otomatik Ã¶lÃ§eklendirme yetenekleri, kurumsal dÃ¼zeyde daÄŸÄ±tÄ±m Ã§Ã¶zÃ¼mleri saÄŸlar.

### Foundry Local: Microsoft'un Edge Ã‡Ã¶zÃ¼mÃ¼

Foundry Local, kurumsal ortamlar iÃ§in kapsamlÄ± edge daÄŸÄ±tÄ±m yetenekleri saÄŸlar:

**Edge Computing Ã–zellikleri**: Kaynak kÄ±sÄ±tlamasÄ± optimizasyonu ile Ã§evrimdÄ±ÅŸÄ± Ã¶ncelikli mimari tasarÄ±mÄ±, yerel model kayÄ±t defteri yÃ¶netimi ve edge-to-cloud senkronizasyon yetenekleri, gÃ¼venilir edge daÄŸÄ±tÄ±mÄ±nÄ± saÄŸlar.

**GÃ¼venlik ve Uyumluluk**: GizliliÄŸi korumak iÃ§in yerel veri iÅŸleme, kurumsal gÃ¼venlik kontrolleri, denetim kaydÄ± ve uyumluluk raporlama ve rol tabanlÄ± eriÅŸim yÃ¶netimi, edge daÄŸÄ±tÄ±mlarÄ± iÃ§in kapsamlÄ± gÃ¼venlik saÄŸlar.

## SLM UygulamasÄ± iÃ§in En Ä°yi Uygulamalar

### Model SeÃ§im KÄ±lavuzlarÄ±

Edge daÄŸÄ±tÄ±mÄ± iÃ§in SLM seÃ§erken ÅŸu faktÃ¶rleri gÃ¶z Ã¶nÃ¼nde bulundurun:

**Parametre SayÄ±sÄ± DÃ¼ÅŸÃ¼nceleri**: Ultra hafif mobil uygulamalar iÃ§in Qwen3-0.6B gibi mikro SLM'leri, dengeli performans senaryolarÄ± iÃ§in Qwen3-1.7B veya Google Gemma3 gibi kÃ¼Ã§Ã¼k SLM'leri ve verimliliÄŸi korurken LLM yeteneklerine yaklaÅŸan Phi-4-mini-3.8B veya Qwen3-4B gibi orta SLM'leri seÃ§in. BitNET modelleri, belirli araÅŸtÄ±rma uygulamalarÄ± iÃ§in deneysel ultra sÄ±kÄ±ÅŸtÄ±rma sunar.

**KullanÄ±m AlanÄ± Uyumu**: Model yeteneklerini belirli uygulama gereksinimlerine uygun hale getirin; yanÄ±t kalitesi, Ã§Ä±karÄ±m hÄ±zÄ±, bellek kÄ±sÄ±tlamalarÄ± ve Ã§evrimdÄ±ÅŸÄ± Ã§alÄ±ÅŸma gereksinimlerini gÃ¶z Ã¶nÃ¼nde bulundurun.

### Optimizasyon Stratejisi SeÃ§imi

**Kuantizasyon YaklaÅŸÄ±mÄ±**: Kalite gereksinimlerine ve donanÄ±m kÄ±sÄ±tlamalarÄ±na gÃ¶re uygun kuantizasyon seviyelerini seÃ§in. Maksimum sÄ±kÄ±ÅŸtÄ±rma iÃ§in Q4_0'Ä± (Qwen3-0.6B mobil daÄŸÄ±tÄ±mÄ± iÃ§in ideal), kalite-sÄ±kÄ±ÅŸtÄ±rma dengesi iÃ§in Q5_1'i (Phi-4-mini-3.8B ve Google Gemma3 iÃ§in uygun) ve orijinal kaliteyi korumak iÃ§in Q8_0'Ä± (Qwen3-4B Ã¼retim ortamlarÄ± iÃ§in Ã¶nerilir) dÃ¼ÅŸÃ¼nÃ¼n. BitNET'in 1-bit kuantizasyonu, Ã¶zel uygulamalar iÃ§in aÅŸÄ±rÄ± sÄ±kÄ±ÅŸtÄ±rma sÄ±nÄ±rÄ±nÄ± temsil eder.

**Ã‡erÃ§eve SeÃ§imi**: Hedef donanÄ±m ve daÄŸÄ±tÄ±m gereksinimlerine gÃ¶re optimizasyon Ã§erÃ§evelerini seÃ§in. CPU'ya optimize edilmiÅŸ daÄŸÄ±tÄ±m iÃ§in Llama.cpp, kapsamlÄ± optimizasyon iÅŸ akÄ±ÅŸlarÄ± iÃ§in Microsoft Olive ve Apple Silicon cihazlarÄ± iÃ§in Apple MLX kullanÄ±n.

## Pratik Model Ã–rnekleri ve KullanÄ±m AlanlarÄ±

### GerÃ§ek DÃ¼nya DaÄŸÄ±tÄ±m SenaryolarÄ±

**Mobil Uygulamalar**: Qwen3-0.6B, minimal bellek ayak izi ile akÄ±llÄ± telefon chatbot uygulamalarÄ±nda mÃ¼kemmel performans gÃ¶sterirken, Google Gemma3 tablet tabanlÄ± eÄŸitim araÃ§larÄ± iÃ§in dengeli performans saÄŸlar. Phi-4-mini-3.8B, mobil Ã¼retkenlik uygulamalarÄ± iÃ§in Ã¼stÃ¼n akÄ±l yÃ¼rÃ¼tme yetenekleri sunar.

**MasaÃ¼stÃ¼ ve Edge Computing**: Qwen3-1.7B, masaÃ¼stÃ¼ asistan uygulamalarÄ± iÃ§in optimal performans saÄŸlar, Phi-4-mini-3.8B geliÅŸtirici araÃ§larÄ± iÃ§in geliÅŸmiÅŸ kod Ã¼retim yetenekleri sunar ve Qwen3-4B, iÅŸ istasyonu ortamlarÄ±nda sofistike belge analizi saÄŸlar.

**AraÅŸtÄ±rma ve Deneysel**: BitNET modelleri, aÅŸÄ±rÄ± dÃ¼ÅŸÃ¼k hassasiyetli Ã§Ä±karÄ±mÄ±n akademik araÅŸtÄ±rma ve aÅŸÄ±rÄ± kaynak kÄ±sÄ±tlamalarÄ± gerektiren kavram kanÄ±tÄ± uygulamalarÄ± iÃ§in keÅŸfedilmesini saÄŸlar.

### Performans Ã–lÃ§Ã¼tleri ve KarÅŸÄ±laÅŸtÄ±rmalar

**Ã‡Ä±karÄ±m HÄ±zÄ±**: Qwen3-0.6B, mobil CPU'larda en hÄ±zlÄ± Ã§Ä±karÄ±m sÃ¼relerini elde eder, Google Gemma3 genel uygulamalar iÃ§in dengeli hÄ±z-kalite oranÄ± saÄŸlar, Phi-4-mini-3.8B karmaÅŸÄ±k gÃ¶revler iÃ§in Ã¼stÃ¼n akÄ±l yÃ¼rÃ¼tme hÄ±zÄ± sunar ve BitNET, Ã¶zel donanÄ±mla teorik maksimum verimlilik saÄŸlar.

**Bellek Gereksinimleri**: Model bellek ayak izleri, Qwen3-0.6B (1GB altÄ±nda kuantize edilmiÅŸ) ile Phi-4-mini-3.8B (yaklaÅŸÄ±k 3-4GB kuantize edilmiÅŸ) arasÄ±nda deÄŸiÅŸir; BitNET, deneysel yapÄ±landÄ±rmalarda 500MB altÄ± ayak izlerine ulaÅŸÄ±r.

## Zorluklar ve Dikkat Edilmesi Gerekenler

### Performans Dengeleri

SLM daÄŸÄ±tÄ±mÄ±, model boyutu, Ã§Ä±karÄ±m hÄ±zÄ± ve Ã§Ä±ktÄ± kalitesi arasÄ±ndaki dengeleri dikkatlice deÄŸerlendirmeyi gerektirir. Ã–rneÄŸin, Qwen3-0.6B olaÄŸanÃ¼stÃ¼ hÄ±z ve verimlilik sunarken, Phi-4-mini-3.8B Ã¼stÃ¼n akÄ±l yÃ¼rÃ¼tme yetenekleri saÄŸlar ancak artan kaynak gereksinimleri ile birlikte gelir. Google Gemma3, Ã§oÄŸu genel uygulama iÃ§in uygun bir orta yol sunar.

### DonanÄ±m UyumluluÄŸu

FarklÄ± edge cihazlar, deÄŸiÅŸen yeteneklere ve kÄ±sÄ±tlamalara sahiptir. Qwen3-0.6B, temel ARM iÅŸlemcilerde verimli Ã§alÄ±ÅŸÄ±rken, Google Gemma3 orta dÃ¼zeyde hesaplama kaynaklarÄ± gerektirir ve Phi-4-mini-3.8B daha Ã¼st dÃ¼zey edge donanÄ±mÄ±ndan faydalanÄ±r. BitNET modelleri, optimal 1-bit iÅŸlemler iÃ§in Ã¶zel donanÄ±m veya yazÄ±lÄ±m uygulamalarÄ± gerektirir.

### GÃ¼venlik ve Gizlilik

SLM'ler, yerel iÅŸlemle gizliliÄŸi artÄ±rÄ±rken, edge ortamlarÄ±nda modelleri ve verileri korumak iÃ§in uygun gÃ¼venlik Ã¶nlemleri uygulanmalÄ±dÄ±r. Bu, Ã¶zellikle Phi-4-mini-3.8B'nin kurumsal ortamlarda veya Qwen3 serisinin hassas verileri iÅŸleyen Ã§ok dilli uygulamalarda daÄŸÄ±tÄ±mÄ± sÄ±rasÄ±nda Ã¶nemlidir.

## SLM GeliÅŸiminde Gelecek Trendler

SLM alanÄ±, model mimarilerindeki, optimizasyon tekniklerindeki ve

---

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±k iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul edilmez.