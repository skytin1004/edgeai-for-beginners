<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "308873206bc5ec5d87e34ca2907d2c9c",
  "translation_date": "2025-09-18T00:01:21+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "tr"
}
-->
# BÃ¶lÃ¼m 2: Yerel Ortamda DaÄŸÄ±tÄ±m - Gizlilik Ã–ncelikli Ã‡Ã¶zÃ¼mler

KÃ¼Ã§Ã¼k Dil Modellerinin (SLM'ler) yerel olarak daÄŸÄ±tÄ±lmasÄ±, gizliliÄŸi koruyan ve maliyet aÃ§Ä±sÄ±ndan etkili yapay zeka Ã§Ã¶zÃ¼mlerine doÄŸru bir paradigma deÄŸiÅŸimini temsil eder. Bu kapsamlÄ± rehber, Ollama ve Microsoft Foundry Local gibi gÃ¼Ã§lÃ¼ Ã§erÃ§eveleri inceleyerek, geliÅŸtiricilerin SLM'lerin tam potansiyelini kullanÄ±rken daÄŸÄ±tÄ±m ortamlarÄ± Ã¼zerinde tam kontrol saÄŸlamalarÄ±nÄ± mÃ¼mkÃ¼n kÄ±lar.

## GiriÅŸ

Bu derste, KÃ¼Ã§Ã¼k Dil Modellerinin yerel ortamlarda geliÅŸmiÅŸ daÄŸÄ±tÄ±m stratejilerini inceleyeceÄŸiz. Yerel yapay zeka daÄŸÄ±tÄ±mÄ±nÄ±n temel kavramlarÄ±nÄ± ele alacak, iki lider platformu (Ollama ve Microsoft Foundry Local) inceleyecek ve Ã¼retime hazÄ±r Ã§Ã¶zÃ¼mler iÃ§in pratik uygulama rehberliÄŸi saÄŸlayacaÄŸÄ±z.

## Ã–ÄŸrenme Hedefleri

Bu dersin sonunda ÅŸunlarÄ± yapabileceksiniz:

- Yerel SLM daÄŸÄ±tÄ±m Ã§erÃ§evelerinin mimarisini ve avantajlarÄ±nÄ± anlayÄ±n.
- Ollama ve Microsoft Foundry Local kullanarak Ã¼retime hazÄ±r daÄŸÄ±tÄ±mlarÄ± uygulayÄ±n.
- Belirli gereksinimler ve kÄ±sÄ±tlamalar temelinde uygun platformu karÅŸÄ±laÅŸtÄ±rÄ±n ve seÃ§in.
- Performans, gÃ¼venlik ve Ã¶lÃ§eklenebilirlik iÃ§in yerel daÄŸÄ±tÄ±mlarÄ± optimize edin.

## Yerel SLM DaÄŸÄ±tÄ±m Mimarilerini Anlamak

Yerel SLM daÄŸÄ±tÄ±mÄ±, bulut baÄŸÄ±mlÄ± yapay zeka hizmetlerinden, gizliliÄŸi koruyan ve yerinde Ã§Ã¶zÃ¼mlere doÄŸru temel bir deÄŸiÅŸimi temsil eder. Bu yaklaÅŸÄ±m, kuruluÅŸlarÄ±n yapay zeka altyapÄ±larÄ± Ã¼zerinde tam kontrol saÄŸlamalarÄ±na olanak tanÄ±rken, veri egemenliÄŸini ve operasyonel baÄŸÄ±msÄ±zlÄ±ÄŸÄ± garanti eder.

### DaÄŸÄ±tÄ±m Ã‡erÃ§evesi SÄ±nÄ±flandÄ±rmalarÄ±

FarklÄ± daÄŸÄ±tÄ±m yaklaÅŸÄ±mlarÄ±nÄ± anlamak, belirli kullanÄ±m durumlarÄ± iÃ§in doÄŸru stratejiyi seÃ§meye yardÄ±mcÄ± olur:

- **GeliÅŸtirme OdaklÄ±**: Deney ve prototipleme iÃ§in kolay kurulum
- **Kurumsal DÃ¼zey**: Kurumsal entegrasyon yetenekleriyle Ã¼retime hazÄ±r Ã§Ã¶zÃ¼mler  
- **Ã‡apraz Platform**: FarklÄ± iÅŸletim sistemleri ve donanÄ±mlar arasÄ±nda evrensel uyumluluk

### Yerel SLM DaÄŸÄ±tÄ±mÄ±nÄ±n Temel AvantajlarÄ±

Yerel SLM daÄŸÄ±tÄ±mÄ±, kurumsal ve gizlilik hassasiyetine sahip uygulamalar iÃ§in ideal olan birkaÃ§ temel avantaj sunar:

**Gizlilik ve GÃ¼venlik**: Yerel iÅŸlem, hassas verilerin kuruluÅŸun altyapÄ±sÄ±nÄ± terk etmemesini saÄŸlar ve GDPR, HIPAA gibi dÃ¼zenleyici gerekliliklere uyumu mÃ¼mkÃ¼n kÄ±lar. SÄ±nÄ±flandÄ±rÄ±lmÄ±ÅŸ ortamlar iÃ§in hava boÅŸluklu daÄŸÄ±tÄ±mlar yapÄ±labilirken, tam denetim izleri gÃ¼venlik gÃ¶zetimini sÃ¼rdÃ¼rÃ¼r.

**Maliyet EtkinliÄŸi**: Token baÅŸÄ±na fiyatlandÄ±rma modellerinin ortadan kaldÄ±rÄ±lmasÄ±, operasyonel maliyetleri Ã¶nemli Ã¶lÃ§Ã¼de azaltÄ±r. Daha dÃ¼ÅŸÃ¼k bant geniÅŸliÄŸi gereksinimleri ve azalan bulut baÄŸÄ±mlÄ±lÄ±ÄŸÄ±, kurumsal bÃ¼tÃ§eleme iÃ§in Ã¶ngÃ¶rÃ¼lebilir maliyet yapÄ±larÄ± saÄŸlar.

**Performans ve GÃ¼venilirlik**: AÄŸ gecikmesi olmadan daha hÄ±zlÄ± Ã§Ä±karÄ±m sÃ¼releri, gerÃ§ek zamanlÄ± uygulamalarÄ± mÃ¼mkÃ¼n kÄ±lar. Ã‡evrimdÄ±ÅŸÄ± iÅŸlevsellik, internet baÄŸlantÄ±sÄ±ndan baÄŸÄ±msÄ±z olarak sÃ¼rekli Ã§alÄ±ÅŸmayÄ± saÄŸlar ve yerel kaynak optimizasyonu tutarlÄ± performans sunar.

## Ollama: Evrensel Yerel DaÄŸÄ±tÄ±m Platformu

### Temel Mimari ve Felsefe

Ollama, Ã§eÅŸitli donanÄ±m yapÄ±landÄ±rmalarÄ± ve iÅŸletim sistemleri arasÄ±nda yerel LLM daÄŸÄ±tÄ±mÄ±nÄ± demokratikleÅŸtiren evrensel, geliÅŸtirici dostu bir platform olarak tasarlanmÄ±ÅŸtÄ±r.

**Teknik Temel**: GÃ¼Ã§lÃ¼ llama.cpp Ã§erÃ§evesi Ã¼zerine inÅŸa edilen Ollama, optimal performans iÃ§in verimli GGUF model formatÄ±nÄ± kullanÄ±r. Ã‡apraz platform uyumluluÄŸu, Windows, macOS ve Linux ortamlarÄ±nda tutarlÄ± davranÄ±ÅŸ saÄŸlar ve akÄ±llÄ± kaynak yÃ¶netimi CPU, GPU ve bellek kullanÄ±mÄ±nÄ± optimize eder.

**TasarÄ±m Felsefesi**: Ollama, iÅŸlevsellikten Ã¶dÃ¼n vermeden sadeliÄŸi Ã¶nceliklendirir ve sÄ±fÄ±r yapÄ±landÄ±rma daÄŸÄ±tÄ±mÄ± sunarak anÄ±nda verimlilik saÄŸlar. Platform, geniÅŸ model uyumluluÄŸunu korurken farklÄ± model mimarileri arasÄ±nda tutarlÄ± API'ler sunar.

### GeliÅŸmiÅŸ Ã–zellikler ve Yetenekler

**Model YÃ¶netim MÃ¼kemmelliÄŸi**: Ollama, otomatik Ã§ekme, Ã¶nbellekleme ve sÃ¼rÃ¼mleme ile kapsamlÄ± model yaÅŸam dÃ¶ngÃ¼sÃ¼ yÃ¶netimi saÄŸlar. Platform, Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral ve Ã¶zel gÃ¶mme modeller dahil olmak Ã¼zere geniÅŸ bir model ekosistemini destekler.

**Modelfiles ile Ã–zelleÅŸtirme**: Ä°leri dÃ¼zey kullanÄ±cÄ±lar, belirli parametreler, sistem istemleri ve davranÄ±ÅŸ deÄŸiÅŸiklikleriyle Ã¶zel model yapÄ±landÄ±rmalarÄ± oluÅŸturabilir. Bu, alan spesifik optimizasyonlarÄ± ve Ã¶zel uygulama gereksinimlerini mÃ¼mkÃ¼n kÄ±lar.

**Performans Optimizasyonu**: Ollama, NVIDIA CUDA, Apple Metal ve OpenCL dahil olmak Ã¼zere mevcut donanÄ±m hÄ±zlandÄ±rmayÄ± otomatik olarak algÄ±lar ve kullanÄ±r. AkÄ±llÄ± bellek yÃ¶netimi, farklÄ± donanÄ±m yapÄ±landÄ±rmalarÄ± arasÄ±nda optimal kaynak kullanÄ±mÄ±nÄ± saÄŸlar.

### Ãœretim Uygulama Stratejileri

**Kurulum ve Ayar**: Ollama, yerel yÃ¼kleyiciler, paket yÃ¶neticileri (WinGet, Homebrew, APT) ve Docker konteynerleri aracÄ±lÄ±ÄŸÄ±yla platformlar arasÄ±nda kolay kurulum saÄŸlar.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Temel Komutlar ve Ä°ÅŸlemler**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**GeliÅŸmiÅŸ YapÄ±landÄ±rma**: Modelfiles, kurumsal gereksinimler iÃ§in sofistike Ã¶zelleÅŸtirmeyi mÃ¼mkÃ¼n kÄ±lar:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### GeliÅŸtirici Entegrasyon Ã–rnekleri

**Python API Entegrasyonu**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**JavaScript/TypeScript Entegrasyonu (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**RESTful API KullanÄ±mÄ± cURL ile**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Performans AyarÄ± ve Optimizasyon

**Bellek ve Ä°ÅŸ ParÃ§acÄ±ÄŸÄ± YapÄ±landÄ±rmasÄ±**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**FarklÄ± DonanÄ±mlar iÃ§in Kuantizasyon SeÃ§imi**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Kurumsal Edge AI Platformu

### Kurumsal DÃ¼zey Mimari

Microsoft Foundry Local, Microsoft ekosistemine derin entegrasyon ile Ã¼retim edge AI daÄŸÄ±tÄ±mlarÄ± iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸ kapsamlÄ± bir kurumsal Ã§Ã¶zÃ¼mÃ¼ temsil eder.

**ONNX TabanlÄ± Temel**: EndÃ¼stri standardÄ± ONNX Runtime Ã¼zerine inÅŸa edilen Foundry Local, Ã§eÅŸitli donanÄ±m mimarileri arasÄ±nda optimize edilmiÅŸ performans saÄŸlar. Platform, Windows ML entegrasyonu ile yerel Windows optimizasyonu sunarken Ã§apraz platform uyumluluÄŸunu korur.

**DonanÄ±m HÄ±zlandÄ±rma MÃ¼kemmelliÄŸi**: Foundry Local, CPU, GPU ve NPU'lar arasÄ±nda akÄ±llÄ± donanÄ±m algÄ±lama ve optimizasyon Ã¶zelliklerine sahiptir. DonanÄ±m satÄ±cÄ±larÄ± (AMD, Intel, NVIDIA, Qualcomm) ile derin iÅŸbirliÄŸi, kurumsal donanÄ±m yapÄ±landÄ±rmalarÄ±nda optimal performans saÄŸlar.

### GeliÅŸmiÅŸ GeliÅŸtirici Deneyimi

**Ã‡oklu ArayÃ¼z EriÅŸimi**: Foundry Local, model yÃ¶netimi ve daÄŸÄ±tÄ±mÄ± iÃ§in gÃ¼Ã§lÃ¼ bir CLI, yerel entegrasyon iÃ§in Ã§ok dilli SDK'lar (Python, NodeJS) ve sorunsuz geÃ§iÅŸ iÃ§in OpenAI uyumluluÄŸuna sahip RESTful API'ler dahil olmak Ã¼zere kapsamlÄ± geliÅŸtirme arayÃ¼zleri saÄŸlar.

**Visual Studio Entegrasyonu**: Platform, model dÃ¶nÃ¼ÅŸtÃ¼rme, kuantizasyon ve optimizasyon araÃ§larÄ±nÄ± geliÅŸtirme ortamÄ±na entegre eden VS Code iÃ§in AI Toolkit ile sorunsuz bir ÅŸekilde entegre olur. Bu entegrasyon, geliÅŸtirme iÅŸ akÄ±ÅŸlarÄ±nÄ± hÄ±zlandÄ±rÄ±r ve daÄŸÄ±tÄ±m karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± azaltÄ±r.

**Model Optimizasyon HattÄ±**: Microsoft Olive entegrasyonu, dinamik kuantizasyon, grafik optimizasyonu ve donanÄ±m spesifik ayarlarÄ± iÃ§eren sofistike model optimizasyon iÅŸ akÄ±ÅŸlarÄ±nÄ± mÃ¼mkÃ¼n kÄ±lar. Azure ML aracÄ±lÄ±ÄŸÄ±yla bulut tabanlÄ± dÃ¶nÃ¼ÅŸtÃ¼rme yetenekleri, bÃ¼yÃ¼k modeller iÃ§in Ã¶lÃ§eklenebilir optimizasyon saÄŸlar.

### Ãœretim Uygulama Stratejileri

**Kurulum ve YapÄ±landÄ±rma**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Model YÃ¶netim Ä°ÅŸlemleri**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**GeliÅŸmiÅŸ DaÄŸÄ±tÄ±m YapÄ±landÄ±rmasÄ±**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Kurumsal Ekosistem Entegrasyonu

**GÃ¼venlik ve Uyumluluk**: Foundry Local, rol tabanlÄ± eriÅŸim kontrolÃ¼, denetim kaydÄ±, uyumluluk raporlama ve ÅŸifrelenmiÅŸ model depolama gibi kurumsal dÃ¼zeyde gÃ¼venlik Ã¶zellikleri saÄŸlar. Microsoft gÃ¼venlik altyapÄ±sÄ± ile entegrasyon, kurumsal gÃ¼venlik politikalarÄ±na uyumu garanti eder.

**YerleÅŸik AI Hizmetleri**: Platform, yerel dil iÅŸleme iÃ§in Phi Silica, gÃ¶rÃ¼ntÃ¼ iyileÅŸtirme ve analiz iÃ§in AI Imaging ve yaygÄ±n kurumsal AI gÃ¶revleri iÃ§in Ã¶zel API'ler gibi kullanÄ±ma hazÄ±r yapay zeka yetenekleri sunar.

## KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz: Ollama vs Foundry Local

### Teknik Mimari KarÅŸÄ±laÅŸtÄ±rmasÄ±

| **Konu** | **Ollama** | **Foundry Local** |
|----------|------------|-------------------|
| **Model FormatÄ±** | GGUF (llama.cpp Ã¼zerinden) | ONNX (ONNX Runtime Ã¼zerinden) |
| **Platform OdaklÄ±lÄ±k** | Evrensel Ã§apraz platform | Windows/Kurumsal optimizasyon |
| **DonanÄ±m Entegrasyonu** | Genel GPU/CPU desteÄŸi | Derin Windows ML, NPU desteÄŸi |
| **Optimizasyon** | llama.cpp kuantizasyonu | Microsoft Olive + ONNX Runtime |
| **Kurumsal Ã–zellikler** | Topluluk odaklÄ± | SLA'larla kurumsal dÃ¼zey |

### Performans Ã–zellikleri

**Ollama Performans GÃ¼Ã§lÃ¼ YÃ¶nleri**:
- llama.cpp optimizasyonu ile olaÄŸanÃ¼stÃ¼ CPU performansÄ±
- FarklÄ± platformlar ve donanÄ±mlar arasÄ±nda tutarlÄ± davranÄ±ÅŸ
- AkÄ±llÄ± model yÃ¼kleme ile verimli bellek kullanÄ±mÄ±
- GeliÅŸtirme ve test senaryolarÄ± iÃ§in hÄ±zlÄ± baÅŸlangÄ±Ã§ sÃ¼releri

**Foundry Local Performans AvantajlarÄ±**:
- Modern Windows donanÄ±mÄ±nda Ã¼stÃ¼n NPU kullanÄ±mÄ±
- SatÄ±cÄ± iÅŸbirlikleriyle optimize edilmiÅŸ GPU hÄ±zlandÄ±rma
- Kurumsal dÃ¼zeyde performans izleme ve optimizasyon
- Ãœretim ortamlarÄ± iÃ§in Ã¶lÃ§eklenebilir daÄŸÄ±tÄ±m yetenekleri

### GeliÅŸtirme Deneyimi Analizi

**Ollama GeliÅŸtirici Deneyimi**:
- AnÄ±nda verimlilik saÄŸlayan minimal kurulum gereksinimleri
- TÃ¼m iÅŸlemler iÃ§in sezgisel komut satÄ±rÄ± arayÃ¼zÃ¼
- GeniÅŸ topluluk desteÄŸi ve dokÃ¼mantasyon
- Modelfiles ile esnek Ã¶zelleÅŸtirme

**Foundry Local GeliÅŸtirici Deneyimi**:
- Visual Studio ekosistemi ile kapsamlÄ± IDE entegrasyonu
- TakÄ±m iÅŸbirliÄŸi Ã¶zellikleriyle kurumsal geliÅŸtirme iÅŸ akÄ±ÅŸlarÄ±
- Microsoft destekli profesyonel destek kanallarÄ±
- GeliÅŸmiÅŸ hata ayÄ±klama ve optimizasyon araÃ§larÄ±

### KullanÄ±m Durumu Optimizasyonu

**Ollama'yÄ± SeÃ§in**:
- TutarlÄ± davranÄ±ÅŸ gerektiren Ã§apraz platform uygulamalarÄ± geliÅŸtirirken
- AÃ§Ä±k kaynak ÅŸeffaflÄ±ÄŸÄ± ve topluluk katkÄ±larÄ±nÄ± Ã¶nceliklendirirken
- SÄ±nÄ±rlÄ± kaynaklar veya bÃ¼tÃ§e kÄ±sÄ±tlamalarÄ±yla Ã§alÄ±ÅŸÄ±rken
- Deneysel veya araÅŸtÄ±rma odaklÄ± uygulamalar oluÅŸtururken
- FarklÄ± mimariler arasÄ±nda geniÅŸ model uyumluluÄŸu gerektirirken

**Foundry Local'Ä± SeÃ§in**:
- KatÄ± performans gereksinimleri olan kurumsal uygulamalar daÄŸÄ±tÄ±rken
- Windows'a Ã¶zgÃ¼ donanÄ±m optimizasyonlarÄ±ndan (NPU, Windows ML) yararlanÄ±rken
- Kurumsal destek, SLA'lar ve uyumluluk Ã¶zellikleri gerektirirken
- Microsoft ekosistemi entegrasyonu ile Ã¼retim uygulamalarÄ± oluÅŸtururken
- GeliÅŸmiÅŸ optimizasyon araÃ§larÄ± ve profesyonel geliÅŸtirme iÅŸ akÄ±ÅŸlarÄ±na ihtiyaÃ§ duyarken

## GeliÅŸmiÅŸ DaÄŸÄ±tÄ±m Stratejileri

### KonteynerleÅŸtirilmiÅŸ DaÄŸÄ±tÄ±m Modelleri

**Ollama KonteynerleÅŸtirme**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Foundry Local Kurumsal DaÄŸÄ±tÄ±m**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Performans Optimizasyon Teknikleri

**Ollama Optimizasyon Stratejileri**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Foundry Local Optimizasyonu**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## GÃ¼venlik ve Uyumluluk HususlarÄ±

### Kurumsal GÃ¼venlik UygulamasÄ±

**Ollama GÃ¼venlik En Ä°yi UygulamalarÄ±**:
- GÃ¼venlik duvarÄ± kurallarÄ± ve VPN eriÅŸimi ile aÄŸ izolasyonu
- Ters proxy entegrasyonu ile kimlik doÄŸrulama
- Model bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ doÄŸrulama ve gÃ¼venli model daÄŸÄ±tÄ±mÄ±
- API eriÅŸimi ve model iÅŸlemleri iÃ§in denetim kaydÄ±

**Foundry Local Kurumsal GÃ¼venlik**:
- Active Directory entegrasyonu ile yerleÅŸik rol tabanlÄ± eriÅŸim kontrolÃ¼
- Uyumluluk raporlamasÄ± ile kapsamlÄ± denetim izleri
- ÅifrelenmiÅŸ model depolama ve gÃ¼venli model daÄŸÄ±tÄ±mÄ±
- Microsoft gÃ¼venlik altyapÄ±sÄ± ile entegrasyon

### Uyumluluk ve DÃ¼zenleyici Gereksinimler

Her iki platform da dÃ¼zenleyici uyumluluÄŸu ÅŸu yollarla destekler:
- Yerel iÅŸlem saÄŸlayan veri yerleÅŸim kontrolleri
- DÃ¼zenleyici raporlama gereksinimleri iÃ§in denetim kaydÄ±
- Hassas veri iÅŸleme iÃ§in eriÅŸim kontrolleri
- Veri koruma iÃ§in dinlenme ve aktarÄ±m sÄ±rasÄ±nda ÅŸifreleme

## Ãœretim DaÄŸÄ±tÄ±mÄ± iÃ§in En Ä°yi Uygulamalar

### Ä°zleme ve GÃ¶zlemlenebilirlik

**Ä°zlenmesi Gereken Temel Metrikler**:
- Model Ã§Ä±karÄ±m gecikmesi ve verimlilik
- Kaynak kullanÄ±mÄ± (CPU, GPU, bellek)
- API yanÄ±t sÃ¼releri ve hata oranlarÄ±
- Model doÄŸruluÄŸu ve performans kaymasÄ±

**Ä°zleme UygulamasÄ±**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### SÃ¼rekli Entegrasyon ve DaÄŸÄ±tÄ±m

**CI/CD Boru HattÄ± Entegrasyonu**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## Gelecek Trendler ve Hususlar

### GeliÅŸen Teknolojiler

Yerel SLM daÄŸÄ±tÄ±m ortamÄ±, birkaÃ§ Ã¶nemli trendle birlikte geliÅŸmeye devam ediyor:

**GeliÅŸmiÅŸ Model Mimarileri**: Dinamik Ã¶lÃ§eklendirme iÃ§in uzman karÄ±ÅŸÄ±mÄ± modeller ve edge daÄŸÄ±tÄ±m iÃ§in Ã¶zel mimariler dahil olmak Ã¼zere daha verimli ve yetenekli oranlara sahip yeni nesil SLM'ler ortaya Ã§Ä±kÄ±yor.

**DonanÄ±m Entegrasyonu**: NPU'lar, Ã¶zel silikon ve edge computing hÄ±zlandÄ±rÄ±cÄ±larÄ± gibi Ã¶zel yapay zeka donanÄ±mlarÄ±yla daha derin entegrasyon, geliÅŸmiÅŸ performans yetenekleri saÄŸlayacak.

**Ekosistem Evrimi**: DaÄŸÄ±tÄ±m platformlarÄ± arasÄ±nda standartlaÅŸma Ã§abalarÄ± ve farklÄ± Ã§erÃ§eveler arasÄ±nda geliÅŸtirilmiÅŸ birlikte Ã§alÄ±ÅŸabilirlik, Ã§ok platformlu daÄŸÄ±tÄ±mlarÄ± basitleÅŸtirecek.

### EndÃ¼stri Benimseme Modelleri

**Kurumsal Benimseme**: Gizlilik gereksinimleri, maliyet optimizasyonu ve dÃ¼zenleyici uyumluluk ihtiyaÃ§larÄ± tarafÄ±ndan yÃ¶nlendirilen artan kurumsal benimseme. HÃ¼kÃ¼met ve savunma sektÃ¶rleri Ã¶zellikle hava boÅŸluklu daÄŸÄ±tÄ±mlara odaklanÄ±yor.

**KÃ¼resel Hususlar**: UluslararasÄ± veri egemenliÄŸi gereksinimleri, Ã¶zellikle sÄ±kÄ± veri koruma dÃ¼zenlemelerine sahip bÃ¶lgelerde yerel daÄŸÄ±tÄ±m benimsemesini yÃ¶nlendiriyor.

## Zorluklar ve Hususlar

### Teknik Zorluklar

**AltyapÄ± Gereksinimleri**: Yerel daÄŸÄ±tÄ±m, dikkatli kapasite planlamasÄ± ve donanÄ±m seÃ§imi gerektirir. KuruluÅŸlar, performans gereksinimlerini maliyet kÄ±sÄ±tlamalarÄ±yla dengelemeli ve bÃ¼yÃ¼yen iÅŸ yÃ¼kleri iÃ§in Ã¶lÃ§eklenebilirliÄŸi saÄŸlamalÄ±dÄ±r.

**ğŸ”§ BakÄ±m ve GÃ¼ncellemeler**: DÃ¼zenli model gÃ¼ncellemeleri, gÃ¼venlik yamalarÄ± ve performans optimizasyonu, Ã¶zel kaynaklar ve uzmanlÄ±k gerektirir. Ãœretim ortamlarÄ± iÃ§in otomatik daÄŸÄ±tÄ±m boru hatlarÄ± gerekli hale gelir.

### GÃ¼venlik HususlarÄ±

**Model GÃ¼venliÄŸi**: Yetkisiz eriÅŸim veya Ã§Ä±karÄ±mdan korumak iÃ§in ÅŸifreleme, eriÅŸim kontrolleri ve denetim kaydÄ± gibi kapsamlÄ± gÃ¼venlik Ã¶nlemleri gereklidir.

**Veri Koruma**: Ã‡Ä±karÄ±m hattÄ± boyunca gÃ¼venli veri iÅŸleme saÄŸlanÄ±rken performans ve kullanÄ±labilirlik standartlarÄ±nÄ± korumak.

## Pratik Uygulama Kontrol Listesi

### âœ… DaÄŸÄ±tÄ±m Ã–ncesi DeÄŸerlendirme

- [ ] DonanÄ±m gereksinimleri analizi ve kapasite planlamasÄ±
- [ ] AÄŸ mimarisi ve gÃ¼venlik gereksinimleri tanÄ±mÄ±
- [ ] Model seÃ§imi ve performans kÄ±yaslamasÄ±
- [ ] Uyumluluk ve dÃ¼zenleyici gereksinimler doÄŸrulamasÄ±

---

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluÄŸu saÄŸlamak iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±k iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul edilmez.