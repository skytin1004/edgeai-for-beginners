<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddaad917d0c16fc3d498a6b4eabc8088",
  "translation_date": "2025-10-09T11:16:53+00:00",
  "source_file": "Workshop/notebooks/quickstart.md",
  "language_code": "tr"
}
-->
# AtÃ¶lye Not Defterleri - HÄ±zlÄ± BaÅŸlangÄ±Ã§ KÄ±lavuzu

## Ä°Ã§indekiler

- [Ã–n KoÅŸullar](../../../../Workshop/notebooks)
- [Ä°lk Kurulum](../../../../Workshop/notebooks)
- [Oturum 04: Model KarÅŸÄ±laÅŸtÄ±rmasÄ±](../../../../Workshop/notebooks)
- [Oturum 05: Ã‡oklu Ajan OrkestratÃ¶rÃ¼](../../../../Workshop/notebooks)
- [Oturum 06: Niyet TabanlÄ± Model YÃ¶nlendirme](../../../../Workshop/notebooks)
- [Ortam DeÄŸiÅŸkenleri](../../../../Workshop/notebooks)
- [Ortak Komutlar](../../../../Workshop/notebooks)

---

## Ã–n KoÅŸullar

### 1. Foundry Local'Ä± YÃ¼kleyin

**Windows:**
```bash
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**Kurulumu DoÄŸrula:**
```bash
foundry --version
```

### 2. Python BaÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± YÃ¼kleyin

```bash
cd Workshop
pip install -r requirements.txt
```

Ya da tek tek yÃ¼kleyin:
```bash
pip install foundry-local-sdk openai numpy requests
```

---

## Ä°lk Kurulum

### Foundry Local Servisini BaÅŸlatÄ±n

**Herhangi bir not defterini Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce gereklidir:**

```bash
# Start the service
foundry service start

# Verify it's running
foundry service status
```

Beklenen Ã§Ä±ktÄ±:
```
âœ… Service started successfully
Endpoint: http://localhost:59959
```

### Modelleri Ä°ndirin ve YÃ¼kleyin

Not defterleri varsayÄ±lan olarak ÅŸu modelleri kullanÄ±r:

```bash
# Download models (first time only - may take several minutes)
foundry model download phi-4-mini
foundry model download qwen2.5-3b
foundry model download phi-3.5-mini
foundry model download qwen2.5-0.5b

# Load models into memory
foundry model run phi-4-mini
foundry model run qwen2.5-3b
foundry model run phi-3.5-mini
```

### Kurulumu DoÄŸrula

```bash
# List loaded models
foundry model ls

# Check service health
curl http://localhost:59959/v1/models
```

---

## Oturum 04: Model KarÅŸÄ±laÅŸtÄ±rmasÄ±

### AmaÃ§
KÃ¼Ã§Ã¼k Dil Modelleri (SLM) ile BÃ¼yÃ¼k Dil Modelleri (LLM) arasÄ±ndaki performansÄ± karÅŸÄ±laÅŸtÄ±rÄ±n.

### HÄ±zlÄ± Kurulum

```bash
# Start service (if not already running)
foundry service start

# Load required models
foundry model run phi-4-mini
foundry model run qwen2.5-3b
```

### Not Defterini Ã‡alÄ±ÅŸtÄ±rma

1. **AÃ§Ä±n** `session04_model_compare.ipynb` dosyasÄ±nÄ± VS Code veya Jupyter'de
2. **Ã‡ekirdeÄŸi yeniden baÅŸlatÄ±n** (Kernel â†’ Restart Kernel)
3. **TÃ¼m hÃ¼creleri sÄ±rayla Ã§alÄ±ÅŸtÄ±rÄ±n**

### Temel YapÄ±landÄ±rma

**VarsayÄ±lan Modeller:**
- **SLM:** `phi-4-mini` (~4GB RAM, daha hÄ±zlÄ±)
- **LLM:** `qwen2.5-3b` (~3GB RAM, bellek optimizasyonlu)

**Ortam DeÄŸiÅŸkenleri (isteÄŸe baÄŸlÄ±):**
```python
import os
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'
```

### Beklenen Ã‡Ä±ktÄ±

```
================================================================================
COMPARISON SUMMARY
================================================================================
Alias                Latency(s)      Tokens     Route               
--------------------------------------------------------------------------------
phi-4-mini           1.234           150        chat.completions    
qwen2.5-3b           2.456           180        chat.completions    
================================================================================

ğŸ’¡ SLM is 1.99x faster than LLM for this prompt
```

### Ã–zelleÅŸtirme

**FarklÄ± modeller kullanÄ±n:**
```python
os.environ['SLM_ALIAS'] = 'phi-3.5-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-1.5b'
```

**Ã–zel istem:**
```python
os.environ['COMPARE_PROMPT'] = 'Explain quantum computing in simple terms'
```

### DoÄŸrulama Kontrol Listesi

- [ ] HÃ¼cre 12 doÄŸru modelleri gÃ¶steriyor (phi-4-mini, qwen2.5-3b)
- [ ] HÃ¼cre 12 doÄŸru uÃ§ noktayÄ± gÃ¶steriyor (port 59959)
- [ ] HÃ¼cre 16 tanÄ±lama geÃ§iyor (âœ… Servis Ã§alÄ±ÅŸÄ±yor)
- [ ] HÃ¼cre 20 Ã¶n kontrol geÃ§iyor (her iki model tamam)
- [ ] HÃ¼cre 22 karÅŸÄ±laÅŸtÄ±rma gecikme deÄŸerleriyle tamamlanÄ±yor
- [ ] HÃ¼cre 24 doÄŸrulama ğŸ‰ TÃœM KONTROLLER GEÃ‡TÄ°! mesajÄ±nÄ± gÃ¶steriyor

### Zaman Tahmini
- **Ä°lk Ã§alÄ±ÅŸtÄ±rma:** 5-10 dakika (model indirmeleri dahil)
- **Sonraki Ã§alÄ±ÅŸtÄ±rmalar:** 1-2 dakika

---

## Oturum 05: Ã‡oklu Ajan OrkestratÃ¶rÃ¼

### AmaÃ§
Foundry Local SDK kullanarak Ã§oklu ajan iÅŸ birliÄŸini gÃ¶sterin - ajanlar birlikte Ã§alÄ±ÅŸarak daha rafine Ã§Ä±ktÄ±lar Ã¼retir.

### HÄ±zlÄ± Kurulum

```bash
# Start service
foundry service start

# Load models
foundry model run phi-4-mini  # Primary model
foundry model run qwen2.5-7b  # Optional: higher quality editor
```

### Not Defterini Ã‡alÄ±ÅŸtÄ±rma

1. **AÃ§Ä±n** `session05_agents_orchestrator.ipynb`
2. **Ã‡ekirdeÄŸi yeniden baÅŸlatÄ±n**
3. **TÃ¼m hÃ¼creleri sÄ±rayla Ã§alÄ±ÅŸtÄ±rÄ±n**

### Temel YapÄ±landÄ±rma

**VarsayÄ±lan Kurulum (Her Ä°ki Ajan Ä°Ã§in AynÄ± Model):**
```python
PRIMARY_ALIAS = 'phi-4-mini'
EDITOR_ALIAS = 'phi-4-mini'  # Uses same model
```

**GeliÅŸmiÅŸ Kurulum (FarklÄ± Modeller):**
```python
import os
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'     # Fast for research
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'      # High quality for editing
```

### Mimari

```
User Question
    â†“
Researcher Agent (phi-4-mini)
  â†’ Gathers bullet points
    â†“
Editor Agent (phi-4-mini or qwen2.5-7b)
  â†’ Refines into executive summary
    â†“
Final Output
```

### Beklenen Ã‡Ä±ktÄ±

```
================================================================================
[Pipeline] Question: Explain why edge AI matters for compliance.
================================================================================

[Stage 1: Research]
Output: â€¢ Edge AI processes data locally, reducing transmission...

[Stage 2: Editorial Refinement]
Output: Executive Summary: Edge AI enhances compliance by keeping data...

[FINAL OUTPUT]
Executive Summary: Edge AI enhances compliance by keeping sensitive data 
on-premises and reduces latency through local processing.

[METADATA]
Models used: {'researcher': 'phi-4-mini', 'editor': 'phi-4-mini'}
```

### UzantÄ±lar

**Daha fazla ajan ekleyin:**
```python
critic = Agent(
    name='Critic',
    system='Review content for accuracy',
    client=client,
    model_id=model_id
)
```

**Toplu test:**
```python
test_questions = [
    "What are benefits of local AI?",
    "How does RAG improve accuracy?",
]

for q in test_questions:
    result = pipeline(q, verbose=False)
    print(result['final'])
```

### Zaman Tahmini
- **Ä°lk Ã§alÄ±ÅŸtÄ±rma:** 3-5 dakika
- **Sonraki Ã§alÄ±ÅŸtÄ±rmalar:** Soru baÅŸÄ±na 1-2 dakika

---

## Oturum 06: Niyet TabanlÄ± Model YÃ¶nlendirme

### AmaÃ§
Tespit edilen niyete gÃ¶re istemleri Ã¶zel modellere akÄ±llÄ±ca yÃ¶nlendirin.

### HÄ±zlÄ± Kurulum

```bash
# Start service
foundry service start

# Load all routing models (CPU variants recommended)
foundry model run phi-4-mini-cpu
foundry model run qwen2.5-0.5b-cpu
foundry model run phi-3.5-mini-cpu
```

**Not:** Oturum 06 maksimum uyumluluk iÃ§in varsayÄ±lan olarak CPU modellerini kullanÄ±r.

### Not Defterini Ã‡alÄ±ÅŸtÄ±rma

1. **AÃ§Ä±n** `session06_models_router.ipynb`
2. **Ã‡ekirdeÄŸi yeniden baÅŸlatÄ±n**
3. **TÃ¼m hÃ¼creleri sÄ±rayla Ã§alÄ±ÅŸtÄ±rÄ±n**

### Temel YapÄ±landÄ±rma

**VarsayÄ±lan Katalog (CPU Modelleri):**
```python
CATALOG = {
    'phi-4-mini-cpu': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b-cpu': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini-cpu': {'capabilities':['code','refactor'],'priority':3},
}
```

**Alternatif (GPU Modelleri):**
```python
# Uncomment GPU catalog in Cell #6 if you have sufficient VRAM (8GB+)
CATALOG = {
    'phi-4-mini': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini': {'capabilities':['code','refactor'],'priority':3},
}
```

### Niyet Tespiti

YÃ¶nlendirici niyeti tespit etmek iÃ§in regex desenlerini kullanÄ±r:

| Niyet | Ã–rnek Desenler | YÃ¶nlendirildiÄŸi Model |
|-------|-----------------|-----------------------|
| `code` | "refactor", "implement function" | phi-3.5-mini-cpu |
| `classification` | "categorize", "classify this" | qwen2.5-0.5b-cpu |
| `summarize` | "summarize", "tl;dr" | phi-4-mini-cpu |
| `general` | DiÄŸer her ÅŸey | phi-4-mini-cpu |

### Beklenen Ã‡Ä±ktÄ±

```
âœ“ Using CPU-optimized models (default configuration)
  Models: phi-4-mini-cpu, qwen2.5-0.5b-cpu, phi-3.5-mini-cpu

Routing prompts to specialized models...
============================================================

Prompt: Refactor this Python function for readability
  Intent: code           | Model: phi-3.5-mini-cpu
  Output: Here's a refactored version...
  Tokens: 156

Prompt: Categorize this email as urgent or normal
  Intent: classification | Model: qwen2.5-0.5b-cpu
  Output: Category: Normal
  Tokens: 45

âœ“ Success! All prompts routed correctly.
```

### Ã–zelleÅŸtirme

**Ã–zel niyet ekleyin:**
```python
import re

# Add to RULES
RULES.append((re.compile('translate|ç¿»è¯‘', re.I), 'translation'))

# Add capability to catalog
CATALOG['phi-4-mini-cpu']['capabilities'].append('translation')
```

**Token takibini etkinleÅŸtirin:**
```python
import os
os.environ['SHOW_USAGE'] = '1'
```

### GPU Modellerine GeÃ§iÅŸ

8GB+ VRAM'e sahipseniz:

1. **HÃ¼cre #6**'da CPU kataloÄŸunu yorum satÄ±rÄ± yapÄ±n
2. GPU kataloÄŸunu yorum satÄ±rÄ± olmaktan Ã§Ä±karÄ±n
3. GPU modellerini yÃ¼kleyin:
   ```bash
   foundry model run phi-4-mini
   foundry model run qwen2.5-0.5b
   foundry model run phi-3.5-mini
   ```
4. Ã‡ekirdeÄŸi yeniden baÅŸlatÄ±n ve not defterini tekrar Ã§alÄ±ÅŸtÄ±rÄ±n

### Zaman Tahmini
- **Ä°lk Ã§alÄ±ÅŸtÄ±rma:** 5-10 dakika (model yÃ¼kleme)
- **Sonraki Ã§alÄ±ÅŸtÄ±rmalar:** Test baÅŸÄ±na 30-60 saniye

---

## Ortam DeÄŸiÅŸkenleri

### Genel YapÄ±landÄ±rma

Jupyter/VS Code'u baÅŸlatmadan Ã¶nce ayarlayÄ±n:

**Windows (Komut Ä°stemi):**
```cmd
set FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
set SHOW_USAGE=1
set RETRY_ON_FAIL=1
```

**Windows (PowerShell):**
```powershell
$env:FOUNDRY_LOCAL_ENDPOINT="http://localhost:59959/v1"
$env:SHOW_USAGE="1"
$env:RETRY_ON_FAIL="1"
```

**macOS/Linux:**
```bash
export FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
export SHOW_USAGE=1
export RETRY_ON_FAIL=1
```

### Not Defteri Ä°Ã§inde YapÄ±landÄ±rma

Herhangi bir not defterinin baÅŸÄ±nda ayarlayÄ±n:

```python
import os

# Foundry Local configuration
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'

# Model selection
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'

# Agent models
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'

# Debugging
os.environ['SHOW_USAGE'] = '1'       # Show token usage
os.environ['RETRY_ON_FAIL'] = '1'    # Enable retries
os.environ['RETRY_BACKOFF'] = '2.0'  # Retry delay
```

---

## Ortak Komutlar

### Servis YÃ¶netimi

```bash
# Start service
foundry service start

# Check status
foundry service status

# Stop service
foundry service stop

# View logs
foundry service logs
```

### Model YÃ¶netimi

```bash
# List all available models in catalog
foundry model catalog

# List loaded models
foundry model ls

# Download a model
foundry model download phi-4-mini

# Load a model
foundry model run phi-4-mini

# Unload a model
foundry model unload phi-4-mini

# Remove a model
foundry model remove phi-4-mini

# Get model info
foundry model info phi-4-mini
```

### UÃ§ NoktalarÄ± Test Etme

```bash
# Check service health
curl http://localhost:59959/health

# List available models via API
curl http://localhost:59959/v1/models

# Test model completion
curl http://localhost:59959/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role":"user","content":"Hello"}],
    "max_tokens": 50
  }'
```

### TanÄ±lama KomutlarÄ±

```bash
# Check everything
foundry --version
foundry service status
foundry model ls
foundry device info

# GPU status (NVIDIA)
nvidia-smi

# NPU status (Qualcomm)
foundry device info
```

---

## En Ä°yi Uygulamalar

### Herhangi Bir Not Defterine BaÅŸlamadan Ã–nce

1. **Servisin Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± kontrol edin:**
   ```bash
   foundry service status
   ```

2. **Modellerin yÃ¼klÃ¼ olduÄŸunu doÄŸrulayÄ±n:**
   ```bash
   foundry model ls
   ```

3. **Not defteri Ã§ekirdeÄŸini yeniden baÅŸlatÄ±n** eÄŸer tekrar Ã§alÄ±ÅŸtÄ±rÄ±yorsanÄ±z

4. **TÃ¼m Ã§Ä±ktÄ±larÄ± temizleyin** temiz bir Ã§alÄ±ÅŸma iÃ§in

### Kaynak YÃ¶netimi

1. **VarsayÄ±lan olarak CPU modellerini kullanÄ±n** uyumluluk iÃ§in
2. **GPU modellerine geÃ§in** yalnÄ±zca 8GB+ VRAM'e sahipseniz
3. **DiÄŸer GPU uygulamalarÄ±nÄ± kapatÄ±n** Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce
4. **Servisi aÃ§Ä±k tutun** not defteri oturumlarÄ± arasÄ±nda
5. **Kaynak kullanÄ±mÄ±nÄ± izleyin** GÃ¶rev YÃ¶neticisi / nvidia-smi ile

### Sorun Giderme

1. **Ã–nce her zaman servisi kontrol edin** kodu hata ayÄ±klamadan Ã¶nce
2. **Ã‡ekirdeÄŸi yeniden baÅŸlatÄ±n** eÄŸer eski yapÄ±landÄ±rma gÃ¶rÃ¼yorsanÄ±z
3. **TanÄ±lama hÃ¼crelerini yeniden Ã§alÄ±ÅŸtÄ±rÄ±n** herhangi bir deÄŸiÅŸiklikten sonra
4. **Model adlarÄ±nÄ±n eÅŸleÅŸtiÄŸini kontrol edin** yÃ¼klenenlerle
5. **UÃ§ nokta portunun eÅŸleÅŸtiÄŸini doÄŸrulayÄ±n** servis durumu ile

---

## HÄ±zlÄ± Referans: Model Takma AdlarÄ±

### YaygÄ±n Modeller

| Takma Ad | Boyut | En Ä°yi KullanÄ±m AlanÄ± | RAM/VRAM | Varyantlar |
|----------|-------|-----------------------|----------|------------|
| `phi-4-mini` | ~4B | Genel sohbet, Ã¶zetleme | 4-6GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `phi-3.5-mini` | ~3.5B | Kod Ã¼retimi, yeniden dÃ¼zenleme | 3-5GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `qwen2.5-3b` | ~3B | Genel gÃ¶revler, verimli | 3-4GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-1.5b` | ~1.5B | HÄ±zlÄ±, dÃ¼ÅŸÃ¼k kaynak | 2-3GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-0.5b` | ~0.5B | SÄ±nÄ±flandÄ±rma, minimal kaynak | 1-2GB | `-cpu`, `-cuda-gpu` |

### Varyant Ä°simlendirme

- **Temel isim** (Ã¶r. `phi-4-mini`): DonanÄ±mÄ±nÄ±za en uygun varyantÄ± otomatik seÃ§er
- **`-cpu`**: CPU iÃ§in optimize edilmiÅŸ, her yerde Ã§alÄ±ÅŸÄ±r
- **`-cuda-gpu`**: NVIDIA GPU iÃ§in optimize edilmiÅŸ, 8GB+ VRAM gerektirir
- **`-npu`**: Qualcomm NPU iÃ§in optimize edilmiÅŸ, NPU sÃ¼rÃ¼cÃ¼leri gerektirir

**Ã–neri:** Temel isimleri (son ek olmadan) kullanÄ±n ve Foundry Local'Ä±n en iyi varyantÄ± otomatik seÃ§mesine izin verin.

---

## BaÅŸarÄ± GÃ¶stergeleri

HazÄ±rsÄ±nÄ±z, eÄŸer:

âœ… `foundry service status` "Ã§alÄ±ÅŸÄ±yor" gÃ¶steriyor  
âœ… `foundry model ls` gerekli modellerinizi gÃ¶steriyor  
âœ… Servis doÄŸru uÃ§ noktada eriÅŸilebilir  
âœ… SaÄŸlÄ±k kontrolÃ¼ 200 OK dÃ¶ndÃ¼rÃ¼yor  
âœ… Not defteri tanÄ±lama hÃ¼creleri geÃ§iyor  
âœ… Ã‡Ä±ktÄ±da baÄŸlantÄ± hatasÄ± yok  

---

## YardÄ±m Alma

### Belgeler
- **Ana Depo:** https://github.com/microsoft/Foundry-Local
- **Python SDK:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python
- **CLI ReferansÄ±:** https://github.com/microsoft/Foundry-Local/blob/main/docs/reference/reference-cli.md
- **Sorun Giderme:** Bu dizindeki `troubleshooting.md` dosyasÄ±na bakÄ±n

### GitHub SorunlarÄ±
- https://github.com/microsoft/Foundry-Local/issues
- https://github.com/microsoft/edgeai-for-beginners/issues

---

**Son GÃ¼ncelleme:** 8 Ekim 2025  
**SÃ¼rÃ¼m:** AtÃ¶lye Not Defterleri 2.0

---

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±k iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.