<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a35d3b47e6ae98ad9b3e89fb73917e90",
  "translation_date": "2025-09-17T23:23:23+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "tr"
}
-->
# BÃ¶lÃ¼m 1: EdgeAI Temelleri

EdgeAI, yapay zeka daÄŸÄ±tÄ±mÄ±nda bir paradigma deÄŸiÅŸimini temsil eder ve yapay zeka yeteneklerini yalnÄ±zca bulut tabanlÄ± iÅŸlemeye gÃ¼venmek yerine doÄŸrudan uÃ§ cihazlara taÅŸÄ±r. EdgeAI'nin, sÄ±nÄ±rlÄ± kaynaklara sahip cihazlarda yerel yapay zeka iÅŸlemesini nasÄ±l mÃ¼mkÃ¼n kÄ±ldÄ±ÄŸÄ±nÄ±, makul bir performansÄ± korurken gizlilik, gecikme ve Ã§evrimdÄ±ÅŸÄ± Ã§alÄ±ÅŸma gibi zorluklarÄ± ele aldÄ±ÄŸÄ±nÄ± anlamak Ã¶nemlidir.

## GiriÅŸ

Bu derste EdgeAI ve temel kavramlarÄ±nÄ± inceleyeceÄŸiz. Geleneksel yapay zeka hesaplama paradigmasÄ±nÄ±, uÃ§ biliÅŸimin zorluklarÄ±nÄ±, EdgeAI'yi mÃ¼mkÃ¼n kÄ±lan temel teknolojileri ve Ã§eÅŸitli endÃ¼strilerdeki pratik uygulamalarÄ± ele alacaÄŸÄ±z.

## Ã–ÄŸrenme Hedefleri

Bu dersin sonunda ÅŸunlarÄ± yapabileceksiniz:

- Geleneksel bulut tabanlÄ± yapay zeka ile EdgeAI yaklaÅŸÄ±mlarÄ± arasÄ±ndaki farkÄ± anlayÄ±n.
- UÃ§ cihazlarda yapay zeka iÅŸlemesini mÃ¼mkÃ¼n kÄ±lan temel teknolojileri tanÄ±mlayÄ±n.
- EdgeAI uygulamalarÄ±nÄ±n avantajlarÄ±nÄ± ve sÄ±nÄ±rlamalarÄ±nÄ± tanÄ±yÄ±n.
- EdgeAI bilgilerini gerÃ§ek dÃ¼nya senaryolarÄ±nda ve kullanÄ±m durumlarÄ±nda uygulayÄ±n.

## Geleneksel Yapay Zeka Hesaplama ParadigmasÄ±nÄ± Anlamak

Geleneksel olarak, Ã¼retken yapay zeka uygulamalarÄ±, bÃ¼yÃ¼k dil modellerini (LLM'ler) etkili bir ÅŸekilde Ã§alÄ±ÅŸtÄ±rmak iÃ§in yÃ¼ksek performanslÄ± hesaplama altyapÄ±sÄ±na dayanÄ±r. KuruluÅŸlar genellikle bu modelleri bulut ortamlarÄ±nda GPU kÃ¼melerinde daÄŸÄ±tÄ±r ve API arayÃ¼zleri aracÄ±lÄ±ÄŸÄ±yla bu yeteneklere eriÅŸir.

Bu merkezi model birÃ§ok uygulama iÃ§in iyi Ã§alÄ±ÅŸÄ±r, ancak uÃ§ biliÅŸim senaryolarÄ±nda doÄŸal sÄ±nÄ±rlamalarÄ± vardÄ±r. Geleneksel yaklaÅŸÄ±m, kullanÄ±cÄ± sorgularÄ±nÄ± uzak sunuculara gÃ¶ndermeyi, gÃ¼Ã§lÃ¼ donanÄ±mlarla iÅŸlemeyi ve sonuÃ§larÄ± internet Ã¼zerinden geri gÃ¶ndermeyi iÃ§erir. Bu yÃ¶ntem, en son modellerin kullanÄ±labilirliÄŸini saÄŸlarken, internet baÄŸlantÄ±sÄ±na baÄŸÄ±mlÄ±lÄ±k yaratÄ±r, gecikme sorunlarÄ± ortaya Ã§Ä±karÄ±r ve hassas verilerin harici sunuculara iletilmesi gerektiÄŸinde gizlilik endiÅŸelerini artÄ±rÄ±r.

Geleneksel yapay zeka hesaplama paradigmalarÄ±yla Ã§alÄ±ÅŸÄ±rken anlamamÄ±z gereken bazÄ± temel kavramlar ÅŸunlardÄ±r:

- **â˜ï¸ Bulut TabanlÄ± Ä°ÅŸleme**: Yapay zeka modelleri, yÃ¼ksek hesaplama kaynaklarÄ±na sahip gÃ¼Ã§lÃ¼ sunucu altyapÄ±sÄ±nda Ã§alÄ±ÅŸÄ±r.
- **ğŸ”Œ API TabanlÄ± EriÅŸim**: Uygulamalar, yapay zeka yeteneklerine yerel iÅŸlem yerine uzak API Ã§aÄŸrÄ±larÄ±yla eriÅŸir.
- **ğŸ›ï¸ Merkezi Model YÃ¶netimi**: Modeller merkezi olarak korunur ve gÃ¼ncellenir, tutarlÄ±lÄ±ÄŸÄ± saÄŸlar ancak aÄŸ baÄŸlantÄ±sÄ± gerektirir.
- **ğŸ“ˆ Kaynak Ã–lÃ§eklenebilirliÄŸi**: Bulut altyapÄ±sÄ±, deÄŸiÅŸen hesaplama taleplerini karÅŸÄ±lamak iÃ§in dinamik olarak Ã¶lÃ§eklenebilir.

## UÃ§ BiliÅŸimin ZorluklarÄ±

DizÃ¼stÃ¼ bilgisayarlar, cep telefonlarÄ± ve Raspberry Pi veya NVIDIA Orin Nano gibi Nesnelerin Ä°nterneti (IoT) cihazlarÄ± gibi uÃ§ cihazlar benzersiz hesaplama kÄ±sÄ±tlamalarÄ± sunar. Bu cihazlar, veri merkezi altyapÄ±sÄ±na kÄ±yasla genellikle sÄ±nÄ±rlÄ± iÅŸlem gÃ¼cÃ¼ne, belleÄŸe ve enerji kaynaklarÄ±na sahiptir.

Geleneksel LLM'leri bu tÃ¼r cihazlarda Ã§alÄ±ÅŸtÄ±rmak, bu donanÄ±m sÄ±nÄ±rlamalarÄ± nedeniyle tarihsel olarak zordu. Ancak, uÃ§ yapay zeka iÅŸlemesine duyulan ihtiyaÃ§ Ã§eÅŸitli senaryolarda giderek daha Ã¶nemli hale gelmiÅŸtir. Ä°nternet baÄŸlantÄ±sÄ±nÄ±n gÃ¼venilmez veya mevcut olmadÄ±ÄŸÄ± durumlarÄ± dÃ¼ÅŸÃ¼nÃ¼n: uzak endÃ¼striyel sahalar, transit halindeki araÃ§lar veya zayÄ±f aÄŸ kapsama alanÄ±na sahip bÃ¶lgeler. AyrÄ±ca, yÃ¼ksek gÃ¼venlik standartlarÄ± gerektiren uygulamalar, Ã¶rneÄŸin tÄ±bbi cihazlar, finansal sistemler veya devlet uygulamalarÄ±, gizliliÄŸi ve uyumluluÄŸu korumak iÃ§in hassas verileri yerel olarak iÅŸlemeyi gerektirebilir.

### UÃ§ BiliÅŸim KÄ±sÄ±tlamalarÄ±

UÃ§ biliÅŸim ortamlarÄ±, geleneksel bulut tabanlÄ± yapay zeka Ã§Ã¶zÃ¼mlerinin karÅŸÄ±laÅŸmadÄ±ÄŸÄ± birkaÃ§ temel kÄ±sÄ±tlamayla karÅŸÄ± karÅŸÄ±yadÄ±r:

- **SÄ±nÄ±rlÄ± Ä°ÅŸlem GÃ¼cÃ¼**: UÃ§ cihazlar genellikle sunucu sÄ±nÄ±fÄ± donanÄ±ma kÄ±yasla daha az CPU Ã§ekirdeÄŸine ve daha dÃ¼ÅŸÃ¼k saat hÄ±zlarÄ±na sahiptir.
- **Bellek KÄ±sÄ±tlamalarÄ±**: UÃ§ cihazlarda mevcut RAM ve depolama kapasitesi Ã¶nemli Ã¶lÃ§Ã¼de azaltÄ±lmÄ±ÅŸtÄ±r.
- **GÃ¼Ã§ SÄ±nÄ±rlamalarÄ±**: Pil ile Ã§alÄ±ÅŸan cihazlar, uzun sÃ¼reli Ã§alÄ±ÅŸma iÃ§in performansÄ± enerji tÃ¼ketimiyle dengelemelidir.
- **Termal YÃ¶netim**: Kompakt form faktÃ¶rleri, yÃ¼k altÄ±nda sÃ¼rekli performansÄ± etkileyen soÄŸutma yeteneklerini sÄ±nÄ±rlar.

## EdgeAI Nedir?

### Kavram: Edge AI TanÄ±mÄ±

Edge AI, yapay zeka algoritmalarÄ±nÄ±n doÄŸrudan uÃ§ cihazlardaâ€”verilerin oluÅŸturulduÄŸu ve toplandÄ±ÄŸÄ± aÄŸÄ±n "uÃ§" noktasÄ±ndaki fiziksel donanÄ±mdaâ€”daÄŸÄ±tÄ±mÄ± ve Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ±nÄ± ifade eder. Bu cihazlar arasÄ±nda akÄ±llÄ± telefonlar, IoT sensÃ¶rleri, akÄ±llÄ± kameralar, otonom araÃ§lar, giyilebilir cihazlar ve endÃ¼striyel ekipmanlar bulunur. Geleneksel yapay zeka sistemlerinin iÅŸleme iÃ§in bulut sunucularÄ±na gÃ¼venmesine kÄ±yasla, Edge AI zekayÄ± doÄŸrudan veri kaynaÄŸÄ±na taÅŸÄ±r.

Edge AI'nin temelinde, yapay zeka iÅŸlemesini merkezileÅŸtirilmiÅŸ veri merkezlerinden uzaklaÅŸtÄ±rmak ve dijital ekosistemimizi oluÅŸturan geniÅŸ cihaz aÄŸÄ±na daÄŸÄ±tmak yer alÄ±r. Bu, yapay zeka sistemlerinin tasarlanma ve daÄŸÄ±tÄ±lma biÃ§iminde temel bir mimari deÄŸiÅŸimi temsil eder.

Edge AI'nin temel kavramsal sÃ¼tunlarÄ± ÅŸunlardÄ±r:

- **YakÄ±nlÄ±k Ä°ÅŸlemesi**: Hesaplama, verinin kaynaklandÄ±ÄŸÄ± yere fiziksel olarak yakÄ±n bir yerde gerÃ§ekleÅŸir.
- **DaÄŸÄ±tÄ±lmÄ±ÅŸ Zeka**: Karar verme yetenekleri birden fazla cihaza daÄŸÄ±tÄ±lÄ±r.
- **Veri EgemenliÄŸi**: Bilgiler yerel kontrol altÄ±nda kalÄ±r, genellikle cihazdan hiÃ§ ayrÄ±lmaz.
- **Otonom Ã‡alÄ±ÅŸma**: Cihazlar, sÃ¼rekli baÄŸlantÄ± gerektirmeden akÄ±llÄ±ca Ã§alÄ±ÅŸabilir.
- **GÃ¶mÃ¼lÃ¼ Yapay Zeka**: Zeka, gÃ¼nlÃ¼k cihazlarÄ±n iÃ§sel bir yeteneÄŸi haline gelir.

### Edge AI Mimari GÃ¶rselleÅŸtirme

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TRADITIONAL AI ARCHITECTURE                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Data Transfer  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   API Response   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Edge Devices â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Cloud Servers â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ End Users â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            EDGE AI ARCHITECTURE                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Direct Response   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Edge Devices with Embedded AI        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ End Users â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â”‚ Sensors â”‚â”€>â”‚ SLM Inference â”‚â”€>â”‚ Local Action â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI, yapay zeka daÄŸÄ±tÄ±mÄ±nda bir paradigma deÄŸiÅŸimini temsil eder ve yapay zeka yeteneklerini yalnÄ±zca bulut tabanlÄ± iÅŸlemeye gÃ¼venmek yerine doÄŸrudan uÃ§ cihazlara taÅŸÄ±r. Bu yaklaÅŸÄ±m, yapay zeka modellerinin sÄ±nÄ±rlÄ± hesaplama kaynaklarÄ±na sahip cihazlarda yerel olarak Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar ve sÃ¼rekli internet baÄŸlantÄ±sÄ± gerektirmeden gerÃ§ek zamanlÄ± Ã§Ä±karÄ±m yetenekleri sunar.

EdgeAI, yapay zeka modellerini daha verimli ve sÄ±nÄ±rlÄ± kaynaklara sahip cihazlarda daÄŸÄ±tÄ±ma uygun hale getirmek iÃ§in tasarlanmÄ±ÅŸ Ã§eÅŸitli teknolojileri ve teknikleri kapsar. AmaÃ§, yapay zeka modellerinin hesaplama ve bellek gereksinimlerini Ã¶nemli Ã¶lÃ§Ã¼de azaltÄ±rken makul bir performansÄ± korumaktÄ±r.

FarklÄ± cihaz tÃ¼rleri ve kullanÄ±m durumlarÄ± iÃ§in EdgeAI uygulamalarÄ±nÄ± mÃ¼mkÃ¼n kÄ±lan temel yaklaÅŸÄ±mlara bir gÃ¶z atalÄ±m.

### Temel EdgeAI Ä°lkeleri

EdgeAI, geleneksel bulut tabanlÄ± yapay zekadan farklÄ± kÄ±lan birkaÃ§ temel ilkeye dayanÄ±r:

- **Yerel Ä°ÅŸleme**: Yapay zeka Ã§Ä±karÄ±mÄ±, harici baÄŸlantÄ± gerektirmeden doÄŸrudan uÃ§ cihazda gerÃ§ekleÅŸir.
- **Kaynak Optimizasyonu**: Modeller, hedef cihazlarÄ±n donanÄ±m kÄ±sÄ±tlamalarÄ±na Ã¶zel olarak optimize edilir.
- **GerÃ§ek ZamanlÄ± Performans**: Zaman aÃ§Ä±sÄ±ndan hassas uygulamalar iÃ§in iÅŸlem, minimum gecikmeyle gerÃ§ekleÅŸir.
- **Gizlilik OdaklÄ± TasarÄ±m**: Hassas veriler cihazda kalÄ±r, gÃ¼venlik ve uyumluluÄŸu artÄ±rÄ±r.

## EdgeAI'yi MÃ¼mkÃ¼n KÄ±lan Temel Teknolojiler

### Model Kuantizasyonu

EdgeAI'deki en Ã¶nemli tekniklerden biri model kuantizasyonudur. Bu iÅŸlem, model parametrelerinin hassasiyetini genellikle 32-bit kayan nokta sayÄ±larÄ±ndan 8-bit tamsayÄ±lar veya daha dÃ¼ÅŸÃ¼k hassasiyet formatlarÄ±na dÃ¼ÅŸÃ¼rmeyi iÃ§erir. Bu hassasiyetin azaltÄ±lmasÄ± endiÅŸe verici gÃ¶rÃ¼nebilir, ancak araÅŸtÄ±rmalar, birÃ§ok yapay zeka modelinin performansÄ±nÄ± Ã¶nemli Ã¶lÃ§Ã¼de azaltmadan koruyabileceÄŸini gÃ¶stermiÅŸtir.

Kuantizasyon, kayan nokta deÄŸerlerinin aralÄ±ÄŸÄ±nÄ± daha kÃ¼Ã§Ã¼k bir dizi ayrÄ±k deÄŸere eÅŸleyerek Ã§alÄ±ÅŸÄ±r. Ã–rneÄŸin, her parametreyi temsil etmek iÃ§in 32 bit kullanmak yerine, kuantizasyon yalnÄ±zca 8 bit kullanabilir, bu da bellek gereksinimlerinde 4 kat azalma saÄŸlar ve genellikle daha hÄ±zlÄ± Ã§Ä±karÄ±m sÃ¼relerine yol aÃ§ar.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

FarklÄ± kuantizasyon teknikleri ÅŸunlarÄ± iÃ§erir:

- **EÄŸitim SonrasÄ± Kuantizasyon (PTQ)**: Model eÄŸitimi tamamlandÄ±ktan sonra uygulanÄ±r, yeniden eÄŸitim gerektirmez.
- **Kuantizasyon FarkÄ±ndalÄ±klÄ± EÄŸitim (QAT)**: Daha iyi doÄŸruluk iÃ§in kuantizasyon etkilerini eÄŸitim sÄ±rasÄ±nda iÃ§erir.
- **Dinamik Kuantizasyon**: AÄŸÄ±rlÄ±klarÄ± int8'e kuantize eder ancak aktivasyonlarÄ± dinamik olarak hesaplar.
- **Statik Kuantizasyon**: Hem aÄŸÄ±rlÄ±klar hem de aktivasyonlar iÃ§in tÃ¼m kuantizasyon parametrelerini Ã¶nceden hesaplar.

EdgeAI daÄŸÄ±tÄ±mlarÄ± iÃ§in uygun kuantizasyon stratejisinin seÃ§imi, hedef cihazÄ±n model mimarisi, performans gereksinimleri ve donanÄ±m yeteneklerine baÄŸlÄ±dÄ±r.

### Model SÄ±kÄ±ÅŸtÄ±rma ve Optimizasyon

Kuantizasyonun Ã¶tesinde, model boyutunu ve hesaplama gereksinimlerini azaltmaya yardÄ±mcÄ± olan Ã§eÅŸitli sÄ±kÄ±ÅŸtÄ±rma teknikleri vardÄ±r. Bunlar ÅŸunlarÄ± iÃ§erir:

**Budama**: Bu teknik, sinir aÄŸlarÄ±ndan gereksiz baÄŸlantÄ±larÄ± veya nÃ¶ronlarÄ± kaldÄ±rÄ±r. Performansa az katkÄ±da bulunan parametreleri belirleyip ortadan kaldÄ±rarak, budama model boyutunu Ã¶nemli Ã¶lÃ§Ã¼de azaltabilir ve doÄŸruluÄŸu koruyabilir.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Bilgi DamÄ±tÄ±mÄ±**: Bu yaklaÅŸÄ±m, daha bÃ¼yÃ¼k bir "Ã¶ÄŸretmen" modelin davranÄ±ÅŸÄ±nÄ± taklit etmek iÃ§in daha kÃ¼Ã§Ã¼k bir "Ã¶ÄŸrenci" modelin eÄŸitilmesini iÃ§erir. Ã–ÄŸrenci modeli, Ã¶ÄŸretmenin Ã§Ä±ktÄ±sÄ±nÄ± yaklaÅŸÄ±k olarak Ã¶ÄŸrenir ve genellikle Ã¶nemli Ã¶lÃ§Ã¼de daha az parametreyle benzer performans elde eder.

**Model Mimari Optimizasyonu**: AraÅŸtÄ±rmacÄ±lar, performansÄ± hesaplama verimliliÄŸiyle dengeleyen MobileNets, EfficientNets gibi uÃ§ daÄŸÄ±tÄ±m iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸ mimariler geliÅŸtirmiÅŸtir.

### KÃ¼Ã§Ã¼k Dil Modelleri (SLM'ler)

EdgeAI'deki yÃ¼kselen bir trend, KÃ¼Ã§Ã¼k Dil Modelleri'nin (SLM'ler) geliÅŸtirilmesidir. Bu modeller, kompakt ve verimli olacak ÅŸekilde tasarlanmÄ±ÅŸ olup, anlamlÄ± doÄŸal dil yetenekleri sunar. SLM'ler, dikkatli mimari seÃ§imler, verimli eÄŸitim teknikleri ve belirli alanlara veya gÃ¶revlere odaklanmÄ±ÅŸ eÄŸitim yoluyla bu verimliliÄŸi saÄŸlar.

Geleneksel yaklaÅŸÄ±mlarÄ±n bÃ¼yÃ¼k modelleri sÄ±kÄ±ÅŸtÄ±rmayÄ± iÃ§ermesine kÄ±yasla, SLM'ler genellikle daha kÃ¼Ã§Ã¼k veri setleri ve uÃ§ daÄŸÄ±tÄ±m iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸ optimize edilmiÅŸ mimarilerle eÄŸitilir. Bu yaklaÅŸÄ±m, yalnÄ±zca daha kÃ¼Ã§Ã¼k deÄŸil, aynÄ± zamanda belirli kullanÄ±m durumlarÄ± iÃ§in daha verimli modellerle sonuÃ§lanabilir.

## EdgeAI iÃ§in DonanÄ±m HÄ±zlandÄ±rma

Modern uÃ§ cihazlar, giderek yapay zeka iÅŸ yÃ¼klerini hÄ±zlandÄ±rmak iÃ§in tasarlanmÄ±ÅŸ Ã¶zel donanÄ±mlar iÃ§ermektedir:

### Sinir Ä°ÅŸleme Birimleri (NPUs)

NPUs, Ã¶zellikle sinir aÄŸÄ± hesaplamalarÄ± iÃ§in tasarlanmÄ±ÅŸ Ã¶zel iÅŸlemcilerdir. Bu Ã§ipler, geleneksel CPU'lardan Ã§ok daha verimli bir ÅŸekilde yapay zeka Ã§Ä±karÄ±m gÃ¶revlerini gerÃ§ekleÅŸtirebilir ve genellikle daha dÃ¼ÅŸÃ¼k gÃ¼Ã§ tÃ¼ketimiyle Ã§alÄ±ÅŸÄ±r. GÃ¼nÃ¼mÃ¼zÃ¼n birÃ§ok akÄ±llÄ± telefonu, dizÃ¼stÃ¼ bilgisayarÄ± ve IoT cihazÄ±, cihazda yapay zeka iÅŸlemesini mÃ¼mkÃ¼n kÄ±lmak iÃ§in NPU'lar iÃ§erir.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

NPU'lara sahip cihazlar ÅŸunlarÄ± iÃ§erir:

- **Apple**: Neural Engine iÃ§eren A-serisi ve M-serisi Ã§ipler
- **Qualcomm**: Hexagon DSP/NPU iÃ§eren Snapdragon iÅŸlemciler
- **Samsung**: NPU iÃ§eren Exynos iÅŸlemciler
- **Intel**: Movidius VPUs ve Habana Labs hÄ±zlandÄ±rÄ±cÄ±lar
- **Microsoft**: Windows Copilot+ PC'ler ile NPU'lar

### ğŸ® GPU HÄ±zlandÄ±rma

UÃ§ cihazlar veri merkezlerinde bulunan gÃ¼Ã§lÃ¼ GPU'lara sahip olmasa da, Ã§oÄŸu yine de yapay zeka iÅŸ yÃ¼klerini hÄ±zlandÄ±rabilen entegre veya ayrÄ±k GPU'lar iÃ§erir. Modern mobil GPU'lar ve entegre grafik iÅŸlemciler, yapay zeka Ã§Ä±karÄ±m gÃ¶revleri iÃ§in Ã¶nemli performans iyileÅŸtirmeleri saÄŸlayabilir.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### CPU Optimizasyonu

Sadece CPU kullanan cihazlar bile EdgeAI'den optimize edilmiÅŸ uygulamalar aracÄ±lÄ±ÄŸÄ±yla faydalanabilir. Modern CPU'lar yapay zeka iÅŸ yÃ¼kleri iÃ§in Ã¶zel talimatlar iÃ§erir ve yazÄ±lÄ±m Ã§erÃ§eveleri, yapay zeka Ã§Ä±karÄ±mÄ± iÃ§in CPU performansÄ±nÄ± en Ã¼st dÃ¼zeye Ã§Ä±karmak Ã¼zere geliÅŸtirilmiÅŸtir.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

EdgeAI ile Ã§alÄ±ÅŸan yazÄ±lÄ±m mÃ¼hendisleri iÃ§in, hedef cihazlarda Ã§Ä±karÄ±m performansÄ±nÄ± ve enerji verimliliÄŸini optimize etmek iÃ§in bu donanÄ±m hÄ±zlandÄ±rma seÃ§eneklerinden nasÄ±l yararlanÄ±lacaÄŸÄ±nÄ± anlamak kritik Ã¶neme sahiptir.

## EdgeAI'nin AvantajlarÄ±

### Gizlilik ve GÃ¼venlik

EdgeAI'nin en Ã¶nemli avantajlarÄ±ndan biri, artÄ±rÄ±lmÄ±ÅŸ gizlilik ve gÃ¼venliktir. Veriler yerel olarak cihazda iÅŸlendiÄŸi iÃ§in hassas bilgiler kullanÄ±cÄ±nÄ±n kontrolÃ¼nden Ã§Ä±kmaz. Bu, kiÅŸisel veriler, tÄ±bbi bilgiler veya gizli iÅŸ verilerini iÅŸleyen uygulamalar iÃ§in Ã¶zellikle Ã¶nemlidir.

### AzaltÄ±lmÄ±ÅŸ Gecikme

EdgeAI, verileri iÅŸleme iÃ§in uzak sunuculara gÃ¶ndermeye olan ihtiyacÄ± ortadan kaldÄ±rarak gecikmeyi Ã¶nemli Ã¶lÃ§Ã¼de azaltÄ±r. Bu, otonom araÃ§lar, endÃ¼striyel otomasyon veya anÄ±nda yanÄ±t gerektiren etkileÅŸimli uygulamalar gibi gerÃ§ek zamanlÄ± uygulamalar iÃ§in Ã§ok Ã¶nemlidir.

### Ã‡evrimdÄ±ÅŸÄ± Ã‡alÄ±ÅŸma YeteneÄŸi

EdgeAI, internet baÄŸlantÄ±sÄ± olmadÄ±ÄŸÄ±nda bile yapay zeka iÅŸlevselliÄŸini mÃ¼mkÃ¼n kÄ±lar. Bu, uzak konumlarda, seyahat sÄ±rasÄ±nda veya aÄŸ gÃ¼venilirliÄŸinin endiÅŸe verici olduÄŸu durumlarda uygulamalar iÃ§in deÄŸerlidir.

### Maliyet VerimliliÄŸi

Bulut tabanlÄ± yapay zeka hizmetlerine olan baÄŸÄ±mlÄ±lÄ±ÄŸÄ± azaltarak, EdgeAI yÃ¼ksek kullanÄ±m hacmine sahip uygulamalar iÃ§in operasyonel maliyetleri dÃ¼ÅŸÃ¼rmeye yardÄ±mcÄ± olabilir. KuruluÅŸlar, sÃ¼rekli API maliyetlerinden kaÃ§Ä±nabilir ve bant geniÅŸliÄŸi gereksinimlerini azaltabilir.

### Ã–lÃ§eklenebilirlik

EdgeAI, hesaplama yÃ¼kÃ¼nÃ¼ veri merkezlerinde merkezileÅŸtirmek yerine uÃ§ cihazlara daÄŸÄ±tÄ±r. Bu, altyapÄ± maliyetlerini azaltmaya ve genel sistem Ã¶lÃ§eklenebilirliÄŸini artÄ±rmaya yardÄ±mcÄ± olabilir.

## EdgeAI UygulamalarÄ±

### AkÄ±llÄ± Cihazlar ve IoT

EdgeAI, sesli asistanlarÄ±n komutlarÄ± yerel olarak iÅŸleyebilmesinden, videoyu buluta gÃ¶ndermeden nesneleri ve insanlarÄ± tanÄ±mlayabilen akÄ±llÄ± kameralara kadar birÃ§ok akÄ±llÄ± cihaz Ã¶zelliÄŸini gÃ¼Ã§lendirir. IoT cihazlarÄ±, kestirimci bakÄ±m, Ã§evresel izleme ve otomatik karar verme iÃ§in EdgeAI kullanÄ±r.

### Mobil Uygulamalar

AkÄ±llÄ± telefonlar ve tabletler, fotoÄŸraf iyileÅŸtirme, gerÃ§ek zamanlÄ± Ã§eviri, artÄ±rÄ±lmÄ±ÅŸ gerÃ§eklik ve kiÅŸiselleÅŸtirilmiÅŸ Ã¶neriler gibi Ã§eÅŸitli Ã¶zellikler iÃ§in EdgeAI kullanÄ±r. Bu uygulamalar, yerel iÅŸlemenin dÃ¼ÅŸÃ¼k gecikme ve gizlilik avantajlarÄ±ndan yararlanÄ±r.

### EndÃ¼striyel Uygulamalar

Ãœretim ve endÃ¼striyel ortamlar, kalite kontrol, kestirimci bakÄ±m ve sÃ¼reÃ§ optimizasyonu iÃ§in EdgeAI kullanÄ±r. Bu uygulamalar genellikle gerÃ§ek zamanlÄ± iÅŸlem gerektirir ve sÄ±nÄ±rlÄ± baÄŸlantÄ± ortamlarÄ±nda Ã§alÄ±ÅŸabilir.

### SaÄŸlÄ±k

TÄ±bbi cihazlar ve saÄŸlÄ±k uygulamalarÄ±, hasta izleme, tanÄ± yardÄ±mÄ± ve tedavi Ã¶nerileri iÃ§in EdgeAI kullanÄ±r. Yerel iÅŸlemenin gizlilik ve gÃ¼venlik avantajlarÄ±, saÄŸlÄ±k uygulamalarÄ±nda Ã¶zellikle Ã¶nemlidir.

## Zorluklar ve SÄ±nÄ±rlamalar

### Performans Tavizleri

EdgeAI genellikle model boyutu, hesaplama verimliliÄŸi ve performans arasÄ±nda tavizler iÃ§erir. Kuantizasyon ve budama gibi teknikler kaynak gereksinimlerini Ã¶nemli Ã¶lÃ§Ã¼de azaltabilirken, model doÄŸruluÄŸunu veya yeteneÄŸini etkileyebilir.

### GeliÅŸtirme KarmaÅŸÄ±klÄ±ÄŸÄ±

EdgeAI uygulamalarÄ± geliÅŸtirmek, Ã¶zel bilgi ve araÃ§lar gerektirir. GeliÅŸtiriciler, optimizasyon tekniklerini, donanÄ±m yeteneklerini ve daÄŸÄ±tÄ±m kÄ±sÄ±tlamalarÄ±nÄ± anlamalÄ±dÄ±r, bu da geliÅŸtirme karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± artÄ±rabilir.

### DonanÄ±m SÄ±nÄ±rlamalarÄ±

UÃ§ donanÄ±mdaki ilerlemelere raÄŸmen, bu cihazlar hala veri merkezi altyapÄ±sÄ±na kÄ±yasla Ã¶nemli sÄ±nÄ±rlamalara sahiptir. TÃ¼m yapay zeka uygulamalarÄ± uÃ§ cihazlarda etkili bir ÅŸekilde daÄŸÄ±tÄ±lamaz ve bazÄ±larÄ± hibrit yaklaÅŸÄ±mlar gerektirebilir.

### Model GÃ¼ncellemeleri ve BakÄ±mÄ±

UÃ§ cihazlarda daÄŸÄ±tÄ±lan yapay zeka modeller
## â¡ï¸ SÄ±rada ne var

- [02: EdgeAI UygulamalarÄ±](02.RealWorldCaseStudies.md)

---

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.