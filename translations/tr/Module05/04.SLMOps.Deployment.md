<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-17T23:56:33+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "tr"
}
-->
# BÃ¶lÃ¼m 4: DaÄŸÄ±tÄ±m - Ãœretime HazÄ±r Model UygulamasÄ±

## Genel BakÄ±ÅŸ

Bu kapsamlÄ± eÄŸitim, Foundry Local kullanarak ince ayar yapÄ±lmÄ±ÅŸ kuantize edilmiÅŸ modellerin daÄŸÄ±tÄ±m sÃ¼recini baÅŸtan sona size rehberlik edecek. Model dÃ¶nÃ¼ÅŸÃ¼mÃ¼, kuantizasyon optimizasyonu ve daÄŸÄ±tÄ±m yapÄ±landÄ±rmasÄ±nÄ± ele alacaÄŸÄ±z.

## Ã–n KoÅŸullar

BaÅŸlamadan Ã¶nce aÅŸaÄŸÄ±daki gereksinimlere sahip olduÄŸunuzdan emin olun:

- âœ… DaÄŸÄ±tÄ±ma hazÄ±r ince ayar yapÄ±lmÄ±ÅŸ bir onnx modeli
- âœ… Windows veya Mac bilgisayar
- âœ… Python 3.10 veya Ã¼stÃ¼
- âœ… En az 8GB boÅŸ RAM
- âœ… Sisteminizde kurulu Foundry Local

## BÃ¶lÃ¼m 1: Ortam Kurulumu

### Gerekli AraÃ§larÄ±n Kurulumu

Terminalinizi aÃ§Ä±n (Windows'ta Komut Ä°stemi, Mac'te Terminal) ve aÅŸaÄŸÄ±daki komutlarÄ± sÄ±rasÄ±yla Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

âš ï¸ **Ã–nemli Not**: AyrÄ±ca [cmake.org](https://cmake.org/download/) adresinden indirilebilecek 3.31 veya daha yeni bir CMake sÃ¼rÃ¼mÃ¼ne ihtiyacÄ±nÄ±z olacak.

## BÃ¶lÃ¼m 2: Model DÃ¶nÃ¼ÅŸÃ¼mÃ¼ ve Kuantizasyon

### DoÄŸru FormatÄ± SeÃ§mek

Ä°nce ayar yapÄ±lmÄ±ÅŸ kÃ¼Ã§Ã¼k dil modelleri iÃ§in **ONNX formatÄ±nÄ±** kullanmanÄ±zÄ± Ã¶neriyoruz Ã§Ã¼nkÃ¼:

- ğŸš€ Daha iyi performans optimizasyonu saÄŸlar
- ğŸ”§ DonanÄ±m baÄŸÄ±msÄ±z daÄŸÄ±tÄ±m sunar
- ğŸ­ Ãœretime hazÄ±r Ã¶zellikler iÃ§erir
- ğŸ“± Platformlar arasÄ± uyumluluk saÄŸlar

### YÃ¶ntem 1: Tek Komutla DÃ¶nÃ¼ÅŸÃ¼m (Ã–nerilen)

Ä°nce ayar yapÄ±lmÄ±ÅŸ modelinizi doÄŸrudan dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in aÅŸaÄŸÄ±daki komutu kullanÄ±n:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Parametre AÃ§Ä±klamasÄ±:**
- `--model_name_or_path`: Ä°nce ayar yapÄ±lmÄ±ÅŸ modelinizin yolu
- `--device cpu`: Optimizasyon iÃ§in CPU kullanÄ±mÄ±
- `--precision int4`: INT4 kuantizasyonu kullanÄ±mÄ± (yaklaÅŸÄ±k %75 boyut azaltma)
- `--output_path`: DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ modelin Ã§Ä±kÄ±ÅŸ yolu

### YÃ¶ntem 2: YapÄ±landÄ±rma DosyasÄ± YaklaÅŸÄ±mÄ± (Ä°leri DÃ¼zey KullanÄ±cÄ±lar)

`finetuned_conversion_config.json` adlÄ± bir yapÄ±landÄ±rma dosyasÄ± oluÅŸturun:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Sonra ÅŸu komutu Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Kuantizasyon SeÃ§enekleri KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Hassasiyet | Dosya Boyutu | Ã‡Ä±karÄ±m HÄ±zÄ± | Model Kalitesi | Ã–nerilen KullanÄ±m |
|------------|--------------|--------------|----------------|-------------------|
| FP16       | Temel Ã— 0.5  | HÄ±zlÄ±        | En Ä°yi         | Ãœst dÃ¼zey donanÄ±m |
| INT8       | Temel Ã— 0.25 | Ã‡ok HÄ±zlÄ±    | Ä°yi            | Dengeli seÃ§im     |
| INT4       | Temel Ã— 0.125| En HÄ±zlÄ±     | Kabul Edilebilir| Kaynak sÄ±nÄ±rlÄ±    |

ğŸ’¡ **Ã–neri**: Ä°lk daÄŸÄ±tÄ±mÄ±nÄ±z iÃ§in INT4 kuantizasyonuyla baÅŸlayÄ±n. Kalite tatmin edici deÄŸilse, INT8 veya FP16 deneyin.

## BÃ¶lÃ¼m 3: Foundry Local DaÄŸÄ±tÄ±m YapÄ±landÄ±rmasÄ±

### Model YapÄ±landÄ±rmasÄ± OluÅŸturma

Foundry Local modeller dizinine gidin:

```bash
foundry cache cd ./models/
```

Model dizin yapÄ±nÄ±zÄ± oluÅŸturun:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Model dizininizde `inference_model.json` yapÄ±landÄ±rma dosyasÄ±nÄ± oluÅŸturun:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Model-Specifik Åablon YapÄ±landÄ±rmalarÄ±

#### Qwen Serisi Modeller Ä°Ã§in:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## BÃ¶lÃ¼m 4: Model Testi ve Optimizasyonu

### Model Kurulumunu DoÄŸrulama

Foundry Local'Ä±n modelinizi tanÄ±yÄ±p tanÄ±madÄ±ÄŸÄ±nÄ± kontrol edin:

```bash
foundry cache ls
```

Listede `your-finetuned-model-int4` gÃ¶rmelisiniz.

### Model Testine BaÅŸlama

```bash
foundry model run your-finetuned-model-int4
```

### Performans Ã–lÃ§Ã¼mleme

Test sÄ±rasÄ±nda Ã¶nemli metrikleri izleyin:

1. **YanÄ±t SÃ¼resi**: Ortalama yanÄ±t sÃ¼resini Ã¶lÃ§Ã¼n
2. **Bellek KullanÄ±mÄ±**: RAM tÃ¼ketimini izleyin
3. **CPU KullanÄ±mÄ±**: Ä°ÅŸlemci yÃ¼kÃ¼nÃ¼ kontrol edin
4. **Ã‡Ä±ktÄ± Kalitesi**: YanÄ±tlarÄ±n alaka dÃ¼zeyini ve tutarlÄ±lÄ±ÄŸÄ±nÄ± deÄŸerlendirin

### Kalite DoÄŸrulama Kontrol Listesi

- âœ… Model, ince ayar yapÄ±lmÄ±ÅŸ alan sorgularÄ±na uygun ÅŸekilde yanÄ±t veriyor
- âœ… YanÄ±t formatÄ± beklenen Ã§Ä±ktÄ± yapÄ±sÄ±na uyuyor
- âœ… Uzun sÃ¼reli kullanÄ±mda bellek sÄ±zÄ±ntÄ±sÄ± yok
- âœ… FarklÄ± giriÅŸ uzunluklarÄ±nda tutarlÄ± performans
- âœ… UÃ§ durumlar ve geÃ§ersiz giriÅŸler dÃ¼zgÃ¼n ÅŸekilde ele alÄ±nÄ±yor

## Ã–zet

Tebrikler! BaÅŸarÄ±yla tamamladÄ±nÄ±z:

- âœ… Ä°nce ayar yapÄ±lmÄ±ÅŸ model format dÃ¶nÃ¼ÅŸÃ¼mÃ¼
- âœ… Model kuantizasyon optimizasyonu
- âœ… Foundry Local daÄŸÄ±tÄ±m yapÄ±landÄ±rmasÄ±
- âœ… Performans ayarÄ± ve sorun giderme

---

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±k iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.