<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7c8a73e11384e3462674273498d0f9a6",
  "translation_date": "2025-10-09T19:18:22+00:00",
  "source_file": "Workshop/Session01-GettingStartedFoundryLocal.md",
  "language_code": "tl"
}
-->
# Session 1: Pagsisimula sa Foundry Local

## Abstrak

Simulan ang iyong paglalakbay sa Foundry Local sa pamamagitan ng pag-install at pag-configure nito sa Windows 11. Matutunan kung paano i-set up ang CLI, paganahin ang hardware acceleration, at i-cache ang mga modelo para sa mabilis na lokal na inference. Ang hands-on na sesyon na ito ay magpapakita kung paano patakbuhin ang mga modelo tulad ng Phi, Qwen, DeepSeek, at GPT-OSS-20B gamit ang mga reproducible na CLI command.

## Mga Layunin sa Pag-aaral

Sa pagtatapos ng sesyon na ito, magagawa mo ang sumusunod:

- **Mag-install at Mag-configure**: I-set up ang Foundry Local sa Windows 11 gamit ang optimal na performance settings
- **Master CLI Operations**: Gamitin ang Foundry Local CLI para sa pamamahala at deployment ng modelo
- **Paganahin ang Hardware Acceleration**: I-configure ang GPU acceleration gamit ang ONNXRuntime o WebGPU
- **Mag-deploy ng Maraming Modelo**: Patakbuhin ang phi-4, GPT-OSS-20B, Qwen, at DeepSeek na mga modelo nang lokal
- **Gumawa ng Unang App**: I-adapt ang mga umiiral na sample upang magamit ang Foundry Local Python SDK

# Subukan ang modelo (non-interactive single prompt)
foundry model run phi-4-mini --prompt "Hello, introduce yourself"

- Windows 11 (22H2 o mas bago)
# Ilista ang mga available na modelo sa catalog (ang mga loaded na modelo ay lilitaw kapag pinatakbo)
foundry model list
## NOTE: Sa kasalukuyan, walang dedikadong `--running` flag; upang makita kung alin ang loaded, mag-initiate ng chat o suriin ang service logs.
- Nakainstall ang Python 3.10+
- Visual Studio Code na may Python extension
- Administrator privileges para sa pag-install

### (Opsyonal) Mga Environment Variable

Gumawa ng `.env` (o i-set sa shell) upang gawing portable ang mga script:
# Ihambing ang mga sagot (non-interactive)
foundry model run gpt-oss-20b --prompt "Explain edge AI in simple terms"
| Variable | Layunin | Halimbawa |
|----------|---------|---------|
| `FOUNDRY_LOCAL_ALIAS` | Preferred na alias ng modelo (ang catalog ay auto-select ng pinakamahusay na variant) | `phi-3.5-mini` |
| `FOUNDRY_LOCAL_ENDPOINT` | Override endpoint (kung hindi, auto mula sa manager) | `http://localhost:5273/v1` |
| `FOUNDRY_LOCAL_STREAM` | Paganahin ang streaming demo | `true` |

> Kung `FOUNDRY_LOCAL_ENDPOINT=auto` (o unset) ito ay derived mula sa SDK manager.

## Demo Flow (30 minuto)

### 1. I-install ang Foundry Local at I-verify ang CLI Setup (10 minuto)

# Ilista ang mga cached na modelo
foundry cache list

```powershell
# Install via winget (recommended)
winget install Microsoft.FoundryLocal

# Or download from Microsoft Learn
# https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install
```

**macOS (Preview / Kung Suportado)**

Kung may native na macOS package na ibinigay (suriin ang opisyal na dokumentasyon para sa pinakabago):

```bash
# Homebrew (if/when available)
brew update
brew install foundry-local  # hypothetical formula name

# Or manual download (tarball)
curl -L -o foundry-local.tar.gz "https://download.microsoft.com/foundry-local/latest/macos/foundry-local.tar.gz"
tar -xzf foundry-local.tar.gz
sudo ./install.sh
```

Kung ang macOS native binaries ay hindi pa available, maaari mo pa ring:
1. Gumamit ng Windows 11 ARM/Intel VM (Parallels / UTM) at sundin ang mga hakbang sa Windows.
2. Patakbuhin ang mga modelo gamit ang container (kung may published na container image) at i-set ang `FOUNDRY_LOCAL_ENDPOINT` sa exposed na port.

**Gumawa ng Python Virtual Environment (Cross‑Platform)**

Windows PowerShell:
```powershell
py -m venv .venv
 .\.venv\Scripts\Activate.ps1
```

macOS / Linux:
```bash
python3 -m venv .venv
source .venv/bin/activate
```

I-upgrade ang pip at i-install ang mga pangunahing dependencies:
```bash
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

#### Hakbang 1.2: I-verify ang Pag-install

```powershell
# Check version
foundry --version

# Initialize configuration
foundry init

# View available commands
foundry --help
```

#### Hakbang 1.3: I-configure ang Environment

```powershell
# Set up Python environment for Module08
cd Module08
py -m venv .venv
.\.venv\Scripts\activate

# Install Foundry Local Python SDK and dependencies
pip install foundry-local-sdk openai requests
```

### SDK Bootstrapping (Inirerekomenda)

Sa halip na manu-manong simulan ang serbisyo at patakbuhin ang mga modelo, ang **Foundry Local Python SDK** ay maaaring mag-bootstrap ng lahat:

```python
from foundry_local import FoundryLocalManager
from openai import OpenAI
import os

alias = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-3.5-mini")

# Bootstraps service + downloads + loads most suitable variant for hardware
manager = FoundryLocalManager(alias)

print("Service running:", manager.is_service_running())
print("Endpoint:", manager.endpoint)
print("Cached models:", manager.list_cached_models())

client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key or "not-needed")

resp = client.chat.completions.create(
    model=manager.get_model_info(alias).id,
    messages=[
        {"role": "system", "content": "You are a helpful local assistant."},
        {"role": "user", "content": "Hello"}
    ],
    max_tokens=120,
    temperature=0.5
)
print(resp.choices[0].message.content)
```

Kung mas gusto mo ang explicit na kontrol, maaari mo pa ring gamitin ang CLI + OpenAI client tulad ng ipinakita sa ibang bahagi.

### 2. Paganahin ang GPU Acceleration (5 minuto)

#### Hakbang 2.1: Suriin ang Hardware Capabilities

```powershell
# Check available compute providers
foundry system info

# List GPU capabilities
foundry system gpu-info
```

#### Hakbang 2.2: I-configure ang Hardware Acceleration

```powershell
# Enable ONNX Runtime GPU (if NVIDIA GPU available)
foundry config set compute.onnx.enable_gpu true

# Enable WebGPU for broader hardware support
foundry config set compute.webgpu.enabled true

# Verify configuration
foundry config list
```

### 3. Patakbuhin ang Mga Modelo Lokal gamit ang CLI (10 minuto)

#### Hakbang 3.1: I-deploy ang Phi-4 Model

```powershell
# Download and run phi-4-mini
foundry model run phi-4-mini

# Test the model (one-shot prompt)
foundry model run phi-4-mini --prompt "Hello, introduce yourself"

# NOTE: There is no `--running` flag; use `foundry model list` and recent activity to infer loaded models.
```

#### Hakbang 3.2: I-deploy ang GPT-OSS-20B

```powershell
# Download and run GPT-OSS-20B
foundry model run gpt-oss-20b

# Compare responses (one-shot prompt)
foundry model run gpt-oss-20b --prompt "Explain edge AI in simple terms"
```

#### Hakbang 3.3: I-load ang Karagdagang Mga Modelo

```powershell
# Download Qwen model family
foundry model download qwen2.5-0.5b
foundry model download qwen2.5-7b

# Download DeepSeek models
foundry model download deepseek-coder-1.3b

# List cached models
foundry cache list
```

### 4. Starter Project: I-adapt ang 01-run-phi para sa Foundry Local (5 minuto)

#### Hakbang 4.1: Gumawa ng Basic Chat Application

Gumawa ng `samples/01-foundry-quickstart/chat_quickstart.py` (na-update upang gamitin ang manager kung available):

```python
#!/usr/bin/env python3
"""
Foundry Local Chat Quickstart
Demo: Basic chat interaction using Foundry Local Python SDK
Reference: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-sdk?pivots=programming-language-python
"""

import os, sys
from openai import OpenAI
try:
    from foundry_local import FoundryLocalManager  # control-plane SDK
except ImportError:
    FoundryLocalManager = None

def main():
    """Main chat function using Foundry Local SDK"""
    
    # Preferred: bootstrap via SDK manager (auto start + download + load)
    alias = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-3.5-mini")
    if FoundryLocalManager:
        manager = FoundryLocalManager(alias)
        endpoint = manager.endpoint
        model_id = manager.get_model_info(alias).id
        api_key = manager.api_key or "not-needed"
    else:
        # Fallback: assume default endpoint & alias already running via CLI
        endpoint = os.getenv("FOUNDRY_LOCAL_ENDPOINT", "http://localhost:5273/v1")
        model_id = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-4-mini")
        api_key = "not-needed"

    client = OpenAI(base_url=endpoint, api_key=api_key)
    
    # Get user input
    if len(sys.argv) > 1:
        user_message = " ".join(sys.argv[1:])
    else:
        user_message = input("Enter your message: ")
    
    try:
        # Make chat completion request
        response = client.chat.completions.create(
            model=model_id,
            messages=[
                {"role": "system", "content": "You are a helpful AI assistant powered by Microsoft Foundry Local."},
                {"role": "user", "content": user_message}
            ],
            max_tokens=500,
            temperature=0.7
        )
        
        # Display response
        print(f"\nModel: {response.model}")
        print(f"Response: {response.choices[0].message.content}")
        print(f"Tokens used: {response.usage.total_tokens if response.usage else 'N/A'}")
        
    except Exception as e:
        print(f"Error: {e}")
        print("\nTroubleshooting:")
    print("1. Ensure Foundry Local is running: foundry status")
    print("2. List models: foundry model list")
    print(f"3. Start model if needed: foundry model run {model_id}")
    print("4. Or let SDK bootstrap: pip install foundry-local-sdk")

if __name__ == "__main__":
    main()
```

#### Hakbang 4.2: Subukan ang Application

```powershell
# Ensure phi-4-mini is running
foundry model run phi-4-mini

# Run the quickstart app
python samples/01-foundry-quickstart/chat_quickstart.py "What is Microsoft Foundry Local?"

# Try interactive mode
python samples/01-foundry-quickstart/chat_quickstart.py
```

## Mga Pangunahing Konseptong Tinalakay

### 1. Foundry Local Architecture

- **Local Inference Engine**: Pinapatakbo ang mga modelo nang buo sa iyong device
- **OpenAI SDK Compatibility**: Seamless na integrasyon sa umiiral na OpenAI code
- **Model Management**: Mag-download, mag-cache, at magpatakbo ng maraming modelo nang epektibo
- **Hardware Optimization**: Gamitin ang GPU, NPU, at CPU acceleration

### 2. CLI Command Reference

```powershell
# Core Commands
foundry --version              # Check installation
# Model Management
foundry model list             # List available models
foundry model unload <name>    # Unload from memory

foundry config list            # Current configuration
```

### 3. Python SDK Integration

```python
# Basic client setup
from foundry_local import FoundryLocalManager
from openai import OpenAI
import os

alias = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-3.5-mini")
manager = FoundryLocalManager(alias)
client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key or "not-needed")

response = client.chat.completions.create(
    model=manager.get_model_info(alias).id,
    messages=[{"role": "user", "content": "Hello!"}],
    max_tokens=50
)
print(response.choices[0].message.content)

# Streaming example
stream = client.chat.completions.create(
    model=manager.get_model_info(alias).id,
    messages=[{"role": "user", "content": "Give a 1-sentence definition of edge AI."}],
    stream=True,
    max_tokens=60,
    temperature=0.4
)
for chunk in stream:
    delta = chunk.choices[0].delta
    if delta and delta.content:
        print(delta.content, end="", flush=True)
print()
```

## Pag-troubleshoot ng Karaniwang Mga Isyu

### Isyu 1: "Foundry command not found"

**Solusyon:**
```powershell
# Restart PowerShell after installation
# Or manually add to PATH
$env:PATH += ";C:\Program Files\Microsoft\FoundryLocal"
```

### Isyu 2: "Model failed to load"

**Solusyon:**
```powershell
# Check available memory
foundry system info

# Try smaller model first
foundry model run phi-4-mini

# Check disk space for model cache
dir "$env:USERPROFILE\.foundry\models"
```

### Isyu 3: "Connection refused on localhost:5273"

**Solusyon:**
```powershell
# Check if service is running
foundry status

# Start service if needed
foundry service start

# Check for port conflicts
netstat -an | findstr 5273
```

## Mga Tip sa Performance Optimization

### 1. Estratehiya sa Pagpili ng Modelo

- **Phi-4-mini**: Pinakamahusay para sa pangkalahatang gawain, mababang memory usage
- **Qwen2.5-0.5b**: Pinakamabilis na inference, minimal na resources
- **GPT-OSS-20B**: Pinakamataas na kalidad, nangangailangan ng mas maraming resources
- **DeepSeek-Coder**: Na-optimize para sa mga programming task

### 2. Hardware Optimization

```powershell
# Enable all acceleration options
foundry config set compute.onnx.enable_gpu true
foundry config set compute.webgpu.enabled true
foundry config set compute.cpu.threads auto

# Optimize memory usage
foundry config set model.cache.max_size 10GB
foundry config set model.preload false
```

### 3. Pagsubaybay sa Performance

```powershell
# Performance & latency measurement
# Use the Python benchmark script (Session 3) instead of legacy 'model stats' or 'model benchmark' commands.
# Example:
set BENCH_MODELS=phi-4-mini,qwen2.5-0.5b
python Workshop\samples\session03\benchmark_oss_models.py

# Re-run after enabling GPU acceleration to compare:
foundry config set compute.onnx.enable_gpu true
python Workshop\samples\session03\benchmark_oss_models.py
```

### Opsyonal na Mga Pagpapahusay

| Pagpapahusay | Ano | Paano |
|-------------|------|-----|
| Shared Utilities | Tanggalin ang duplicate na client/bootstrap logic | Gamitin ang `Workshop/samples/workshop_utils.py` (`get_client`, `chat_once`) |
| Token Usage Visibility | Turuan ang cost/efficiency thinking nang maaga | I-set ang `SHOW_USAGE=1` upang i-print ang prompt/completion/total tokens |
| Deterministic Comparisons | Stable na benchmarking at regression checks | Gamitin ang `temperature=0`, `top_p=1`, consistent na prompt text |
| First-Token Latency | Sukatin ang perceived responsiveness | I-adapt ang benchmark script gamit ang streaming (`BENCH_STREAM=1`) |
| Retry on Transient Errors | Resilient na demos sa cold start | `RETRY_ON_FAIL=1` (default) at i-adjust ang `RETRY_BACKOFF` |
| Smoke Testing | Mabilis na sanity check sa mga key flow | Patakbuhin ang `python Workshop/tests/smoke.py` bago ang workshop |
| Model Alias Profiles | Mabilis na mag-pivot ng model set sa pagitan ng mga machine | Panatilihin ang `.env` na may `FOUNDRY_LOCAL_ALIAS`, `SLM_ALIAS`, `LLM_ALIAS` |
| Caching Efficiency | Iwasan ang paulit-ulit na warmups sa multi-sample run | Utilities cache managers; i-reuse sa mga script/notebooks |
| First Run Warmup | Bawasan ang p95 latency spikes | Magpatakbo ng maliit na prompt pagkatapos ng `FoundryLocalManager` creation |

Halimbawa ng deterministic warm baseline (PowerShell):

```powershell
set FOUNDRY_LOCAL_ALIAS=phi-4-mini
set SHOW_USAGE=1
python Workshop\samples\session01\chat_bootstrap.py "List two privacy benefits of local inference." | Out-Null
python Workshop\samples\session01\chat_bootstrap.py "List two privacy benefits of local inference."
```

Dapat mong makita ang katulad na output at identical na token counts sa pangalawang run, na nagpapatunay ng determinism.

## Mga Susunod na Hakbang

Pagkatapos makumpleto ang sesyon na ito:

1. **I-explore ang Session 2**: Gumawa ng AI solutions gamit ang Azure AI Foundry RAG
2. **Subukan ang Iba't Ibang Modelo**: Mag-eksperimento sa Qwen, DeepSeek, at iba pang pamilya ng modelo
3. **I-optimize ang Performance**: Fine-tune ang mga setting para sa iyong partikular na hardware
4. **Gumawa ng Custom Applications**: Gamitin ang Foundry Local SDK sa iyong sariling mga proyekto

## Karagdagang Mga Mapagkukunan

### Dokumentasyon
- [Foundry Local Python SDK Reference](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-sdk?pivots=programming-language-python)
- [Foundry Local Installation Guide](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)
- [Model Catalog](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/models)

### Sample Code
- [Module08 Sample 01](./samples/01/README.md) - REST Chat Quickstart
- [Module08 Sample 02](./samples/02/README.md) - OpenAI SDK Integration
- [Module08 Sample 03](./samples/03/README.md) - Model Discovery & Benchmarking

### Komunidad
- [Foundry Local GitHub Discussions](https://github.com/microsoft/Foundry-Local/discussions)
- [Azure AI Community](https://techcommunity.microsoft.com/category/artificialintelligence)

---

**Tagal ng Sisyon**: 30 minuto hands-on + 15 minuto Q&A  
**Antas ng Kahirapan**: Baguhan  
**Mga Kinakailangan**: Windows 11, Python 3.10+, Administrator access  

## Sample Scenario at Workshop Mapping

| Workshop Script / Notebook | Scenario | Layunin | Halimbawa ng Input(s) | Dataset na Kailangan |
|----------------------------|----------|------|------------------|----------------|
| `samples/session01/chat_bootstrap.py` / `notebooks/session01_chat_bootstrap.ipynb` | Internal IT team na nag-evaluate ng on‑device inference para sa privacy assessment portal | Patunayan na ang lokal na SLM ay tumutugon sa sub‑second latency sa standard prompts | "List two benefits of local inference." | Wala (single prompt) |
| Quickstart adaptation code block | Developer na nagmamigrate ng umiiral na OpenAI script sa Foundry Local | Ipakita ang drop‑in compatibility | "Give two benefits of local inference." | Inline prompt lamang |

### Scenario Narrative
Ang security & compliance squad ay kailangang mag-validate kung ang sensitibong prototype data ay maaaring ma-proseso nang lokal. Pinapatakbo nila ang bootstrap script gamit ang ilang prompt (privacy, latency, cost) gamit ang deterministic na temperature=0 mode upang makuha ang baseline outputs para sa susunod na comparison (Session 3 benchmarking at Session 4 SLM vs LLM contrast).

### Minimal Prompt Set JSON (opsyonal)
```json
[
    "List two benefits of local inference.",
    "Summarize why keeping data on device improves privacy.",
    "Give one trade‑off when choosing an SLM over a large model."
]
```

Gamitin ang listahan na ito upang gumawa ng reproducible evaluation loop o upang mag-seed ng future regression test harness.

---

**Paunawa**:  
Ang dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, pakitandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa orihinal nitong wika ang dapat ituring na opisyal na sanggunian. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na dulot ng paggamit ng pagsasaling ito.