<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-01T20:41:59+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "bn"
}
-->
# ржЕржзрзНржпрж╛ржпрж╝ рзи: Qwen ржкрж░рж┐ржмрж╛рж░ рж╕ржорзНржкрж░рзНржХрзЗ ржорзМрж▓рж┐ржХ ржзрж╛рж░ржгрж╛

Qwen ржоржбрзЗрж▓ ржкрж░рж┐ржмрж╛рж░ Alibaba Cloud-ржПрж░ ржмрзГрж╣рзО ржнрж╛рж╖рж╛ ржоржбрзЗрж▓ ржПржмржВ ржорж╛рж▓рзНржЯрж┐ржорзЛржбрж╛рж▓ AI-ржПрж░ ржкрзНрж░рждрж┐ рж╕ржоржирзНржмрж┐ржд ржжрзГрж╖рзНржЯрж┐ржнржЩрзНржЧрж┐ ржЙржкрж╕рзНржерж╛ржкржи ржХрж░рзЗред ржПржЯрж┐ ржкрзНрж░ржорж╛ржг ржХрж░рзЗ ржпрзЗ ржУржкрзЗржи-рж╕рзЛрж░рзНрж╕ ржоржбрзЗрж▓ржЧрзБрж▓рж┐ ржЕрж╕рж╛ржзрж╛рж░ржг ржХрж░рзНржоржХрзНрж╖ржорждрж╛ ржЕрж░рзНржЬржи ржХрж░рждрзЗ ржкрж╛рж░рзЗ ржПржмржВ ржмрж┐ржнрж┐ржирзНржи рж╕рзНржерж╛ржкржирж╛рж░ ржкрж░рж┐рж╕рзНржерж┐рждрж┐рждрзЗ рж╕рж╣ржЬрж▓ржнрзНржп рж╣рждрзЗ ржкрж╛рж░рзЗред Qwen ржкрж░рж┐ржмрж╛рж░ ржХрзАржнрж╛ржмрзЗ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА AI рж╕ржХрзНрж╖ржорждрж╛ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗ ржПржмржВ ржиржоржирзАржпрж╝ рж╕рзНржерж╛ржкржирж╛рж░ ржмрж┐ржХрж▓рзНржкржЧрзБрж▓рж┐ рж╕ржХрзНрж╖ржо ржХрж░рзЗ рждрж╛ ржмрзЛржЭрж╛ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг, ржПржХржЗрж╕рж╛ржерзЗ ржмрж┐ржнрж┐ржирзНржи ржХрж╛ржЬрзЗрж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ ржкрзНрж░рждрж┐ржпрзЛржЧрж┐рждрж╛ржорзВрж▓ржХ ржХрж░рзНржоржХрзНрж╖ржорждрж╛ ржмржЬрж╛ржпрж╝ рж░рж╛ржЦрзЗред

## ржбрзЗржнрзЗрж▓ржкрж╛рж░ржжрзЗрж░ ржЬржирзНржп рж╕ржорзНржкржж

### Hugging Face ржоржбрзЗрж▓ рж░рж┐ржкрзЛржЬрж┐ржЯрж░рж┐
ржирж┐рж░рзНржмрж╛ржЪрж┐ржд Qwen ржкрж░рж┐ржмрж╛рж░рзЗрж░ ржоржбрзЗрж▓ржЧрзБрж▓рж┐ [Hugging Face](https://huggingface.co/models?search=qwen)-ржПрж░ ржорж╛ржзрзНржпржорзЗ ржЙржкрж▓ржмрзНржз, ржпрж╛ ржПржЗ ржоржбрзЗрж▓ржЧрзБрж▓рж┐рж░ ржХрж┐ржЫрзБ ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯрзЗ ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗред ржЖржкржирж┐ ржЙржкрж▓ржмрзНржз ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯржЧрзБрж▓рж┐ ржЕржирзНржмрзЗрж╖ржг ржХрж░рждрзЗ ржкрж╛рж░рзЗржи, ржЖржкржирж╛рж░ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржмрзНржпржмрж╣рж╛рж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ ржПржЧрзБрж▓рж┐ ржлрж╛ржЗржи-ржЯрж┐ржЙржи ржХрж░рждрзЗ ржкрж╛рж░рзЗржи ржПржмржВ ржмрж┐ржнрж┐ржирзНржи ржлрзНрж░рзЗржоржУржпрж╝рж╛рж░рзНржХрзЗрж░ ржорж╛ржзрзНржпржорзЗ рж╕рзНржерж╛ржкржи ржХрж░рждрзЗ ржкрж╛рж░рзЗржиред

### рж╕рзНржерж╛ржирзАржпрж╝ ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ ржЯрзБрж▓рж╕
рж╕рзНржерж╛ржирзАржпрж╝ ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ ржПржмржВ ржкрж░рзАржХрзНрж╖рж╛рж░ ржЬржирзНржп, ржЖржкржирж┐ [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) ржмрзНржпржмрж╣рж╛рж░ ржХрж░рждрзЗ ржкрж╛рж░рзЗржи, ржпрж╛ ржЖржкржирж╛рж░ ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ ржорзЗрж╢рж┐ржирзЗ ржЙржкрж▓ржмрзНржз Qwen ржоржбрзЗрж▓ржЧрзБрж▓рж┐ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬржб ржХрж░рзНржоржХрзНрж╖ржорждрж╛рж░ рж╕рж╛ржерзЗ ржЪрж╛рж▓рж╛ржирзЛрж░ рж╕рзБржпрзЛржЧ ржжрзЗржпрж╝ред

### ржбржХрзБржорзЗржирзНржЯрзЗрж╢ржи рж╕ржорзНржкржж
- [Qwen ржоржбрзЗрж▓ ржбржХрзБржорзЗржирзНржЯрзЗрж╢ржи](https://huggingface.co/docs/transformers/model_doc/qwen)
- [ржПржЬ рж╕рзНржерж╛ржкржирж╛рж░ ржЬржирзНржп Qwen ржоржбрзЗрж▓ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи](https://github.com/microsoft/olive)

## ржкрж░рж┐ржЪрж┐рждрж┐

ржПржЗ ржЯрж┐ржЙржЯрзЛрж░рж┐ржпрж╝рж╛рж▓рзЗ ржЖржорж░рж╛ Alibaba-ржПрж░ Qwen ржоржбрзЗрж▓ ржкрж░рж┐ржмрж╛рж░ ржПржмржВ ржПрж░ ржорзМрж▓рж┐ржХ ржзрж╛рж░ржгрж╛ржЧрзБрж▓рж┐ ржЕржирзНржмрзЗрж╖ржг ржХрж░ржмред ржЖржорж░рж╛ Qwen ржкрж░рж┐ржмрж╛рж░рзЗрж░ ржмрж┐ржмрж░рзНрждржи, ржЙржжрзНржнрж╛ржмржирзА ржкрзНрж░рж╢рж┐ржХрзНрж╖ржг ржкржжрзНржзрждрж┐ ржпрж╛ Qwen ржоржбрзЗрж▓ржЧрзБрж▓рж┐ржХрзЗ ржХрж╛рж░рзНржпржХрж░ ржХрж░рзЗ рждрзЛрж▓рзЗ, ржкрж░рж┐ржмрж╛рж░рзЗрж░ ржорзВрж▓ ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯ ржПржмржВ ржмрж┐ржнрж┐ржирзНржи ржкрж░рж┐рж╕рзНржерж┐рждрж┐рждрзЗ ржмрзНржпржмрж╣рж╛рж░рж┐ржХ ржкрзНрж░ржпрж╝рзЛржЧржЧрзБрж▓рж┐ ржирж┐ржпрж╝рзЗ ржЖрж▓рзЛржЪржирж╛ ржХрж░ржмред

## рж╢рзЗржЦрж╛рж░ рж▓ржХрзНрж╖рзНржп

ржПржЗ ржЯрж┐ржЙржЯрзЛрж░рж┐ржпрж╝рж╛рж▓ рж╢рзЗрж╖рзЗ ржЖржкржирж┐ рж╕ржХрзНрж╖ржо рж╣ржмрзЗржи:

- Alibaba-ржПрж░ Qwen ржоржбрзЗрж▓ ржкрж░рж┐ржмрж╛рж░рзЗрж░ ржиржХрж╢рж╛ ржжрж░рзНрж╢ржи ржПржмржВ ржмрж┐ржмрж░рзНрждржи ржмрзБржЭрждрзЗ
- ржорзВрж▓ ржЙржжрзНржнрж╛ржмржиржЧрзБрж▓рж┐ ржЪрж┐рж╣рзНржирж┐ржд ржХрж░рждрзЗ ржпрж╛ Qwen ржоржбрзЗрж▓ржЧрзБрж▓рж┐ржХрзЗ ржмрж┐ржнрж┐ржирзНржи ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░ рж╕рж╛ржЗржЬрзЗ ржЙржЪрзНржЪ ржХрж░рзНржоржХрзНрж╖ржорждрж╛ ржЕрж░рзНржЬржирзЗ рж╕ржХрзНрж╖ржо ржХрж░рзЗ
- ржмрж┐ржнрж┐ржирзНржи Qwen ржоржбрзЗрж▓ ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯрзЗрж░ рж╕рзБржмрж┐ржзрж╛ ржПржмржВ рж╕рзАржорж╛ржмржжрзНржзрждрж╛ ржЪрж┐ржирждрзЗ
- ржмрж╛рж╕рзНрждржм ржЬрзАржмржирзЗрж░ ржкрж░рж┐рж╕рзНржерж┐рждрж┐рждрзЗ ржЙржкржпрзБржХрзНржд ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯ ржирж┐рж░рзНржмрж╛ржЪржи ржХрж░рждрзЗ Qwen ржоржбрзЗрж▓ рж╕ржорзНржкрж░рзНржХрзЗ ржЬрзНржЮрж╛ржи ржкрзНрж░ржпрж╝рзЛржЧ ржХрж░рждрзЗ

## ржЖржзрзБржирж┐ржХ AI ржоржбрзЗрж▓ рж▓рзНржпрж╛ржирзНржбрж╕рзНржХрзЗржк ржмрзЛржЭрж╛

AI рж▓рзНржпрж╛ржирзНржбрж╕рзНржХрзЗржк ржЙрж▓рзНрж▓рзЗржЦржпрзЛржЧрзНржпржнрж╛ржмрзЗ ржмрж┐ржХрж╢рж┐ржд рж╣ржпрж╝рзЗржЫрзЗ, ржпрзЗржЦрж╛ржирзЗ ржмрж┐ржнрж┐ржирзНржи рж╕ржВрж╕рзНржерж╛ ржнрж╛рж╖рж╛ ржоржбрзЗрж▓ ржЙржирзНржиржпрж╝ржирзЗрж░ ржЬржирзНржп ржмрж┐ржнрж┐ржирзНржи ржкржжрзНржзрждрж┐ ржЕржирзБрж╕рж░ржг ржХрж░ржЫрзЗред ржХрж┐ржЫрзБ рж╕ржВрж╕рзНржерж╛ ржорж╛рж▓рж┐ржХрж╛ржирж╛ржзрзАржи ржХрзНрж▓рзЛржЬржб-рж╕рзЛрж░рзНрж╕ ржоржбрзЗрж▓рзЗрж░ ржЙржкрж░ ржЬрзЛрж░ ржжрзЗржпрж╝, ржЕржирзНржпрж░рж╛ ржУржкрзЗржи-рж╕рзЛрж░рзНрж╕ ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕ржпрзЛржЧрзНржпрждрж╛ ржПржмржВ рж╕рзНржмржЪрзНржЫрждрж╛рж░ ржЙржкрж░ ржЧрзБрж░рзБрждрзНржм ржжрзЗржпрж╝ред ржкрзНрж░ржЪрж▓рж┐ржд ржкржжрзНржзрждрж┐рждрзЗ рж╣ржпрж╝ ржмрж┐рж╢рж╛рж▓ ржорж╛рж▓рж┐ржХрж╛ржирж╛ржзрзАржи ржоржбрзЗрж▓ржЧрзБрж▓рж┐ рж╢рзБржзрзБржорж╛рждрзНрж░ API-ржПрж░ ржорж╛ржзрзНржпржорзЗ ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕ржпрзЛржЧрзНржп ржерж╛ржХрзЗ ржЕржержмрж╛ ржУржкрзЗржи-рж╕рзЛрж░рзНрж╕ ржоржбрзЗрж▓ржЧрзБрж▓рж┐ ржпрж╛ рж╕ржХрзНрж╖ржорждрж╛рж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ ржкрж┐ржЫрж┐ржпрж╝рзЗ ржерж╛ржХрждрзЗ ржкрж╛рж░рзЗред

ржПржЗ ржкрзНржпрж╛рж░рж╛ржбрж╛ржЗржоржЯрж┐ рж╕ржВрж╕рзНржерж╛ржЧрзБрж▓рж┐рж░ ржЬржирзНржп ржЪрзНржпрж╛рж▓рзЗржЮрзНржЬ рждрзИрж░рж┐ ржХрж░рзЗ ржпрж╛рж░рж╛ рждрж╛ржжрзЗрж░ ржбрзЗржЯрж╛, ржЦрж░ржЪ ржПржмржВ рж╕рзНржерж╛ржкржирж╛рж░ ржиржоржирзАржпрж╝рждрж╛ ржмржЬрж╛ржпрж╝ рж░рзЗржЦрзЗ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА AI рж╕ржХрзНрж╖ржорждрж╛ ржЪрж╛ржпрж╝ред ржкрзНрж░ржЪрж▓рж┐ржд ржкржжрзНржзрждрж┐ ржкрзНрж░рж╛ржпрж╝ржЗ ржХрж╛ржЯрж┐ржВ-ржПржЬ ржХрж░рзНржоржХрзНрж╖ржорждрж╛ ржПржмржВ ржмрзНржпржмрж╣рж╛рж░рж┐ржХ рж╕рзНржерж╛ржкржирж╛рж░ ржмрж┐ржмрзЗржЪржирж╛рж░ ржоржзрзНржпрзЗ ржПржХржЯрж┐ ржмрзЗржЫрзЗ ржирзЗржУржпрж╝рж╛рж░ ржкрзНрж░ржпрж╝рзЛржЬржи рждрзИрж░рж┐ ржХрж░рзЗред

## рж╕рж╣ржЬрж▓ржнрзНржп AI ржЙрзОржХрж░рзНрж╖рждрж╛рж░ ржЪрзНржпрж╛рж▓рзЗржЮрзНржЬ

ржЙржЪрзНржЪ-ржорж╛ржирзЗрж░, рж╕рж╣ржЬрж▓ржнрзНржп AI-ржПрж░ ржкрзНрж░ржпрж╝рзЛржЬржи ржмрж┐ржнрж┐ржирзНржи ржкрж░рж┐рж╕рзНржерж┐рждрж┐рждрзЗ ржХрзНрж░ржоржмрж░рзНржзржорж╛ржи ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг рж╣ржпрж╝рзЗ ржЙржарзЗржЫрзЗред ржПржоржи ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржиржЧрзБрж▓рж┐ ржмрж┐ржмрзЗржЪржирж╛ ржХрж░рзБржи ржпрзЗржЦрж╛ржирзЗ ржмрж┐ржнрж┐ржирзНржи рж╕ржВрж╕рзНржерж╛рж░ ржкрзНрж░ржпрж╝рзЛржЬржирзЗрж░ ржЬржирзНржп ржиржоржирзАржпрж╝ рж╕рзНржерж╛ржкржирж╛рж░ ржмрж┐ржХрж▓рзНржк ржкрзНрж░ржпрж╝рзЛржЬржи, ржпрзЗржЦрж╛ржирзЗ API ржЦрж░ржЪ ржЙрж▓рзНрж▓рзЗржЦржпрзЛржЧрзНржп рж╣ржпрж╝рзЗ ржЙржарждрзЗ ржкрж╛рж░рзЗ, ржмрзИрж╢рзНржмрж┐ржХ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржирзЗрж░ ржЬржирзНржп ржмрж╣рзБржнрж╛рж╖рж┐ржХ рж╕ржХрзНрж╖ржорждрж╛, ржЕржержмрж╛ ржХрзЛржбрж┐ржВ ржПржмржВ ржЧржгрж┐рждрзЗрж░ ржорждрзЛ ржмрж┐рж╢рзЗрж╖рж╛ржпрж╝рж┐ржд ржбрзЛржорзЗржЗржи ржжржХрзНрж╖рждрж╛ред

### ржорзВрж▓ рж╕рзНржерж╛ржкржирж╛рж░ ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝рждрж╛

ржЖржзрзБржирж┐ржХ AI рж╕рзНржерж╛ржкржирж╛ржЧрзБрж▓рж┐ ржХржпрж╝рзЗржХржЯрж┐ ржорзМрж▓рж┐ржХ ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝рждрж╛рж░ ржорзБржЦрзЛржорзБржЦрж┐ рж╣ржпрж╝ ржпрж╛ ржмрзНржпржмрж╣рж╛рж░рж┐ржХ ржкрзНрж░ржпрж╝рзЛржЧ рж╕рзАржорж┐ржд ржХрж░рзЗ:

- **ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕ржпрзЛржЧрзНржпрждрж╛**: рж╕рзНржмржЪрзНржЫрждрж╛ ржПржмржВ ржХрж╛рж╕рзНржЯржорж╛ржЗржЬрзЗрж╢ржирзЗрж░ ржЬржирзНржп ржУржкрзЗржи-рж╕рзЛрж░рзНрж╕ ржЙржкрж▓ржмрзНржзрждрж╛
- **ржЦрж░ржЪ ржХрж╛рж░рзНржпржХрж╛рж░рж┐рждрж╛**: ржмрж┐ржнрж┐ржирзНржи ржмрж╛ржЬрзЗржЯрзЗрж░ ржЬржирзНржп ржпрзБржХрзНрждрж┐рж╕ржЩрзНржЧржд ржХржорзНржкрж┐ржЙржЯрзЗрж╢ржирж╛рж▓ ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝рждрж╛
- **ржиржоржирзАржпрж╝рждрж╛**: ржмрж┐ржнрж┐ржирзНржи рж╕рзНржерж╛ржкржирж╛рж░ ржкрж░рж┐рж╕рзНржерж┐рждрж┐рж░ ржЬржирзНржп ржПржХрж╛ржзрж┐ржХ ржоржбрзЗрж▓ рж╕рж╛ржЗржЬ
- **ржмрзИрж╢рзНржмрж┐ржХ ржкрзМржБржЫрж╛ржирзЛ**: рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА ржмрж╣рзБржнрж╛рж╖рж┐ржХ ржПржмржВ ржЖржирзНрждржГрж╕рж╛ржВрж╕рзНржХрзГрждрж┐ржХ рж╕ржХрзНрж╖ржорждрж╛
- **ржмрж┐рж╢рзЗрж╖рж╛ржпрж╝ржи**: ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржмрзНржпржмрж╣рж╛рж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ ржбрзЛржорзЗржЗржи-ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯ

## Qwen ржоржбрзЗрж▓ ржжрж░рзНрж╢ржи

Qwen ржоржбрзЗрж▓ ржкрж░рж┐ржмрж╛рж░ AI ржоржбрзЗрж▓ ржЙржирзНржиржпрж╝ржирзЗрж░ ржкрзНрж░рждрж┐ ржПржХржЯрж┐ рж╕ржоржирзНржмрж┐ржд ржжрзГрж╖рзНржЯрж┐ржнржЩрзНржЧрж┐ ржЙржкрж╕рзНржерж╛ржкржи ржХрж░рзЗ, ржпрж╛ ржУржкрзЗржи-рж╕рзЛрж░рзНрж╕ ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕ржпрзЛржЧрзНржпрждрж╛, ржмрж╣рзБржнрж╛рж╖рж┐ржХ рж╕ржХрзНрж╖ржорждрж╛ ржПржмржВ ржмрзНржпржмрж╣рж╛рж░рж┐ржХ рж╕рзНржерж╛ржкржирж╛ржХрзЗ ржЕржЧрзНрж░рж╛ржзрж┐ржХрж╛рж░ ржжрзЗржпрж╝, ржПржХржЗрж╕рж╛ржерзЗ ржкрзНрж░рждрж┐ржпрзЛржЧрж┐рждрж╛ржорзВрж▓ржХ ржХрж░рзНржоржХрзНрж╖ржорждрж╛ ржмрзИрж╢рж┐рж╖рзНржЯрзНржп ржмржЬрж╛ржпрж╝ рж░рж╛ржЦрзЗред Qwen ржоржбрзЗрж▓ржЧрзБрж▓рж┐ ржмрж┐ржнрж┐ржирзНржи ржоржбрзЗрж▓ рж╕рж╛ржЗржЬ, ржЙржЪрзНржЪ-ржорж╛ржирзЗрж░ ржкрзНрж░рж╢рж┐ржХрзНрж╖ржг ржкржжрзНржзрждрж┐ ржПржмржВ ржмрж┐ржнрж┐ржирзНржи ржбрзЛржорзЗржЗржирзЗрж░ ржЬржирзНржп ржмрж┐рж╢рзЗрж╖рж╛ржпрж╝рж┐ржд ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯрзЗрж░ ржорж╛ржзрзНржпржорзЗ ржПржЯрж┐ ржЕрж░рзНржЬржи ржХрж░рзЗред

Qwen ржкрж░рж┐ржмрж╛рж░ ржХрж░рзНржоржХрзНрж╖ржорждрж╛-ржжржХрзНрж╖рждрж╛рж░ рж╕рзНржкрзЗржХржЯрзНрж░рж╛ржорзЗрж░ ржЬрзБржбрж╝рзЗ ржмрж┐ржХрж▓рзНржкржЧрзБрж▓рж┐ ржкрзНрж░ржжрж╛ржи ржХрж░рж╛рж░ ржЬржирзНржп ржмрж┐ржнрж┐ржирзНржи ржкржжрзНржзрждрж┐ ржЕржирзНрждрж░рзНржнрзБржХрзНржд ржХрж░рзЗ, ржпрж╛ ржорзЛржмрж╛ржЗрж▓ ржбрж┐ржнрж╛ржЗрж╕ ржерзЗржХрзЗ ржПржирзНржЯрж╛рж░ржкрзНрж░рж╛ржЗржЬ рж╕рж╛рж░рзНржнрж╛рж░ ржкрж░рзНржпржирзНржд рж╕рзНржерж╛ржкржи рж╕ржХрзНрж╖ржо ржХрж░рзЗ ржПржмржВ ржЕрж░рзНржержмрж╣ AI рж╕ржХрзНрж╖ржорждрж╛ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗред рж▓ржХрзНрж╖рзНржп рж╣рж▓ ржЙржЪрзНржЪ-ржорж╛ржирзЗрж░ AI-рждрзЗ ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕ ржЧржгрждрж╛ржирзНрждрзНрж░рж┐ржХ ржХрж░рж╛ ржПржмржВ рж╕рзНржерж╛ржкржирж╛рж░ ржкржЫржирзНржжржЧрзБрж▓рж┐рждрзЗ ржиржоржирзАржпрж╝рждрж╛ ржкрзНрж░ржжрж╛ржи ржХрж░рж╛ред

### Qwen-ржПрж░ ржорзВрж▓ ржиржХрж╢рж╛ ржирзАрждрж┐ржорж╛рж▓рж╛

Qwen ржоржбрзЗрж▓ржЧрзБрж▓рж┐ ржХржпрж╝рзЗржХржЯрж┐ ржорзМрж▓рж┐ржХ ржирзАрждрж┐рж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржХрж░рзЗ рждрзИрж░рж┐ ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗ ржпрж╛ рждрж╛ржжрзЗрж░ ржЕржирзНржпрж╛ржирзНржп ржнрж╛рж╖рж╛ ржоржбрзЗрж▓ ржкрж░рж┐ржмрж╛рж░ ржерзЗржХрзЗ ржЖрж▓рж╛ржжрж╛ ржХрж░рзЗ:

- **ржкрзНрж░ржержорзЗ ржУржкрзЗржи-рж╕рзЛрж░рзНрж╕**: ржЧржмрзЗрж╖ржгрж╛ ржПржмржВ ржмрж╛ржгрж┐ржЬрзНржпрж┐ржХ ржмрзНржпржмрж╣рж╛рж░рзЗрж░ ржЬржирзНржп рж╕ржорзНржкрзВрж░рзНржг рж╕рзНржмржЪрзНржЫрждрж╛ ржПржмржВ ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕ржпрзЛржЧрзНржпрждрж╛
- **рж╕ржоржЧрзНрж░ ржкрзНрж░рж╢рж┐ржХрзНрж╖ржг**: ржПржХрж╛ржзрж┐ржХ ржнрж╛рж╖рж╛ ржПржмржВ ржбрзЛржорзЗржЗржи ржХржнрж╛рж░ ржХрж░рзЗ ржмрж┐рж╢рж╛рж▓, ржмрзИржЪрж┐рждрзНрж░рзНржпржоржпрж╝ ржбрзЗржЯрж╛рж╕рзЗржЯрзЗ ржкрзНрж░рж╢рж┐ржХрзНрж╖ржг
- **рж╕рзНржХрзЗрж▓ржпрзЛржЧрзНржп ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░**: ржмрж┐ржнрж┐ржирзНржи ржХржорзНржкрж┐ржЙржЯрзЗрж╢ржирж╛рж▓ ржкрзНрж░ржпрж╝рзЛржЬржирзЗрж░ рж╕рж╛ржерзЗ ржорж┐рж▓рж╛ржирзЛрж░ ржЬржирзНржп ржПржХрж╛ржзрж┐ржХ ржоржбрзЗрж▓ рж╕рж╛ржЗржЬ
- **ржмрж┐рж╢рзЗрж╖рж╛ржпрж╝рж┐ржд ржЙрзОржХрж░рзНрж╖рждрж╛**: ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржХрж╛ржЬрзЗрж░ ржЬржирзНржп ржЕржкрзНржЯрж┐ржорж╛ржЗржЬржб ржбрзЛржорзЗржЗржи-ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯ

## Qwen ржкрж░рж┐ржмрж╛рж░ржХрзЗ рж╕ржХрзНрж╖ржо ржХрж░рж╛рж░ ржорзВрж▓ ржкрзНрж░ржпрзБржХрзНрждрж┐

### ржмрж┐рж╢рж╛рж▓ рж╕рзНржХрзЗрж▓ ржкрзНрж░рж╢рж┐ржХрзНрж╖ржг

Qwen ржкрж░рж┐ржмрж╛рж░рзЗрж░ ржПржХржЯрж┐ рж╕ржВржЬрзНржЮрж╛ржпрж╝рж┐ржд ржжрж┐ржХ рж╣рж▓ ржоржбрзЗрж▓ ржЙржирзНржиржпрж╝ржирзЗ ржкрзНрж░рж╢рж┐ржХрзНрж╖ржг ржбрзЗржЯрж╛ ржПржмржВ ржХржорзНржкрж┐ржЙржЯрзЗрж╢ржирж╛рж▓ рж╕ржорзНржкржжрзЗрж░ ржмрж┐рж╢рж╛рж▓ рж╕рзНржХрзЗрж▓ред Qwen ржоржбрзЗрж▓ржЧрзБрж▓рж┐ ржЯрзНрж░рж┐рж▓рж┐ржпрж╝ржи ржЯрзЛржХрзЗржи ржЬрзБржбрж╝рзЗ рж╕рж╛ржмржзрж╛ржирзЗ ржХрж┐ржЙрж░рзЗржЯ ржХрж░рж╛, ржмрж╣рзБржнрж╛рж╖рж┐ржХ ржбрзЗржЯрж╛рж╕рзЗржЯ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ, ржпрж╛ ржмрзНржпрж╛ржкржХ ржмрж┐рж╢рзНржм ржЬрзНржЮрж╛ржи ржПржмржВ ржпрзБржХрзНрждрж┐ рж╕ржХрзНрж╖ржорждрж╛ ржкрзНрж░ржжрж╛ржи ржХрж░рж╛рж░ ржЬржирзНржп ржбрж┐ржЬрж╛ржЗржи ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗред

ржПржЗ ржкржжрзНржзрждрж┐ ржЙржЪрзНржЪ-ржорж╛ржирзЗрж░ ржУржпрж╝рзЗржм рж╕рж╛ржоржЧрзНрж░рзА, ржПржХрж╛ржбрзЗржорж┐ржХ рж╕рж╛рж╣рж┐рждрзНржп, ржХрзЛржб рж░рж┐ржкрзЛржЬрж┐ржЯрж░рж┐ ржПржмржВ ржмрж╣рзБржнрж╛рж╖рж┐ржХ рж╕ржорзНржкржж ржПржХрждрзНрж░рж┐ржд ржХрж░рзЗ ржХрж╛ржЬ ржХрж░рзЗред ржкрзНрж░рж╢рж┐ржХрзНрж╖ржг ржкржжрзНржзрждрж┐ ржмрж┐ржнрж┐ржирзНржи ржбрзЛржорзЗржЗржи ржПржмржВ ржнрж╛рж╖рж╛рж░ ржЬрзБржбрж╝рзЗ ржЬрзНржЮрж╛ржирзЗрж░ ржмрж┐рж╕рзНрждрзГрждрж┐ ржПржмржВ ржЧржнрзАрж░рждрж╛рж░ ржЙржкрж░ ржЬрзЛрж░ ржжрзЗржпрж╝ред

### ржЙржирзНржиржд ржпрзБржХрзНрждрж┐ ржПржмржВ ржЪрж┐ржирзНрждрж╛

рж╕рж╛ржорзНржкрзНрж░рждрж┐ржХ Qwen ржоржбрзЗрж▓ржЧрзБрж▓рж┐ ржЙржирзНржиржд ржпрзБржХрзНрждрж┐ рж╕ржХрзНрж╖ржорждрж╛ ржЕржирзНрждрж░рзНржнрзБржХрзНржд ржХрж░рзЗ ржпрж╛ ржЬржЯрж┐рж▓ ржмрж╣рзБ-ржзрж╛ржкрзЗрж░ рж╕ржорж╕рзНржпрж╛ рж╕ржорж╛ржзрж╛ржи рж╕ржХрзНрж╖ржо ржХрж░рзЗ:

**Thinking Mode (Qwen3)**: ржоржбрзЗрж▓ржЧрзБрж▓рж┐ ржЪрзВржбрж╝рж╛ржирзНржд ржЙрждрзНрждрж░ ржжрзЗржУржпрж╝рж╛рж░ ржЖржЧрзЗ ржмрж┐рж╕рзНрждрж╛рж░рж┐ржд ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ ржпрзБржХрзНрждрж┐ ржХрж░рждрзЗ ржкрж╛рж░рзЗ, ржпрж╛ ржорж╛ржирзБрж╖рзЗрж░ рж╕ржорж╕рзНржпрж╛ рж╕ржорж╛ржзрж╛ржирзЗрж░ ржкржжрзНржзрждрж┐рж░ ржорждрзЛред

**Dual-Mode Operation**: рж╕рж╣ржЬ ржкрзНрж░рж╢рзНржирзЗрж░ ржЬржирзНржп ржжрзНрж░рзБржд ржкрзНрж░рждрж┐ржХрзНрж░рж┐ржпрж╝рж╛ ржорзЛржб ржПржмржВ ржЬржЯрж┐рж▓ рж╕ржорж╕рзНржпрж╛рж░ ржЬржирзНржп ржЧржнрзАрж░ ржЪрж┐ржирзНрждрж╛ ржорзЛржбрзЗрж░ ржоржзрзНржпрзЗ рж╕рзНржпрзБржЗржЪ ржХрж░рж╛рж░ ржХрзНрж╖ржорждрж╛ред

**Chain-of-Thought Integration**: ржпрзБржХрзНрждрж┐ ржзрж╛ржкржЧрзБрж▓рж┐рж░ ржкрзНрж░рж╛ржХрзГрждрж┐ржХ рж╕ржВржпрзЛржЬржи ржпрж╛ ржЬржЯрж┐рж▓ ржХрж╛ржЬржЧрзБрж▓рж┐рждрзЗ рж╕рзНржмржЪрзНржЫрждрж╛ ржПржмржВ ржирж┐рж░рзНржнрзБрж▓рждрж╛ ржЙржирзНржиржд ржХрж░рзЗред

### ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░рж╛рж▓ ржЙржжрзНржнрж╛ржмржи

Qwen ржкрж░рж┐ржмрж╛рж░ ржХрж░рзНржоржХрзНрж╖ржорждрж╛ ржПржмржВ ржжржХрзНрж╖рждрж╛рж░ ржЬржирзНржп ржбрж┐ржЬрж╛ржЗржи ржХрж░рж╛ ржмрзЗрж╢ ржХржпрж╝рзЗржХржЯрж┐ ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░рж╛рж▓ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи ржЕржирзНрждрж░рзНржнрзБржХрзНржд ржХрж░рзЗ:

**рж╕рзНржХрзЗрж▓ржпрзЛржЧрзНржп ржбрж┐ржЬрж╛ржЗржи**: ржоржбрзЗрж▓ рж╕рж╛ржЗржЬ ржЬрзБржбрж╝рзЗ рж╕рж╛ржоржЮрзНржЬрж╕рзНржпржкрзВрж░рзНржг ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░ ржпрж╛ рж╕рж╣ржЬ рж╕рзНржХрзЗрж▓рж┐ржВ ржПржмржВ рждрзБрж▓ржирж╛ рж╕ржХрзНрж╖ржо ржХрж░рзЗред

**ржорж╛рж▓рзНржЯрж┐ржорзЛржбрж╛рж▓ ржЗржирзНржЯрж┐ржЧрзНрж░рзЗрж╢ржи**: ржЯрзЗржХрзНрж╕ржЯ, ржнрж┐рж╢ржи ржПржмржВ ржЕржбрж┐ржУ ржкрзНрж░рж╕рзЗрж╕рж┐ржВ рж╕ржХрзНрж╖ржорждрж╛рж░ ржПржХрзАржнрзВржд ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░рзЗ ржирж┐рж░рзНржмрж┐ржШрзНржи рж╕ржВржпрзЛржЬржиред

**рж╕рзНржерж╛ржкржи ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи**: ржмрж┐ржнрж┐ржирзНржи рж╣рж╛рж░рзНржбржУржпрж╝рзНржпрж╛рж░ ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржирзЗрж░ ржЬржирзНржп ржПржХрж╛ржзрж┐ржХ ржХрзЛржпрж╝рж╛ржирзНржЯрж╛ржЗржЬрзЗрж╢ржи ржмрж┐ржХрж▓рзНржк ржПржмржВ рж╕рзНржерж╛ржкржи ржлрж░ржорзНржпрж╛ржЯред

## ржоржбрзЗрж▓ рж╕рж╛ржЗржЬ ржПржмржВ рж╕рзНржерж╛ржкржирж╛рж░ ржмрж┐ржХрж▓рзНржк

ржЖржзрзБржирж┐ржХ рж╕рзНржерж╛ржкржи ржкрж░рж┐ржмрзЗрж╢ржЧрзБрж▓рж┐ Qwen ржоржбрзЗрж▓ржЧрзБрж▓рж┐рж░ ржиржоржирзАржпрж╝рждрж╛ ржерзЗржХрзЗ ржЙржкржХрзГржд рж╣ржпрж╝ ржпрж╛ ржмрж┐ржнрж┐ржирзНржи ржХржорзНржкрж┐ржЙржЯрзЗрж╢ржирж╛рж▓ ржкрзНрж░ржпрж╝рзЛржЬржирзЗрж░ рж╕рж╛ржерзЗ ржорж╛ржирж╛ржирж╕ржЗ:

### ржЫрзЛржЯ ржоржбрзЗрж▓ (0.5B-3B)

Qwen ржжржХрзНрж╖ ржЫрзЛржЯ ржоржбрзЗрж▓ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗ ржпрж╛ ржПржЬ рж╕рзНржерж╛ржкржи, ржорзЛржмрж╛ржЗрж▓ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи ржПржмржВ рж╕рзАржорж┐ржд рж╕ржорзНржкржж ржкрж░рж┐ржмрзЗрж╢рзЗрж░ ржЬржирзНржп ржЙржкржпрзБржХрзНржд, ржПржХржЗрж╕рж╛ржерзЗ ржЪрж┐рждрзНрждрж╛ржХрж░рзНрж╖ржХ рж╕ржХрзНрж╖ржорждрж╛ ржмржЬрж╛ржпрж╝ рж░рж╛ржЦрзЗред

### ржорж╛ржЭрж╛рж░рж┐ ржоржбрзЗрж▓ (7B-32B)

ржорж╛ржЭрж╛рж░рж┐-ржкрж░рж┐рж╕рж░рзЗрж░ ржоржбрзЗрж▓ржЧрзБрж▓рж┐ ржкрзЗрж╢рж╛ржжрж╛рж░ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржирзЗрж░ ржЬржирзНржп ржЙржирзНржиржд рж╕ржХрзНрж╖ржорждрж╛ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗ, ржХрж░рзНржоржХрзНрж╖ржорждрж╛ ржПржмржВ ржХржорзНржкрж┐ржЙржЯрзЗрж╢ржирж╛рж▓ ржкрзНрж░ржпрж╝рзЛржЬржирзЗрж░ ржоржзрзНржпрзЗ ржЪржорзОржХрж╛рж░ ржнрж╛рж░рж╕рж╛ржорзНржп ржкрзНрж░ржжрж╛ржи ржХрж░рзЗред

### ржмржбрж╝ ржоржбрзЗрж▓ (72B+)

ржкрзВрж░рзНржг-рж╕рзНржХрзЗрж▓ ржоржбрзЗрж▓ржЧрзБрж▓рж┐ рж╕рж░рзНржмрзЛржЪрзНржЪ рж╕ржХрзНрж╖ржорждрж╛ ржкрзНрж░ржпрж╝рзЛржЬржи ржПржоржи ржЪрж╛рж╣рж┐ржжрж╛ржкрзВрж░рзНржг ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи, ржЧржмрзЗрж╖ржгрж╛ ржПржмржВ ржПржирзНржЯрж╛рж░ржкрзНрж░рж╛ржЗржЬ рж╕рзНржерж╛ржкржирж╛рж░ ржЬржирзНржп ржЕрждрзНржпрж╛ржзрзБржирж┐ржХ ржХрж░рзНржоржХрзНрж╖ржорждрж╛ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗред

## Qwen ржоржбрзЗрж▓ ржкрж░рж┐ржмрж╛рж░рзЗрж░ рж╕рзБржмрж┐ржзрж╛

### ржУржкрзЗржи-рж╕рзЛрж░рзНрж╕ ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕ржпрзЛржЧрзНржпрждрж╛

Qwen ржоржбрзЗрж▓ржЧрзБрж▓рж┐ рж╕ржорзНржкрзВрж░рзНржг рж╕рзНржмржЪрзНржЫрждрж╛ ржПржмржВ ржХрж╛рж╕рзНржЯржорж╛ржЗржЬрзЗрж╢ржи рж╕ржХрзНрж╖ржорждрж╛ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗ, ржпрж╛ рж╕ржВрж╕рзНржерж╛ржЧрзБрж▓рж┐ржХрзЗ рждрж╛ржжрзЗрж░ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржкрзНрж░ржпрж╝рзЛржЬржирзЗрж░ ржЬржирзНржп ржоржбрзЗрж▓ржЧрзБрж▓рж┐ ржмрзБржЭрждрзЗ, ржкрж░рж┐ржмрж░рзНрждржи ржХрж░рждрзЗ ржПржмржВ ржорж╛ржирж┐ржпрж╝рзЗ ржирж┐рждрзЗ рж╕ржХрзНрж╖ржо ржХрж░рзЗ, ржнрзЗржирзНржбрж░ рж▓ржХ-ржЗржи ржЫрж╛ржбрж╝рж╛ржЗред

### рж╕рзНржерж╛ржкржирж╛рж░ ржиржоржирзАржпрж╝рждрж╛

ржоржбрзЗрж▓ рж╕рж╛ржЗржЬрзЗрж░ ржкрж░рж┐рж╕рж░ ржорзЛржмрж╛ржЗрж▓ ржбрж┐ржнрж╛ржЗрж╕ ржерзЗржХрзЗ ржЙржЪрзНржЪ-рж╕ржорзНржкржирзНржи рж╕рж╛рж░рзНржнрж╛рж░ ржкрж░рзНржпржирзНржд ржмрж┐ржнрж┐ржирзНржи рж╣рж╛рж░рзНржбржУржпрж╝рзНржпрж╛рж░ ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржирзЗрж░ ржЬрзБржбрж╝рзЗ рж╕рзНржерж╛ржкржи рж╕ржХрзНрж╖ржо ржХрж░рзЗ, рж╕ржВрж╕рзНржерж╛ржЧрзБрж▓рж┐ржХрзЗ рждрж╛ржжрзЗрж░ AI ржЕржмржХрж╛ржарж╛ржорзЛ ржкржЫржирзНржжржЧрзБрж▓рж┐рждрзЗ ржиржоржирзАржпрж╝рждрж╛ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗред

### ржмрж╣рзБржнрж╛рж╖рж┐ржХ ржЙрзОржХрж░рзНрж╖рждрж╛

Qwen ржоржбрзЗрж▓ржЧрзБрж▓рж┐ ржмрж╣рзБржнрж╛рж╖рж┐ржХ ржмрзЛржЭрж╛ ржПржмржВ ржкрзНрж░ржЬржирзНржорзЗ ржЪржорзО
- Qwen3-235B-A22B ржЕржирзНржпрж╛ржирзНржп рж╢рзАрж░рзНрж╖рж╕рзНржерж╛ржирзАржпрж╝ ржоржбрзЗрж▓ ржпрзЗржоржи DeepSeek-R1, o1, o3-mini, Grok-3, ржПржмржВ Gemini-2.5-Pro ржПрж░ рж╕рж╛ржерзЗ рждрзБрж▓ржирж╛ржпрж╝ ржХрзЛржбрж┐ржВ, ржЧржгрж┐ржд, ржПржмржВ рж╕рж╛ржзрж╛рж░ржг рж╕ржХрзНрж╖ржорждрж╛рж░ ржмрзЗржЮрзНржЪржорж╛рж░рзНржХ ржорзВрж▓рзНржпрж╛ржпрж╝ржирзЗ ржкрзНрж░рждрж┐ржпрзЛржЧрж┐рждрж╛ржорзВрж▓ржХ ржлрж▓рж╛ржлрж▓ ржЕрж░рзНржЬржи ржХрж░рзЗржЫрзЗред
- Qwen3-30B-A3B, QwQ-32B ржПрж░ ржЪрзЗржпрж╝рзЗ рззрзж ржЧрзБржг ржмрзЗрж╢рж┐ рж╕ржХрзНрж░рж┐ржпрж╝ ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржПржЧрж┐ржпрж╝рзЗ рж░ржпрж╝рзЗржЫрзЗред
- Qwen3-4B, Qwen2.5-72B-Instruct ржПрж░ ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕рзЗрж░ рж╕рж╛ржерзЗ ржкрзНрж░рждрж┐ржжрзНржмржирзНржжрзНржмрж┐рждрж╛ ржХрж░рждрзЗ ржкрж╛рж░рзЗред

**ржжржХрзНрж╖рждрж╛рж░ ржЕрж░рзНржЬржирж╕ржорзВрж╣:**
- Qwen3-MoE ржмрзЗрж╕ ржоржбрзЗрж▓ржЧрзБрж▓рж┐ Qwen2.5 ржбрзЗржирзНрж╕ ржмрзЗрж╕ ржоржбрзЗрж▓ржЧрзБрж▓рж┐рж░ рж╕ржорждрзБрж▓рзНржп ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржЕрж░рзНржЬржи ржХрж░рзЗржЫрзЗ, ржпрзЗржЦрж╛ржирзЗ ржорж╛рждрзНрж░ рззрзж% рж╕ржХрзНрж░рж┐ржпрж╝ ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗред
- ржбрзЗржирзНрж╕ ржоржбрзЗрж▓рзЗрж░ рждрзБрж▓ржирж╛ржпрж╝ ржкрзНрж░рж╢рж┐ржХрзНрж╖ржг ржПржмржВ ржЗржиржлрж╛рж░рзЗржирзНрж╕рзЗ ржЙрж▓рзНрж▓рзЗржЦржпрзЛржЧрзНржп ржЦрж░ржЪ рж╕рж╛рж╢рзНрж░ржпрж╝ред

**ржмрж╣рзБржнрж╛рж╖рж┐ржХ рж╕ржХрзНрж╖ржорждрж╛:**
- Qwen3 ржоржбрзЗрж▓ржЧрзБрж▓рж┐ рззрззрзпржЯрж┐ ржнрж╛рж╖рж╛ ржПржмржВ ржЙржкржнрж╛рж╖рж╛ рж╕ржорж░рзНржержи ржХрж░рзЗред
- ржмрж┐ржнрж┐ржирзНржи ржнрж╛рж╖рж╛ржЧржд ржПржмржВ рж╕рж╛ржВрж╕рзНржХрзГрждрж┐ржХ ржкрзНрж░рзЗржХрзНрж╖рж╛ржкржЯрзЗ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ред

**ржкрзНрж░рж╢рж┐ржХрзНрж╖ржгрзЗрж░ рж╕рзНржХрзЗрж▓:**
- Qwen3 ржкрзНрж░рж╛ржпрж╝ рзйрзм ржЯрзНрж░рж┐рж▓рж┐ржпрж╝ржи ржЯрзЛржХрзЗржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ, ржпрж╛ Qwen2.5 ржПрж░ рззрзо ржЯрзНрж░рж┐рж▓рж┐ржпрж╝ржи ржЯрзЛржХрзЗржирзЗрж░ рждрзБрж▓ржирж╛ржпрж╝ ржжрзНржмрж┐ржЧрзБржг, ржПржмржВ ржПржЯрж┐ рззрззрзпржЯрж┐ ржнрж╛рж╖рж╛ ржУ ржЙржкржнрж╛рж╖рж╛ ржХржнрж╛рж░ ржХрж░рзЗред

### ржоржбрзЗрж▓ рждрзБрж▓ржирж╛ ржорзНржпрж╛ржЯрзНрж░рж┐ржХрзНрж╕

| ржоржбрзЗрж▓ рж╕рж┐рж░рж┐ржЬ | ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░ рж░рзЗржЮрзНржЬ | ржХржиржЯрзЗржХрзНрж╕ржЯ ржжрзИрж░рзНржШрзНржп | ржорзВрж▓ рж╢ржХрзНрждрж┐ | рж╕рзЗрж░рж╛ ржмрзНржпржмрж╣рж╛рж░ ржХрзНрж╖рзЗрждрзНрж░ |
|--------------|------------------|----------------|---------------|----------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | ржнрж╛рж░рж╕рж╛ржорзНржпржкрзВрж░рзНржг ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕, ржмрж╣рзБржнрж╛рж╖рж┐ржХ | рж╕рж╛ржзрж╛рж░ржг ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи, ржкрзНрж░рзЛржбрж╛ржХрж╢ржи ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | ржХрзЛржб ржЬрзЗржирж╛рж░рзЗрж╢ржи, ржкрзНрж░рзЛржЧрзНрж░рж╛ржорж┐ржВ | рж╕ржлржЯржУржпрж╝рзНржпрж╛рж░ ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ, ржХрзЛржбрж┐ржВ рж╕рж╣рж╛ржпрж╝рждрж╛ |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | ржЧржгрж┐рждрзАржпрж╝ ржпрзБржХрзНрждрж┐ | рж╢рж┐ржХрзНрж╖рж╛ржорзВрж▓ржХ ржкрзНрж▓рзНржпрж╛ржЯржлрж░рзНржо, STEM ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи |
| **Qwen2.5-VL** | ржмрж┐ржнрж┐ржирзНржи | ржкрж░рж┐ржмрж░рзНрждржирж╢рзАрж▓ | ржнрж┐рж╢ржи-рж▓рзНржпрж╛ржЩрзНржЧрзБржпрж╝рзЗржЬ ржмрзЛржЭрж╛ржкржбрж╝рж╛ | ржорж╛рж▓рзНржЯрж┐ржорзЛржбрж╛рж▓ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи, ржЗржорзЗржЬ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг |
| **Qwen3** | 0.6B-235B | ржкрж░рж┐ржмрж░рзНрждржирж╢рзАрж▓ | ржЙржирзНржиржд ржпрзБржХрзНрждрж┐, ржЪрж┐ржирзНрждрж╛рж░ ржорзЛржб | ржЬржЯрж┐рж▓ ржпрзБржХрзНрждрж┐, ржЧржмрзЗрж╖ржгрж╛ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи |
| **Qwen3 MoE** | 30B-235B ржорзЛржЯ | ржкрж░рж┐ржмрж░рзНрждржирж╢рзАрж▓ | ржжржХрзНрж╖ ржмрзГрж╣рзО-рж╕рзНржХрзЗрж▓ ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ | ржПржирзНржЯрж╛рж░ржкрзНрж░рж╛ржЗржЬ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи, ржЙржЪрзНржЪ-ржХрзНрж╖ржорждрж╛рж░ ржкрзНрж░ржпрж╝рзЛржЬржи |

## ржоржбрзЗрж▓ ржирж┐рж░рзНржмрж╛ржЪржи ржЧрж╛ржЗржб

### рж╕рж╛ржзрж╛рж░ржг ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржирзЗрж░ ржЬржирзНржп
- **Qwen2.5-0.5B/1.5B**: ржорзЛржмрж╛ржЗрж▓ ржЕрзНржпрж╛ржк, ржПржЬ ржбрж┐ржнрж╛ржЗрж╕, рж░рж┐ржпрж╝рзЗрж▓-ржЯрж╛ржЗржо ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи
- **Qwen2.5-3B/7B**: рж╕рж╛ржзрж╛рж░ржг ржЪрзНржпрж╛ржЯржмржЯ, ржХржирзНржЯрзЗржирзНржЯ ржЬрзЗржирж╛рж░рзЗрж╢ржи, ржкрзНрж░рж╢рзНржирзЛрждрзНрждрж░ рж╕рж┐рж╕рзНржЯрзЗржо

### ржЧржгрж┐ржд ржПржмржВ ржпрзБржХрзНрждрж┐ рж╕ржВржХрзНрж░рж╛ржирзНржд ржХрж╛ржЬрзЗрж░ ржЬржирзНржп
- **Qwen2.5-Math**: ржЧржгрж┐ржд рж╕ржорж╕рзНржпрж╛ рж╕ржорж╛ржзрж╛ржи ржПржмржВ STEM рж╢рж┐ржХрзНрж╖рж╛
- **Qwen3 with Thinking Mode**: ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг ржкрзНрж░ржпрж╝рзЛржЬржи ржПржоржи ржЬржЯрж┐рж▓ ржпрзБржХрзНрждрж┐

### ржкрзНрж░рзЛржЧрзНрж░рж╛ржорж┐ржВ ржПржмржВ ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯрзЗрж░ ржЬржирзНржп
- **Qwen2.5-Coder**: ржХрзЛржб ржЬрзЗржирж╛рж░рзЗрж╢ржи, ржбрж┐ржмрж╛ржЧрж┐ржВ, ржкрзНрж░рзЛржЧрзНрж░рж╛ржорж┐ржВ рж╕рж╣рж╛ржпрж╝рждрж╛
- **Qwen3**: ржпрзБржХрзНрждрж┐ рж╕ржХрзНрж╖ржорждрж╛рж░ рж╕рж╛ржерзЗ ржЙржирзНржиржд ржкрзНрж░рзЛржЧрзНрж░рж╛ржорж┐ржВ ржХрж╛ржЬ

### ржорж╛рж▓рзНржЯрж┐ржорзЛржбрж╛рж▓ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржирзЗрж░ ржЬржирзНржп
- **Qwen2.5-VL**: ржЗржорзЗржЬ ржмрзЛржЭрж╛ржкржбрж╝рж╛, ржнрж┐ржЬрзНржпрзБржпрж╝рж╛рж▓ ржкрзНрж░рж╢рзНржирзЛрждрзНрждрж░
- **Qwen-Audio**: ржЕржбрж┐ржУ ржкрзНрж░рж╕рзЗрж╕рж┐ржВ ржПржмржВ рж╕рзНржкрж┐ржЪ ржмрзЛржЭрж╛ржкржбрж╝рж╛

### ржПржирзНржЯрж╛рж░ржкрзНрж░рж╛ржЗржЬ ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯрзЗрж░ ржЬржирзНржп
- **Qwen2.5-32B/72B**: ржЙржЪрзНржЪ-ржХрзНрж╖ржорждрж╛рж░ ржнрж╛рж╖рж╛ ржмрзЛржЭрж╛ржкржбрж╝рж╛
- **Qwen3-235B-A22B**: ржЪрж╛рж╣рж┐ржжрж╛ржкрзВрж░рзНржг ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржирзЗрж░ ржЬржирзНржп рж╕рж░рзНржмрзЛржЪрзНржЪ рж╕ржХрзНрж╖ржорждрж╛

## ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ ржкрзНрж▓рзНржпрж╛ржЯржлрж░рзНржо ржПржмржВ ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕рж┐ржмрж┐рж▓рж┐ржЯрж┐
### ржХрзНрж▓рж╛ржЙржб ржкрзНрж▓рзНржпрж╛ржЯржлрж░рзНржо
- **Hugging Face Hub**: рж╕ржорзНржкрзНрж░ржжрж╛ржпрж╝рзЗрж░ рж╕рж╣рж╛ржпрж╝рждрж╛ рж╕рж╣ ржмрж┐рж╕рзНрждрзГржд ржоржбрзЗрж▓ рж╕ржВржЧрзНрж░рж╣рж╢рж╛рж▓рж╛
- **ModelScope**: ржЖрж▓рж┐ржмрж╛ржмрж╛рж░ ржоржбрзЗрж▓ ржкрзНрж▓рзНржпрж╛ржЯржлрж░рзНржо, ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи ржЯрзБрж▓ рж╕рж╣
- **ржмрж┐ржнрж┐ржирзНржи ржХрзНрж▓рж╛ржЙржб ржкрзНрж░ржжрж╛ржиржХрж╛рж░рзА**: рж╕рзНржЯрзНржпрж╛ржирзНржбрж╛рж░рзНржб ML ржкрзНрж▓рзНржпрж╛ржЯржлрж░рзНржорзЗрж░ ржорж╛ржзрзНржпржорзЗ рж╕ржорж░рзНржержи

### рж╕рзНржерж╛ржирзАржпрж╝ ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ ржлрзНрж░рзЗржоржУржпрж╝рж╛рж░рзНржХ
- **Transformers**: рж╕рж╣ржЬ ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯрзЗрж░ ржЬржирзНржп рж╕рзНржЯрзНржпрж╛ржирзНржбрж╛рж░рзНржб Hugging Face ржЗржирзНржЯрж┐ржЧрзНрж░рзЗрж╢ржи
- **vLLM**: ржкрзНрж░рзЛржбрж╛ржХрж╢ржи ржкрж░рж┐ржмрзЗрж╢рзЗрж░ ржЬржирзНржп ржЙржЪрзНржЪ-ржХрзНрж╖ржорждрж╛рж░ рж╕рж╛рж░рзНржнрж┐ржВ
- **Ollama**: рж╕рзНржерж╛ржирзАржпрж╝ ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ ржПржмржВ ржмрзНржпржмрж╕рзНржерж╛ржкржирж╛рж░ рж╕рж░рж▓рзАржХрж░ржг
- **ONNX Runtime**: ржмрж┐ржнрж┐ржирзНржи рж╣рж╛рж░рзНржбржУржпрж╝рзНржпрж╛рж░рзЗрж░ ржЬржирзНржп ржХрзНрж░рж╕-ржкрзНрж▓рзНржпрж╛ржЯржлрж░рзНржо ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи
- **llama.cpp**: ржмрж┐ржнрж┐ржирзНржи ржкрзНрж▓рзНржпрж╛ржЯржлрж░рзНржорзЗрж░ ржЬржирзНржп ржжржХрзНрж╖ C++ ржЗржоржкрзНрж▓рж┐ржорзЗржирзНржЯрзЗрж╢ржи

### рж╢рзЗржЦрж╛рж░ рж╕ржорзНржкржж
- **Qwen ржбржХрзБржорзЗржирзНржЯрзЗрж╢ржи**: ржЕржлрж┐рж╕рж┐ржпрж╝рж╛рж▓ ржбржХрзБржорзЗржирзНржЯрзЗрж╢ржи ржПржмржВ ржоржбрзЗрж▓ ржХрж╛рж░рзНржб
- **Hugging Face Model Hub**: ржЗржирзНржЯрж╛рж░ржЕрзНржпрж╛ржХржЯрж┐ржн ржбрзЗржорзЛ ржПржмржВ рж╕ржорзНржкрзНрж░ржжрж╛ржпрж╝ ржЙржжрж╛рж╣рж░ржг
- **ржЧржмрзЗрж╖ржгрж╛ ржкрзЗржкрж╛рж░**: ржЧржнрзАрж░ ржмрзЛржЭрж╛рж░ ржЬржирзНржп arxiv-ржП ржЯрзЗржХржирж┐ржХрзНржпрж╛рж▓ ржкрзЗржкрж╛рж░
- **ржХржорж┐ржЙржирж┐ржЯрж┐ ржлрзЛрж░рж╛ржо**: рж╕ржХрзНрж░рж┐ржпрж╝ рж╕ржорзНржкрзНрж░ржжрж╛ржпрж╝рзЗрж░ рж╕рж╣рж╛ржпрж╝рждрж╛ ржПржмржВ ржЖрж▓рзЛржЪржирж╛

### Qwen ржоржбрзЗрж▓ ржжрж┐ржпрж╝рзЗ рж╢рзБрж░рзБ ржХрж░рж╛

#### ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ ржкрзНрж▓рзНржпрж╛ржЯржлрж░рзНржо
1. **Hugging Face Transformers**: рж╕рзНржЯрзНржпрж╛ржирзНржбрж╛рж░рзНржб ржкрж╛ржЗржержи ржЗржирзНржЯрж┐ржЧрзНрж░рзЗрж╢ржи ржжрж┐ржпрж╝рзЗ рж╢рзБрж░рзБ ржХрж░рзБржи
2. **ModelScope**: ржЖрж▓рж┐ржмрж╛ржмрж╛рж░ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬржб ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ ржЯрзБрж▓ржЧрзБрж▓рж┐ ржЕржирзНржмрзЗрж╖ржг ржХрж░рзБржи
3. **рж╕рзНржерж╛ржирзАржпрж╝ ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ**: Ollama ржмрж╛ рж╕рж░рж╛рж╕рж░рж┐ Transformers ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ рж╕рзНржерж╛ржирзАржпрж╝ ржЯрзЗрж╕рзНржЯрж┐ржВ ржХрж░рзБржи

#### рж╢рзЗржЦрж╛рж░ ржкрже
1. **ржорзВрж▓ ржзрж╛рж░ржгрж╛ ржмрзЛржЭрж╛**: Qwen ржкрж░рж┐ржмрж╛рж░рзЗрж░ ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░ ржПржмржВ рж╕ржХрзНрж╖ржорждрж╛ ржЕржзрзНржпржпрж╝ржи ржХрж░рзБржи
2. **ржмрж┐ржнрж┐ржирзНржи ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯ ржирж┐ржпрж╝рзЗ ржкрж░рзАржХрзНрж╖рж╛ ржХрж░рзБржи**: ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржЯрзНрж░рзЗржб-ржЕржл ржмрзЛржЭрж╛рж░ ржЬржирзНржп ржмрж┐ржнрж┐ржирзНржи ржоржбрзЗрж▓ рж╕рж╛ржЗржЬ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рзБржи
3. **ржЗржоржкрзНрж▓рж┐ржорзЗржирзНржЯрзЗрж╢ржи ржЕржирзБрж╢рзАрж▓ржи ржХрж░рзБржи**: ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ ржкрж░рж┐ржмрзЗрж╢рзЗ ржоржбрзЗрж▓ ржбрж┐ржкрзНрж▓ржпрж╝ ржХрж░рзБржи
4. **ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬ ржХрж░рзБржи**: ржкрзНрж░рзЛржбрж╛ржХрж╢ржи ржмрзНржпржмрж╣рж╛рж░рзЗрж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ ржлрж╛ржЗржи-ржЯрж┐ржЙржи ржХрж░рзБржи

#### рж╕рзЗрж░рж╛ ржЕржирзБрж╢рзАрж▓ржи
- **ржЫрзЛржЯ ржерзЗржХрзЗ рж╢рзБрж░рзБ ржХрж░рзБржи**: ржкрзНрж░рж╛ржержорж┐ржХ ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯрзЗрж░ ржЬржирзНржп ржЫрзЛржЯ ржоржбрзЗрж▓ (1.5B-7B) ржжрж┐ржпрж╝рзЗ рж╢рзБрж░рзБ ржХрж░рзБржи
- **ржЪрзНржпрж╛ржЯ ржЯрзЗржоржкрзНрж▓рзЗржЯ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи**: рж╕рж░рзНржмрзЛрждрзНрждржо ржлрж▓рж╛ржлрж▓рзЗрж░ ржЬржирзНржп рж╕ржарж┐ржХ ржлрж░ржорзНржпрж╛ржЯрж┐ржВ ржкрзНрж░ржпрж╝рзЛржЧ ржХрж░рзБржи
- **рж░рж┐рж╕рзЛрж░рзНрж╕ ржоржирж┐ржЯрж░ ржХрж░рзБржи**: ржорзЗржорж░рж┐ ржмрзНржпржмрж╣рж╛рж░ ржПржмржВ ржЗржиржлрж╛рж░рзЗржирзНрж╕ рж╕рзНржкрж┐ржб ржЯрзНрж░рзНржпрж╛ржХ ржХрж░рзБржи
- **ржмрж┐рж╢рзЗрж╖рж╛ржпрж╝ржи ржмрж┐ржмрзЗржЪржирж╛ ржХрж░рзБржи**: ржкрзНрж░ржпрж╝рзЛржЬржи рж╣рж▓рзЗ ржбрзЛржорзЗржЗржи-ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯ ржирж┐рж░рзНржмрж╛ржЪржи ржХрж░рзБржи

## ржЙржирзНржиржд ржмрзНржпржмрж╣рж╛рж░рзЗрж░ ржкрзНржпрж╛ржЯрж╛рж░рзНржи

### ржлрж╛ржЗржи-ржЯрж┐ржЙржирж┐ржВ ржЙржжрж╛рж╣рж░ржг

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### ржмрж┐рж╢рзЗрж╖рж╛ржпрж╝рж┐ржд ржкрзНрж░ржорзНржкржЯ ржЗржЮрзНржЬрж┐ржирж┐ржпрж╝рж╛рж░рж┐ржВ

**ржЬржЯрж┐рж▓ ржпрзБржХрзНрждрж┐ ржХрж╛ржЬрзЗрж░ ржЬржирзНржп:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**ржХржиржЯрзЗржХрзНрж╕ржЯ рж╕рж╣ ржХрзЛржб ржЬрзЗржирж╛рж░рзЗрж╢ржирзЗрж░ ржЬржирзНржп:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### ржмрж╣рзБржнрж╛рж╖рж┐ржХ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ф╕нцЦЗ)",
        "es": "Spanish (Espa├▒ol)",
        "fr": "French (Fran├зais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (цЧецЬмшкЮ)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ЁЯФз ржкрзНрж░рзЛржбрж╛ржХрж╢ржи ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ ржкрзНржпрж╛ржЯрж╛рж░рзНржи

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи ржХрзМрж╢рж▓

### ржорзЗржорж░рж┐ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### ржЗржиржлрж╛рж░рзЗржирзНрж╕ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## рж╕рзЗрж░рж╛ ржЕржирзБрж╢рзАрж▓ржи ржПржмржВ ржирж┐рж░рзНржжрзЗрж╢рж┐ржХрж╛

### ржирж┐рж░рж╛ржкрждрзНрждрж╛ ржПржмржВ ржЧрзЛржкржирзАржпрж╝рждрж╛

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### ржоржирж┐ржЯрж░рж┐ржВ ржПржмржВ ржорзВрж▓рзНржпрж╛ржпрж╝ржи

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## ржЙржкрж╕ржВрж╣рж╛рж░

Qwen ржоржбрзЗрж▓ ржкрж░рж┐ржмрж╛рж░ ржПржХржЯрж┐ ржмрж┐рж╕рзНрждрзГржд ржкржжрзНржзрждрж┐рж░ ржкрзНрж░рждрж┐ржирж┐ржзрж┐рждрзНржм ржХрж░рзЗ ржпрж╛ AI ржкрзНрж░ржпрзБржХрзНрждрж┐ржХрзЗ ржЧржгрждрж╛ржирзНрждрзНрж░рж┐ржХ ржХрж░рж╛рж░ ржкрж╛рж╢рж╛ржкрж╛рж╢рж┐ ржмрж┐ржнрж┐ржирзНржи ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржирзЗ ржкрзНрж░рждрж┐ржпрзЛржЧрж┐рждрж╛ржорзВрж▓ржХ ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржмржЬрж╛ржпрж╝ рж░рж╛ржЦрзЗред ржПрж░ ржУржкрзЗржи-рж╕рзЛрж░рзНрж╕ ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕рж┐ржмрж┐рж▓рж┐ржЯрж┐, ржмрж╣рзБржнрж╛рж╖рж┐ржХ рж╕ржХрзНрж╖ржорждрж╛, ржПржмржВ ржиржоржирзАржпрж╝ ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ ржЕржкрж╢ржиржЧрзБрж▓рж┐рж░ ржорж╛ржзрзНржпржорзЗ, Qwen рж╕ржВрж╕рзНржерж╛ ржПржмржВ ржбрзЗржнрзЗрж▓ржкрж╛рж░ржжрзЗрж░ рждрж╛ржжрзЗрж░ рж╕ржорзНржкржж ржмрж╛ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝рждрж╛ ржирж┐рж░рзНржмрж┐рж╢рзЗрж╖рзЗ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА AI рж╕ржХрзНрж╖ржорждрж╛ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛рж░ рж╕рзБржпрзЛржЧ ржжрзЗржпрж╝ред

### ржорзВрж▓ ржмрж┐рж╖ржпрж╝ржЧрзБрж▓рзЛ

**ржУржкрзЗржи рж╕рзЛрж░рзНрж╕ ржЙрзОржХрж░рзНрж╖рждрж╛**: Qwen ржкрзНрж░ржорж╛ржг ржХрж░рзЗ ржпрзЗ ржУржкрзЗржи-рж╕рзЛрж░рзНрж╕ ржоржбрзЗрж▓ржЧрзБрж▓рж┐ ржорж╛рж▓рж┐ржХрж╛ржирж╛ржзрзАржи ржмрж┐ржХрж▓рзНржкржЧрзБрж▓рж┐рж░ рж╕рж╛ржерзЗ ржкрзНрж░рждрж┐ржпрзЛржЧрж┐рждрж╛ржорзВрж▓ржХ ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржЕрж░рзНржЬржи ржХрж░рждрзЗ ржкрж╛рж░рзЗ, ржкрж╛рж╢рж╛ржкрж╛рж╢рж┐ рж╕рзНржмржЪрзНржЫрждрж╛, ржХрж╛рж╕рзНржЯржорж╛ржЗржЬрзЗрж╢ржи, ржПржмржВ ржирж┐ржпрж╝ржирзНрждрзНрж░ржг ржкрзНрж░ржжрж╛ржи ржХрж░рзЗред

**рж╕рзНржХрзЗрж▓ржпрзЛржЧрзНржп ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░**: 0.5B ржерзЗржХрзЗ 235B ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░ ржкрж░рзНржпржирзНржд ржкрж░рж┐рж╕рж░ ржорзЛржмрж╛ржЗрж▓ ржбрж┐ржнрж╛ржЗрж╕ ржерзЗржХрзЗ ржПржирзНржЯрж╛рж░ржкрзНрж░рж╛ржЗржЬ ржХрзНрж▓рж╛рж╕рзНржЯрж╛рж░ ржкрж░рзНржпржирзНржд рж╕ржорж╕рзНржд ржХржорзНржкрж┐ржЙржЯрзЗрж╢ржирж╛рж▓ ржкрж░рж┐ржмрзЗрж╢рзЗ ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ рж╕ржХрзНрж╖ржо ржХрж░рзЗред

**ржмрж┐рж╢рзЗрж╖рж╛ржпрж╝рж┐ржд рж╕ржХрзНрж╖ржорждрж╛**: Qwen-Coder, Qwen-Math, ржПржмржВ Qwen-VL ржПрж░ ржорждрзЛ ржбрзЛржорзЗржЗржи-ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯржЧрзБрж▓рж┐ рж╕рж╛ржзрж╛рж░ржг ржнрж╛рж╖рж╛ ржмрзЛржЭрж╛рж░ ржкрж╛рж╢рж╛ржкрж╛рж╢рж┐ ржмрж┐рж╢рзЗрж╖ржЬрзНржЮ ржжржХрзНрж╖рждрж╛ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗред

**ржЧрзНрж▓рзЛржмрж╛рж▓ ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕рж┐ржмрж┐рж▓рж┐ржЯрж┐**: рззрззрзп+ ржнрж╛рж╖рж╛рж░ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА ржмрж╣рзБржнрж╛рж╖рж┐ржХ рж╕ржорж░рзНржержи Qwen-ржХрзЗ ржЖржирзНрждрж░рзНржЬрж╛рждрж┐ржХ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи ржПржмржВ ржмрзИржЪрж┐рждрзНрж░рзНржпржоржпрж╝ ржмрзНржпржмрж╣рж╛рж░ржХрж╛рж░рзА ржнрж┐рждрзНрждрж┐рж░ ржЬржирзНржп ржЙржкржпрзБржХрзНржд ржХрж░рзЗ рждрзЛрж▓рзЗред

**ржирж┐рж░ржмржЪрзНржЫрж┐ржирзНржи ржЙржжрзНржнрж╛ржмржи**: Qwen 1.0 ржерзЗржХрзЗ Qwen3 ржкрж░рзНржпржирзНржд ржмрж┐ржмрж░рзНрждржи рж╕ржХрзНрж╖ржорждрж╛, ржжржХрзНрж╖рждрж╛, ржПржмржВ ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ ржЕржкрж╢ржиржЧрзБрж▓рж┐рждрзЗ ржзрж╛рж░рж╛ржмрж╛рж╣рж┐ржХ ржЙржирзНржирждрж┐ ржкрзНрж░ржжрж░рзНрж╢ржи ржХрж░рзЗред

### ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржжрзГрж╖рзНржЯрж┐ржнржЩрзНржЧрж┐

Qwen ржкрж░рж┐ржмрж╛рж░ ржмрж┐ржХрж╢рж┐ржд рж╣рждрзЗ ржерж╛ржХрж╛ржпрж╝ ржЖржорж░рж╛ ржЖрж╢рж╛ ржХрж░рждрзЗ ржкрж╛рж░рж┐:

- **ржЙржирзНржиржд ржжржХрзНрж╖рждрж╛**: ржЖрж░ржУ ржнрж╛рж▓ ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕-ржкрзНрж░рждрж┐-ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░ ржЕржирзБржкрж╛рждрзЗрж░ ржЬржирзНржп ржзрж╛рж░рж╛ржмрж╛рж╣рж┐ржХ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи
- **ржмрж░рзНржзрж┐ржд ржорж╛рж▓рзНржЯрж┐ржорзЛржбрж╛рж▓ рж╕ржХрзНрж╖ржорждрж╛**: ржЖрж░ржУ ржЙржирзНржиржд ржнрж┐рж╢ржи, ржЕржбрж┐ржУ, ржПржмржВ ржЯрзЗржХрзНрж╕ржЯ ржкрзНрж░рж╕рзЗрж╕рж┐ржВ ржЗржирзНржЯрж┐ржЧрзНрж░рзЗрж╢ржи
- **ржЙржирзНржиржд ржпрзБржХрзНрждрж┐**: ржЙржирзНржиржд ржЪрж┐ржирзНрждрж╛рж░ ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛ ржПржмржВ ржмрж╣рзБ-ржзрж╛ржк рж╕ржорж╕рзНржпрж╛ рж╕ржорж╛ржзрж╛ржирзЗрж░ рж╕ржХрзНрж╖ржорждрж╛
- **ржЙржирзНржиржд ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ ржЯрзБрж▓**: ржмрж┐ржнрж┐ржирзНржи ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ ржкрж░рж┐рж╕рзНржерж┐рждрж┐рж░ ржЬржирзНржп ржЙржирзНржиржд ржлрзНрж░рзЗржоржУржпрж╝рж╛рж░рзНржХ ржПржмржВ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи ржЯрзБрж▓
- **ржХржорж┐ржЙржирж┐ржЯрж┐ ржмрзГржжрзНржзрж┐**: ржЯрзБрж▓, ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи, ржПржмржВ рж╕ржорзНржкрзНрж░ржжрж╛ржпрж╝рзЗрж░ ржЕржмржжрж╛ржиржЧрзБрж▓рж┐рж░ ржмрж┐рж╕рзНрждрзГржд ржЗржХрзЛрж╕рж┐рж╕рзНржЯрзЗржо

### ржкрж░ржмрж░рзНрждрзА ржкржжржХрзНрж╖рзЗржк

ржЖржкржирж┐ ржпржжрж┐ ржПржХржЯрж┐ ржЪрзНржпрж╛ржЯржмржЯ рждрзИрж░рж┐ ржХрж░ржЫрзЗржи, рж╢рж┐ржХрзНрж╖рж╛ржорзВрж▓ржХ ржЯрзБрж▓ ржбрзЗржнрзЗрж▓ржк ржХрж░ржЫрзЗржи, ржХрзЛржбрж┐ржВ рж╕рж╣ржХрж╛рж░рзА рждрзИрж░рж┐ ржХрж░ржЫрзЗржи, ржмрж╛ ржмрж╣рзБржнрж╛рж╖рж┐ржХ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржирзЗ ржХрж╛ржЬ ржХрж░ржЫрзЗржи, Qwen ржкрж░рж┐ржмрж╛рж░ рж╕рзНржХрзЗрж▓ржпрзЛржЧрзНржп рж╕ржорж╛ржзрж╛ржи ржкрзНрж░ржжрж╛ржи ржХрж░рзЗ ржпрж╛ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА рж╕ржорзНржкрзНрж░ржжрж╛ржпрж╝рзЗрж░ рж╕рж╣рж╛ржпрж╝рждрж╛ ржПржмржВ ржмрж┐рж╕рзНрждрзГржд ржбржХрзБржорзЗржирзНржЯрзЗрж╢ржи рж╕рж╣ ржЖрж╕рзЗред

рж╕рж░рзНржмрж╢рзЗрж╖ ржЖржкржбрзЗржЯ, ржоржбрзЗрж▓ рж░рж┐рж▓рж┐ржЬ, ржПржмржВ ржмрж┐рж╕рзНрждрж╛рж░рж┐ржд ржЯрзЗржХржирж┐ржХрзНржпрж╛рж▓ ржбржХрзБржорзЗржирзНржЯрзЗрж╢ржирзЗрж░ ржЬржирзНржп, Hugging Face-ржП ржЕржлрж┐рж╕рж┐ржпрж╝рж╛рж▓ Qwen рж░рж┐ржкрзЛржЬрж┐ржЯрж░рж┐ржЧрзБрж▓рж┐ ржжрзЗржЦрзБржи ржПржмржВ рж╕ржХрзНрж░рж┐ржпрж╝ рж╕ржорзНржкрзНрж░ржжрж╛ржпрж╝рзЗрж░ ржЖрж▓рзЛржЪржирж╛ ржПржмржВ ржЙржжрж╛рж╣рж░ржгржЧрзБрж▓рж┐ ржЕржирзНржмрзЗрж╖ржг ржХрж░рзБржиред

AI ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯрзЗрж░ ржнржмрж┐рж╖рзНржпрзО ржПржоржи ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕ржпрзЛржЧрзНржп, рж╕рзНржмржЪрзНржЫ, ржПржмржВ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА ржЯрзБрж▓ржЧрзБрж▓рж┐рждрзЗ ржирж┐рж╣рж┐ржд ржпрж╛ рж╕ржорж╕рзНржд рж╕рзЗржХрзНржЯрж░ ржПржмржВ рж╕рзНржХрзЗрж▓рзЗ ржЙржжрзНржнрж╛ржмржи рж╕ржХрзНрж╖ржо ржХрж░рзЗред Qwen ржкрж░рж┐ржмрж╛рж░ ржПржЗ ржжрзГрж╖рзНржЯрж┐ржнржЩрзНржЧрж┐рж░ ржЙржжрж╛рж╣рж░ржг ржжрзЗржпрж╝, рж╕ржВрж╕рзНржерж╛ ржПржмржВ ржбрзЗржнрзЗрж▓ржкрж╛рж░ржжрзЗрж░ ржкрж░ржмрж░рзНрждрзА ржкрзНрж░ржЬржирзНржорзЗрж░ AI-ржЪрж╛рж▓рж┐ржд ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи рждрзИрж░рж┐ ржХрж░рж╛рж░ ржЬржирзНржп ржнрж┐рждрзНрждрж┐ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗред

## ржЕрждрж┐рж░рж┐ржХрзНржд рж╕ржорзНржкржж

- **ржЕржлрж┐рж╕рж┐ржпрж╝рж╛рж▓ ржбржХрзБржорзЗржирзНржЯрзЗрж╢ржи**: [Qwen Documentation](https://qwen.readthedocs.io/)
- **ржоржбрзЗрж▓ рж╣рж╛ржм**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **ржЯрзЗржХржирж┐ржХрзНржпрж╛рж▓ ржкрзЗржкрж╛рж░**: [Qwen Research Publications](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **ржХржорж┐ржЙржирж┐ржЯрж┐**: [GitHub Discussions and Issues](https://github.com/QwenLM/)
- **ModelScope ржкрзНрж▓рзНржпрж╛ржЯржлрж░рзНржо**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## рж╢рзЗржЦрж╛рж░ ржлрж▓рж╛ржлрж▓

ржПржЗ ржоржбрж┐ржЙрж▓ рж╕ржорзНржкржирзНржи ржХрж░рж╛рж░ ржкрж░рзЗ ржЖржкржирж┐ рж╕ржХрзНрж╖ржо рж╣ржмрзЗржи:

1. Qwen ржоржбрзЗрж▓ ржкрж░рж┐ржмрж╛рж░рзЗрж░ ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░рж╛рж▓ рж╕рзБржмрж┐ржзрж╛ ржПржмржВ ржПрж░ ржУржкрзЗржи-рж╕рзЛрж░рзНрж╕ ржкржжрзНржзрждрж┐рж░ ржмрзНржпрж╛ржЦрзНржпрж╛ ржХрж░рждрзЗ
2. ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝рждрж╛ ржПржмржВ рж░рж┐рж╕рзЛрж░рзНрж╕ рж╕рзАржорж╛ржмржжрзНржзрждрж╛рж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржХрж░рзЗ рж╕ржарж┐ржХ Qwen ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯ ржирж┐рж░рзНржмрж╛ржЪржи ржХрж░рждрзЗ
3. ржмрж┐ржнрж┐ржирзНржи ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ ржкрж░рж┐рж╕рзНржерж┐рждрж┐рждрзЗ Qwen ржоржбрзЗрж▓ ржмрж╛рж╕рзНрждржмрж╛ржпрж╝ржи ржХрж░рждрзЗ ржПржмржВ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬржб ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржирзЗрж░ рж╕рж╛ржерзЗ ржХрж╛ржЬ ржХрж░рждрзЗ
4. Qwen ржоржбрзЗрж▓рзЗрж░ ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржЙржирзНржиржд ржХрж░рждрзЗ ржХрзЛржпрж╝рж╛ржирзНржЯрж╛ржЗржЬрзЗрж╢ржи ржПржмржВ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи ржХрзМрж╢рж▓ ржкрзНрж░ржпрж╝рзЛржЧ ржХрж░рждрзЗ
5. Qwen ржкрж░рж┐ржмрж╛рж░рзЗрж░ ржоржзрзНржпрзЗ ржоржбрзЗрж▓ рж╕рж╛ржЗржЬ, ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕, ржПржмржВ рж╕ржХрзНрж╖ржорждрж╛рж░ ржоржзрзНржпрзЗ ржЯрзНрж░рзЗржб-ржЕржл ржорзВрж▓рзНржпрж╛ржпрж╝ржи ржХрж░рждрзЗ

## ржкрж░ржмрж░рзНрждрзА ржХрзА

- [03: Gemma Family Fundamentals](03.GemmaFamily.md)

---

**ржЕрж╕рзНржмрзАржХрзГрждрж┐**:  
ржПржЗ ржиржерж┐ржЯрж┐ AI ржЕржирзБржмрж╛ржж ржкрж░рж┐рж╖рзЗржмрж╛ [Co-op Translator](https://github.com/Azure/co-op-translator) ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржЕржирзБржмрж╛ржж ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗред ржЖржорж░рж╛ ржпржерж╛рж╕рж╛ржзрзНржп рж╕ржарж┐ржХрждрж╛рж░ ржЬржирзНржп ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рж┐, рждржмрзЗ ржЕржирзБржЧрзНрж░рж╣ ржХрж░рзЗ ржоржирзЗ рж░рж╛ржЦржмрзЗржи ржпрзЗ рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ ржЕржирзБржмрж╛ржжрзЗ рждрзНрж░рзБржЯрж┐ ржмрж╛ ржЕрж╕ржЩрзНржЧрждрж┐ ржерж╛ржХрждрзЗ ржкрж╛рж░рзЗред ржорзВрж▓ ржнрж╛рж╖рж╛ржпрж╝ ржерж╛ржХрж╛ ржиржерж┐ржЯрж┐ржХрзЗ ржкрзНрж░рж╛ржорж╛ржгрж┐ржХ ржЙрзОрж╕ рж╣рж┐рж╕рзЗржмрзЗ ржмрж┐ржмрзЗржЪржирж╛ ржХрж░рж╛ ржЙржЪрж┐рждред ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг рждржерзНржпрзЗрж░ ржЬржирзНржп, ржкрзЗрж╢рж╛ржжрж╛рж░ ржорж╛ржиржм ржЕржирзБржмрж╛ржж рж╕рзБржкрж╛рж░рж┐рж╢ ржХрж░рж╛ рж╣ржпрж╝ред ржПржЗ ржЕржирзБржмрж╛ржж ржмрзНржпржмрж╣рж╛рж░рзЗрж░ ржлрж▓рзЗ ржХрзЛржирзЛ ржнрзБрж▓ ржмрзЛржЭрж╛ржмрзБржЭрж┐ ржмрж╛ ржнрзБрж▓ ржмрзНржпрж╛ржЦрзНржпрж╛ рж╣рж▓рзЗ ржЖржорж░рж╛ ржжрж╛ржпрж╝ржмржжрзНржз ржерж╛ржХржм ржирж╛ред