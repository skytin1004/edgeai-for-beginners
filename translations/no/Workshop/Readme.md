<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "48d0fb38be925084a6ebd957d4b045e5",
  "translation_date": "2025-10-09T14:34:36+00:00",
  "source_file": "Workshop/Readme.md",
  "language_code": "no"
}
-->
# EdgeAI for Nybegynnere - Workshop

> **Praktisk l√¶ringssti for √• bygge produksjonsklare Edge AI-applikasjoner**
>
> Mestre lokal AI-distribusjon med Microsoft Foundry Local, fra f√∏rste chat-komplettering til multi-agent orkestrering i 6 progressive √∏kter.

---

## üéØ Introduksjon

Velkommen til **EdgeAI for Nybegynnere Workshop** - din praktiske guide til √• bygge intelligente applikasjoner som kj√∏rer helt p√• lokal maskinvare. Denne workshopen forvandler teoretiske Edge AI-konsepter til praktiske ferdigheter gjennom gradvis utfordrende √∏velser med Microsoft Foundry Local og Small Language Models (SLMs).

### Hvorfor denne workshopen?

**Edge AI-revolusjonen er her**

Organisasjoner over hele verden skifter fra skybasert AI til edge computing av tre viktige grunner:

1. **Personvern og samsvar** - Behandle sensitiv data lokalt uten √• sende den til skyen (HIPAA, GDPR, finansielle reguleringer)
2. **Ytelse** - Fjern nettverksforsinkelser (50-500ms lokalt vs 500-2000ms sky-rundtur)
3. **Kostnadskontroll** - Fjern kostnader per token for API og skaler uten skyutgifter

**Men Edge AI er annerledes**

√Ö kj√∏re AI lokalt krever nye ferdigheter:
- Modellvalg og optimalisering for ressursbegrensninger
- Lokal tjenestestyring og maskinvareakselerasjon
- Prompt engineering for mindre modeller
- Produksjonsdistribusjonsm√∏nstre for edge-enheter

**Denne workshopen gir deg disse ferdighetene**

I 6 fokuserte √∏kter (~3 timer totalt) vil du g√• fra "Hello World" til √• distribuere produksjonsklare multi-agent systemer - alt kj√∏rer lokalt p√• din maskin.

---

## üìö L√¶ringsm√•l

Ved √• fullf√∏re denne workshopen vil du kunne:

### Kjernekompetanser
1. **Distribuere og administrere lokale AI-tjenester**
   - Installere og konfigurere Microsoft Foundry Local
   - Velge passende modeller for edge-distribusjon
   - Administrere modellens livssyklus (nedlasting, lasting, caching)
   - Overv√•ke ressursbruk og optimalisere ytelse

2. **Bygge AI-drevne applikasjoner**
   - Implementere OpenAI-kompatible chat-kompletteringer lokalt
   - Designe effektive prompts for Small Language Models
   - H√•ndtere streaming-responser for bedre brukeropplevelse
   - Integrere lokale modeller i eksisterende applikasjoner

3. **Skape RAG (Retrieval Augmented Generation)-systemer**
   - Bygge semantisk s√∏k med embeddings
   - Forankre LLM-responser i domenespesifikk kunnskap
   - Evaluere RAG-kvalitet med industristandardmetrikker
   - Skalere fra prototype til produksjon

4. **Optimalisere modellens ytelse**
   - Benchmarke flere modeller for ditt brukstilfelle
   - M√•le forsinkelse, gjennomstr√∏mning og tid til f√∏rste token
   - Velge optimale modeller basert p√• hastighet/kvalitet-avveininger
   - Sammenligne SLM vs LLM-avveininger i reelle scenarioer

5. **Orkestrere multi-agent systemer**
   - Designe spesialiserte agenter for ulike oppgaver
   - Implementere agentminne og kontekststyring
   - Koordinere agenter i komplekse arbeidsflyter
   - Rute foresp√∏rsler intelligent mellom flere modeller

6. **Distribuere produksjonsklare l√∏sninger**
   - Implementere feilh√•ndtering og retry-logikk
   - Overv√•ke tokenbruk og systemressurser
   - Bygge skalerbare arkitekturer med modell-som-verkt√∏y-m√∏nstre
   - Planlegge migreringsveier fra edge til hybrid (edge + sky)

---

## üéì L√¶ringsresultater

### Hva du vil bygge

Ved slutten av denne workshopen vil du ha laget:

| √òkt | Leveranse | Demonstrerte ferdigheter |
|-----|-----------|--------------------------|
| **1** | Chat-applikasjon med streaming | Tjenesteoppsett, grunnleggende kompletteringer, streaming UX |
| **2** | RAG-system med evaluering | Embeddings, semantisk s√∏k, kvalitetsmetrikker |
| **3** | Multi-modell benchmark-suite | Ytelsesm√•ling, modell-sammenligning |
| **4** | SLM vs LLM-komparator | Avveiningsanalyse, optimaliseringsstrategier |
| **5** | Multi-agent orkestrator | Agentdesign, minneh√•ndtering, koordinering |
| **6** | Intelligent rutingssystem | Intensjonsdeteksjon, modellvalg, skalerbarhet |

### Kompetansematrise

| Ferdighetsniv√• | √òkt 1-2 | √òkt 3-4 | √òkt 5-6 |
|----------------|---------|---------|---------|
| **Nybegynner** | ‚úÖ Oppsett & grunnleggende | ‚ö†Ô∏è Utfordrende | ‚ùå For avansert |
| **Mellomniv√•** | ‚úÖ Rask gjennomgang | ‚úÖ Kjerneoppl√¶ring | ‚ö†Ô∏è Strekkm√•l |
| **Avansert** | ‚úÖ Lett gjennomgang | ‚úÖ Finpussing | ‚úÖ Produksjonsm√∏nstre |

### Karriereklare ferdigheter

**Etter denne workshopen vil du v√¶re klar til √•:**

‚úÖ **Bygge personvern-fokuserte applikasjoner**
- Helseapplikasjoner som h√•ndterer PHI/PII lokalt
- Finansielle tjenester med samsvarskrav
- Regjeringssystemer med krav til datasuverenitet

‚úÖ **Optimalisere for edge-milj√∏er**
- IoT-enheter med begrensede ressurser
- Offline-f√∏rst mobilapplikasjoner
- Lav-latens sanntidssystemer

‚úÖ **Designe intelligente arkitekturer**
- Multi-agent systemer for komplekse arbeidsflyter
- Hybrid edge-sky distribusjoner
- Kostnadsoptimalisert AI-infrastruktur

‚úÖ **Lede Edge AI-initiativer**
- Evaluere Edge AI-tilpasning for prosjekter
- Velge passende modeller og rammeverk
- Arkitektere skalerbare lokale AI-l√∏sninger

---

## üó∫Ô∏è Workshop-struktur

### √òktoversikt (6 √∏kter √ó 30 minutter = 3 timer)

| √òkt | Tema | Fokus | Varighet |
|-----|------|-------|----------|
| **1** | Komme i gang med Foundry Local | Installere, validere, f√∏rste kompletteringer | 30 min |
| **2** | Bygge AI-l√∏sninger med RAG | Prompt engineering, embeddings, evaluering | 30 min |
| **3** | √Öpne kildekode-modeller | Modelloppdagelse, benchmarking, valg | 30 min |
| **4** | Banebrytende modeller | SLM vs LLM, optimalisering, rammeverk | 30 min |
| **5** | AI-drevne agenter | Agentdesign, orkestrering, minne | 30 min |
| **6** | Modeller som verkt√∏y | Ruting, kjeding, skaleringsstrategier | 30 min |

---

## üöÄ Hurtigstart

### Forutsetninger

**Systemkrav:**
- **OS**: Windows 10/11, macOS 11+, eller Linux (Ubuntu 20.04+)
- **RAM**: Minimum 8GB, anbefalt 16GB+
- **Lagring**: 10GB+ ledig plass for modeller
- **CPU**: Moderne prosessor med AVX2-st√∏tte
- **GPU** (valgfritt): CUDA-kompatibel eller Qualcomm NPU for akselerasjon

**Programvarekrav:**
- **Python 3.8+** ([Last ned](https://www.python.org/downloads/))
- **Microsoft Foundry Local** ([Installasjonsveiledning](../../../Workshop))
- **Git** ([Last ned](https://git-scm.com/downloads))
- **Visual Studio Code** (anbefalt) ([Last ned](https://code.visualstudio.com/))

### Oppsett i 3 steg

#### 1. Installer Foundry Local

**Windows:**
```powershell
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**Verifiser installasjon:**
```bash
foundry --version
foundry service status
```

#### 2. Klon repository & installer avhengigheter

```bash
# Clone repository
git clone https://github.com/microsoft/edgeai-for-beginners.git
cd edgeai-for-beginners/Workshop

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.\.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

#### 3. Kj√∏r ditt f√∏rste eksempel

```bash
# Start Foundry Local and load a model
foundry model run phi-4-mini

# Run the chat bootstrap sample
cd samples/session01
python chat_bootstrap.py "What is edge AI?"
```

**‚úÖ Suksess!** Du b√∏r se en streaming-respons om edge AI.

---

## üì¶ Workshop-ressurser

### Python-eksempler

Progressive praktiske eksempler som demonstrerer hvert konsept:

| √òkt | Eksempel | Beskrivelse | Kj√∏retid |
|-----|----------|-------------|----------|
| 1 | [`chat_bootstrap.py`](../../../Workshop/samples/session01/chat_bootstrap.py) | Grunnleggende & streaming chat | ~30s |
| 2 | [`rag_pipeline.py`](../../../Workshop/samples/session02/rag_pipeline.py) | RAG med embeddings | ~45s |
| 2 | [`rag_eval_ragas.py`](../../../Workshop/samples/session02/rag_eval_ragas.py) | RAG-kvalitetsevaluering | ~60s |
| 3 | [`benchmark_oss_models.py`](../../../Workshop/samples/session03/benchmark_oss_models.py) | Multi-modell benchmarking | ~2-3m |
| 4 | [`model_compare.py`](../../../Workshop/samples/session04/model_compare.py) | SLM vs LLM-sammenligning | ~45s |
| 5 | [`agents_orchestrator.py`](../../../Workshop/samples/session05/agents_orchestrator.py) | Multi-agent system | ~60s |
| 6 | [`models_router.py`](../../../Workshop/samples/session06/models_router.py) | Intensjonsbasert ruting | ~45s |
| 6 | [`models_pipeline.py`](../../../Workshop/samples/session06/models_pipeline.py) | Multi-trinns pipeline | ~60s |

### Jupyter Notebooks

Interaktiv utforskning med forklaringer og visualiseringer:

| √òkt | Notebook | Beskrivelse | Vanskelighetsgrad |
|-----|----------|-------------|-------------------|
| 1 | [`session01_chat_bootstrap.ipynb`](./notebooks/session01_chat_bootstrap.ipynb) | Chat-grunnleggende & streaming | ‚≠ê Nybegynner |
| 2 | [`session02_rag_pipeline.ipynb`](./notebooks/session02_rag_pipeline.ipynb) | Bygg RAG-system | ‚≠ê‚≠ê Mellomniv√• |
| 2 | [`session02_rag_eval_ragas.ipynb`](./notebooks/session02_rag_eval_ragas.ipynb) | Evaluere RAG-kvalitet | ‚≠ê‚≠ê Mellomniv√• |
| 3 | [`session03_benchmark_oss_models.ipynb`](./notebooks/session03_benchmark_oss_models.ipynb) | Modellbenchmarking | ‚≠ê‚≠ê Mellomniv√• |
| 4 | [`session04_model_compare.ipynb`](./notebooks/session04_model_compare.ipynb) | Modell-sammenligning | ‚≠ê‚≠ê Mellomniv√• |
| 5 | [`session05_agents_orchestrator.ipynb`](./notebooks/session05_agents_orchestrator.ipynb) | Agent-orkestrering | ‚≠ê‚≠ê‚≠ê Avansert |
| 6 | [`session06_models_router.ipynb`](./notebooks/session06_models_router.ipynb) | Intensjonsruting | ‚≠ê‚≠ê‚≠ê Avansert |
| 6 | [`session06_models_pipeline.ipynb`](./notebooks/session06_models_pipeline.ipynb) | Pipeline-orkestrering | ‚≠ê‚≠ê‚≠ê Avansert |

### Dokumentasjon

Omfattende guider og referanser:

| Dokument | Beskrivelse | Bruk n√•r |
|----------|-------------|----------|
| [QUICK_START.md](./QUICK_START.md) | Hurtigoppsett-guide | Starter fra bunnen |
| [QUICK_REFERENCE.md](./QUICK_REFERENCE.md) | Kommando- & API-hurtigreferanse | Trenger raske svar |
| [FOUNDRY_SDK_QUICKREF.md](./FOUNDRY_SDK_QUICKREF.md) | SDK-m√∏nstre & eksempler | Skriver kode |
| [ENV_CONFIGURATION.md](./ENV_CONFIGURATION.md) | Veiledning for milj√∏variabler | Konfigurerer eksempler |
| [SAMPLES_UPDATE_SUMMARY.md](./SAMPLES_UPDATE_SUMMARY.md) | Seneste forbedringer i eksempler | Forst√• endringer |
| [SDK_MIGRATION_NOTES.md](./SDK_MIGRATION_NOTES.md) | Migreringsveiledning | Oppgraderer kode |
| [notebooks/TROUBLESHOOTING.md](./notebooks/TROUBLESHOOTING.md) | Vanlige problemer & l√∏sninger | Feils√∏ker problemer |

---

## üéì L√¶ringssti-anbefalinger

### For nybegynnere (3-4 timer)
1. ‚úÖ √òkt 1: Komme i gang (fokus p√• oppsett og grunnleggende chat)
2. ‚úÖ √òkt 2: RAG-grunnleggende (hopp over evaluering i starten)
3. ‚úÖ √òkt 3: Enkel benchmarking (kun 2 modeller)
4. ‚è≠Ô∏è Hopp over √∏kter 4-6 forel√∏pig
5. üîÑ G√• tilbake til √∏kter 4-6 etter √• ha bygget din f√∏rste applikasjon

### For utviklere p√• mellomniv√• (3 timer)
1. ‚ö° √òkt 1: Rask validering av oppsett
2. ‚úÖ √òkt 2: Fullf√∏r RAG-pipeline med evaluering
3. ‚úÖ √òkt 3: Full benchmark-suite
4. ‚úÖ √òkt 4: Modelloptimalisering
5. ‚úÖ √òkter 5-6: Fokus p√• arkitekturm√∏nstre

### For avanserte ut√∏vere (2-3 timer)
1. ‚ö° √òkter 1-3: Rask gjennomgang og validering
2. ‚úÖ √òkt 4: Dypdykk i optimalisering
3. ‚úÖ √òkt 5: Multi-agent arkitektur
4. ‚úÖ √òkt 6: Produksjonsm√∏nstre og skalering
5. üöÄ Utvid: Bygg tilpasset rutingslogikk og hybrid distribusjoner

---

## Workshop-√∏ktpakke (Fokuserte 30-minutters laboratorier)

Hvis du f√∏lger det konsentrerte 6-√∏kt workshop-formatet, bruk disse dedikerte guidene (hver tilsvarer og utfyller de bredere modul-dokumentene ovenfor):

| Workshop-√∏kt | Veiledning | Kjernefokus |
|--------------|------------|-------------|
| 1 | [Session01-GettingStartedFoundryLocal](./Session01-GettingStartedFoundryLocal.md) | Installere, validere, kj√∏re phi & GPT-OSS-20B, akselerasjon |
| 2 | [Session02-BuildAISolutionsRAG](./Session02-BuildAISolutionsRAG.md) | Prompt engineering, RAG-m√∏nstre, CSV & dokumentforankring, migrering |
| 3 | [Session03-OpenSourceModels](./Session03-OpenSourceModels.md) | Hugging Face-integrasjon, benchmarking, modellvalg |
| 4 | [Session04-CuttingEdgeModels](./Session04-CuttingEdgeModels.md) | SLM vs LLM, WebGPU, Chainlit RAG, ONNX-akselerasjon |
| 5 | [Session05-AIPoweredAgents](./Session05-AIPoweredAgents.md) | Agentroller, minne, verkt√∏y, orkestrering |
| 6 | [Session06-ModelsAsTools](./Session06-ModelsAsTools.md) | Ruting, kjeding, skaleringsvei til Azure |
Hver sesjonsfil inkluderer: sammendrag, l√¶ringsm√•l, 30-minutters demooppsett, startprosjekt, valideringssjekkliste, feils√∏king og referanser til den offisielle Foundry Local Python SDK.

### Eksempelskript

Installer workshop-avhengigheter (Windows):

```powershell
cd Workshop
py -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt
```

macOS / Linux:

```bash
cd Workshop
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Hvis Foundry Local-tjenesten kj√∏rer p√• en annen (Windows) maskin eller VM fra macOS, eksporter endepunktet:

```bash
export FOUNDRY_LOCAL_ENDPOINT=http://<windows-host>:5273/v1
```

| Sesjon | Skript(er) | Beskrivelse |
|--------|------------|-------------|
| 1 | `samples/session01/chat_bootstrap.py` | Bootstrap-tjeneste & streaming-chat |
| 2 | `samples/session02/rag_pipeline.py` | Minimal RAG (in-memory embeddings) |
|   | `samples/session02/rag_eval_ragas.py` | RAG-evaluering med ragas-metrikker |
| 3 | `samples/session03/benchmark_oss_models.py` | Multi-modell latens- og gjennomstr√∏mningsbenchmarking |
| 4 | `samples/session04/model_compare.py` | SLM vs LLM-sammenligning (latens & eksempelutdata) |
| 5 | `samples/session05/agents_orchestrator.py` | To-agent forsknings- ‚Üí redaksjonell pipeline |
| 6 | `samples/session06/models_router.py` | Intent-basert rutingdemo |
|   | `samples/session06/models_pipeline.py` | Multi-trinns plan/utf√∏r/forbedre-kjede |

### Milj√∏variabler (Felles for eksempler)

| Variabel | Form√•l | Eksempel |
|----------|--------|----------|
| `FOUNDRY_LOCAL_ALIAS` | Standard enkeltmodell-alias for grunnleggende eksempler | `phi-4-mini` |
| `SLM_ALIAS` / `LLM_ALIAS` | Eksplisitt SLM vs st√∏rre modell for sammenligning | `phi-4-mini` / `gpt-oss-20b` |
| `BENCH_MODELS` | Kommaliste over aliaser som skal benchmarkes | `qwen2.5-0.5b,gemma-2-2b,mistral-7b` |
| `BENCH_ROUNDS` | Benchmark-repetisjoner per modell | `3` |
| `BENCH_PROMPT` | Prompt brukt i benchmarking | `Explain retrieval augmented generation briefly.` |
| `EMBED_MODEL` | Sentence-transformers embedding-modell | `sentence-transformers/all-MiniLM-L6-v2` |
| `RAG_QUESTION` | Overstyr testsp√∏rsm√•l for RAG-pipeline | `Why use RAG with local inference?` |
| `AGENT_QUESTION` | Overstyr sp√∏rsm√•l for agent-pipeline | `Explain why edge AI matters for compliance.` |
| `AGENT_MODEL_PRIMARY` | Modellalias for forskningsagent | `phi-4-mini` |
| `AGENT_MODEL_EDITOR` | Modellalias for redakt√∏ragent (kan v√¶re forskjellig) | `gpt-oss-20b` |
| `SHOW_USAGE` | N√•r `1`, skriver ut tokenbruk per fullf√∏ring | `1` |
| `RETRY_ON_FAIL` | N√•r `1`, pr√∏v p√• nytt √©n gang ved midlertidige chatfeil | `1` |
| `RETRY_BACKOFF` | Sekunder √• vente f√∏r ny fors√∏k | `1.0` |

Hvis en variabel ikke er satt, faller skriptene tilbake til fornuftige standardverdier. For enkeltmodell-demoer trenger du vanligvis bare `FOUNDRY_LOCAL_ALIAS`.

### Verkt√∏ysmodul

Alle eksempler deler n√• en hjelper `samples/workshop_utils.py` som tilbyr:

* Hurtigbufret `FoundryLocalManager` + OpenAI-klientoppretting
* `chat_once()` hjelper med valgfri retry + brukerapportering
* Enkel tokenbruk-rapportering (aktiver via `SHOW_USAGE=1`)

Dette reduserer duplisering og fremhever beste praksis for effektiv lokal modellorkestrering.

## Valgfrie forbedringer (p√• tvers av sesjoner)

| Tema | Forbedring | Sesjoner | Milj√∏ / Toggle |
|------|------------|----------|----------------|
| Determinisme | Fast temperatur + stabile promptsett | 1‚Äì6 | Sett `temperature=0`, `top_p=1` |
| Synlighet av tokenbruk | Konsistent kostnad/effektivitet undervisning | 1‚Äì6 | `SHOW_USAGE=1` |
| Streaming av f√∏rste token | Opplevd latensmetrisk | 1,3,4,6 | `BENCH_STREAM=1` (benchmark) |
| Retry-robusthet | H√•ndterer midlertidig kaldstart | Alle | `RETRY_ON_FAIL=1` + `RETRY_BACKOFF` |
| Multi-modell agenter | Heterogen rolle-spesialisering | 5 | `AGENT_MODEL_PRIMARY`, `AGENT_MODEL_EDITOR` |
| Adaptiv ruting | Intent + kostnadsheuristikk | 6 | Utvid router med eskaleringslogikk |
| Vektorminne | Langsiktig semantisk gjenkalling | 2,5,6 | Integrer FAISS/Chroma embedding-indeks |
| Sporingsutdrag | Revisjon & evaluering | 2,5,6 | Legg til JSON-linjer per trinn |
| Kvalitetsrubrikker | Kvalitativ sporing | 3‚Äì6 | Sekund√¶re scoringsprompter |
| R√∏yktester | Rask validering f√∏r workshop | Alle | `python Workshop/tests/smoke.py` |

### Deterministisk rask start

```powershell
set FOUNDRY_LOCAL_ALIAS=phi-4-mini
set SHOW_USAGE=1
python Workshop\tests\smoke.py
```

Forvent stabile tokenantall p√• tvers av gjentatte identiske input.

### RAG-evaluering (Sesjon 2)

Bruk `rag_eval_ragas.py` for √• beregne svarrelevans, troverdighet og kontekstpresisjon p√• et lite syntetisk datasett:

```powershell
python samples/session02/rag_eval_ragas.py
```

Utvid ved √• levere en st√∏rre JSONL med sp√∏rsm√•l, kontekster og fasitsvar, og konverter deretter til et Hugging Face `Dataset`.

## CLI Kommando N√∏yaktighetsvedlegg

Workshopen bruker kun dokumenterte / stabile Foundry Local CLI-kommandoer.

### Refererte stabile kommandoer

| Kategori | Kommando | Form√•l |
|----------|----------|--------|
| Kjerne | `foundry --version` | Vis installert versjon |
| Kjerne | `foundry init` | Initialiser konfigurasjon |
| Tjeneste | `foundry service start` | Start lokal tjeneste (hvis ikke automatisk) |
| Tjeneste | `foundry status` | Vis tjenestestatus |
| Modeller | `foundry model list` | List katalog / tilgjengelige modeller |
| Modeller | `foundry model download <alias>` | Last ned modellvekter til hurtigbuffer |
| Modeller | `foundry model run <alias>` | Start (last) en modell lokalt; kombiner med `--prompt` for √©n-gangs |
| Modeller | `foundry model unload <alias>` / `foundry model stop <alias>` | Fjern en modell fra minnet (hvis st√∏ttet) |
| Hurtigbuffer | `foundry cache list` | List hurtigbufrede (nedlastede) modeller |
| System | `foundry system info` | Maskinvare- og akselerasjonskapabiliteter snapshot |
| System | `foundry system gpu-info` | GPU-diagnostisk info |
| Konfigurasjon | `foundry config list` | Vis gjeldende konfigurasjonsverdier |
| Konfigurasjon | `foundry config set <key> <value>` | Oppdater konfigurasjon |

### √ân-gangs promptm√∏nster

I stedet for en utg√•tt `model chat` underkommando, bruk:

```powershell
foundry model run <alias> --prompt "Your question here"
```

Dette utf√∏rer en enkelt prompt/svar-syklus og avslutter deretter.

### Fjernede / unng√•tte m√∏nstre

| Utg√•tt / Udokumentert | Erstatning / Veiledning |
|------------------------|-------------------------|
| `foundry model chat <model> "..."` | `foundry model run <model> --prompt "..."` |
| `foundry model list --running` | Bruk vanlig `foundry model list` + nylig aktivitet / logger |
| `foundry model list --cached` | `foundry cache list` |
| `foundry model stats <model>` | Bruk benchmark Python-skript + OS-verkt√∏y (Task Manager / `nvidia-smi`) |
| `foundry model benchmark ...` | `samples/session03/benchmark_oss_models.py` |

### Benchmarking & Telemetri

- Latens, p95, tokens/sek: `samples/session03/benchmark_oss_models.py`
- F√∏rste-token latens (streaming): sett `BENCH_STREAM=1`
- Ressursbruk: OS-monitorer (Task Manager, Activity Monitor, `nvidia-smi`) + `foundry system info`.

Etter hvert som nye CLI-telemetrikommandoer stabiliseres, kan de enkelt integreres med minimale endringer i sesjonsmarkeringer.

### Automatisk lint-vakt

En automatisk linter forhindrer gjeninnf√∏ring av utg√•tte CLI-m√∏nstre inne i avgrensede kodeblokker i markdown-filer:

Skript: `Workshop/scripts/lint_markdown_cli.py`

Utg√•tte m√∏nstre blokkeres inne i kodegjerder.

Anbefalte erstatninger:
| Utg√•tt | Erstatning |
|--------|------------|
| `foundry model chat <a> "..."` | `foundry model run <a> --prompt "..."` |
| `model list --running` | `model list` |
| `model list --cached` | `cache list` |
| `model stats` | Benchmark-skript + systemverkt√∏y |
| `model benchmark` | `samples/session03/benchmark_oss_models.py` |
| `model list --available` | `model list` |

Kj√∏r lokalt:
```powershell
python Workshop\scripts\lint_markdown_cli.py --verbose
```

GitHub Action: `.github/workflows/markdown-cli-lint.yml` kj√∏rer ved hver push & PR.

Valgfri pre-commit hook:
```bash
echo "python Workshop/scripts/lint_markdown_cli.py" > .git/hooks/pre-commit
chmod +x .git/hooks/pre-commit
```

## Rask CLI ‚Üí SDK Migrasjonstabell

| Oppgave | CLI √ân-liner | SDK (Python) Ekvivalent | Notater |
|---------|--------------|-------------------------|---------|
| Kj√∏r en modell √©n gang (prompt) | `foundry model run phi-4-mini --prompt "Hello"` | `manager=FoundryLocalManager("phi-4-mini"); client=OpenAI(base_url=manager.endpoint, api_key=manager.api_key or "not-needed"); client.chat.completions.create(model=manager.get_model_info("phi-4-mini").id, messages=[{"role":"user","content":"Hello"}])` | SDK starter tjeneste & caching automatisk |
| Last ned (hurtigbuffer) modell | `foundry model download qwen2.5-0.5b` | `FoundryLocalManager("qwen2.5-0.5b")  # triggers download/load` | Manager velger beste variant hvis aliaset peker til flere bygg |
| List katalog | `foundry model list` | `# bruk manager for hvert alias eller oppretthold kjent liste` | CLI samler; SDK for √∏yeblikket per-alias-instansiering |
| List hurtigbufrede modeller | `foundry cache list` | `manager.list_cached_models()` | Etter manager-init (hvilket som helst alias) |
| Aktiver GPU-akselerasjon | `foundry config set compute.onnx.enable_gpu true` | `# CLI handling; SDK antar at konfigurasjonen allerede er brukt` | Konfigurasjon er en ekstern sideeffekt |
| Hent endepunkt-URL | (implisitt) | `manager.endpoint` | Brukes til √• opprette OpenAI-kompatibel klient |
| Varm opp en modell | `foundry model run <alias>` deretter f√∏rste prompt | `chat_once(alias, messages=[...])` (verkt√∏y) | Verkt√∏y h√•ndterer initial kald latens oppvarming |
| M√•l latens | `python benchmark_oss_models.py` | `import benchmark_oss_models` (eller nytt eksporteringsskript) | Foretrekk skript for konsistente metrikker |
| Stopp / fjern modell | `foundry model unload <alias>` | (Ikke eksponert ‚Äì start tjeneste / prosess p√• nytt) | Vanligvis ikke n√∏dvendig for workshopflyt |
| Hent tokenbruk | (vis utdata) | `resp.usage.total_tokens` | Tilbys hvis backend returnerer bruksobjekt |

## Benchmark Markdown Eksport

Bruk skriptet `Workshop/scripts/export_benchmark_markdown.py` for √• kj√∏re en fersk benchmark (samme logikk som `samples/session03/benchmark_oss_models.py`) og generere en GitHub-vennlig Markdown-tabell pluss r√• JSON.

### Eksempel

```powershell
python Workshop\scripts\export_benchmark_markdown.py --models "qwen2.5-0.5b,gemma-2-2b,mistral-7b" --prompt "Explain retrieval augmented generation briefly." --rounds 3 --output benchmark_report.md
```

Genererte filer:
| Fil | Innhold |
|-----|---------|
| `benchmark_report.md` | Markdown-tabell + tolkningshint |
| `benchmark_report.json` | R√• metrikker-array (for diffing / trendsporing) |

Sett `BENCH_STREAM=1` i milj√∏et for √• inkludere f√∏rste-token latens hvis st√∏ttet.

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, v√¶r oppmerksom p√• at automatiske oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.