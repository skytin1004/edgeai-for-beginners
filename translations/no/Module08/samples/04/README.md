<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "562ac0eae12d808c9f45fbb77eb5c84f",
  "translation_date": "2025-09-24T23:23:00+00:00",
  "source_file": "Module08/samples/04/README.md",
  "language_code": "no"
}
-->
# Eksempel 04: Produksjonsklare Chat-applikasjoner med Chainlit

Et omfattende eksempel som viser flere tilnÃ¦rminger til Ã¥ bygge produksjonsklare chat-applikasjoner ved bruk av Microsoft Foundry Local, med moderne webgrensesnitt, strÃ¸mmende svar og avanserte nettleserteknologier.

## Hva er inkludert

- **ðŸš€ Chainlit Chat App** (`app.py`): Produksjonsklar chat-applikasjon med strÃ¸mming
- **ðŸŒ WebGPU Demo** (`webgpu-demo/`): Nettleserbasert AI-inferens med maskinvareakselerasjon
- **ðŸŽ¨ Open WebUI-integrasjon** (`open-webui-guide.md`): Profesjonelt ChatGPT-lignende grensesnitt
- **ðŸ“š Pedagogisk Notebook** (`chainlit_app.ipynb`): Interaktive lÃ¦ringsmaterialer

## Kom i gang

### 1. Chainlit Chat-applikasjon

```cmd
# Navigate to Module08 directory
cd Module08

# Start your model
foundry model run phi-4-mini

# Run Chainlit app (using port 8080 to avoid conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Ã…pnes pÃ¥: `http://localhost:8080`

### 2. WebGPU Nettleser-demo

```cmd
# Navigate to WebGPU demo
cd Module08\samples\04\webgpu-demo

# Serve the demo
python -m http.server 5173
```

Ã…pnes pÃ¥: `http://localhost:5173`

### 3. Open WebUI-oppsett

```cmd
# Run Open WebUI with Docker
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

Ã…pnes pÃ¥: `http://localhost:3000`

## ArkitekturmÃ¸nstre

### Lokal vs sky beslutningsmatrise

| Scenario | Anbefaling | Begrunnelse |
|----------|------------|-------------|
| **PersonvernfÃ¸lsomme data** | ðŸ  Lokal (Foundry) | Data forlater aldri enheten |
| **Kompleks resonnering** | â˜ï¸ Sky (Azure OpenAI) | Tilgang til stÃ¸rre modeller |
| **Sanntidschat** | ðŸ  Lokal (Foundry) | Lavere ventetid, raskere svar |
| **Dokumentanalyse** | ðŸ”„ Hybrid | Lokal for utvinning, sky for analyse |
| **Kodegenerering** | ðŸ  Lokal (Foundry) | Personvern + spesialiserte modeller |
| **Forskningsoppgaver** | â˜ï¸ Sky (Azure OpenAI) | Bred kunnskapsbase nÃ¸dvendig |

### Teknologisammenligning

| Teknologi | BruksomrÃ¥de | Fordeler | Ulemper |
|-----------|-------------|----------|---------|
| **Chainlit** | Python-utviklere, rask prototyping | Enkel oppsett, stÃ¸tte for strÃ¸mming | Kun Python |
| **WebGPU** | Maksimalt personvern, offline-scenarier | Nettleserbasert, ingen server nÃ¸dvendig | Begrenset modellstÃ¸rrelse |
| **Open WebUI** | Produksjonsdistribusjon, team | Profesjonelt grensesnitt, brukerstyring | Krever Docker |

## Forutsetninger

- **Foundry Local**: Installert og kjÃ¸rer ([Last ned](https://aka.ms/foundry-local-installer))
- **Python**: 3.10+ med virtuelt miljÃ¸
- **Modell**: Minst Ã©n lastet (`foundry model run phi-4-mini`)
- **Nettleser**: Chrome/Edge med WebGPU-stÃ¸tte for demoer
- **Docker**: For Open WebUI (valgfritt)

## Installasjon og oppsett

### 1. Python-miljÃ¸oppsett

```cmd
# Navigate to Module08 directory
cd Module08

# Create and activate virtual environment
py -m venv .venv
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Foundry Local-oppsett

```cmd
# Verify Foundry Local installation
foundry --version

# Start the service
foundry service start

# Load a model
foundry model run phi-4-mini

# Verify model is running
foundry service ps
```

## Eksempelapplikasjoner

### Chainlit Chat-applikasjon

**Funksjoner:**
- ðŸš€ **SanntidsstrÃ¸mming**: Tokens vises mens de genereres
- ðŸ›¡ï¸ **Robust feilbehandling**: GrasiÃ¸s nedgradering og gjenoppretting
- ðŸŽ¨ **Moderne grensesnitt**: Profesjonelt chat-grensesnitt rett ut av boksen
- ðŸ”§ **Fleksibel konfigurasjon**: MiljÃ¸variabler og automatisk deteksjon
- ðŸ“± **Responsivt design**: Fungerer pÃ¥ bÃ¥de desktop og mobile enheter

**Kom i gang:**
```cmd
# Run with default settings (recommended)
chainlit run samples\04\app.py -w --port 8080

# Use specific model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Manual endpoint configuration
set BASE_URL=http://localhost:51211
set API_KEY=your-api-key
chainlit run samples\04\app.py -w --port 8080
```

### WebGPU Nettleser-demo

**Funksjoner:**
- ðŸŒ **Nettleserbasert AI**: Ingen server nÃ¸dvendig, kjÃ¸rer helt i nettleseren
- âš¡ **WebGPU-akselerasjon**: Maskinvareakselerasjon nÃ¥r tilgjengelig
- ðŸ”’ **Maksimalt personvern**: Ingen data forlater enheten din
- ðŸŽ¯ **Ingen installasjon**: Fungerer i enhver kompatibel nettleser
- ðŸ”„ **GrasiÃ¸s fallback**: Faller tilbake til CPU hvis WebGPU ikke er tilgjengelig

**KjÃ¸ring:**
```cmd
cd samples\04\webgpu-demo
python -m http.server 5173
# Open http://localhost:5173
```

### Open WebUI-integrasjon

**Funksjoner:**
- ðŸŽ¨ **ChatGPT-lignende grensesnitt**: Profesjonelt, kjent UI
- ðŸ‘¥ **StÃ¸tte for flere brukere**: Brukerkontoer og samtalehistorikk
- ðŸ“ **Filbehandling**: Last opp og analyser dokumenter
- ðŸ”„ **Modellbytte**: Enkel bytting mellom ulike modeller
- ðŸ³ **Docker-distribusjon**: Produksjonsklar containerbasert oppsett

**Raskt oppsett:**
```cmd
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

## Konfigurasjonsreferanse

### MiljÃ¸variabler

| Variabel | Beskrivelse | Standard | Eksempel |
|----------|-------------|----------|----------|
| `MODEL` | Modellalias som skal brukes | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | Foundry Local-endepunkt | Automatisk oppdaget | `http://localhost:51211` |
| `API_KEY` | API-nÃ¸kkel (valgfritt for lokal) | `""` | `your-api-key` |

## FeilsÃ¸king

### Vanlige problemer

**Chainlit-applikasjon:**

1. **Tjenesten er ikke tilgjengelig:**
   ```cmd
   # Check Foundry Local status
   foundry service status
   foundry service ps
   
   # Validate API endpoint (note: port 51211)
   curl http://localhost:51211/v1/models
   ```

2. **Portkonflikter:**
   ```cmd
   # Check what's using port 8080
   netstat -ano | findstr :8080
   
   # Use different port if needed
   chainlit run samples\04\app.py -w --port 3000
   ```

3. **Problemer med Python-miljÃ¸:**
   ```cmd
   # Verify correct interpreter in VS Code
   # Ctrl+Shift+P â†’ Python: Select Interpreter
   # Choose: Module08/.venv/Scripts/python.exe
   
   # Reinstall dependencies
   pip install -r requirements.txt
   ```

**WebGPU-demo:**

1. **WebGPU ikke stÃ¸ttet:**
   - Oppdater til Chrome/Edge 113+
   - Aktiver WebGPU: `chrome://flags/#enable-unsafe-webgpu`
   - Sjekk GPU-status: `chrome://gpu`
   - Demoen faller automatisk tilbake til CPU

2. **Feil ved modelllasting:**
   - SÃ¸rg for internettforbindelse for modellnedlasting
   - Sjekk nettleserkonsollen for CORS-feil
   - Verifiser at du serverer via HTTP (ikke file://)

**Open WebUI:**

1. **Tilkobling nektet:**
   ```cmd
   # Check Docker is running
   docker --version
   
   # Check container status
   docker ps | findstr open-webui
   
   # View container logs
   docker logs open-webui
   ```

2. **Modeller vises ikke:**
   ```cmd
   # Verify Foundry Local endpoint
   curl http://localhost:51211/v1/models
   
   # Restart Open WebUI
   docker restart open-webui
   ```

### Valideringssjekkliste

```cmd
# âœ… 1. Foundry Local Setup
foundry --version                    # Should show version
foundry service status               # Should show "running"
foundry model list                   # Should show loaded models
curl http://localhost:51211/v1/models  # Should return JSON

# âœ… 2. Python Environment  
python --version                     # Should be 3.10+
pip list | findstr chainlit         # Should show chainlit package
pip list | findstr openai           # Should show openai package

# âœ… 3. Application Testing
chainlit run samples\04\app.py -w --port 8080  # Should open browser
# Test WebGPU demo at localhost:5173
# Test Open WebUI at localhost:3000
```

## Avansert bruk

### Ytelsesoptimalisering

**Chainlit:**
- Bruk strÃ¸mming for bedre opplevd ytelse
- Implementer tilkoblingspooling for hÃ¸y samtidighet
- Cache modellresponser for gjentatte forespÃ¸rsler
- OvervÃ¥k minnebruk med store samtalehistorikker

**WebGPU:**
- Bruk WebGPU for maksimal personvern og hastighet
- Implementer modellkvantisering for mindre modeller
- Bruk Web Workers for bakgrunnsprosessering
- Cache kompilerte modeller i nettleserlagring

**Open WebUI:**
- Bruk persistente volumer for samtalehistorikk
- Konfigurer ressursgrenser for Docker-container
- Implementer backup-strategier for brukerdata
- Sett opp reverse proxy for SSL-terminering

### IntegrasjonsmÃ¸nstre

**Hybrid Lokal/Sky:**
```python
# Route based on complexity and privacy requirements
async def intelligent_routing(prompt: str, metadata: dict):
    if metadata.get("contains_pii"):
        return await foundry_local_completion(prompt)  # Privacy-sensitive
    elif len(prompt.split()) > 200:
        return await azure_openai_completion(prompt)   # Complex reasoning
    else:
        return await foundry_local_completion(prompt)  # Default local
```

**Multi-modal Pipeline:**
```python
# Combine different AI capabilities
async def analyze_document(file_path: str):
    # 1. OCR with WebGPU (browser-based)
    text = await webgpu_ocr(file_path)
    
    # 2. Analysis with Foundry Local (private)
    summary = await foundry_local_analyze(text)
    
    # 3. Enhancement with cloud (if needed)
    if summary.confidence < 0.8:
        summary = await azure_openai_enhance(summary)
    
    return summary
```

## Produksjonsdistribusjon

### Sikkerhetsvurderinger

- **API-nÃ¸kler**: Bruk miljÃ¸variabler, aldri hardkod
- **Nettverk**: Bruk HTTPS i produksjon, vurder VPN for teamtilgang
- **Tilgangskontroll**: Implementer autentisering for Open WebUI
- **Datapersonvern**: GjennomgÃ¥ hva som forblir lokalt vs. sendes til skyen
- **Oppdateringer**: Hold Foundry Local og containere oppdatert

### OvervÃ¥king og vedlikehold

- **Helsetester**: Implementer endepunktovervÃ¥king
- **Logging**: Sentraliser logger fra alle komponenter
- **Metrikker**: Spor responstider, feilrater, ressursbruk
- **Backup**: Regelmessig backup av samtaledata og konfigurasjoner

## Referanser og ressurser

### Dokumentasjon
- [Chainlit Dokumentasjon](https://docs.chainlit.io/) - Komplett rammeverksguide
- [Foundry Local Dokumentasjon](https://learn.microsoft.com/azure/ai-foundry/foundry-local/) - Offisiell Microsoft-dokumentasjon
- [ONNX Runtime Web](https://onnxruntime.ai/docs/get-started/with-javascript/web.html) - WebGPU-integrasjon
- [Open WebUI Dokumentasjon](https://docs.openwebui.com/) - Avansert konfigurasjon

### Eksempelfiler
- [`app.py`](../../../../../Module08/samples/04/app.py) - Produksjonsklar Chainlit-applikasjon
- [`chainlit_app.ipynb`](./chainlit_app.ipynb) - Pedagogisk notebook
- [`webgpu-demo/`](../../../../../Module08/samples/04/webgpu-demo) - Nettleserbasert AI-inferens
- [`open-webui-guide.md`](./open-webui-guide.md) - Komplett Open WebUI-oppsett

### Relaterte eksempler
- [Sesjon 4 Dokumentasjon](../../04.CuttingEdgeModels.md) - Komplett sesjonsguide
- [Foundry Local Eksempler](https://github.com/microsoft/foundry-local/tree/main/samples) - Offisielle eksempler

---

