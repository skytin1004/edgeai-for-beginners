<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-17T15:54:52+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "pl"
}
-->
# Sekcja 4: WdroÅ¼enie - Implementacja Modelu Gotowego do Produkcji

## PrzeglÄ…d

Ten kompleksowy poradnik poprowadzi CiÄ™ przez caÅ‚y proces wdraÅ¼ania dostrojonych modeli kwantyzowanych za pomocÄ… Foundry Local. OmÃ³wimy konwersjÄ™ modelu, optymalizacjÄ™ kwantyzacji oraz konfiguracjÄ™ wdroÅ¼enia od poczÄ…tku do koÅ„ca.

## Wymagania wstÄ™pne

Przed rozpoczÄ™ciem upewnij siÄ™, Å¼e masz:

- âœ… Dostrojony model w formacie onnx gotowy do wdroÅ¼enia
- âœ… Komputer z systemem Windows lub Mac
- âœ… Python 3.10 lub nowszy
- âœ… Co najmniej 8GB dostÄ™pnej pamiÄ™ci RAM
- âœ… Zainstalowany Foundry Local na Twoim systemie

## CzÄ™Å›Ä‡ 1: Konfiguracja Å›rodowiska

### Instalacja wymaganych narzÄ™dzi

OtwÃ³rz terminal (Command Prompt na Windows, Terminal na Mac) i uruchom nastÄ™pujÄ…ce polecenia w kolejnoÅ›ci:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

âš ï¸ **WaÅ¼na uwaga**: Potrzebujesz rÃ³wnieÅ¼ CMake w wersji 3.31 lub nowszej, ktÃ³ry moÅ¼na pobraÄ‡ ze strony [cmake.org](https://cmake.org/download/).

## CzÄ™Å›Ä‡ 2: Konwersja i kwantyzacja modelu

### WybÃ³r odpowiedniego formatu

Dla dostrojonych maÅ‚ych modeli jÄ™zykowych zalecamy uÅ¼ycie formatu **ONNX**, poniewaÅ¼ oferuje:

- ğŸš€ LepszÄ… optymalizacjÄ™ wydajnoÅ›ci
- ğŸ”§ NiezaleÅ¼noÅ›Ä‡ od sprzÄ™tu
- ğŸ­ MoÅ¼liwoÅ›ci gotowe do produkcji
- ğŸ“± KompatybilnoÅ›Ä‡ miÄ™dzyplatformowa

### Metoda 1: Konwersja jednym poleceniem (zalecana)

UÅ¼yj nastÄ™pujÄ…cego polecenia, aby bezpoÅ›rednio przekonwertowaÄ‡ dostrojony model:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**WyjaÅ›nienie parametrÃ³w:**
- `--model_name_or_path`: ÅšcieÅ¼ka do dostrojonego modelu
- `--device cpu`: UÅ¼ycie CPU do optymalizacji
- `--precision int4`: UÅ¼ycie kwantyzacji INT4 (redukcja rozmiaru o okoÅ‚o 75%)
- `--output_path`: ÅšcieÅ¼ka wyjÅ›ciowa dla przekonwertowanego modelu

### Metoda 2: PodejÅ›cie z plikiem konfiguracyjnym (zaawansowani uÅ¼ytkownicy)

UtwÃ³rz plik konfiguracyjny o nazwie `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

NastÄ™pnie uruchom:

```bash
olive run --config ./finetuned_conversion_config.json
```

### PorÃ³wnanie opcji kwantyzacji

| Precyzja | Rozmiar pliku | SzybkoÅ›Ä‡ inferencji | JakoÅ›Ä‡ modelu | Zalecane zastosowanie |
|----------|---------------|---------------------|---------------|-----------------------|
| FP16     | Podstawowy Ã— 0.5 | Szybka | Najlepsza | Zaawansowany sprzÄ™t |
| INT8     | Podstawowy Ã— 0.25 | Bardzo szybka | Dobra | ZrÃ³wnowaÅ¼ony wybÃ³r |
| INT4     | Podstawowy Ã— 0.125 | Najszybsza | Akceptowalna | Ograniczone zasoby |

ğŸ’¡ **Rekomendacja**: Zacznij od kwantyzacji INT4 przy pierwszym wdroÅ¼eniu. JeÅ›li jakoÅ›Ä‡ nie bÄ™dzie zadowalajÄ…ca, sprÃ³buj INT8 lub FP16.

## CzÄ™Å›Ä‡ 3: Konfiguracja wdroÅ¼enia w Foundry Local

### Tworzenie konfiguracji modelu

PrzejdÅº do katalogu modeli Foundry Local:

```bash
foundry cache cd ./models/
```

UtwÃ³rz strukturÄ™ katalogÃ³w dla swojego modelu:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

UtwÃ³rz plik konfiguracyjny `inference_model.json` w katalogu modelu:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Szablony konfiguracji specyficzne dla modelu

#### Dla modeli z serii Qwen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## CzÄ™Å›Ä‡ 4: Testowanie i optymalizacja modelu

### Weryfikacja instalacji modelu

SprawdÅº, czy Foundry Local rozpoznaje TwÃ³j model:

```bash
foundry cache ls
```

PowinieneÅ› zobaczyÄ‡ `your-finetuned-model-int4` na liÅ›cie.

### RozpoczÄ™cie testowania modelu

```bash
foundry model run your-finetuned-model-int4
```

### Benchmarking wydajnoÅ›ci

Monitoruj kluczowe metryki podczas testowania:

1. **Czas odpowiedzi**: Mierz Å›redni czas na odpowiedÅº
2. **ZuÅ¼ycie pamiÄ™ci**: Monitoruj wykorzystanie RAM
3. **Wykorzystanie CPU**: Sprawdzaj obciÄ…Å¼enie procesora
4. **JakoÅ›Ä‡ wyjÅ›cia**: OceÅ„ trafnoÅ›Ä‡ i spÃ³jnoÅ›Ä‡ odpowiedzi

### Lista kontrolna walidacji jakoÅ›ci

- âœ… Model odpowiednio reaguje na zapytania z dostrojonej domeny
- âœ… Format odpowiedzi odpowiada oczekiwanej strukturze wyjÅ›ciowej
- âœ… Brak wyciekÃ³w pamiÄ™ci podczas dÅ‚ugotrwaÅ‚ego uÅ¼ytkowania
- âœ… SpÃ³jna wydajnoÅ›Ä‡ dla rÃ³Å¼nych dÅ‚ugoÅ›ci wejÅ›Ä‡
- âœ… Poprawne obsÅ‚ugiwanie przypadkÃ³w brzegowych i nieprawidÅ‚owych danych wejÅ›ciowych

## Podsumowanie

Gratulacje! PomyÅ›lnie ukoÅ„czyÅ‚eÅ›:

- âœ… KonwersjÄ™ formatu dostrojonego modelu
- âœ… OptymalizacjÄ™ kwantyzacji modelu
- âœ… KonfiguracjÄ™ wdroÅ¼enia w Foundry Local
- âœ… Strojenie wydajnoÅ›ci i rozwiÄ…zywanie problemÃ³w

---

**ZastrzeÅ¼enie**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ staramy siÄ™ zapewniÄ‡ dokÅ‚adnoÅ›Ä‡, prosimy pamiÄ™taÄ‡, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego jÄ™zyku ÅºrÃ³dÅ‚owym powinien byÄ‡ uznawany za autorytatywne ÅºrÃ³dÅ‚o. W przypadku informacji krytycznych zaleca siÄ™ skorzystanie z profesjonalnego tÅ‚umaczenia przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z uÅ¼ycia tego tÅ‚umaczenia.