<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "be25052ac4c842765e7f6f7eb4d7dcc5",
  "translation_date": "2025-10-20T09:45:35+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "pl"
}
-->
# Sekcja 1: Podstawy EdgeAI

EdgeAI to nowy sposÃ³b wdraÅ¼ania sztucznej inteligencji, ktÃ³ry przenosi jej moÅ¼liwoÅ›ci bezpoÅ›rednio na urzÄ…dzenia brzegowe, zamiast polegaÄ‡ wyÅ‚Ä…cznie na przetwarzaniu w chmurze. WaÅ¼ne jest zrozumienie, jak EdgeAI umoÅ¼liwia lokalne przetwarzanie AI na urzÄ…dzeniach o ograniczonych zasobach, jednoczeÅ›nie utrzymujÄ…c odpowiedniÄ… wydajnoÅ›Ä‡ i rozwiÄ…zujÄ…c problemy takie jak prywatnoÅ›Ä‡, opÃ³Åºnienia czy dziaÅ‚anie offline.

## Wprowadzenie

W tej lekcji zgÅ‚Ä™bimy temat EdgeAI i jego podstawowe koncepcje. OmÃ³wimy tradycyjny paradygmat obliczeÅ„ AI, wyzwania zwiÄ…zane z przetwarzaniem na brzegu sieci, kluczowe technologie umoÅ¼liwiajÄ…ce EdgeAI oraz praktyczne zastosowania w rÃ³Å¼nych branÅ¼ach.

## Cele nauki

Po zakoÅ„czeniu tej lekcji bÄ™dziesz w stanie:

- ZrozumieÄ‡ rÃ³Å¼nicÄ™ miÄ™dzy tradycyjnym podejÅ›ciem AI opartym na chmurze a podejÅ›ciem EdgeAI.
- ZidentyfikowaÄ‡ kluczowe technologie umoÅ¼liwiajÄ…ce przetwarzanie AI na urzÄ…dzeniach brzegowych.
- RozpoznaÄ‡ korzyÅ›ci i ograniczenia wdroÅ¼eÅ„ EdgeAI.
- ZastosowaÄ‡ wiedzÄ™ o EdgeAI w rzeczywistych scenariuszach i przypadkach uÅ¼ycia.

## Zrozumienie tradycyjnego paradygmatu obliczeÅ„ AI

Tradycyjnie aplikacje generatywnej AI opierajÄ… siÄ™ na infrastrukturze obliczeniowej o wysokiej wydajnoÅ›ci, aby skutecznie uruchamiaÄ‡ duÅ¼e modele jÄ™zykowe (LLM). Organizacje zazwyczaj wdraÅ¼ajÄ… te modele na klastrach GPU w Å›rodowiskach chmurowych, uzyskujÄ…c dostÄ™p do ich moÅ¼liwoÅ›ci za poÅ›rednictwem interfejsÃ³w API.

Ten scentralizowany model sprawdza siÄ™ w wielu aplikacjach, ale ma wady w scenariuszach przetwarzania na brzegu sieci. Tradycyjne podejÅ›cie polega na przesyÅ‚aniu zapytaÅ„ uÅ¼ytkownika do zdalnych serwerÃ³w, przetwarzaniu ich za pomocÄ… wydajnego sprzÄ™tu i zwracaniu wynikÃ³w przez internet. ChociaÅ¼ metoda ta zapewnia dostÄ™p do najnowoczeÅ›niejszych modeli, tworzy zaleÅ¼noÅ›ci od Å‚Ä…cznoÅ›ci internetowej, wprowadza opÃ³Åºnienia i rodzi obawy dotyczÄ…ce prywatnoÅ›ci, gdy wraÅ¼liwe dane muszÄ… byÄ‡ przesyÅ‚ane na zewnÄ™trzne serwery.

Istnieje kilka kluczowych koncepcji, ktÃ³re naleÅ¼y zrozumieÄ‡, pracujÄ…c z tradycyjnymi paradygmatami obliczeÅ„ AI, mianowicie:

- **â˜ï¸ Przetwarzanie w chmurze**: Modele AI dziaÅ‚ajÄ… na potÄ™Å¼nej infrastrukturze serwerowej z duÅ¼ymi zasobami obliczeniowymi.
- **ğŸ”Œ DostÄ™p przez API**: Aplikacje uzyskujÄ… dostÄ™p do moÅ¼liwoÅ›ci AI za poÅ›rednictwem zdalnych wywoÅ‚aÅ„ API, zamiast lokalnego przetwarzania.
- **ğŸ›ï¸ Scentralizowane zarzÄ…dzanie modelami**: Modele sÄ… utrzymywane i aktualizowane centralnie, co zapewnia spÃ³jnoÅ›Ä‡, ale wymaga Å‚Ä…cznoÅ›ci sieciowej.
- **ğŸ“ˆ SkalowalnoÅ›Ä‡ zasobÃ³w**: Infrastruktura chmurowa moÅ¼e dynamicznie skalowaÄ‡ siÄ™, aby sprostaÄ‡ zmiennym wymaganiom obliczeniowym.

## Wyzwania przetwarzania na brzegu sieci

UrzÄ…dzenia brzegowe, takie jak laptopy, telefony komÃ³rkowe czy urzÄ…dzenia Internetu Rzeczy (IoT) jak Raspberry Pi i NVIDIA Orin Nano, majÄ… unikalne ograniczenia obliczeniowe. Zazwyczaj dysponujÄ… one mniejszÄ… mocÄ… obliczeniowÄ…, pamiÄ™ciÄ… i zasobami energetycznymi w porÃ³wnaniu z infrastrukturÄ… centrÃ³w danych.

Uruchamianie tradycyjnych LLM na takich urzÄ…dzeniach byÅ‚o historycznie trudne z powodu tych ograniczeÅ„ sprzÄ™towych. Jednak potrzeba przetwarzania AI na brzegu sieci staje siÄ™ coraz bardziej istotna w rÃ³Å¼nych scenariuszach. WeÅºmy pod uwagÄ™ sytuacje, w ktÃ³rych Å‚Ä…cznoÅ›Ä‡ internetowa jest zawodna lub niedostÄ™pna, takie jak odlegÅ‚e miejsca przemysÅ‚owe, pojazdy w ruchu czy obszary o sÅ‚abym zasiÄ™gu sieci. Dodatkowo aplikacje wymagajÄ…ce wysokich standardÃ³w bezpieczeÅ„stwa, takie jak urzÄ…dzenia medyczne, systemy finansowe czy aplikacje rzÄ…dowe, mogÄ… potrzebowaÄ‡ lokalnego przetwarzania danych w celu zachowania prywatnoÅ›ci i zgodnoÅ›ci z przepisami.

### Kluczowe ograniczenia przetwarzania na brzegu sieci

Åšrodowiska przetwarzania na brzegu sieci napotykajÄ… kilka fundamentalnych ograniczeÅ„, ktÃ³rych nie doÅ›wiadczajÄ… tradycyjne rozwiÄ…zania AI oparte na chmurze:

- **Ograniczona moc obliczeniowa**: UrzÄ…dzenia brzegowe zazwyczaj majÄ… mniej rdzeni CPU i niÅ¼sze taktowanie w porÃ³wnaniu z serwerami.
- **Ograniczenia pamiÄ™ci**: DostÄ™pna pamiÄ™Ä‡ RAM i pojemnoÅ›Ä‡ magazynowa sÄ… znacznie mniejsze na urzÄ…dzeniach brzegowych.
- **Ograniczenia energetyczne**: UrzÄ…dzenia zasilane bateriami muszÄ… rÃ³wnowaÅ¼yÄ‡ wydajnoÅ›Ä‡ z zuÅ¼yciem energii, aby dziaÅ‚aÄ‡ przez dÅ‚uÅ¼szy czas.
- **ZarzÄ…dzanie termiczne**: Kompaktowe formy urzÄ…dzeÅ„ ograniczajÄ… moÅ¼liwoÅ›ci chÅ‚odzenia, co wpÅ‚ywa na wydajnoÅ›Ä‡ pod obciÄ…Å¼eniem.

## Czym jest EdgeAI?

### Koncepcja: Definicja Edge AI

Edge AI odnosi siÄ™ do wdraÅ¼ania i wykonywania algorytmÃ³w sztucznej inteligencji bezpoÅ›rednio na urzÄ…dzeniach brzegowychâ€”fizycznym sprzÄ™cie znajdujÄ…cym siÄ™ na "krawÄ™dzi" sieci, blisko miejsca, gdzie dane sÄ… generowane i zbierane. Do tych urzÄ…dzeÅ„ naleÅ¼Ä… smartfony, czujniki IoT, inteligentne kamery, pojazdy autonomiczne, urzÄ…dzenia noszone oraz sprzÄ™t przemysÅ‚owy. W przeciwieÅ„stwie do tradycyjnych systemÃ³w AI, ktÃ³re polegajÄ… na serwerach chmurowych do przetwarzania, Edge AI przenosi inteligencjÄ™ bezpoÅ›rednio do ÅºrÃ³dÅ‚a danych.

W swojej istocie Edge AI polega na decentralizacji przetwarzania AI, przenoszÄ…c je z centralnych centrÃ³w danych i rozpraszajÄ…c w rozlegÅ‚ej sieci urzÄ…dzeÅ„, ktÃ³re tworzÄ… nasz cyfrowy ekosystem. To fundamentalna zmiana w architekturze projektowania i wdraÅ¼ania systemÃ³w AI.

Kluczowe filary koncepcyjne Edge AI obejmujÄ…:

- **Przetwarzanie w pobliÅ¼u**: Obliczenia odbywajÄ… siÄ™ fizycznie blisko miejsca, gdzie dane powstajÄ….
- **Zdecentralizowana inteligencja**: ZdolnoÅ›ci decyzyjne sÄ… rozproszone na wiele urzÄ…dzeÅ„.
- **SuwerennoÅ›Ä‡ danych**: Informacje pozostajÄ… pod lokalnÄ… kontrolÄ…, czÄ™sto nigdy nie opuszczajÄ…c urzÄ…dzenia.
- **Autonomiczne dziaÅ‚anie**: UrzÄ…dzenia mogÄ… dziaÅ‚aÄ‡ inteligentnie bez koniecznoÅ›ci staÅ‚ej Å‚Ä…cznoÅ›ci.
- **Wbudowana inteligencja**: Inteligencja staje siÄ™ integralnÄ… czÄ™Å›ciÄ… codziennych urzÄ…dzeÅ„.

### Wizualizacja architektury Edge AI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TRADITIONAL AI ARCHITECTURE                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Data Transfer  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   API Response   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Edge Devices â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Cloud Servers â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ End Users â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            EDGE AI ARCHITECTURE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Direct Response  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Edge Devices with Embedded AI         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ End Users â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â”‚ Sensors â”‚â”€>â”‚ SLM Inference â”‚â”€>â”‚ Local Action â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI reprezentuje zmianÄ™ paradygmatu w wdraÅ¼aniu sztucznej inteligencji, przenoszÄ…c jej moÅ¼liwoÅ›ci bezpoÅ›rednio na urzÄ…dzenia brzegowe, zamiast polegaÄ‡ wyÅ‚Ä…cznie na przetwarzaniu w chmurze. PodejÅ›cie to umoÅ¼liwia uruchamianie modeli AI lokalnie na urzÄ…dzeniach o ograniczonych zasobach obliczeniowych, zapewniajÄ…c moÅ¼liwoÅ›ci wnioskowania w czasie rzeczywistym bez koniecznoÅ›ci staÅ‚ej Å‚Ä…cznoÅ›ci internetowej.

EdgeAI obejmuje rÃ³Å¼ne technologie i techniki zaprojektowane w celu uczynienia modeli AI bardziej efektywnymi i odpowiednimi do wdroÅ¼enia na urzÄ…dzeniach o ograniczonych zasobach. Celem jest utrzymanie odpowiedniej wydajnoÅ›ci przy jednoczesnym znacznym zmniejszeniu wymagaÅ„ obliczeniowych i pamiÄ™ciowych modeli AI.

Przyjrzyjmy siÄ™ podstawowym podejÅ›ciom umoÅ¼liwiajÄ…cym wdroÅ¼enia EdgeAI na rÃ³Å¼nych typach urzÄ…dzeÅ„ i w rÃ³Å¼nych przypadkach uÅ¼ycia.

### Podstawowe zasady EdgeAI

EdgeAI opiera siÄ™ na kilku fundamentalnych zasadach, ktÃ³re odrÃ³Å¼niajÄ… go od tradycyjnego AI opartego na chmurze:

- **Lokalne przetwarzanie**: Wnioskowanie AI odbywa siÄ™ bezpoÅ›rednio na urzÄ…dzeniu brzegowym, bez potrzeby zewnÄ™trznej Å‚Ä…cznoÅ›ci.
- **Optymalizacja zasobÃ³w**: Modele sÄ… specjalnie zoptymalizowane pod kÄ…tem ograniczeÅ„ sprzÄ™towych docelowych urzÄ…dzeÅ„.
- **WydajnoÅ›Ä‡ w czasie rzeczywistym**: Przetwarzanie odbywa siÄ™ z minimalnym opÃ³Åºnieniem dla aplikacji wymagajÄ…cych szybkiej reakcji.
- **PrywatnoÅ›Ä‡ w standardzie**: WraÅ¼liwe dane pozostajÄ… na urzÄ…dzeniu, co zwiÄ™ksza bezpieczeÅ„stwo i zgodnoÅ›Ä‡ z przepisami.

## Kluczowe technologie umoÅ¼liwiajÄ…ce EdgeAI

### Kwantyzacja modelu

JednÄ… z najwaÅ¼niejszych technik w EdgeAI jest kwantyzacja modelu. Proces ten polega na redukcji precyzji parametrÃ³w modelu, zazwyczaj z 32-bitowych liczb zmiennoprzecinkowych do 8-bitowych liczb caÅ‚kowitych lub jeszcze niÅ¼szych formatÃ³w precyzji. ChociaÅ¼ taka redukcja precyzji moÅ¼e wydawaÄ‡ siÄ™ niepokojÄ…ca, badania wykazaÅ‚y, Å¼e wiele modeli AI moÅ¼e utrzymaÄ‡ swojÄ… wydajnoÅ›Ä‡ nawet przy znacznie zmniejszonej precyzji.

Kwantyzacja dziaÅ‚a poprzez mapowanie zakresu wartoÅ›ci zmiennoprzecinkowych na mniejszy zestaw wartoÅ›ci dyskretnych. Na przykÅ‚ad, zamiast uÅ¼ywaÄ‡ 32 bitÃ³w do reprezentowania kaÅ¼dego parametru, kwantyzacja moÅ¼e uÅ¼ywaÄ‡ tylko 8 bitÃ³w, co prowadzi do 4-krotnego zmniejszenia wymagaÅ„ pamiÄ™ciowych i czÄ™sto szybszego czasu wnioskowania.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

RÃ³Å¼ne techniki kwantyzacji obejmujÄ…:

- **Kwantyzacja po treningu (PTQ)**: Stosowana po treningu modelu, bez koniecznoÅ›ci ponownego trenowania.
- **Trening uwzglÄ™dniajÄ…cy kwantyzacjÄ™ (QAT)**: UwzglÄ™dnia efekty kwantyzacji podczas treningu dla lepszej dokÅ‚adnoÅ›ci.
- **Dynamiczna kwantyzacja**: Kwantyzuje wagi do int8, ale oblicza aktywacje dynamicznie.
- **Statyczna kwantyzacja**: WstÄ™pnie oblicza wszystkie parametry kwantyzacji zarÃ³wno dla wag, jak i aktywacji.

W przypadku wdroÅ¼eÅ„ EdgeAI wybÃ³r odpowiedniej strategii kwantyzacji zaleÅ¼y od konkretnej architektury modelu, wymagaÅ„ wydajnoÅ›ciowych i moÅ¼liwoÅ›ci sprzÄ™towych docelowego urzÄ…dzenia.

### Kompresja i optymalizacja modelu

OprÃ³cz kwantyzacji, rÃ³Å¼ne techniki kompresji pomagajÄ… zmniejszyÄ‡ rozmiar modelu i wymagania obliczeniowe. NaleÅ¼Ä… do nich:

**Przycinanie**: Technika ta usuwa niepotrzebne poÅ‚Ä…czenia lub neurony z sieci neuronowych. IdentyfikujÄ…c i eliminujÄ…c parametry, ktÃ³re majÄ… niewielki wpÅ‚yw na wydajnoÅ›Ä‡ modelu, przycinanie moÅ¼e znacznie zmniejszyÄ‡ rozmiar modelu, zachowujÄ…c jednoczeÅ›nie dokÅ‚adnoÅ›Ä‡.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Destylacja wiedzy**: PodejÅ›cie to polega na trenowaniu mniejszego modelu "ucznia", aby naÅ›ladowaÅ‚ zachowanie wiÄ™kszego modelu "nauczyciela". Model uczeÅ„ uczy siÄ™ naÅ›ladowaÄ‡ wyniki nauczyciela, czÄ™sto osiÄ…gajÄ…c podobnÄ… wydajnoÅ›Ä‡ przy znacznie mniejszej liczbie parametrÃ³w.

**Optymalizacja architektury modelu**: Naukowcy opracowali specjalne architektury zaprojektowane specjalnie do wdroÅ¼eÅ„ na brzegu, takie jak MobileNets, EfficientNets i inne lekkie architektury, ktÃ³re rÃ³wnowaÅ¼Ä… wydajnoÅ›Ä‡ z efektywnoÅ›ciÄ… obliczeniowÄ….

### MaÅ‚e modele jÄ™zykowe (SLM)

Nowym trendem w EdgeAI jest rozwÃ³j maÅ‚ych modeli jÄ™zykowych (SLM). Modele te sÄ… projektowane od podstaw, aby byÅ‚y kompaktowe i wydajne, jednoczeÅ›nie oferujÄ…c znaczÄ…ce moÅ¼liwoÅ›ci w zakresie jÄ™zyka naturalnego. SLM osiÄ…gajÄ… to dziÄ™ki starannym wyborom architektonicznym, efektywnym technikom treningowym i skoncentrowanemu treningowi na okreÅ›lonych domenach lub zadaniach.

W przeciwieÅ„stwie do tradycyjnych podejÅ›Ä‡, ktÃ³re polegajÄ… na kompresji duÅ¼ych modeli, SLM sÄ… czÄ™sto trenowane na mniejszych zbiorach danych i zoptymalizowanych architekturach zaprojektowanych specjalnie do wdroÅ¼eÅ„ na brzegu. PodejÅ›cie to moÅ¼e prowadziÄ‡ do modeli, ktÃ³re sÄ… nie tylko mniejsze, ale takÅ¼e bardziej wydajne w okreÅ›lonych przypadkach uÅ¼ycia.

## Przyspieszenie sprzÄ™towe dla EdgeAI

Nowoczesne urzÄ…dzenia brzegowe coraz czÄ™Å›ciej zawierajÄ… specjalistyczny sprzÄ™t zaprojektowany do przyspieszania obciÄ…Å¼eÅ„ AI:

### Jednostki przetwarzania neuronowego (NPU)

NPU to specjalistyczne procesory zaprojektowane specjalnie do obliczeÅ„ zwiÄ…zanych z sieciami neuronowymi. UkÅ‚ady te mogÄ… wykonywaÄ‡ zadania wnioskowania AI znacznie bardziej efektywnie niÅ¼ tradycyjne CPU, czÄ™sto przy niÅ¼szym zuÅ¼yciu energii. Wiele nowoczesnych smartfonÃ³w, laptopÃ³w i urzÄ…dzeÅ„ IoT zawiera teraz NPU, aby umoÅ¼liwiÄ‡ przetwarzanie AI na urzÄ…dzeniu.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

UrzÄ…dzenia z NPU obejmujÄ…:

- **Apple**: Chipy serii A i M z Neural Engine
- **Qualcomm**: Procesory Snapdragon z Hexagon DSP/NPU
- **Samsung**: Procesory Exynos z NPU
- **Intel**: Movidius VPUs i akceleratory Habana Labs
- **Microsoft**: Komputery Windows Copilot+ z NPU

### ğŸ® Przyspieszenie GPU

ChociaÅ¼ urzÄ…dzenia brzegowe mogÄ… nie mieÄ‡ potÄ™Å¼nych GPU znajdujÄ…cych siÄ™ w centrach danych, wiele z nich nadal zawiera zintegrowane lub dedykowane GPU, ktÃ³re mogÄ… przyspieszaÄ‡ obciÄ…Å¼enia AI. Nowoczesne mobilne GPU i zintegrowane procesory graficzne mogÄ… zapewniÄ‡ znaczÄ…ce poprawy wydajnoÅ›ci dla zadaÅ„ wnioskowania AI.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### Optymalizacja CPU

Nawet urzÄ…dzenia wyposaÅ¼one wyÅ‚Ä…cznie w CPU mogÄ… korzystaÄ‡ z EdgeAI dziÄ™ki zoptymalizowanym implementacjom. Nowoczesne CPU zawierajÄ… specjalistyczne instrukcje dla obciÄ…Å¼eÅ„ AI, a opracowano oprogramowanie maksymalizujÄ…ce wydajnoÅ›Ä‡ CPU dla wnioskowania AI.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

Dla inÅ¼ynierÃ³w oprogramowania pracujÄ…cych z EdgeAI, zrozumienie, jak wykorzystaÄ‡ te opcje przyspieszenia sprzÄ™towego, jest kluczowe dla optymalizacji wydajnoÅ›ci wnioskowania i efektywnoÅ›ci energetycznej na docelowych urzÄ…dzeniach.

## KorzyÅ›ci EdgeAI

### PrywatnoÅ›Ä‡ i bezpieczeÅ„stwo

JednÄ… z najwaÅ¼niejszych zalet EdgeAI jest zwiÄ™kszona prywatnoÅ›Ä‡ i bezpieczeÅ„stwo. PrzetwarzajÄ…c dane lokalnie na urzÄ…dzeniu, wraÅ¼liwe informacje nigdy nie opuszczajÄ… kontroli uÅ¼ytkownika. Jest to szczegÃ³lnie waÅ¼ne dla aplikacji obsÅ‚ugujÄ…cych dane osobowe, informacje medyczne czy poufne dane biznesowe.

### Zmniejszone opÃ³Åºnienia

EdgeAI eliminuje koniecznoÅ›Ä‡ przesyÅ‚ania danych do zdalnych serwerÃ³w w celu przetwarzania, co znacznie zmniejsza opÃ³Åºnienia. Jest to kluczowe dla aplikacji w czasie rzeczywistym, takich jak pojazdy autonomiczne, automatyzacja przemysÅ‚owa czy interaktywne aplikacje wymagajÄ…ce natychmiastowych odpowiedzi.

### MoÅ¼liwoÅ›Ä‡ dziaÅ‚ania offline

EdgeAI umoÅ¼liwia funkcjonalnoÅ›Ä‡ AI nawet wtedy, gdy Å‚Ä…cznoÅ›Ä‡ internetowa jest niedostÄ™pna. Jest to cenne dla aplikacji w odlegÅ‚ych lokalizacjach, podczas podrÃ³Å¼y lub w sytuacjach, gdy niezawodnoÅ›Ä‡ sieci jest problemem.

### EfektywnoÅ›Ä‡ kosztowa

ZmniejszajÄ…c zaleÅ¼noÅ›Ä‡ od usÅ‚ug AI opartych na chmurze, EdgeAI moÅ¼e pomÃ³c obniÅ¼yÄ‡ koszty operacyjne, szczegÃ³lnie dla aplikacji o duÅ¼ym natÄ™Å¼eniu uÅ¼ytkowania. Organizacje mogÄ… uniknÄ…Ä‡ ciÄ…gÅ‚ych kosztÃ³w API i zmniejszyÄ‡ wymagania dotyczÄ…ce przepustowoÅ›ci.

### SkalowalnoÅ›Ä‡

EdgeAI rozkÅ‚ada obciÄ…Å¼enie obliczeniowe na urzÄ…dzenia brzegowe, zamiast centralizowaÄ‡ je w centrach danych. MoÅ¼e to pomÃ³c zmniejszyÄ‡ koszty infrastruktury i poprawiÄ‡ ogÃ³lnÄ… skalowalnoÅ›Ä‡ systemu.

## Zastosowania EdgeAI

### Inteligentne urzÄ…dzenia i IoT

EdgeAI zasila wiele funkcji inteligentnych urzÄ…dzeÅ„, od asystentÃ³w gÅ‚osowych, ktÃ³re mogÄ… przetwarzaÄ‡ polecenia lokalnie, po inteligentne kamery, ktÃ³re mogÄ… identyfikowaÄ‡ obiekty i osoby bez przesyÅ‚ania wideo do chmury. UrzÄ…dzenia IoT wykorzystujÄ… EdgeAI do predykcyjnej konserwacji, monitorowania
- [02: Zastosowania EdgeAI](02.RealWorldCaseStudies.md)

---

**ZastrzeÅ¼enie**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ staramy siÄ™ zapewniÄ‡ dokÅ‚adnoÅ›Ä‡, prosimy pamiÄ™taÄ‡, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego rodzimym jÄ™zyku powinien byÄ‡ uznawany za autorytatywne ÅºrÃ³dÅ‚o. W przypadku informacji krytycznych zaleca siÄ™ skorzystanie z profesjonalnego tÅ‚umaczenia przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z uÅ¼ycia tego tÅ‚umaczenia.