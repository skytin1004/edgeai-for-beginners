<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-10-01T00:54:48+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "vi"
}
-->
# Bu·ªïi 4: X√¢y d·ª±ng ·ª©ng d·ª•ng chat s·∫£n xu·∫•t v·ªõi Chainlit

## T·ªïng quan

Bu·ªïi h·ªçc n√†y t·∫≠p trung v√†o vi·ªác x√¢y d·ª±ng c√°c ·ª©ng d·ª•ng chat s·∫µn s√†ng cho s·∫£n xu·∫•t b·∫±ng Chainlit v√† Microsoft Foundry Local. B·∫°n s·∫Ω h·ªçc c√°ch t·∫°o giao di·ªán web hi·ªán ƒë·∫°i cho c√°c cu·ªôc tr√≤ chuy·ªán AI, tri·ªÉn khai ph·∫£n h·ªìi theo lu·ªìng v√† tri·ªÉn khai c√°c ·ª©ng d·ª•ng chat m·∫°nh m·∫Ω v·ªõi x·ª≠ l√Ω l·ªói v√† thi·∫øt k·∫ø tr·∫£i nghi·ªám ng∆∞·ªùi d√πng ph√π h·ª£p.

**Nh·ªØng g√¨ b·∫°n s·∫Ω x√¢y d·ª±ng:**
- **·ª®ng d·ª•ng Chat Chainlit**: Giao di·ªán web hi·ªán ƒë·∫°i v·ªõi ph·∫£n h·ªìi theo lu·ªìng
- **Demo WebGPU**: Suy lu·∫≠n tr√™n tr√¨nh duy·ªát cho c√°c ·ª©ng d·ª•ng ∆∞u ti√™n quy·ªÅn ri√™ng t∆∞  
- **T√≠ch h·ª£p Open WebUI**: Giao di·ªán chat chuy√™n nghi·ªáp v·ªõi Foundry Local
- **M√¥ h√¨nh s·∫£n xu·∫•t**: X·ª≠ l√Ω l·ªói, gi√°m s√°t v√† chi·∫øn l∆∞·ª£c tri·ªÉn khai

## M·ª•c ti√™u h·ªçc t·∫≠p

- X√¢y d·ª±ng ·ª©ng d·ª•ng chat s·∫µn s√†ng cho s·∫£n xu·∫•t v·ªõi Chainlit
- Tri·ªÉn khai ph·∫£n h·ªìi theo lu·ªìng ƒë·ªÉ c·∫£i thi·ªán tr·∫£i nghi·ªám ng∆∞·ªùi d√πng
- L√†m ch·ªß c√°c m√¥ h√¨nh t√≠ch h·ª£p SDK Foundry Local
- √Åp d·ª•ng x·ª≠ l√Ω l·ªói ph√π h·ª£p v√† gi·∫£m thi·ªÉu t√°c ƒë·ªông m·ªôt c√°ch nh·∫π nh√†ng
- Tri·ªÉn khai v√† c·∫•u h√¨nh ·ª©ng d·ª•ng chat cho c√°c m√¥i tr∆∞·ªùng kh√°c nhau
- Hi·ªÉu c√°c m·∫´u giao di·ªán web hi·ªán ƒë·∫°i cho AI h·ªôi tho·∫°i

## Y√™u c·∫ßu tr∆∞·ªõc

- **Foundry Local**: ƒê√£ c√†i ƒë·∫∑t v√† ch·∫°y ([H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: Phi√™n b·∫£n 3.10 ho·∫∑c m·ªõi h∆°n v·ªõi kh·∫£ nƒÉng t·∫°o m√¥i tr∆∞·ªùng ·∫£o
- **M√¥ h√¨nh**: √çt nh·∫•t m·ªôt m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i (`foundry model run phi-4-mini`)
- **Tr√¨nh duy·ªát**: Tr√¨nh duy·ªát web hi·ªán ƒë·∫°i h·ªó tr·ª£ WebGPU (Chrome/Edge)
- **Docker**: ƒê·ªÉ t√≠ch h·ª£p Open WebUI (t√πy ch·ªçn)

## Ph·∫ßn 1: Hi·ªÉu v·ªÅ ·ª©ng d·ª•ng chat hi·ªán ƒë·∫°i

### T·ªïng quan ki·∫øn tr√∫c

```
User Browser ‚Üê‚Üí Chainlit UI ‚Üê‚Üí Python Backend ‚Üê‚Üí Foundry Local ‚Üê‚Üí AI Model
      ‚Üì              ‚Üì              ‚Üì              ‚Üì            ‚Üì
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### C√¥ng ngh·ªá ch√≠nh

**M·∫´u SDK Foundry Local:**
- `FoundryLocalManager(alias)`: Qu·∫£n l√Ω d·ªãch v·ª• t·ª± ƒë·ªông
- `manager.endpoint` v√† `manager.api_key`: Chi ti·∫øt k·∫øt n·ªëi
- `manager.get_model_info(alias).id`: Nh·∫≠n di·ªán m√¥ h√¨nh

**Framework Chainlit:**
- `@cl.on_chat_start`: Kh·ªüi t·∫°o phi√™n chat
- `@cl.on_message`: X·ª≠ l√Ω tin nh·∫Øn t·ª´ ng∆∞·ªùi d√πng  
- `cl.Message().stream_token()`: Ph√°t tr·ª±c ti·∫øp theo th·ªùi gian th·ª±c
- T·ª± ƒë·ªông t·∫°o giao di·ªán ng∆∞·ªùi d√πng v√† qu·∫£n l√Ω WebSocket

## Ph·∫ßn 2: Ma tr·∫≠n quy·∫øt ƒë·ªãnh gi·ªØa Local v√† Cloud

### ƒê·∫∑c ƒëi·ªÉm hi·ªáu su·∫•t

| Kh√≠a c·∫°nh | Local (Foundry) | Cloud (Azure OpenAI) |
|-----------|-----------------|----------------------|
| **ƒê·ªô tr·ªÖ** | üöÄ 50-200ms (kh√¥ng m·∫°ng) | ‚è±Ô∏è 200-2000ms (ph·ª• thu·ªôc m·∫°ng) |
| **Quy·ªÅn ri√™ng t∆∞** | üîí D·ªØ li·ªáu kh√¥ng r·ªùi kh·ªèi thi·∫øt b·ªã | ‚ö†Ô∏è D·ªØ li·ªáu g·ª≠i l√™n cloud |
| **Chi ph√≠** | üí∞ Mi·ªÖn ph√≠ sau ph·∫ßn c·ª©ng | üí∏ T√≠nh ph√≠ theo token |
| **Offline** | ‚úÖ Ho·∫°t ƒë·ªông kh√¥ng c·∫ßn internet | ‚ùå Y√™u c·∫ßu internet |
| **K√≠ch th∆∞·ªõc m√¥ h√¨nh** | ‚ö†Ô∏è Gi·ªõi h·∫°n b·ªüi ph·∫ßn c·ª©ng | ‚úÖ Truy c·∫≠p m√¥ h√¨nh l·ªõn nh·∫•t |
| **Kh·∫£ nƒÉng m·ªü r·ªông** | ‚ö†Ô∏è Ph·ª• thu·ªôc ph·∫ßn c·ª©ng | ‚úÖ M·ªü r·ªông kh√¥ng gi·ªõi h·∫°n |

### M·∫´u chi·∫øn l∆∞·ª£c lai

**∆Øu ti√™n Local v·ªõi d·ª± ph√≤ng:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**ƒê·ªãnh tuy·∫øn theo nhi·ªám v·ª•:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Ph·∫ßn 3: M·∫´u 04 - ·ª®ng d·ª•ng Chat Chainlit

### B·∫Øt ƒë·∫ßu nhanh

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

·ª®ng d·ª•ng t·ª± ƒë·ªông m·ªü t·∫°i `http://localhost:8080` v·ªõi giao di·ªán chat hi·ªán ƒë·∫°i.

### Tri·ªÉn khai c·ªët l√µi

·ª®ng d·ª•ng M·∫´u 04 minh h·ªça c√°c m·∫´u s·∫µn s√†ng cho s·∫£n xu·∫•t:

**Kh√°m ph√° d·ªãch v·ª• t·ª± ƒë·ªông:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**X·ª≠ l√Ω chat theo lu·ªìng:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### T√πy ch·ªçn c·∫•u h√¨nh

**Bi·∫øn m√¥i tr∆∞·ªùng:**

| Bi·∫øn | M√¥ t·∫£ | M·∫∑c ƒë·ªãnh | V√≠ d·ª• |
|------|-------|----------|-------|
| `MODEL` | B√≠ danh m√¥ h√¨nh s·ª≠ d·ª•ng | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Endpoint Foundry Local | T·ª± ƒë·ªông ph√°t hi·ªán | `http://localhost:51211` |
| `API_KEY` | API key (t√πy ch·ªçn cho local) | `""` | `your-api-key` |

**S·ª≠ d·ª•ng n√¢ng cao:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Ph·∫ßn 4: T·∫°o v√† s·ª≠ d·ª•ng Jupyter Notebooks

### T·ªïng quan v·ªÅ h·ªó tr·ª£ Notebook

M·∫´u 04 bao g·ªìm m·ªôt notebook Jupyter to√†n di·ªán (`chainlit_app.ipynb`) cung c·∫•p:

- **üìö N·ªôi dung gi√°o d·ª•c**: T√†i li·ªáu h·ªçc t·∫≠p t·ª´ng b∆∞·ªõc
- **üî¨ Kh√°m ph√° t∆∞∆°ng t√°c**: Ch·∫°y v√† th·ª≠ nghi·ªám v·ªõi c√°c √¥ m√£
- **üìä Minh h·ªça tr·ª±c quan**: Bi·ªÉu ƒë·ªì, s∆° ƒë·ªì v√† h√¨nh ·∫£nh ƒë·∫ßu ra
- **üõ†Ô∏è C√¥ng c·ª• ph√°t tri·ªÉn**: Ki·ªÉm tra v√† g·ª° l·ªói

### T·∫°o notebook c·ªßa ri√™ng b·∫°n

#### B∆∞·ªõc 1: Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng Jupyter

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### B∆∞·ªõc 2: T·∫°o notebook m·ªõi

**S·ª≠ d·ª•ng VS Code:**
1. M·ªü VS Code trong th∆∞ m·ª•c Module08
2. T·∫°o t·ªáp m·ªõi v·ªõi ph·∫ßn m·ªü r·ªông `.ipynb`
3. Ch·ªçn kernel "Foundry Local" khi ƒë∆∞·ª£c nh·∫Øc
4. B·∫Øt ƒë·∫ßu th√™m c√°c √¥ v·ªõi n·ªôi dung c·ªßa b·∫°n

**S·ª≠ d·ª•ng Jupyter Lab:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Th·ª±c h√†nh c·∫•u tr√∫c notebook

#### T·ªï ch·ª©c √¥

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("‚úÖ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### V√≠ d·ª• v√† b√†i t·∫≠p t∆∞∆°ng t√°c

#### B√†i t·∫≠p 1: Ki·ªÉm tra c·∫•u h√¨nh client

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\nüß™ Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'‚úÖ Success' if result['status'] == 'ok' else '‚ùå Failed'}")
```

#### B√†i t·∫≠p 2: M√¥ ph·ªèng ph·∫£n h·ªìi theo lu·ªìng

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("üåä Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n‚úÖ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Ph·∫ßn 5: Demo suy lu·∫≠n tr√¨nh duy·ªát WebGPU

### T·ªïng quan

WebGPU cho ph√©p ch·∫°y c√°c m√¥ h√¨nh AI tr·ª±c ti·∫øp tr√™n tr√¨nh duy·ªát ƒë·ªÉ t·ªëi ƒëa h√≥a quy·ªÅn ri√™ng t∆∞ v√† tr·∫£i nghi·ªám kh√¥ng c·∫ßn c√†i ƒë·∫∑t. M·∫´u n√†y minh h·ªça ONNX Runtime Web v·ªõi th·ª±c thi WebGPU.

### B∆∞·ªõc 1: Ki·ªÉm tra h·ªó tr·ª£ WebGPU

**Y√™u c·∫ßu tr√¨nh duy·ªát:**
- Chrome/Edge 113+ v·ªõi WebGPU ƒë∆∞·ª£c b·∫≠t
- Ki·ªÉm tra: `chrome://gpu` ‚Üí x√°c nh·∫≠n tr·∫°ng th√°i "WebGPU"
- Ki·ªÉm tra b·∫±ng m√£: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### B∆∞·ªõc 2: T·∫°o demo WebGPU

T·∫°o th∆∞ m·ª•c: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>üöÄ WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '‚ùå WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'üîç WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('‚úÖ ONNX Runtime session created with WebGPU');
        log(`üìä Input names: ${session.inputNames.join(', ')}`);
        log(`üìä Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '‚úÖ WebGPU inference complete!';
        log(`üéØ Predicted class: ${maxIdx}`);
        log(`üìà Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `‚ùå Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### B∆∞·ªõc 3: Ch·∫°y demo

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Ph·∫ßn 6: T√≠ch h·ª£p Open WebUI

### T·ªïng quan

Open WebUI cung c·∫•p giao di·ªán chuy√™n nghi·ªáp gi·ªëng ChatGPT k·∫øt n·ªëi v·ªõi API t∆∞∆°ng th√≠ch OpenAI c·ªßa Foundry Local.

### B∆∞·ªõc 1: Y√™u c·∫ßu tr∆∞·ªõc

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### B∆∞·ªõc 2: Thi·∫øt l·∫≠p Docker (Khuy·∫øn ngh·ªã)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**L∆∞u √Ω:** `host.docker.internal` cho ph√©p c√°c container Docker truy c·∫≠p m√°y ch·ªß tr√™n Windows.

### B∆∞·ªõc 3: C·∫•u h√¨nh

1. **M·ªü tr√¨nh duy·ªát:** ƒêi·ªÅu h∆∞·ªõng ƒë·∫øn `http://localhost:3000`
2. **Thi·∫øt l·∫≠p ban ƒë·∫ßu:** T·∫°o t√†i kho·∫£n admin
3. **C·∫•u h√¨nh m√¥ h√¨nh:**
   - Settings ‚Üí Models ‚Üí OpenAI API  
   - Base URL: `http://host.docker.internal:51211/v1`
   - API Key: `foundry-local-key` (b·∫•t k·ª≥ gi√° tr·ªã n√†o ƒë·ªÅu ƒë∆∞·ª£c)
4. **Ki·ªÉm tra k·∫øt n·ªëi:** C√°c m√¥ h√¨nh s·∫Ω xu·∫•t hi·ªán trong danh s√°ch th·∫£ xu·ªëng

### X·ª≠ l√Ω s·ª± c·ªë

**C√°c v·∫•n ƒë·ªÅ th∆∞·ªùng g·∫∑p:**

1. **K·∫øt n·ªëi b·ªã t·ª´ ch·ªëi:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **M√¥ h√¨nh kh√¥ng xu·∫•t hi·ªán:**
   - X√°c minh m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i: `foundry model list`
   - Ki·ªÉm tra ph·∫£n h·ªìi API: `curl http://localhost:51211/v1/models`
   - Kh·ªüi ƒë·ªông l·∫°i container Open WebUI

## Ph·∫ßn 7: C√¢n nh·∫Øc tri·ªÉn khai s·∫£n xu·∫•t

### C·∫•u h√¨nh m√¥i tr∆∞·ªùng

**Thi·∫øt l·∫≠p ph√°t tri·ªÉn:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Tri·ªÉn khai s·∫£n xu·∫•t:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### C√°c v·∫•n ƒë·ªÅ c·ªïng th∆∞·ªùng g·∫∑p v√† gi·∫£i ph√°p

**NgƒÉn ch·∫∑n xung ƒë·ªôt c·ªïng 51211:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Gi√°m s√°t hi·ªáu su·∫•t

**Tri·ªÉn khai ki·ªÉm tra s·ª©c kh·ªèe:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## T√≥m t·∫Øt

Bu·ªïi 4 ƒë√£ ƒë·ªÅ c·∫≠p ƒë·∫øn vi·ªác x√¢y d·ª±ng c√°c ·ª©ng d·ª•ng Chainlit s·∫µn s√†ng cho s·∫£n xu·∫•t d√†nh cho AI h·ªôi tho·∫°i. B·∫°n ƒë√£ h·ªçc v·ªÅ:

- ‚úÖ **Framework Chainlit**: Giao di·ªán hi·ªán ƒë·∫°i v√† h·ªó tr·ª£ ph√°t tr·ª±c ti·∫øp cho ·ª©ng d·ª•ng chat
- ‚úÖ **T√≠ch h·ª£p Foundry Local**: S·ª≠ d·ª•ng SDK v√† c√°c m·∫´u c·∫•u h√¨nh  
- ‚úÖ **Suy lu·∫≠n WebGPU**: AI tr√™n tr√¨nh duy·ªát ƒë·ªÉ t·ªëi ƒëa h√≥a quy·ªÅn ri√™ng t∆∞
- ‚úÖ **Thi·∫øt l·∫≠p Open WebUI**: Tri·ªÉn khai giao di·ªán chat chuy√™n nghi·ªáp
- ‚úÖ **M√¥ h√¨nh s·∫£n xu·∫•t**: X·ª≠ l√Ω l·ªói, gi√°m s√°t v√† m·ªü r·ªông

·ª®ng d·ª•ng M·∫´u 04 minh h·ªça c√°c th·ª±c ti·ªÖn t·ªët nh·∫•t ƒë·ªÉ x√¢y d·ª±ng giao di·ªán chat m·∫°nh m·∫Ω t·∫≠n d·ª•ng c√°c m√¥ h√¨nh AI c·ª•c b·ªô th√¥ng qua Microsoft Foundry Local ƒë·ªìng th·ªùi mang l·∫°i tr·∫£i nghi·ªám ng∆∞·ªùi d√πng xu·∫•t s·∫Øc.

## T√†i li·ªáu tham kh·∫£o

- **[M·∫´u 04: ·ª®ng d·ª•ng Chainlit](samples/04/README.md)**: ·ª®ng d·ª•ng ho√†n ch·ªânh v·ªõi t√†i li·ªáu
- **[Notebook gi√°o d·ª•c Chainlit](samples/04/chainlit_app.ipynb)**: T√†i li·ªáu h·ªçc t·∫≠p t∆∞∆°ng t√°c
- **[T√†i li·ªáu Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: T√†i li·ªáu n·ªÅn t·∫£ng ƒë·∫ßy ƒë·ªß
- **[T√†i li·ªáu Chainlit](https://docs.chainlit.io/)**: T√†i li·ªáu ch√≠nh th·ª©c c·ªßa framework
- **[H∆∞·ªõng d·∫´n t√≠ch h·ª£p Open WebUI](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: H∆∞·ªõng d·∫´n ch√≠nh th·ª©c

---

**Tuy√™n b·ªë mi·ªÖn tr·ª´ tr√°ch nhi·ªám**:  
T√†i li·ªáu n√†y ƒë√£ ƒë∆∞·ª£c d·ªãch b·∫±ng d·ªãch v·ª• d·ªãch thu·∫≠t AI [Co-op Translator](https://github.com/Azure/co-op-translator). M·∫∑c d√π ch√∫ng t√¥i c·ªë g·∫Øng ƒë·∫£m b·∫£o ƒë·ªô ch√≠nh x√°c, xin l∆∞u √Ω r·∫±ng c√°c b·∫£n d·ªãch t·ª± ƒë·ªông c√≥ th·ªÉ ch·ª©a l·ªói ho·∫∑c kh√¥ng ch√≠nh x√°c. T√†i li·ªáu g·ªëc b·∫±ng ng√¥n ng·ªØ b·∫£n ƒë·ªãa n√™n ƒë∆∞·ª£c coi l√† ngu·ªìn th√¥ng tin ch√≠nh th·ª©c. ƒê·ªëi v·ªõi c√°c th√¥ng tin quan tr·ªçng, khuy·∫øn ngh·ªã s·ª≠ d·ª•ng d·ªãch v·ª• d·ªãch thu·∫≠t chuy√™n nghi·ªáp b·ªüi con ng∆∞·ªùi. Ch√∫ng t√¥i kh√¥ng ch·ªãu tr√°ch nhi·ªám cho b·∫•t k·ª≥ s·ª± hi·ªÉu l·∫ßm ho·∫∑c di·ªÖn gi·∫£i sai n√†o ph√°t sinh t·ª´ vi·ªác s·ª≠ d·ª•ng b·∫£n d·ªãch n√†y.