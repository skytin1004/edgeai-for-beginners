{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af028554",
   "metadata": {},
   "source": [
    "# рдЗрдВрдЯреЗрдВрдЯ-рдЖрдзрд╛рд░рд┐рдд рдореЙрдбрд▓ рд░рд╛рдЙрдЯрд░ Foundry Local SDK рдХреЗ рд╕рд╛рде\n",
    "\n",
    "**CPU-рдСрдкреНрдЯрд┐рдорд╛рдЗрдЬрд╝реНрдб рдорд▓реНрдЯреА-рдореЙрдбрд▓ рд░рд╛рдЙрдЯрд┐рдВрдЧ рд╕рд┐рд╕реНрдЯрдо**\n",
    "\n",
    "рдпрд╣ рдиреЛрдЯрдмреБрдХ рдПрдХ рдмреБрджреНрдзрд┐рдорд╛рди рд░рд╛рдЙрдЯрд┐рдВрдЧ рд╕рд┐рд╕реНрдЯрдо рдХрд╛ рдкреНрд░рджрд░реНрд╢рди рдХрд░рддрд╛ рд╣реИ рдЬреЛ рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдХреЗ рдЗрд░рд╛рджреЗ рдХреЗ рдЖрдзрд╛рд░ рдкрд░ рд╕рдмрд╕реЗ рдЕрдЪреНрдЫрд╛ рдЫреЛрдЯрд╛ рднрд╛рд╖рд╛ рдореЙрдбрд▓ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдк рд╕реЗ рдЪреБрдирддрд╛ рд╣реИред рдпрд╣ рдЙрди рдкрд░рд┐рджреГрд╢реНрдпреЛрдВ рдХреЗ рд▓рд┐рдП рдЖрджрд░реНрд╢ рд╣реИ рдЬрд╣рд╛рдВ рдЖрдк рдХрдИ рд╡рд┐рд╢реЗрд╖ рдореЙрдбрд▓реЛрдВ рдХрд╛ рдХреБрд╢рд▓рддрд╛рдкреВрд░реНрд╡рдХ рдЙрдкрдпреЛрдЧ рдХрд░рдирд╛ рдЪрд╛рд╣рддреЗ рд╣реИрдВред\n",
    "\n",
    "## ЁЯОп рдЖрдк рдХреНрдпрд╛ рд╕реАрдЦреЗрдВрдЧреЗ\n",
    "\n",
    "- **рдЗрд░рд╛рджрд╛ рдкрд╣рдЪрд╛рди**: рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдк рд╕реЗ рдкреНрд░реЙрдореНрдкреНрдЯреНрд╕ рдХреЛ рд╡рд░реНрдЧреАрдХреГрдд рдХрд░реЗрдВ (рдХреЛрдб, рд╕рд╛рд░рд╛рдВрд╢, рд╡рд░реНрдЧреАрдХрд░рдг, рд╕рд╛рдорд╛рдиреНрдп)\n",
    "- **рд╕реНрдорд╛рд░реНрдЯ рдореЙрдбрд▓ рдЪрдпрди**: рдкреНрд░рддреНрдпреЗрдХ рдХрд╛рд░реНрдп рдХреЗ рд▓рд┐рдП рд╕рдмрд╕реЗ рд╕рдХреНрд╖рдо рдореЙрдбрд▓ рдХрд╛ рдЪрдпрди рдХрд░реЗрдВ\n",
    "- **CPU рдСрдкреНрдЯрд┐рдорд╛рдЗрдЬрд╝реЗрд╢рди**: рдореЗрдореЛрд░реА-рдХреБрд╢рд▓ рдореЙрдбрд▓ рдЬреЛ рдХрд┐рд╕реА рднреА рд╣рд╛рд░реНрдбрд╡реЗрдпрд░ рдкрд░ рдХрд╛рдо рдХрд░рддреЗ рд╣реИрдВ\n",
    "- **рдорд▓реНрдЯреА-рдореЙрдбрд▓ рдкреНрд░рдмрдВрдзрди**: `--retain true` рдХреЗ рд╕рд╛рде рдХрдИ рдореЙрдбрд▓ рд▓реЛрдб рд░рдЦреЗрдВ\n",
    "- **рдкреНрд░реЛрдбрдХреНрд╢рди рдкреИрдЯрд░реНрди**: рдкреБрдирдГ рдкреНрд░рдпрд╛рд╕ рд▓реЙрдЬрд┐рдХ, рддреНрд░реБрдЯрд┐ рд╣реИрдВрдбрд▓рд┐рдВрдЧ, рдФрд░ рдЯреЛрдХрди рдЯреНрд░реИрдХрд┐рдВрдЧ\n",
    "\n",
    "## ЁЯУЛ рдкрд░рд┐рджреГрд╢реНрдп рдХрд╛ рдЕрд╡рд▓реЛрдХрди\n",
    "\n",
    "рдпрд╣ рдкреИрдЯрд░реНрди рдирд┐рдореНрдирд▓рд┐рдЦрд┐рдд рдХрд╛ рдкреНрд░рджрд░реНрд╢рди рдХрд░рддрд╛ рд╣реИ:\n",
    "\n",
    "1. **рдЗрд░рд╛рджрд╛ рдкрд╣рдЪрд╛рди**: рдкреНрд░рддреНрдпреЗрдХ рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдкреНрд░реЙрдореНрдкреНрдЯ рдХреЛ рд╡рд░реНрдЧреАрдХреГрдд рдХрд░реЗрдВ (рдХреЛрдб, рд╕рд╛рд░рд╛рдВрд╢, рд╡рд░реНрдЧреАрдХрд░рдг, рдпрд╛ рд╕рд╛рдорд╛рдиреНрдп)\n",
    "2. **рдореЙрдбрд▓ рдЪрдпрди**: рдХреНрд╖рдорддрд╛рдУрдВ рдХреЗ рдЖрдзрд╛рд░ рдкрд░ рд╕рдмрд╕реЗ рдЙрдкрдпреБрдХреНрдд рдЫреЛрдЯреЗ рднрд╛рд╖рд╛ рдореЙрдбрд▓ рдХреЛ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдк рд╕реЗ рдЪреБрдиреЗрдВ\n",
    "3. **рд╕реНрдерд╛рдиреАрдп рдирд┐рд╖реНрдкрд╛рджрди**: Foundry Local рд╕реЗрд╡рд╛ рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ рд╕реНрдерд╛рдиреАрдп рд░реВрдк рд╕реЗ рдЪрд▓ рд░рд╣реЗ рдореЙрдбрд▓реЛрдВ рдХреЛ рд░реВрдЯ рдХрд░реЗрдВ\n",
    "4. **рдПрдХреАрдХреГрдд рдЗрдВрдЯрд░рдлрд╝реЗрд╕**: рдПрдХрд▓ рдЪреИрдЯ рдПрдВрдЯреНрд░реА рдкреЙрдЗрдВрдЯ рдЬреЛ рдХрдИ рд╡рд┐рд╢реЗрд╖ рдореЙрдбрд▓реЛрдВ рдХреЛ рд░реВрдЯ рдХрд░рддрд╛ рд╣реИ\n",
    "\n",
    "**рдЖрджрд░реНрд╢ рдЙрдкрдпреЛрдЧ рдХреЗ рд▓рд┐рдП**: рдХрдИ рд╡рд┐рд╢реЗрд╖ рдореЙрдбрд▓реЛрдВ рдХреЗ рд╕рд╛рде рдПрдЬ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ, рдЬрд╣рд╛рдВ рдЖрдк рдореИрдиреНрдпреБрдЕрд▓ рдореЙрдбрд▓ рдЪрдпрди рдХреЗ рдмрд┐рдирд╛ рдмреБрджреНрдзрд┐рдорд╛рди рдЕрдиреБрд░реЛрдз рд░реВрдЯрд┐рдВрдЧ рдЪрд╛рд╣рддреЗ рд╣реИрдВред\n",
    "\n",
    "## ЁЯФз рдЖрд╡рд╢реНрдпрдХрддрд╛рдПрдБ\n",
    "\n",
    "- **Foundry Local** рдЗрдВрд╕реНрдЯреЙрд▓ рдФрд░ рд╕реЗрд╡рд╛ рдЪрд╛рд▓реВ рд╣реЛрдиреА рдЪрд╛рд╣рд┐рдП\n",
    "- **Python 3.8+** рдФрд░ pip\n",
    "- **8GB+ RAM** (рдХрдИ рдореЙрдбрд▓реЛрдВ рдХреЗ рд▓рд┐рдП 16GB+ рдЕрдиреБрд╢рдВрд╕рд┐рдд)\n",
    "- **workshop_utils** рдореЙрдбреНрдпреВрд▓ (../samples/ рдореЗрдВ)\n",
    "\n",
    "## ЁЯЪА рддреНрд╡рд░рд┐рдд рд╢реБрд░реБрдЖрдд\n",
    "\n",
    "рдиреЛрдЯрдмреБрдХ рдирд┐рдореНрдирд▓рд┐рдЦрд┐рдд рдХрд░реЗрдЧрд╛:\n",
    "1. рдЖрдкрдХреЗ рд╕рд┐рд╕реНрдЯрдо рдХреА рдореЗрдореЛрд░реА рдХрд╛ рдкрддрд╛ рд▓рдЧрд╛рдПрдЧрд╛\n",
    "2. рдЙрдкрдпреБрдХреНрдд CPU рдореЙрдбрд▓реЛрдВ рдХреА рд╕рд┐рдлрд╛рд░рд┐рд╢ рдХрд░реЗрдЧрд╛\n",
    "3. `--retain true` рдХреЗ рд╕рд╛рде рдореЙрдбрд▓ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдк рд╕реЗ рд▓реЛрдб рдХрд░реЗрдЧрд╛\n",
    "4. рд╕рднреА рдореЙрдбрд▓реЛрдВ рдХреА рддреИрдпрд╛рд░реА рдХреА рдкреБрд╖реНрдЯрд┐ рдХрд░реЗрдЧрд╛\n",
    "5. рдкрд░реАрдХреНрд╖рдг рдкреНрд░реЙрдореНрдкреНрдЯреНрд╕ рдХреЛ рд╡рд┐рд╢реЗрд╖ рдореЙрдбрд▓реЛрдВ рдкрд░ рд░реВрдЯ рдХрд░реЗрдЧрд╛\n",
    "\n",
    "**рдЕрдиреБрдорд╛рдирд┐рдд рд╕реЗрдЯрдЕрдк рд╕рдордп**: 5-7 рдорд┐рдирдЯ (рдореЙрдбрд▓ рд▓реЛрдбрд┐рдВрдЧ рд╕рд╣рд┐рдд)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa55f0",
   "metadata": {},
   "source": [
    "## ЁЯУж рдЪрд░рдг 1: рдбрд┐рдкреЗрдВрдбреЗрдВрд╕реАрдЬрд╝ рдЗрдВрд╕реНрдЯреЙрд▓ рдХрд░реЗрдВ\n",
    "\n",
    "рдЖрдзрд┐рдХрд╛рд░рд┐рдХ Foundry Local SDK рдФрд░ рдЖрд╡рд╢реНрдпрдХ рд▓рд╛рдЗрдмреНрд░реЗрд░реАрдЬрд╝ рдЗрдВрд╕реНрдЯреЙрд▓ рдХрд░реЗрдВ:\n",
    "\n",
    "- **foundry-local-sdk**: рд╕реНрдерд╛рдиреАрдп рдореЙрдбрд▓ рдкреНрд░рдмрдВрдзрди рдХреЗ рд▓рд┐рдП рдЖрдзрд┐рдХрд╛рд░рд┐рдХ Python SDK\n",
    "- **openai**: рдЪреИрдЯ рдкреВрд░реНрдгрддрд╛ рдХреЗ рд▓рд┐рдП OpenAI-рд╕рдВрдЧрдд API\n",
    "- **psutil**: рд╕рд┐рд╕реНрдЯрдо рдореЗрдореЛрд░реА рдХрд╛ рдкрддрд╛ рд▓рдЧрд╛рдиреЗ рдФрд░ рдирд┐рдЧрд░рд╛рдиреА рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2929c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "!pip install -q foundry-local-sdk openai psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990799e7",
   "metadata": {},
   "source": [
    "## ЁЯТ╗ рдЪрд░рдг 2: рд╕рд┐рд╕реНрдЯрдо рдореЗрдореЛрд░реА рдХрд╛ рдкрддрд╛ рд▓рдЧрд╛рдирд╛\n",
    "\n",
    "рдЙрдкрд▓рдмреНрдз рд╕рд┐рд╕реНрдЯрдо рдореЗрдореЛрд░реА рдХрд╛ рдкрддрд╛ рд▓рдЧрд╛рдПрдВ рддрд╛рдХрд┐ рдпрд╣ рдирд┐рд░реНрдзрд╛рд░рд┐рдд рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХреЗ рдХрд┐ рдХреМрди рд╕реЗ CPU рдореЙрдбрд▓ рдХреБрд╢рд▓рддрд╛рдкреВрд░реНрд╡рдХ рдЪрд▓ рд╕рдХрддреЗ рд╣реИрдВред рдпрд╣ рдЖрдкрдХреЗ рд╣рд╛рд░реНрдбрд╡реЗрдпрд░ рдХреЗ рд▓рд┐рдП рдЖрджрд░реНрд╢ рдореЙрдбрд▓ рдЪрдпрди рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░рддрд╛ рд╣реИред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ff58f1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯЦея╕П  System Memory Information\n",
      "======================================================================\n",
      "Total Memory:     63.30 GB\n",
      "Available Memory: 16.19 GB\n",
      "\n",
      "тЬЕ High Memory System (32GB+)\n",
      "   Can run 3-4 models simultaneously\n",
      "\n",
      "ЁЯУЛ Recommended Model Aliases for Your System:\n",
      "   тАв phi-4-mini\n",
      "   тАв phi-3.5-mini\n",
      "   тАв qwen2.5-0.5b\n",
      "   тАв qwen2.5-coder-0.5b\n",
      "\n",
      "ЁЯТб About Model Aliases:\n",
      "   тЬУ Use base alias (e.g., phi-4-mini, not phi-4-mini-cpu)\n",
      "   тЬУ Foundry Local automatically selects CPU variant for your hardware\n",
      "   тЬУ No GPU required - optimized for CPU inference\n",
      "   тЬУ Predictable memory usage and consistent performance\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get system memory information\n",
    "total_memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "\n",
    "print('ЁЯЦея╕П  System Memory Information')\n",
    "print('=' * 70)\n",
    "print(f'Total Memory:     {total_memory_gb:.2f} GB')\n",
    "print(f'Available Memory: {available_memory_gb:.2f} GB')\n",
    "print()\n",
    "\n",
    "# Recommend models based on available memory\n",
    "# Using model aliases - Foundry Local will automatically select CPU variant\n",
    "model_aliases = []\n",
    "\n",
    "if total_memory_gb >= 32:\n",
    "    model_aliases = ['phi-4-mini', 'phi-3.5-mini', 'qwen2.5-0.5b', 'qwen2.5-coder-0.5b']\n",
    "    print('тЬЕ High Memory System (32GB+)')\n",
    "    print('   Can run 3-4 models simultaneously')\n",
    "elif total_memory_gb >= 16:\n",
    "    model_aliases = ['phi-4-mini', 'qwen2.5-0.5b', 'phi-3.5-mini']\n",
    "    print('тЬЕ Medium Memory System (16-32GB)')\n",
    "    print('   Can run 2-3 models simultaneously')\n",
    "elif total_memory_gb >= 8:\n",
    "    model_aliases = ['qwen2.5-0.5b', 'phi-3.5-mini']\n",
    "    print('тЪая╕П  Lower Memory System (8-16GB)')\n",
    "    print('   Recommended: 2 smaller models')\n",
    "else:\n",
    "    model_aliases = ['qwen2.5-0.5b']\n",
    "    print('тЪая╕П  Limited Memory System (<8GB)')\n",
    "    print('   Recommended: Use only smallest model')\n",
    "\n",
    "print()\n",
    "print('ЁЯУЛ Recommended Model Aliases for Your System:')\n",
    "for model in model_aliases:\n",
    "    print(f'   тАв {model}')\n",
    "\n",
    "print()\n",
    "print('ЁЯТб About Model Aliases:')\n",
    "print('   тЬУ Use base alias (e.g., phi-4-mini, not phi-4-mini-cpu)')\n",
    "print('   тЬУ Foundry Local automatically selects CPU variant for your hardware')\n",
    "print('   тЬУ No GPU required - optimized for CPU inference')\n",
    "print('   тЬУ Predictable memory usage and consistent performance')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69590b94",
   "metadata": {},
   "source": [
    "## ЁЯдЦ рдЪрд░рдг 3: рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рдореЙрдбрд▓ рд▓реЛрдбрд┐рдВрдЧ\n",
    "\n",
    "рдпрд╣ рд╕реЗрд▓ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдк рд╕реЗ:\n",
    "1. Foundry Local рд╕реЗрд╡рд╛ рд╢реБрд░реВ рдХрд░рддрд╛ рд╣реИ (рдпрджрд┐ рдпрд╣ рдЪрд╛рд▓реВ рдирд╣реАрдВ рд╣реИ)\n",
    "2. рдЕрдиреБрд╢рдВрд╕рд┐рдд рдореЙрдбрд▓реЛрдВ рдХреЛ `--retain true` рдХреЗ рд╕рд╛рде рд▓реЛрдб рдХрд░рддрд╛ рд╣реИ (рдореЗрдореЛрд░реА рдореЗрдВ рдХрдИ рдореЙрдбрд▓ рдмрдирд╛рдП рд░рдЦрддрд╛ рд╣реИ)\n",
    "3. SDK рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдХреЗ рд╕рднреА рдореЙрдбрд▓реЛрдВ рдХреА рддреИрдпрд╛рд░реА рдХреА рдкреБрд╖реНрдЯрд┐ рдХрд░рддрд╛ рд╣реИ\n",
    "\n",
    "тП▒я╕П **рдЕрдкреЗрдХреНрд╖рд┐рдд рд╕рдордп**: рд╕рднреА рдореЙрдбрд▓реЛрдВ рдХреЗ рд▓рд┐рдП 3-5 рдорд┐рдирдЯ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "543fd976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯЪА Automatic Model Loading with SDK Verification\n",
      "======================================================================\n",
      "ЁЯУЛ Loading 3 models: ['phi-4-mini', 'phi-3.5-mini', 'qwen2.5-0.5b']\n",
      "ЁЯТб Using model aliases - Foundry will load CPU variants automatically\n",
      "\n",
      "ЁЯУб Step 1: Checking Foundry Local service...\n",
      "   тЬЕ Service is already running\n",
      "\n",
      "ЁЯдЦ Step 2: Loading models with retention...\n",
      "   [1/3] Starting phi-4-mini...\n",
      "       тЬЕ phi-4-mini loading in background\n",
      "   [2/3] Starting phi-3.5-mini...\n",
      "       тЬЕ phi-3.5-mini loading in background\n",
      "   [3/3] Starting qwen2.5-0.5b...\n",
      "       тЬЕ qwen2.5-0.5b loading in background\n",
      "\n",
      "тЬЕ Step 3: Verifying models (this may take 2-3 minutes)...\n",
      "======================================================================\n",
      "\n",
      "   Attempt 1/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 2/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 3/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 4/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 5/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 6/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 7/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 8/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 9/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 10/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 11/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 12/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 13/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 14/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 15/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 16/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 17/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 18/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 19/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 20/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 21/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 22/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 23/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 24/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 25/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 26/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 27/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 28/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 29/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 30/30...\n",
      "   тЪая╕П  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   тЪая╕П  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "======================================================================\n",
      "ЁЯУж Final Status: 0/3 models ready\n",
      "   тЭМ phi-4-mini - NOT READY\n",
      "   тЭМ phi-3.5-mini - NOT READY\n",
      "   тЭМ qwen2.5-0.5b - NOT READY\n",
      "\n",
      "тЪая╕П  Some models not ready. Check: foundry model ls\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add samples directory for workshop_utils (Foundry SDK pattern)\n",
    "sys.path.append(os.path.join('..', 'samples'))\n",
    "\n",
    "print('ЁЯЪА Automatic Model Loading with SDK Verification')\n",
    "print('=' * 70)\n",
    "\n",
    "# Use top 3 recommended models (aliases)\n",
    "# Foundry will automatically load CPU variants\n",
    "REQUIRED_MODELS = model_aliases[:3]\n",
    "print(f'ЁЯУЛ Loading {len(REQUIRED_MODELS)} models: {REQUIRED_MODELS}')\n",
    "print('ЁЯТб Using model aliases - Foundry will load CPU variants automatically')\n",
    "print()\n",
    "\n",
    "# Step 1: Ensure Foundry Local service is running\n",
    "print('ЁЯУб Step 1: Checking Foundry Local service...')\n",
    "try:\n",
    "    result = subprocess.run(['foundry', 'service', 'status'], \n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print('   тЬЕ Service is already running')\n",
    "    else:\n",
    "        print('   тЪЩя╕П  Starting Foundry Local service...')\n",
    "        subprocess.run(['foundry', 'service', 'start'], \n",
    "                      capture_output=True, text=True, timeout=30)\n",
    "        time.sleep(5)\n",
    "        print('   тЬЕ Service started')\n",
    "except Exception as e:\n",
    "    print(f'   тЪая╕П  Could not verify service: {e}')\n",
    "    print('   ЁЯТб Try manually: foundry service start')\n",
    "\n",
    "# Step 2: Load each model with --retain true\n",
    "print(f'\\nЁЯдЦ Step 2: Loading models with retention...')\n",
    "for i, model in enumerate(REQUIRED_MODELS, 1):\n",
    "    print(f'   [{i}/{len(REQUIRED_MODELS)}] Starting {model}...')\n",
    "    try:\n",
    "        subprocess.Popen(['foundry', 'model', 'run', model, '--retain', 'true'],\n",
    "                        stdout=subprocess.DEVNULL,\n",
    "                        stderr=subprocess.DEVNULL)\n",
    "        print(f'       тЬЕ {model} loading in background')\n",
    "    except Exception as e:\n",
    "        print(f'       тЭМ Error starting {model}: {e}')\n",
    "\n",
    "# Step 3: Verify models are ready\n",
    "print(f'\\nтЬЕ Step 3: Verifying models (this may take 2-3 minutes)...')\n",
    "print('=' * 70)\n",
    "\n",
    "try:\n",
    "    from workshop_utils import get_client\n",
    "    \n",
    "    ready_models = []\n",
    "    max_attempts = 30\n",
    "    attempt = 0\n",
    "    \n",
    "    while len(ready_models) < len(REQUIRED_MODELS) and attempt < max_attempts:\n",
    "        attempt += 1\n",
    "        print(f'\\n   Attempt {attempt}/{max_attempts}...')\n",
    "        \n",
    "        for model in REQUIRED_MODELS:\n",
    "            if model in ready_models:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                manager, client, model_id = get_client(model, None)\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "                    max_tokens=5,\n",
    "                    temperature=0\n",
    "                )\n",
    "                \n",
    "                if response and response.choices:\n",
    "                    ready_models.append(model)\n",
    "                    print(f'   тЬЕ {model} is READY')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_msg = str(e).lower()\n",
    "                if 'connection' in error_msg or 'timeout' in error_msg:\n",
    "                    print(f'   тП│ {model} still loading...')\n",
    "                else:\n",
    "                    print(f'   тЪая╕П  {model} error: {str(e)[:60]}...')\n",
    "        \n",
    "        if len(ready_models) == len(REQUIRED_MODELS):\n",
    "            break\n",
    "            \n",
    "        if len(ready_models) < len(REQUIRED_MODELS):\n",
    "            time.sleep(10)\n",
    "    \n",
    "    # Final status\n",
    "    print('\\n' + '=' * 70)\n",
    "    print(f'ЁЯУж Final Status: {len(ready_models)}/{len(REQUIRED_MODELS)} models ready')\n",
    "    \n",
    "    for model in REQUIRED_MODELS:\n",
    "        if model in ready_models:\n",
    "            print(f'   тЬЕ {model} - READY (retained in memory)')\n",
    "        else:\n",
    "            print(f'   тЭМ {model} - NOT READY')\n",
    "    \n",
    "    if len(ready_models) == len(REQUIRED_MODELS):\n",
    "        print('\\nЁЯОЙ All models loaded and verified!')\n",
    "        print('   тЬЕ Ready for intent-based routing')\n",
    "    else:\n",
    "        print(f'\\nтЪая╕П  Some models not ready. Check: foundry model ls')\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f'\\nтЭМ Cannot import workshop_utils: {e}')\n",
    "    print('   ЁЯТб Ensure workshop_utils.py is in ../samples/')\n",
    "except Exception as e:\n",
    "    print(f'\\nтЭМ Verification error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682909b",
   "metadata": {},
   "source": [
    "## ЁЯОп рдЪрд░рдг 4: рдЗрд░рд╛рджрд╛ рдкрд╣рдЪрд╛рди рдФрд░ рдореЙрдбрд▓ рдХреИрдЯрд▓реЙрдЧ рдХреЙрдиреНрдлрд╝рд┐рдЧрд░ рдХрд░реЗрдВ\n",
    "\n",
    "рд░реВрдЯрд┐рдВрдЧ рд╕рд┐рд╕реНрдЯрдо рдХреЛ рд╕реЗрдЯ рдХрд░реЗрдВ:\n",
    "- **рдЗрд░рд╛рджрд╛ рдирд┐рдпрдо**: рдкреНрд░реЙрдореНрдкреНрдЯ рдХреЛ рд╡рд░реНрдЧреАрдХреГрдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП Regex рдкреИрдЯрд░реНрди\n",
    "- **рдореЙрдбрд▓ рдХреИрдЯрд▓реЙрдЧ**: рдореЙрдбрд▓ рдХреА рдХреНрд╖рдорддрд╛рдУрдВ рдХреЛ рдЗрд░рд╛рджрд╛ рд╢реНрд░реЗрдгрд┐рдпреЛрдВ рд╕реЗ рдЬреЛрдбрд╝рддрд╛ рд╣реИ\n",
    "- **рдкреНрд░рд╛рдердорд┐рдХрддрд╛ рдкреНрд░рдгрд╛рд▓реА**: рдЬрдм рдХрдИ рдореЙрдбрд▓ рдореЗрд▓ рдЦрд╛рддреЗ рд╣реИрдВ рддреЛ рдореЙрдбрд▓ рдЪрдпрди рдирд┐рд░реНрдзрд╛рд░рд┐рдд рдХрд░рддрд╛ рд╣реИ\n",
    "\n",
    "**CPU рдореЙрдбрд▓ рдХреЗ рдлрд╛рдпрджреЗ**:\n",
    "- тЬЕ GPU рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рдирд╣реАрдВ\n",
    "- тЬЕ рд╕реНрдерд┐рд░ рдкреНрд░рджрд░реНрд╢рди\n",
    "- тЬЕ рдХрдо рдмрд┐рдЬрд▓реА рдХреА рдЦрдкрдд\n",
    "- тЬЕ рдЕрдиреБрдорд╛рдирд┐рдд рдореЗрдореЛрд░реА рдЙрдкрдпреЛрдЧ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3620a4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯУЛ Active Model Catalog (Hardware-Optimized Aliases)\n",
      "======================================================================\n",
      "ЁЯТб Using model aliases - Foundry automatically selects CPU variants\n",
      "\n",
      "   тАв phi-4-mini\n",
      "     Capabilities: general, summarize, reasoning\n",
      "     Priority: 3\n",
      "\n",
      "   тАв qwen2.5-0.5b\n",
      "     Capabilities: classification, fast, general\n",
      "     Priority: 1\n",
      "\n",
      "   тАв phi-3.5-mini\n",
      "     Capabilities: code, refactor, technical\n",
      "     Priority: 2\n",
      "\n",
      "   тАв qwen2.5-coder-0.5b\n",
      "     Capabilities: code, programming, debug\n",
      "     Priority: 1\n",
      "\n",
      "тЬЕ Intent detection and model selection configured\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ЁЯТб Using model aliases - Foundry automatically selects CPU variants\n",
      "\n",
      "   тАв phi-4-mini\n",
      "     Capabilities: general, summarize, reasoning\n",
      "     Priority: 3\n",
      "\n",
      "   тАв qwen2.5-0.5b\n",
      "     Capabilities: classification, fast, general\n",
      "     Priority: 1\n",
      "\n",
      "   тАв phi-3.5-mini\n",
      "     Capabilities: code, refactor, technical\n",
      "     Priority: 2\n",
      "\n",
      "   тАв qwen2.5-coder-0.5b\n",
      "     Capabilities: code, programming, debug\n",
      "     Priority: 1\n",
      "\n",
      "тЬЕ Intent detection and model selection configured\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Model capability catalog (maps model aliases to capabilities)\n",
    "# Use base aliases - Foundry Local will automatically select CPU variants\n",
    "CATALOG = {\n",
    "    'phi-4-mini': {\n",
    "        'capabilities': ['general', 'summarize', 'reasoning'],\n",
    "        'priority': 3\n",
    "    },\n",
    "    'qwen2.5-0.5b': {\n",
    "        'capabilities': ['classification', 'fast', 'general'],\n",
    "        'priority': 1\n",
    "    },\n",
    "    'phi-3.5-mini': {\n",
    "        'capabilities': ['code', 'refactor', 'technical'],\n",
    "        'priority': 2\n",
    "    },\n",
    "    'qwen2.5-coder-0.5b': {\n",
    "        'capabilities': ['code', 'programming', 'debug'],\n",
    "        'priority': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Filter to only include models recommended for this system\n",
    "CATALOG = {k: v for k, v in CATALOG.items() if k in model_aliases}\n",
    "\n",
    "print('ЁЯУЛ Active Model Catalog (Hardware-Optimized Aliases)')\n",
    "print('=' * 70)\n",
    "print('ЁЯТб Using model aliases - Foundry automatically selects CPU variants')\n",
    "print()\n",
    "for model, info in CATALOG.items():\n",
    "    caps = ', '.join(info['capabilities'])\n",
    "    print(f'   тАв {model}')\n",
    "    print(f'     Capabilities: {caps}')\n",
    "    print(f'     Priority: {info[\"priority\"]}')\n",
    "    print()\n",
    "\n",
    "# Intent detection rules (regex pattern -> intent label)\n",
    "INTENT_RULES = [\n",
    "    (re.compile(r'code|refactor|function|debug|program', re.I), 'code'),\n",
    "    (re.compile(r'summar|abstract|tl;?dr|brief', re.I), 'summarize'),\n",
    "    (re.compile(r'classif|categor|label|sentiment', re.I), 'classification'),\n",
    "    (re.compile(r'explain|teach|describe', re.I), 'general'),\n",
    "]\n",
    "\n",
    "def detect_intent(prompt: str) -> str:\n",
    "    \"\"\"Detect intent from prompt using regex patterns.\n",
    "    \n",
    "    Args:\n",
    "        prompt: User input text\n",
    "        \n",
    "    Returns:\n",
    "        Intent label: 'code', 'summarize', 'classification', or 'general'\n",
    "    \"\"\"\n",
    "    for pattern, intent in INTENT_RULES:\n",
    "        if pattern.search(prompt):\n",
    "            return intent\n",
    "    return 'general'\n",
    "\n",
    "def pick_model(intent: str) -> str:\n",
    "    \"\"\"Select best model for intent based on capabilities and priority.\n",
    "    \n",
    "    Args:\n",
    "        intent: Detected intent category\n",
    "        \n",
    "    Returns:\n",
    "        Model alias string, or first available model if no match\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        (alias, info['priority']) \n",
    "        for alias, info in CATALOG.items() \n",
    "        if intent in info['capabilities']\n",
    "    ]\n",
    "    \n",
    "    if candidates:\n",
    "        # Sort by priority (higher = better)\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return candidates[0][0]\n",
    "    \n",
    "    # Fallback to first available model\n",
    "    return list(CATALOG.keys())[0] if CATALOG else None\n",
    "\n",
    "print('тЬЕ Intent detection and model selection configured')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb6d09",
   "metadata": {},
   "source": [
    "## ЁЯзк рдЪрд░рдг 5: рдЗрд░рд╛рджрд╛ рдкрд╣рдЪрд╛рди рдХрд╛ рдкрд░реАрдХреНрд╖рдг рдХрд░реЗрдВ\n",
    "\n",
    "рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░реЗрдВ рдХрд┐ рдЗрд░рд╛рджрд╛ рдкрд╣рдЪрд╛рди рдкреНрд░рдгрд╛рд▓реА рд╡рд┐рднрд┐рдиреНрди рдкреНрд░рдХрд╛рд░ рдХреЗ рд╕рдВрдХреЗрддреЛрдВ рдХреЛ рд╕рд╣реА рдврдВрдЧ рд╕реЗ рд╡рд░реНрдЧреАрдХреГрдд рдХрд░рддреА рд╣реИред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0fd85468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯзк Testing Intent Detection\n",
      "======================================================================\n",
      "\n",
      "Prompt: Refactor this Python function for better readabili...\n",
      "   Intent: code            тЖТ Model: phi-3.5-mini\n",
      "\n",
      "Prompt: Summarize the key points of this article...\n",
      "   Intent: summarize       тЖТ Model: phi-4-mini\n",
      "\n",
      "Prompt: Classify this customer feedback as positive or neg...\n",
      "   Intent: classification  тЖТ Model: qwen2.5-0.5b\n",
      "\n",
      "Prompt: Explain how edge AI differs from cloud AI...\n",
      "   Intent: general         тЖТ Model: phi-4-mini\n",
      "\n",
      "Prompt: Write a function to calculate fibonacci numbers...\n",
      "   Intent: code            тЖТ Model: phi-3.5-mini\n",
      "\n",
      "Prompt: Give me a brief overview of small language models...\n",
      "   Intent: summarize       тЖТ Model: phi-4-mini\n",
      "\n",
      "======================================================================\n",
      "тЬЕ Intent detection working correctly\n"
     ]
    }
   ],
   "source": [
    "# Test intent detection with sample prompts\n",
    "test_prompts = [\n",
    "    'Refactor this Python function for better readability',\n",
    "    'Summarize the key points of this article',\n",
    "    'Classify this customer feedback as positive or negative',\n",
    "    'Explain how edge AI differs from cloud AI',\n",
    "    'Write a function to calculate fibonacci numbers',\n",
    "    'Give me a brief overview of small language models'\n",
    "]\n",
    "\n",
    "print('ЁЯзк Testing Intent Detection')\n",
    "print('=' * 70)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    intent = detect_intent(prompt)\n",
    "    model = pick_model(intent)\n",
    "    print(f'\\nPrompt: {prompt[:50]}...')\n",
    "    print(f'   Intent: {intent:15s} тЖТ Model: {model}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('тЬЕ Intent detection working correctly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6a08b",
   "metadata": {},
   "source": [
    "## ЁЯЪА рдЪрд░рдг 6: рд░реВрдЯрд┐рдВрдЧ рдлрд╝рдВрдХреНрд╢рди рд▓рд╛рдЧреВ рдХрд░реЗрдВ\n",
    "\n",
    "рдореБрдЦреНрдп рд░реВрдЯрд┐рдВрдЧ рдлрд╝рдВрдХреНрд╢рди рдмрдирд╛рдПрдВ рдЬреЛ:\n",
    "1. рдкреНрд░реЙрдореНрдкреНрдЯ рд╕реЗ рдЗрд░рд╛рджрд╛ рдкрд╣рдЪрд╛рдирддрд╛ рд╣реИ\n",
    "2. рд╕рдмрд╕реЗ рдЙрдкрдпреБрдХреНрдд рдореЙрдбрд▓ рдХрд╛ рдЪрдпрди рдХрд░рддрд╛ рд╣реИ\n",
    "3. Foundry Local SDK рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ рдЕрдиреБрд░реЛрдз рдирд┐рд╖реНрдкрд╛рджрд┐рдд рдХрд░рддрд╛ рд╣реИ\n",
    "4. рдЯреЛрдХрди рдЙрдкрдпреЛрдЧ рдФрд░ рддреНрд░реБрдЯрд┐рдпреЛрдВ рдХреЛ рдЯреНрд░реИрдХ рдХрд░рддрд╛ рд╣реИ\n",
    "\n",
    "**workshop_utils рдкреИрдЯрд░реНрди рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддрд╛ рд╣реИ**:\n",
    "- рдПрдХреНрд╕рдкреЛрдиреЗрдВрд╢рд┐рдпрд▓ рдмреИрдХрдСрдл рдХреЗ рд╕рд╛рде рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рдкреБрдирдГ рдкреНрд░рдпрд╛рд╕\n",
    "- OpenAI-рд╕рдВрдЧрдд API\n",
    "- рдЯреЛрдХрди рдЯреНрд░реИрдХрд┐рдВрдЧ рдФрд░ рддреНрд░реБрдЯрд┐ рдкреНрд░рдмрдВрдзрди\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "24cc251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Routing function ready\n",
      "   Using Foundry Local SDK via workshop_utils\n",
      "   Token tracking: Enabled\n",
      "   Retry logic: Automatic with exponential backoff\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from workshop_utils import chat_once\n",
    "\n",
    "# Fix RETRY_BACKOFF environment variable if it has comments\n",
    "if 'RETRY_BACKOFF' in os.environ:\n",
    "    retry_val = os.environ['RETRY_BACKOFF'].strip().split()[0]\n",
    "    try:\n",
    "        float(retry_val)\n",
    "        os.environ['RETRY_BACKOFF'] = retry_val\n",
    "    except ValueError:\n",
    "        os.environ['RETRY_BACKOFF'] = '1.0'\n",
    "\n",
    "def route(prompt: str, max_tokens: int = 200, temperature: float = 0.7):\n",
    "    \"\"\"Route prompt to appropriate model based on intent.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Detect intent using regex patterns\n",
    "    2. Select best model by capability + priority\n",
    "    3. Execute via Foundry Local SDK\n",
    "    \n",
    "    Args:\n",
    "        prompt: User input text\n",
    "        max_tokens: Maximum tokens in response\n",
    "        temperature: Sampling temperature (0-1)\n",
    "        \n",
    "    Returns:\n",
    "        Dict with: intent, model, output, tokens, usage, error\n",
    "    \"\"\"\n",
    "    intent = detect_intent(prompt)\n",
    "    model_alias = pick_model(intent)\n",
    "    \n",
    "    if not model_alias:\n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'model': None,\n",
    "            'output': '',\n",
    "            'tokens': None,\n",
    "            'usage': {},\n",
    "            'error': 'No suitable model found'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Call Foundry Local via workshop_utils\n",
    "        text, usage = chat_once(\n",
    "            model_alias,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        # Extract token information\n",
    "        usage_info = {}\n",
    "        if usage:\n",
    "            usage_info['prompt_tokens'] = getattr(usage, 'prompt_tokens', None)\n",
    "            usage_info['completion_tokens'] = getattr(usage, 'completion_tokens', None)\n",
    "            usage_info['total_tokens'] = getattr(usage, 'total_tokens', None)\n",
    "        \n",
    "        # Estimate if not provided\n",
    "        if not usage_info.get('total_tokens'):\n",
    "            est_prompt = len(prompt) // 4\n",
    "            est_completion = len(text or '') // 4\n",
    "            usage_info['estimated_tokens'] = est_prompt + est_completion\n",
    "        \n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'model': model_alias,\n",
    "            'output': (text or '').strip(),\n",
    "            'tokens': usage_info.get('total_tokens') or usage_info.get('estimated_tokens'),\n",
    "            'usage': usage_info,\n",
    "            'error': None\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'model': model_alias,\n",
    "            'output': '',\n",
    "            'tokens': None,\n",
    "            'usage': {},\n",
    "            'error': f'{type(e).__name__}: {str(e)}'\n",
    "        }\n",
    "\n",
    "print('тЬЕ Routing function ready')\n",
    "print('   Using Foundry Local SDK via workshop_utils')\n",
    "print('   Token tracking: Enabled')\n",
    "print('   Retry logic: Automatic with exponential backoff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5c915",
   "metadata": {},
   "source": [
    "## ЁЯОп рдЪрд░рдг 7: рд░реВрдЯрд┐рдВрдЧ рдкрд░реАрдХреНрд╖рдг рдЪрд▓рд╛рдПрдВ\n",
    "\n",
    "рд╡рд┐рднрд┐рдиреНрди рдкреНрд░реЙрдореНрдкреНрдЯреНрд╕ рдХреЗ рд╕рд╛рде рдкреВрд░реЗ рд░реВрдЯрд┐рдВрдЧ рд╕рд┐рд╕реНрдЯрдо рдХрд╛ рдкрд░реАрдХреНрд╖рдг рдХрд░реЗрдВ рддрд╛рдХрд┐ рдпрд╣ рдкреНрд░рджрд░реНрд╢рд┐рдд рд╣реЛ рд╕рдХреЗ:\n",
    "- рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рдЗрд░рд╛рджрд╛ рдкрд╣рдЪрд╛рди\n",
    "- рдмреБрджреНрдзрд┐рдорд╛рди рдореЙрдбрд▓ рдЪрдпрди\n",
    "- рдХрдИ рдореЙрдбрд▓реЛрдВ рдХреЗ рд╕рд╛рде рд░реВрдЯрд┐рдВрдЧ рдФрд░ рдореЙрдбрд▓реЛрдВ рдХреЛ рдмрдирд╛рдП рд░рдЦрдирд╛\n",
    "- рдЯреЛрдХрди рдЯреНрд░реИрдХрд┐рдВрдЧ рдФрд░ рдкреНрд░рджрд░реНрд╢рди рдореЗрдЯреНрд░рд┐рдХреНрд╕\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c46ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯОп Running Intent-Based Routing Tests\n",
      "================================================================================\n",
      "\n",
      "[1/6] Testing prompt...\n",
      "Prompt: Refactor this Python function to make it more efficient and readable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Expected Intent: code\n",
      "   Detected Intent: code тЬЕ\n",
      "   Selected Model:  phi-3.5-mini\n",
      "   тЬЕ Response: To refactor a Python function for efficiency and readability, I would need to see the specific funct...\n",
      "   ЁЯУК Tokens: ~158 (estimated)\n",
      "\n",
      "[2/6] Testing prompt...\n",
      "Prompt: Summarize the key benefits of using small language models at the edge\n",
      "   Expected Intent: summarize\n",
      "   Detected Intent: summarize тЬЕ\n",
      "   Selected Model:  phi-4-mini\n",
      "   тЭМ Error: APIConnectionError: Connection error.\n",
      "\n",
      "[3/6] Testing prompt...\n",
      "Prompt: Classify this user feedback: The app is slow but the UI looks great\n",
      "   Expected Intent: classification\n",
      "   Detected Intent: classification тЬЕ\n",
      "   Selected Model:  qwen2.5-0.5b\n",
      "   тЭМ Error: APIConnectionError: Connection error.\n",
      "\n",
      "[4/6] Testing prompt...\n",
      "Prompt: Explain the difference between local and cloud inference\n",
      "   Expected Intent: general\n",
      "   Detected Intent: general тЬЕ\n",
      "   Selected Model:  phi-4-mini\n",
      "   тЭМ Error: APIConnectionError: Connection error.\n",
      "\n",
      "[5/6] Testing prompt...\n",
      "Prompt: Write a Python function to calculate the Fibonacci sequence\n"
     ]
    }
   ],
   "source": [
    "# Test prompts covering all intent categories\n",
    "test_cases = [\n",
    "    {\n",
    "        'prompt': 'Refactor this Python function to make it more efficient and readable',\n",
    "        'expected_intent': 'code'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Summarize the key benefits of using small language models at the edge',\n",
    "        'expected_intent': 'summarize'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Classify this user feedback: The app is slow but the UI looks great',\n",
    "        'expected_intent': 'classification'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Explain the difference between local and cloud inference',\n",
    "        'expected_intent': 'general'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Write a Python function to calculate the Fibonacci sequence',\n",
    "        'expected_intent': 'code'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Give me a brief overview of the Phi model family',\n",
    "        'expected_intent': 'summarize'\n",
    "    }\n",
    "]\n",
    "\n",
    "print('ЁЯОп Running Intent-Based Routing Tests')\n",
    "print('=' * 80)\n",
    "\n",
    "results = []\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f'\\n[{i}/{len(test_cases)}] Testing prompt...')\n",
    "    print(f'Prompt: {test[\"prompt\"]}')\n",
    "    \n",
    "    result = route(test['prompt'], max_tokens=150)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f'   Expected Intent: {test[\"expected_intent\"]}')\n",
    "    print(f'   Detected Intent: {result[\"intent\"]} {\"тЬЕ\" if result[\"intent\"] == test[\"expected_intent\"] else \"тЪая╕П\"}')\n",
    "    print(f'   Selected Model:  {result[\"model\"]}')\n",
    "    \n",
    "    if result['error']:\n",
    "        print(f'   тЭМ Error: {result[\"error\"]}')\n",
    "    else:\n",
    "        output_preview = result['output'][:100] + '...' if len(result['output']) > 100 else result['output']\n",
    "        print(f'   тЬЕ Response: {output_preview}')\n",
    "        \n",
    "        tokens = result.get('tokens', 0)\n",
    "        if tokens:\n",
    "            usage = result.get('usage', {})\n",
    "            if 'estimated_tokens' in usage:\n",
    "                print(f'   ЁЯУК Tokens: ~{tokens} (estimated)')\n",
    "            else:\n",
    "                print(f'   ЁЯУК Tokens: {tokens}')\n",
    "\n",
    "# Summary statistics\n",
    "print('\\n' + '=' * 80)\n",
    "print('ЁЯУК ROUTING SUMMARY')\n",
    "print('=' * 80)\n",
    "\n",
    "success_count = sum(1 for r in results if not r['error'])\n",
    "total_tokens = sum(r.get('tokens', 0) or 0 for r in results if not r['error'])\n",
    "intent_accuracy = sum(1 for i, r in enumerate(results) if r['intent'] == test_cases[i]['expected_intent'])\n",
    "\n",
    "print(f'Total Prompts:        {len(results)}')\n",
    "print(f'тЬЕ Successful:         {success_count}/{len(results)}')\n",
    "print(f'тЭМ Failed:             {len(results) - success_count}')\n",
    "print(f'ЁЯОп Intent Accuracy:    {intent_accuracy}/{len(results)} ({intent_accuracy/len(results)*100:.1f}%)')\n",
    "print(f'ЁЯУК Total Tokens Used:  {total_tokens}')\n",
    "\n",
    "# Model usage distribution\n",
    "print('\\nЁЯУЛ Model Usage Distribution:')\n",
    "model_counts = {}\n",
    "for r in results:\n",
    "    if r['model']:\n",
    "        model_counts[r['model']] = model_counts.get(r['model'], 0) + 1\n",
    "\n",
    "for model, count in sorted(model_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / len(results)) * 100\n",
    "    print(f'   тАв {model}: {count} requests ({percentage:.1f}%)')\n",
    "\n",
    "if success_count == len(results):\n",
    "    print('\\nЁЯОЙ All routing tests passed successfully!')\n",
    "else:\n",
    "    print(f'\\nтЪая╕П  {len(results) - success_count} test(s) failed')\n",
    "    print('   Check Foundry Local service: foundry service status')\n",
    "    print('   Verify models loaded: foundry model ls')\n",
    "\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764811e",
   "metadata": {},
   "source": [
    "## ЁЯФз рдЪрд░рдг 8: рдЗрдВрдЯрд░рдПрдХреНрдЯрд┐рд╡ рдкрд░реАрдХреНрд╖рдг\n",
    "\n",
    "рдЕрдкрдиреЗ рдЦреБрдж рдХреЗ рдкреНрд░реЙрдореНрдкреНрдЯ рдЖрдЬрд╝рдорд╛рдПрдВ рдФрд░ рд░реВрдЯрд┐рдВрдЧ рд╕рд┐рд╕реНрдЯрдо рдХреЛ рдХрд╛рдо рдХрд░рддреЗ рд╣реБрдП рджреЗрдЦреЗрдВ!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fdd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯОп Interactive Routing Test\n",
      "================================================================================\n",
      "Your prompt: Explain how model quantization reduces memory usage\n",
      "\n",
      "Detected Intent: general\n",
      "Selected Model:  phi-4-mini\n",
      "\n",
      "тЬЕ Response:\n",
      "--------------------------------------------------------------------------------\n",
      "Model quantization is a technique used to reduce the memory footprint of a machine learning model, particularly deep learning models. It works by converting the high-precision weights of a neural network, typically represented as 32-bit floating-point numbers, into lower-precision representations, such as 8-bit integers or even binary values.\n",
      "\n",
      "\n",
      "The primary reason for quantization is to decrease the amount of memory required to store the model's parameters. Since floating-point numbers take up more space than integers, by quantizing the weights, we can significantly reduce the model's size. This reduction in size not only saves memory but also can lead to faster computation during inference, as integer operations are generally faster than floating-point operations on many hardware platforms.\n",
      "\n",
      "\n",
      "However, quantization can introduce some loss of accuracy because the lower precision representation may not capture the full range of values that the floating-point representation can. To mitigate this, techniques such as quantization-aware training can be used, where the model is trained with quantization in mind,\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ЁЯУК Tokens used: 292\n",
      "\n",
      "ЁЯТб Try different prompts to test routing behavior!\n",
      "Detected Intent: general\n",
      "Selected Model:  phi-4-mini\n",
      "\n",
      "тЬЕ Response:\n",
      "--------------------------------------------------------------------------------\n",
      "Model quantization is a technique used to reduce the memory footprint of a machine learning model, particularly deep learning models. It works by converting the high-precision weights of a neural network, typically represented as 32-bit floating-point numbers, into lower-precision representations, such as 8-bit integers or even binary values.\n",
      "\n",
      "\n",
      "The primary reason for quantization is to decrease the amount of memory required to store the model's parameters. Since floating-point numbers take up more space than integers, by quantizing the weights, we can significantly reduce the model's size. This reduction in size not only saves memory but also can lead to faster computation during inference, as integer operations are generally faster than floating-point operations on many hardware platforms.\n",
      "\n",
      "\n",
      "However, quantization can introduce some loss of accuracy because the lower precision representation may not capture the full range of values that the floating-point representation can. To mitigate this, techniques such as quantization-aware training can be used, where the model is trained with quantization in mind,\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ЁЯУК Tokens used: 292\n",
      "\n",
      "ЁЯТб Try different prompts to test routing behavior!\n"
     ]
    }
   ],
   "source": [
    "# Interactive testing - modify the prompt and run this cell\n",
    "custom_prompt = \"Explain how model quantization reduces memory usage\"\n",
    "\n",
    "print('ЁЯОп Interactive Routing Test')\n",
    "print('=' * 80)\n",
    "print(f'Your prompt: {custom_prompt}')\n",
    "print()\n",
    "\n",
    "result = route(custom_prompt, max_tokens=200)\n",
    "\n",
    "print(f'Detected Intent: {result[\"intent\"]}')\n",
    "print(f'Selected Model:  {result[\"model\"]}')\n",
    "print()\n",
    "\n",
    "if result['error']:\n",
    "    print(f'тЭМ Error: {result[\"error\"]}')\n",
    "else:\n",
    "    print('тЬЕ Response:')\n",
    "    print('-' * 80)\n",
    "    print(result['output'])\n",
    "    print('-' * 80)\n",
    "    \n",
    "    if result['tokens']:\n",
    "        print(f'\\nЁЯУК Tokens used: {result[\"tokens\"]}')\n",
    "\n",
    "print('\\nЁЯТб Try different prompts to test routing behavior!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c17226c",
   "metadata": {},
   "source": [
    "## ЁЯУК рдЪрд░рдг 9: рдкреНрд░рджрд░реНрд╢рди рд╡рд┐рд╢реНрд▓реЗрд╖рдг\n",
    "\n",
    "рд░реВрдЯрд┐рдВрдЧ рд╕рд┐рд╕реНрдЯрдо рдХреЗ рдкреНрд░рджрд░реНрд╢рди рдФрд░ рдореЙрдбрд▓ рдЙрдкрдпреЛрдЧ рдХрд╛ рд╡рд┐рд╢реНрд▓реЗрд╖рдг рдХрд░реЗрдВред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЪб Performance Benchmark\n",
      "================================================================================\n",
      "\n",
      "Prompt: Write a hello world function...\n",
      "   Model: phi-3.5-mini\n",
      "   Time: 3.31s\n",
      "   Tokens: 60\n",
      "\n",
      "Prompt: Write a hello world function...\n",
      "   Model: phi-3.5-mini\n",
      "   Time: 3.31s\n",
      "   Tokens: 60\n",
      "\n",
      "Prompt: Summarize: AI at the edge is powerful...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 84\n",
      "\n",
      "Prompt: Summarize: AI at the edge is powerful...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 84\n",
      "\n",
      "Prompt: Classify: Good product...\n",
      "   Model: qwen2.5-0.5b\n",
      "   Time: 7.21s\n",
      "   Tokens: 69\n",
      "\n",
      "Prompt: Classify: Good product...\n",
      "   Model: qwen2.5-0.5b\n",
      "   Time: 7.21s\n",
      "   Tokens: 69\n",
      "\n",
      "Prompt: Explain edge computing...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 72\n",
      "\n",
      "================================================================================\n",
      "ЁЯУК Performance Statistics:\n",
      "   Average response time: 27.46s\n",
      "   Fastest response:      3.31s\n",
      "   Slowest response:      49.67s\n",
      "\n",
      "ЁЯТб Note: First request may be slower due to model initialization\n",
      "================================================================================\n",
      "\n",
      "Prompt: Explain edge computing...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 72\n",
      "\n",
      "================================================================================\n",
      "ЁЯУК Performance Statistics:\n",
      "   Average response time: 27.46s\n",
      "   Fastest response:      3.31s\n",
      "   Slowest response:      49.67s\n",
      "\n",
      "ЁЯТб Note: First request may be slower due to model initialization\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Performance benchmark\n",
    "benchmark_prompts = [\n",
    "    'Write a hello world function',\n",
    "    'Summarize: AI at the edge is powerful',\n",
    "    'Classify: Good product',\n",
    "    'Explain edge computing'\n",
    "]\n",
    "\n",
    "print('тЪб Performance Benchmark')\n",
    "print('=' * 80)\n",
    "\n",
    "timings = []\n",
    "for prompt in benchmark_prompts:\n",
    "    start = time.time()\n",
    "    result = route(prompt, max_tokens=50)\n",
    "    duration = time.time() - start\n",
    "    timings.append(duration)\n",
    "    \n",
    "    print(f'\\nPrompt: {prompt[:40]}...')\n",
    "    print(f'   Model: {result[\"model\"]}')\n",
    "    print(f'   Time: {duration:.2f}s')\n",
    "    if result.get('tokens'):\n",
    "        print(f'   Tokens: {result[\"tokens\"]}')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('ЁЯУК Performance Statistics:')\n",
    "print(f'   Average response time: {sum(timings)/len(timings):.2f}s')\n",
    "print(f'   Fastest response:      {min(timings):.2f}s')\n",
    "print(f'   Slowest response:      {max(timings):.2f}s')\n",
    "print('\\nЁЯТб Note: First request may be slower due to model initialization')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db64ff",
   "metadata": {},
   "source": [
    "## ЁЯОУ рдореБрдЦреНрдп рдмрд╛рддреЗрдВ рдФрд░ рдЕрдЧрд▓реЗ рдХрджрдо\n",
    "\n",
    "### тЬЕ рдЖрдкрдиреЗ рдХреНрдпрд╛ рд╕реАрдЦрд╛\n",
    "\n",
    "1. **рдЗрдВрдЯреЗрдВрдЯ-рдЖрдзрд╛рд░рд┐рдд рд░реВрдЯрд┐рдВрдЧ**: рдкреНрд░реЙрдореНрдкреНрдЯ рдХреЛ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдк рд╕реЗ рд╡рд░реНрдЧреАрдХреГрдд рдХрд░реЗрдВ рдФрд░ рд╡рд┐рд╢реЗрд╖ рдореЙрдбрд▓реЛрдВ рдкрд░ рд░реВрдЯ рдХрд░реЗрдВ  \n",
    "2. **рдореЗрдореЛрд░реА-рдЕрд╡реЗрдпрд░ рдЪрдпрди**: рдЙрдкрд▓рдмреНрдз рд╕рд┐рд╕реНрдЯрдо RAM рдХреЗ рдЖрдзрд╛рд░ рдкрд░ CPU рдореЙрдбрд▓ рдЪреБрдиреЗрдВ  \n",
    "3. **рдорд▓реНрдЯреА-рдореЙрдбрд▓ рд░рд┐рдЯреЗрдВрд╢рди**: `--retain true` рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдХреЗ рдХрдИ рдореЙрдбрд▓ рд▓реЛрдб рд░рдЦреЗрдВ  \n",
    "4. **рдкреНрд░реЛрдбрдХреНрд╢рди рдкреИрдЯрд░реНрди**: рд░реАрдЯреНрд░рд╛рдИ рд▓реЙрдЬрд┐рдХ, рдПрд░рд░ рд╣реИрдВрдбрд▓рд┐рдВрдЧ, рдФрд░ рдЯреЛрдХрди рдЯреНрд░реИрдХрд┐рдВрдЧ  \n",
    "5. **CPU рдСрдкреНрдЯрд┐рдорд╛рдЗрдЬреЗрд╢рди**: GPU рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рдХреЗ рдмрд┐рдирд╛ рдХреБрд╢рд▓рддрд╛ рд╕реЗ рддреИрдирд╛рдд рдХрд░реЗрдВ  \n",
    "\n",
    "### ЁЯЪА рдкреНрд░рдпреЛрдЧ рдХреЗ рд╡рд┐рдЪрд╛рд░\n",
    "\n",
    "1. **рдХрд╕реНрдЯрдо рдЗрдВрдЯреЗрдВрдЯ рдЬреЛрдбрд╝реЗрдВ**:  \n",
    "   ```python\n",
    "   INTENT_RULES.append(\n",
    "       (re.compile(r'translate|convert', re.I), 'translation')\n",
    "   )\n",
    "   ```\n",
    "  \n",
    "\n",
    "2. **рдЕрддрд┐рд░рд┐рдХреНрдд рдореЙрдбрд▓ рд▓реЛрдб рдХрд░реЗрдВ**:  \n",
    "   ```bash\n",
    "   foundry model run llama-3.2-1b-cpu --retain true\n",
    "   ```\n",
    "  \n",
    "\n",
    "3. **рдореЙрдбрд▓ рдЪрдпрди рдХреЛ рдЯреНрдпреВрди рдХрд░реЗрдВ**:  \n",
    "   - CATALOG рдореЗрдВ рдкреНрд░рд╛рдердорд┐рдХрддрд╛ рдорд╛рди рд╕рдорд╛рдпреЛрдЬрд┐рдд рдХрд░реЗрдВ  \n",
    "   - рдЕрдзрд┐рдХ рдХреНрд╖рдорддрд╛ рдЯреИрдЧ рдЬреЛрдбрд╝реЗрдВ  \n",
    "   - рдлреЙрд▓рдмреИрдХ рд░рдгрдиреАрддрд┐рдпрд╛рдБ рд▓рд╛рдЧреВ рдХрд░реЗрдВ  \n",
    "\n",
    "4. **рдкреНрд░рджрд░реНрд╢рди рдХреА рдирд┐рдЧрд░рд╛рдиреА рдХрд░реЗрдВ**:  \n",
    "   ```python\n",
    "   import psutil\n",
    "   print(f\"Memory: {psutil.virtual_memory().percent}%\")\n",
    "   ```\n",
    "  \n",
    "\n",
    "### ЁЯУЪ рдЕрддрд┐рд░рд┐рдХреНрдд рд╕рдВрд╕рд╛рдзрди\n",
    "\n",
    "- **Foundry Local SDK**: https://github.com/microsoft/Foundry-Local  \n",
    "- **рд╡рд░реНрдХрд╢реЙрдк рд╕реИрдВрдкрд▓реНрд╕**: ../samples/  \n",
    "- **рдПрдЬ AI рдХреЛрд░реНрд╕**: ../../Module08/  \n",
    "\n",
    "### ЁЯТб рд╕рд░реНрд╡реЛрддреНрддрдо рдкреНрд░рдерд╛рдПрдБ\n",
    "\n",
    "тЬЕ рдХреНрд░реЙрд╕-рдкреНрд▓реЗрдЯрдлрд╝реЙрд░реНрдо рд╡реНрдпрд╡рд╣рд╛рд░ рдХреЗ рд▓рд┐рдП CPU рдореЙрдбрд▓ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ  \n",
    "тЬЕ рдХрдИ рдореЙрдбрд▓ рд▓реЛрдб рдХрд░рдиреЗ рд╕реЗ рдкрд╣рд▓реЗ рд╕рд┐рд╕реНрдЯрдо рдореЗрдореЛрд░реА рдХреА рдЬрд╛рдВрдЪ рдХрд░реЗрдВ  \n",
    "тЬЕ рд░реВрдЯрд┐рдВрдЧ рдкрд░рд┐рджреГрд╢реНрдпреЛрдВ рдХреЗ рд▓рд┐рдП `--retain true` рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ  \n",
    "тЬЕ рдЙрдЪрд┐рдд рдПрд░рд░ рд╣реИрдВрдбрд▓рд┐рдВрдЧ рдФрд░ рд░реАрдЯреНрд░рд╛рдИ рд▓рд╛рдЧреВ рдХрд░реЗрдВ  \n",
    "тЬЕ рд▓рд╛рдЧрдд/рдкреНрд░рджрд░реНрд╢рди рдЕрдиреБрдХреВрд▓рди рдХреЗ рд▓рд┐рдП рдЯреЛрдХрди рдЙрдкрдпреЛрдЧ рдХреЛ рдЯреНрд░реИрдХ рдХрд░реЗрдВ  \n",
    "\n",
    "---\n",
    "\n",
    "**ЁЯОЙ рдмрдзрд╛рдИ рд╣реЛ!** рдЖрдкрдиреЗ Foundry Local SDK рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдХреЗ CPU-рдСрдкреНрдЯрд┐рдорд╛рдЗрдЬрд╝реНрдб рдореЙрдбрд▓ рдХреЗ рд╕рд╛рде рдПрдХ рдкреНрд░реЛрдбрдХреНрд╢рди-рд░реЗрдбреА рдЗрдВрдЯреЗрдВрдЯ-рдЖрдзрд╛рд░рд┐рдд рдореЙрдбрд▓ рд░рд╛рдЙрдЯрд░ рдмрдирд╛ рд▓рд┐рдпрд╛ рд╣реИ!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**рдЕрд╕реНрд╡реАрдХрд░рдг**:  \nрдпрд╣ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝ AI рдЕрдиреБрд╡рд╛рдж рд╕реЗрд╡рд╛ [Co-op Translator](https://github.com/Azure/co-op-translator) рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдХреЗ рдЕрдиреБрд╡рд╛рджрд┐рдд рдХрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИред рдЬрдмрдХрд┐ рд╣рдо рд╕рдЯреАрдХрддрд╛ рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░рдиреЗ рдХрд╛ рдкреНрд░рдпрд╛рд╕ рдХрд░рддреЗ рд╣реИрдВ, рдХреГрдкрдпрд╛ рдзреНрдпрд╛рди рджреЗрдВ рдХрд┐ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рдЕрдиреБрд╡рд╛рдж рдореЗрдВ рддреНрд░реБрдЯрд┐рдпрд╛рдВ рдпрд╛ рдЕрд╢реБрджреНрдзрд┐рдпрд╛рдВ рд╣реЛ рд╕рдХрддреА рд╣реИрдВред рдореВрд▓ рднрд╛рд╖рд╛ рдореЗрдВ рдЙрдкрд▓рдмреНрдз рдореВрд▓ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝ рдХреЛ рдкреНрд░рд╛рдорд╛рдгрд┐рдХ рд╕реНрд░реЛрдд рдорд╛рдирд╛ рдЬрд╛рдирд╛ рдЪрд╛рд╣рд┐рдПред рдорд╣рддреНрд╡рдкреВрд░реНрдг рдЬрд╛рдирдХрд╛рд░реА рдХреЗ рд▓рд┐рдП, рдкреЗрд╢реЗрд╡рд░ рдорд╛рдирд╡ рдЕрдиреБрд╡рд╛рдж рдХреА рд╕рд┐рдлрд╛рд░рд┐рд╢ рдХреА рдЬрд╛рддреА рд╣реИред рдЗрд╕ рдЕрдиреБрд╡рд╛рдж рдХреЗ рдЙрдкрдпреЛрдЧ рд╕реЗ рдЙрддреНрдкрдиреНрди рдХрд┐рд╕реА рднреА рдЧрд▓рддрдлрд╣рдореА рдпрд╛ рдЧрд▓рдд рд╡реНрдпрд╛рдЦреНрдпрд╛ рдХреЗ рд▓рд┐рдП рд╣рдо рдЙрддреНрддрд░рджрд╛рдпреА рдирд╣реАрдВ рд╣реИрдВред\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "149e1ff0f023ecf1f4221663a7928ff5",
   "translation_date": "2025-10-08T22:09:37+00:00",
   "source_file": "Workshop/notebooks/session06_models_router.ipynb",
   "language_code": "hi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}