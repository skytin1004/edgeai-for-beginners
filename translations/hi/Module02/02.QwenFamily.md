<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-17T14:46:34+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "hi"
}
-->
# рдЕрдиреБрднрд╛рдЧ 2: Qwen рдкрд░рд┐рд╡рд╛рд░ рдХреА рдореВрд▓ рдмрд╛рддреЗрдВ

Qwen рдореЙрдбрд▓ рдкрд░рд┐рд╡рд╛рд░ Alibaba Cloud рдХрд╛ рд╡реНрдпрд╛рдкрдХ рджреГрд╖реНрдЯрд┐рдХреЛрдг рд╣реИ, рдЬреЛ рдмрдбрд╝реЗ рднрд╛рд╖рд╛ рдореЙрдбрд▓ рдФрд░ рдорд▓реНрдЯреАрдореЙрдбрд▓ AI рдореЗрдВ рдЙрддреНрдХреГрд╖реНрдЯ рдкреНрд░рджрд░реНрд╢рди рдХреЛ рджрд░реНрд╢рд╛рддрд╛ рд╣реИред рдпрд╣ рджрд┐рдЦрд╛рддрд╛ рд╣реИ рдХрд┐ рдУрдкрди-рд╕реЛрд░реНрд╕ рдореЙрдбрд▓ рд╡рд┐рднрд┐рдиреНрди рдкрд░рд┐рдирд┐рдпреЛрдЬрди рдкрд░рд┐рджреГрд╢реНрдпреЛрдВ рдореЗрдВ рдкрд╣реБрдВрдЪ рдпреЛрдЧреНрдп рд░рд╣рддреЗ рд╣реБрдП рдЙрд▓реНрд▓реЗрдЦрдиреАрдп рдкреНрд░рджрд░реНрд╢рди рдкреНрд░рд╛рдкреНрдд рдХрд░ рд╕рдХрддреЗ рд╣реИрдВред рдпрд╣ рд╕рдордЭрдирд╛ рдорд╣рддреНрд╡рдкреВрд░реНрдг рд╣реИ рдХрд┐ Qwen рдкрд░рд┐рд╡рд╛рд░ рд╢рдХреНрддрд┐рд╢рд╛рд▓реА AI рдХреНрд╖рдорддрд╛рдУрдВ рдХреЛ рд▓рдЪреАрд▓реЗ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рд╡рд┐рдХрд▓реНрдкреЛрдВ рдХреЗ рд╕рд╛рде рд╕рдХреНрд╖рдо рдХрд░рддрд╛ рд╣реИ, рдЬрдмрдХрд┐ рд╡рд┐рд╡рд┐рдз рдХрд╛рд░реНрдпреЛрдВ рдореЗрдВ рдкреНрд░рддрд┐рд╕реНрдкрд░реНрдзреА рдкреНрд░рджрд░реНрд╢рди рдмрдирд╛рдП рд░рдЦрддрд╛ рд╣реИред

## рдбреЗрд╡рд▓рдкрд░реНрд╕ рдХреЗ рд▓рд┐рдП рд╕рдВрд╕рд╛рдзрди

### Hugging Face рдореЙрдбрд▓ рд░рд┐рдкреЙрдЬрд┐рдЯрд░реА
рдЪрдпрдирд┐рдд Qwen рдкрд░рд┐рд╡рд╛рд░ рдХреЗ рдореЙрдбрд▓ [Hugging Face](https://huggingface.co/models?search=qwen) рдкрд░ рдЙрдкрд▓рдмреНрдз рд╣реИрдВ, рдЬреЛ рдЗрди рдореЙрдбрд▓реЛрдВ рдХреЗ рдХреБрдЫ рд╡реЗрд░рд┐рдПрдВрдЯреНрд╕ рддрдХ рдкрд╣реБрдВрдЪ рдкреНрд░рджрд╛рди рдХрд░рддреЗ рд╣реИрдВред рдЖрдк рдЙрдкрд▓рдмреНрдз рд╡реЗрд░рд┐рдПрдВрдЯреНрд╕ рдХрд╛ рдкрддрд╛ рд▓рдЧрд╛ рд╕рдХрддреЗ рд╣реИрдВ, рдЙрдиреНрд╣реЗрдВ рдЕрдкрдиреЗ рд╡рд┐рд╢рд┐рд╖реНрдЯ рдЙрдкрдпреЛрдЧ рдорд╛рдорд▓реЛрдВ рдХреЗ рд▓рд┐рдП рдлрд╛рдЗрди-рдЯреНрдпреВрди рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ, рдФрд░ рд╡рд┐рднрд┐рдиреНрди рдлреНрд░реЗрдорд╡рд░реНрдХреНрд╕ рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ рдЙрдиреНрд╣реЗрдВ рдкрд░рд┐рдирд┐рдпреЛрдЬрд┐рдд рдХрд░ рд╕рдХрддреЗ рд╣реИрдВред

### рд▓реЛрдХрд▓ рдбреЗрд╡рд▓рдкрдореЗрдВрдЯ рдЯреВрд▓реНрд╕
рд╕реНрдерд╛рдиреАрдп рд╡рд┐рдХрд╛рд╕ рдФрд░ рдкрд░реАрдХреНрд╖рдг рдХреЗ рд▓рд┐рдП, рдЖрдк [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ рддрд╛рдХрд┐ рдЙрдкрд▓рдмреНрдз Qwen рдореЙрдбрд▓ рдХреЛ рдЕрдкрдиреЗ рд╡рд┐рдХрд╛рд╕ рдорд╢реАрди рдкрд░ рдЕрдиреБрдХреВрд▓рд┐рдд рдкреНрд░рджрд░реНрд╢рди рдХреЗ рд╕рд╛рде рдЪрд▓рд╛ рд╕рдХреЗрдВред

### рджрд╕реНрддрд╛рд╡реЗрдЬрд╝реАрдХрд░рдг рд╕рдВрд╕рд╛рдзрди
- [Qwen рдореЙрдбрд▓ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝реАрдХрд░рдг](https://huggingface.co/docs/transformers/model_doc/qwen)
- [рдПрдЬ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рдХреЗ рд▓рд┐рдП Qwen рдореЙрдбрд▓ рдХрд╛ рдЕрдиреБрдХреВрд▓рди](https://github.com/microsoft/olive)

## рдкрд░рд┐рдЪрдп

рдЗрд╕ рдЯреНрдпреВрдЯреЛрд░рд┐рдпрд▓ рдореЗрдВ, рд╣рдо Alibaba рдХреЗ Qwen рдореЙрдбрд▓ рдкрд░рд┐рд╡рд╛рд░ рдФрд░ рдЗрд╕рдХреА рдореВрд▓ рдЕрд╡рдзрд╛рд░рдгрд╛рдУрдВ рдХрд╛ рдкрддрд╛ рд▓рдЧрд╛рдПрдВрдЧреЗред рд╣рдо Qwen рдкрд░рд┐рд╡рд╛рд░ рдХреЗ рд╡рд┐рдХрд╛рд╕, рдирд╡реАрди рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдкрджреНрдзрддрд┐рдпреЛрдВ, рдкреНрд░рдореБрдЦ рд╡реЗрд░рд┐рдПрдВрдЯреНрд╕ рдФрд░ рд╡рд┐рднрд┐рдиреНрди рдкрд░рд┐рджреГрд╢реНрдпреЛрдВ рдореЗрдВ рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рдЕрдиреБрдкреНрд░рдпреЛрдЧреЛрдВ рдХреЛ рдХрд╡рд░ рдХрд░реЗрдВрдЧреЗред

## рд╕реАрдЦрдиреЗ рдХреЗ рдЙрджреНрджреЗрд╢реНрдп

рдЗрд╕ рдЯреНрдпреВрдЯреЛрд░рд┐рдпрд▓ рдХреЗ рдЕрдВрдд рддрдХ, рдЖрдк:

- Alibaba рдХреЗ Qwen рдореЙрдбрд▓ рдкрд░рд┐рд╡рд╛рд░ рдХреЗ рдбрд┐рдЬрд╝рд╛рдЗрди рджрд░реНрд╢рди рдФрд░ рд╡рд┐рдХрд╛рд╕ рдХреЛ рд╕рдордЭ рдкрд╛рдПрдВрдЧреЗ
- рдЙрди рдкреНрд░рдореБрдЦ рдирд╡рд╛рдЪрд╛рд░реЛрдВ рдХреА рдкрд╣рдЪрд╛рди рдХрд░ рдкрд╛рдПрдВрдЧреЗ рдЬреЛ Qwen рдореЙрдбрд▓ рдХреЛ рд╡рд┐рднрд┐рдиреНрди рдкреИрд░рд╛рдореАрдЯрд░ рдЖрдХрд╛рд░реЛрдВ рдореЗрдВ рдЙрдЪреНрдЪ рдкреНрд░рджрд░реНрд╢рди рдкреНрд░рд╛рдкреНрдд рдХрд░рдиреЗ рдореЗрдВ рд╕рдХреНрд╖рдо рдмрдирд╛рддреЗ рд╣реИрдВ
- рд╡рд┐рднрд┐рдиреНрди Qwen рдореЙрдбрд▓ рд╡реЗрд░рд┐рдПрдВрдЯреНрд╕ рдХреЗ рд▓рд╛рдн рдФрд░ рд╕реАрдорд╛рдУрдВ рдХреЛ рдкрд╣рдЪрд╛рди рдкрд╛рдПрдВрдЧреЗ
- рд╡рд╛рд╕реНрддрд╡рд┐рдХ рджреБрдирд┐рдпрд╛ рдХреЗ рдкрд░рд┐рджреГрд╢реНрдпреЛрдВ рдХреЗ рд▓рд┐рдП рдЙрдкрдпреБрдХреНрдд рд╡реЗрд░рд┐рдПрдВрдЯреНрд╕ рдХрд╛ рдЪрдпрди рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП Qwen рдореЙрдбрд▓ рдХреЗ рдЬреНрдЮрд╛рди рдХреЛ рд▓рд╛рдЧреВ рдХрд░ рдкрд╛рдПрдВрдЧреЗ

## рдЖрдзреБрдирд┐рдХ AI рдореЙрдбрд▓ рдкрд░рд┐рджреГрд╢реНрдп рдХреЛ рд╕рдордЭрдирд╛

AI рдкрд░рд┐рджреГрд╢реНрдп рдореЗрдВ рдорд╣рддреНрд╡рдкреВрд░реНрдг рд╡рд┐рдХрд╛рд╕ рд╣реБрдЖ рд╣реИ, рдЬрд┐рд╕рдореЗрдВ рд╡рд┐рднрд┐рдиреНрди рд╕рдВрдЧрдарди рднрд╛рд╖рд╛ рдореЙрдбрд▓ рд╡рд┐рдХрд╛рд╕ рдХреЗ рд▓рд┐рдП рдЕрд▓рдЧ-рдЕрд▓рдЧ рджреГрд╖реНрдЯрд┐рдХреЛрдг рдЕрдкрдирд╛ рд░рд╣реЗ рд╣реИрдВред рдХреБрдЫ рдорд╛рд▓рд┐рдХрд╛рдирд╛ рдмрдВрдж-рд╕реНрд░реЛрдд рдореЙрдбрд▓реЛрдВ рдкрд░ рдзреНрдпрд╛рди рдХреЗрдВрджреНрд░рд┐рдд рдХрд░рддреЗ рд╣реИрдВ, рдЬрдмрдХрд┐ рдЕрдиреНрдп рдУрдкрди-рд╕реЛрд░реНрд╕ рдкрд╣реБрдВрдЪ рдФрд░ рдкрд╛рд░рджрд░реНрд╢рд┐рддрд╛ рдкрд░ рдЬреЛрд░ рджреЗрддреЗ рд╣реИрдВред рдкрд╛рд░рдВрдкрд░рд┐рдХ рджреГрд╖реНрдЯрд┐рдХреЛрдг рдореЗрдВ рдпрд╛ рддреЛ рд╡рд┐рд╢рд╛рд▓ рдорд╛рд▓рд┐рдХрд╛рдирд╛ рдореЙрдбрд▓ рд╢рд╛рдорд┐рд▓ рд╣реЛрддреЗ рд╣реИрдВ рдЬреЛ рдХреЗрд╡рд▓ API рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ рд╕реБрд▓рдн рд╣реЛрддреЗ рд╣реИрдВ рдпрд╛ рдУрдкрди-рд╕реЛрд░реНрд╕ рдореЙрдбрд▓ рдЬреЛ рдХреНрд╖рдорддрд╛рдУрдВ рдореЗрдВ рдкреАрдЫреЗ рд░рд╣ рд╕рдХрддреЗ рд╣реИрдВред

рдпрд╣ рдкреНрд░рддрд┐рдорд╛рди рдЙрди рд╕рдВрдЧрдардиреЛрдВ рдХреЗ рд▓рд┐рдП рдЪреБрдиреМрддрд┐рдпрд╛рдВ рдкреИрджрд╛ рдХрд░рддрд╛ рд╣реИ рдЬреЛ рдЕрдкрдиреЗ рдбреЗрдЯрд╛, рд▓рд╛рдЧрдд рдФрд░ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рд▓рдЪреАрд▓реЗрдкрди рдкрд░ рдирд┐рдпрдВрддреНрд░рдг рдмрдирд╛рдП рд░рдЦрддреЗ рд╣реБрдП рд╢рдХреНрддрд┐рд╢рд╛рд▓реА AI рдХреНрд╖рдорддрд╛рдУрдВ рдХреА рддрд▓рд╛рд╢ рдХрд░рддреЗ рд╣реИрдВред рдкрд╛рд░рдВрдкрд░рд┐рдХ рджреГрд╖реНрдЯрд┐рдХреЛрдг рдЕрдХреНрд╕рд░ рдЕрддреНрдпрд╛рдзреБрдирд┐рдХ рдкреНрд░рджрд░реНрд╢рди рдФрд░ рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рд╡рд┐рдЪрд╛рд░реЛрдВ рдХреЗ рдмреАрдЪ рдЪрдпрди рдХрд░рдиреЗ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реЛрддреА рд╣реИред

## рд╕реБрд▓рдн AI рдЙрддреНрдХреГрд╖реНрдЯрддрд╛ рдХреА рдЪреБрдиреМрддреА

рд╡рд┐рднрд┐рдиреНрди рдкрд░рд┐рджреГрд╢реНрдпреЛрдВ рдореЗрдВ рдЙрдЪреНрдЪ рдЧреБрдгрд╡рддреНрддрд╛ рд╡рд╛рд▓реЗ, рд╕реБрд▓рдн AI рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рддреЗрдЬреА рд╕реЗ рдорд╣рддреНрд╡рдкреВрд░реНрдг рд╣реЛ рдЧрдИ рд╣реИред рдЙрди рдЕрдиреБрдкреНрд░рдпреЛрдЧреЛрдВ рдкрд░ рд╡рд┐рдЪрд╛рд░ рдХрд░реЗрдВ рдЬрд┐рдирдХреЗ рд▓рд┐рдП рд╡рд┐рднрд┐рдиреНрди рд╕рдВрдЧрдардирд╛рддреНрдордХ рдЖрд╡рд╢реНрдпрдХрддрд╛рдУрдВ рдХреЗ рд▓рд┐рдП рд▓рдЪреАрд▓реЗ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рд╡рд┐рдХрд▓реНрдкреЛрдВ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реЛрддреА рд╣реИ, рд▓рд╛рдЧрдд рдкреНрд░рднрд╛рд╡реА рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рдЬрд╣рд╛рдВ API рд▓рд╛рдЧрдд рдорд╣рддреНрд╡рдкреВрд░реНрдг рд╣реЛ рд╕рдХрддреА рд╣реИ, рд╡реИрд╢реНрд╡рд┐рдХ рдЕрдиреБрдкреНрд░рдпреЛрдЧреЛрдВ рдХреЗ рд▓рд┐рдП рдмрд╣реБрднрд╛рд╖реА рдХреНрд╖рдорддрд╛рдПрдВ, рдпрд╛ рдХреЛрдбрд┐рдВрдЧ рдФрд░ рдЧрдгрд┐рдд рдЬреИрд╕реЗ рдХреНрд╖реЗрддреНрд░реЛрдВ рдореЗрдВ рд╡рд┐рд╢реЗрд╖ рдбреЛрдореЗрди рд╡рд┐рд╢реЗрд╖рдЬреНрдЮрддрд╛ред

### рдкреНрд░рдореБрдЦ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рдЖрд╡рд╢реНрдпрдХрддрд╛рдПрдВ

рдЖрдзреБрдирд┐рдХ AI рдкрд░рд┐рдирд┐рдпреЛрдЬрди рдХрдИ рдореМрд▓рд┐рдХ рдЖрд╡рд╢реНрдпрдХрддрд╛рдУрдВ рдХрд╛ рд╕рд╛рдордирд╛ рдХрд░рддреЗ рд╣реИрдВ рдЬреЛ рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рдЙрдкрдпреЛрдЧрд┐рддрд╛ рдХреЛ рд╕реАрдорд┐рдд рдХрд░рддреЗ рд╣реИрдВ:

- **рд╕реБрд▓рднрддрд╛**: рдкрд╛рд░рджрд░реНрд╢рд┐рддрд╛ рдФрд░ рдЕрдиреБрдХреВрд▓рди рдХреЗ рд▓рд┐рдП рдУрдкрди-рд╕реЛрд░реНрд╕ рдЙрдкрд▓рдмреНрдзрддрд╛
- **рд▓рд╛рдЧрдд рдкреНрд░рднрд╛рд╡рд╢реАрд▓рддрд╛**: рд╡рд┐рднрд┐рдиреНрди рдмрдЬрдЯреЛрдВ рдХреЗ рд▓рд┐рдП рдЙрдЪрд┐рдд рдЧрдгрдирд╛ рдЖрд╡рд╢реНрдпрдХрддрд╛рдПрдВ
- **рд▓рдЪреАрд▓рд╛рдкрди**: рд╡рд┐рднрд┐рдиреНрди рдкрд░рд┐рдирд┐рдпреЛрдЬрди рдкрд░рд┐рджреГрд╢реНрдпреЛрдВ рдХреЗ рд▓рд┐рдП рдХрдИ рдореЙрдбрд▓ рдЖрдХрд╛рд░
- **рд╡реИрд╢реНрд╡рд┐рдХ рдкрд╣реБрдВрдЪ**: рдордЬрдмреВрдд рдмрд╣реБрднрд╛рд╖реА рдФрд░ рд╕рд╛рдВрд╕реНрдХреГрддрд┐рдХ рдХреНрд╖рдорддрд╛рдПрдВ
- **рд╡рд┐рд╢реЗрд╖рдЬреНрдЮрддрд╛**: рд╡рд┐рд╢реЗрд╖ рдЙрдкрдпреЛрдЧ рдорд╛рдорд▓реЛрдВ рдХреЗ рд▓рд┐рдП рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рд╡реЗрд░рд┐рдПрдВрдЯреНрд╕

## Qwen рдореЙрдбрд▓ рджрд░реНрд╢рди

Qwen рдореЙрдбрд▓ рдкрд░рд┐рд╡рд╛рд░ AI рдореЙрдбрд▓ рд╡рд┐рдХрд╛рд╕ рдХреЗ рд▓рд┐рдП рдПрдХ рд╡реНрдпрд╛рдкрдХ рджреГрд╖реНрдЯрд┐рдХреЛрдг рдХрд╛ рдкреНрд░рддрд┐рдирд┐рдзрд┐рддреНрд╡ рдХрд░рддрд╛ рд╣реИ, рдЬреЛ рдУрдкрди-рд╕реЛрд░реНрд╕ рд╕реБрд▓рднрддрд╛, рдмрд╣реБрднрд╛рд╖реА рдХреНрд╖рдорддрд╛рдУрдВ рдФрд░ рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рдХреЛ рдкреНрд░рд╛рдердорд┐рдХрддрд╛ рджреЗрддрд╛ рд╣реИ, рдЬрдмрдХрд┐ рдкреНрд░рддрд┐рд╕реНрдкрд░реНрдзреА рдкреНрд░рджрд░реНрд╢рди рд╡рд┐рд╢реЗрд╖рддрд╛рдУрдВ рдХреЛ рдмрдирд╛рдП рд░рдЦрддрд╛ рд╣реИред Qwen рдореЙрдбрд▓ рд╡рд┐рд╡рд┐рдз рдореЙрдбрд▓ рдЖрдХрд╛рд░реЛрдВ, рдЙрдЪреНрдЪ-рдЧреБрдгрд╡рддреНрддрд╛ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдкрджреНрдзрддрд┐рдпреЛрдВ рдФрд░ рд╡рд┐рднрд┐рдиреНрди рдбреЛрдореЗрди рдХреЗ рд▓рд┐рдП рд╡рд┐рд╢реЗрд╖ рд╡реЗрд░рд┐рдПрдВрдЯреНрд╕ рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ рдЗрд╕реЗ рдкреНрд░рд╛рдкреНрдд рдХрд░рддреЗ рд╣реИрдВред

Qwen рдкрд░рд┐рд╡рд╛рд░ рдкреНрд░рджрд░реНрд╢рди-рджрдХреНрд╖рддрд╛ рд╕реНрдкреЗрдХреНрдЯреНрд░рдо рдореЗрдВ рд╡рд┐рдХрд▓реНрдк рдкреНрд░рджрд╛рди рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдбрд┐рдЬрд╝рд╛рдЗрди рдХрд┐рдП рдЧрдП рд╡рд┐рднрд┐рдиреНрди рджреГрд╖реНрдЯрд┐рдХреЛрдгреЛрдВ рдХреЛ рд╢рд╛рдорд┐рд▓ рдХрд░рддрд╛ рд╣реИ, рдЬрд┐рд╕рд╕реЗ рдореЛрдмрд╛рдЗрд▓ рдЙрдкрдХрд░рдгреЛрдВ рд╕реЗ рд▓реЗрдХрд░ рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЬрд╝ рд╕рд░реНрд╡рд░реЛрдВ рддрдХ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рд╕рдХреНрд╖рдо рд╣реЛрддрд╛ рд╣реИ, рдЬрдмрдХрд┐ рд╕рд╛рд░реНрдердХ AI рдХреНрд╖рдорддрд╛рдПрдВ рдкреНрд░рджрд╛рди рдХрд░рддрд╛ рд╣реИред рд▓рдХреНрд╖реНрдп рдЙрдЪреНрдЪ-рдЧреБрдгрд╡рддреНрддрд╛ рд╡рд╛рд▓реЗ AI рддрдХ рдкрд╣реБрдВрдЪ рдХреЛ рд▓реЛрдХрддрд╛рдВрддреНрд░рд┐рдХ рдмрдирд╛рдирд╛ рд╣реИ, рдЬрдмрдХрд┐ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рд╡рд┐рдХрд▓реНрдкреЛрдВ рдореЗрдВ рд▓рдЪреАрд▓рд╛рдкрди рдкреНрд░рджрд╛рди рдХрд░рдирд╛ рд╣реИред

### Qwen рдбрд┐рдЬрд╝рд╛рдЗрди рдХреЗ рдореБрдЦреНрдп рд╕рд┐рджреНрдзрд╛рдВрдд

Qwen рдореЙрдбрд▓ рдХрдИ рдореМрд▓рд┐рдХ рд╕рд┐рджреНрдзрд╛рдВрддреЛрдВ рдкрд░ рдЖрдзрд╛рд░рд┐рдд рд╣реИрдВ рдЬреЛ рдЙрдиреНрд╣реЗрдВ рдЕрдиреНрдп рднрд╛рд╖рд╛ рдореЙрдбрд▓ рдкрд░рд┐рд╡рд╛рд░реЛрдВ рд╕реЗ рдЕрд▓рдЧ рдХрд░рддреЗ рд╣реИрдВ:

- **рдУрдкрди-рд╕реЛрд░реНрд╕ рдкреНрд░рд╛рдердорд┐рдХрддрд╛**: рдЕрдиреБрд╕рдВрдзрд╛рди рдФрд░ рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рдЙрдкрдпреЛрдЧ рдХреЗ рд▓рд┐рдП рдкреВрд░реНрдг рдкрд╛рд░рджрд░реНрд╢рд┐рддрд╛ рдФрд░ рд╕реБрд▓рднрддрд╛
- **рд╡реНрдпрд╛рдкрдХ рдкреНрд░рд╢рд┐рдХреНрд╖рдг**: рдХрдИ рднрд╛рд╖рд╛рдУрдВ рдФрд░ рдбреЛрдореЗрди рдХреЛ рдХрд╡рд░ рдХрд░рдиреЗ рд╡рд╛рд▓реЗ рд╡рд┐рд╢рд╛рд▓, рд╡рд┐рд╡рд┐рдз рдбреЗрдЯрд╛рд╕реЗрдЯреНрд╕ рдкрд░ рдкреНрд░рд╢рд┐рдХреНрд╖рдг
- **рд╕реНрдХреЗрд▓реЗрдмрд▓ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░**: рд╡рд┐рднрд┐рдиреНрди рдЧрдгрдирд╛ рдЖрд╡рд╢реНрдпрдХрддрд╛рдУрдВ рд╕реЗ рдореЗрд▓ рдЦрд╛рдиреЗ рдХреЗ рд▓рд┐рдП рдХрдИ рдореЙрдбрд▓ рдЖрдХрд╛рд░
- **рд╡рд┐рд╢реЗрд╖рдЬреНрдЮрддрд╛ рдореЗрдВ рдЙрддреНрдХреГрд╖реНрдЯрддрд╛**: рд╡рд┐рд╢реЗрд╖ рдХрд╛рд░реНрдпреЛрдВ рдХреЗ рд▓рд┐рдП рдЕрдиреБрдХреВрд▓рд┐рдд рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рд╡реЗрд░рд┐рдПрдВрдЯреНрд╕

## Qwen рдкрд░рд┐рд╡рд╛рд░ рдХреЛ рд╕рдХреНрд╖рдо рдХрд░рдиреЗ рд╡рд╛рд▓реА рдкреНрд░рдореБрдЦ рддрдХрдиреАрдХреЗрдВ

### рдмрдбрд╝реЗ рдкреИрдорд╛рдиреЗ рдкрд░ рдкреНрд░рд╢рд┐рдХреНрд╖рдг

Qwen рдкрд░рд┐рд╡рд╛рд░ рдХреА рдкрд░рд┐рднрд╛рд╖рд┐рдд рд╡рд┐рд╢реЗрд╖рддрд╛рдУрдВ рдореЗрдВ рд╕реЗ рдПрдХ рдореЙрдбрд▓ рд╡рд┐рдХрд╛рд╕ рдореЗрдВ рдирд┐рд╡реЗрд╢ рдХрд┐рдП рдЧрдП рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдбреЗрдЯрд╛ рдФрд░ рдЧрдгрдирд╛ рд╕рдВрд╕рд╛рдзрдиреЛрдВ рдХрд╛ рд╡рд┐рд╢рд╛рд▓ рдкреИрдорд╛рдирд╛ рд╣реИред Qwen рдореЙрдбрд▓ рд╕рд╛рд╡рдзрд╛рдиреАрдкреВрд░реНрд╡рдХ рдХреНрдпреВрд░реЗрдЯ рдХрд┐рдП рдЧрдП, рдмрд╣реБрднрд╛рд╖реА рдбреЗрдЯрд╛рд╕реЗрдЯреНрд╕ рдХрд╛ рд▓рд╛рдн рдЙрдард╛рддреЗ рд╣реИрдВ рдЬреЛ рдЯреНрд░рд┐рд▓рд┐рдпрдиреЛрдВ рдЯреЛрдХрди рддрдХ рдлреИрд▓реЗ рд╣реБрдП рд╣реИрдВ, рдЬреЛ рд╡реНрдпрд╛рдкрдХ рд╡рд┐рд╢реНрд╡ рдЬреНрдЮрд╛рди рдФрд░ рддрд░реНрдХ рдХреНрд╖рдорддрд╛рдПрдВ рдкреНрд░рджрд╛рди рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдбрд┐рдЬрд╝рд╛рдЗрди рдХрд┐рдП рдЧрдП рд╣реИрдВред

рдпрд╣ рджреГрд╖реНрдЯрд┐рдХреЛрдг рдЙрдЪреНрдЪ-рдЧреБрдгрд╡рддреНрддрд╛ рд╡рд╛рд▓реЗ рд╡реЗрдм рд╕рд╛рдордЧреНрд░реА, рд╢реИрдХреНрд╖рдгрд┐рдХ рд╕рд╛рд╣рд┐рддреНрдп, рдХреЛрдб рд░рд┐рдкреЙрдЬрд┐рдЯрд░реА рдФрд░ рдмрд╣реБрднрд╛рд╖реА рд╕рдВрд╕рд╛рдзрдиреЛрдВ рдХреЛ рдЬреЛрдбрд╝рдХрд░ рдХрд╛рдо рдХрд░рддрд╛ рд╣реИред рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдкрджреНрдзрддрд┐ рд╡рд┐рднрд┐рдиреНрди рдбреЛрдореЗрди рдФрд░ рднрд╛рд╖рд╛рдУрдВ рдореЗрдВ рдЬреНрдЮрд╛рди рдХреА рдЧрд╣рд░рд╛рдИ рдФрд░ рдЪреМрдбрд╝рд╛рдИ рджреЛрдиреЛрдВ рдкрд░ рдЬреЛрд░ рджреЗрддреА рд╣реИред

### рдЙрдиреНрдирдд рддрд░реНрдХ рдФрд░ рд╕реЛрдЪ

рд╣рд╛рд▓ рдХреЗ Qwen рдореЙрдбрд▓ рдкрд░рд┐рд╖реНрдХреГрдд рддрд░реНрдХ рдХреНрд╖рдорддрд╛рдУрдВ рдХреЛ рд╢рд╛рдорд┐рд▓ рдХрд░рддреЗ рд╣реИрдВ рдЬреЛ рдЬрдЯрд┐рд▓ рдмрд╣реБ-рдЪрд░рдг рд╕рдорд╕реНрдпрд╛ рд╕рдорд╛рдзрд╛рди рдХреЛ рд╕рдХреНрд╖рдо рдХрд░рддреЗ рд╣реИрдВ:

**рдерд┐рдВрдХрд┐рдВрдЧ рдореЛрдб (Qwen3)**: рдореЙрдбрд▓ рдЕрдВрддрд┐рдо рдЙрддреНрддрд░ рдкреНрд░рджрд╛рди рдХрд░рдиреЗ рд╕реЗ рдкрд╣рд▓реЗ рд╡рд┐рд╕реНрддреГрдд рдЪрд░рдг-рджрд░-рдЪрд░рдг рддрд░реНрдХ рдореЗрдВ рд╕рдВрд▓рдЧреНрди рд╣реЛ рд╕рдХрддреЗ рд╣реИрдВ, рдЬреЛ рдорд╛рдирд╡ рд╕рдорд╕реНрдпрд╛ рд╕рдорд╛рдзрд╛рди рджреГрд╖реНрдЯрд┐рдХреЛрдг рдХреЗ рд╕рдорд╛рди рд╣реИред

**рдбреБрдЕрд▓-рдореЛрдб рдСрдкрд░реЗрд╢рди**: рд╕рд░рд▓ рдкреНрд░рд╢реНрдиреЛрдВ рдХреЗ рд▓рд┐рдП рддреНрд╡рд░рд┐рдд рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рдореЛрдб рдФрд░ рдЬрдЯрд┐рд▓ рд╕рдорд╕реНрдпрд╛рдУрдВ рдХреЗ рд▓рд┐рдП рдЧрд╣рди рд╕реЛрдЪ рдореЛрдб рдХреЗ рдмреАрдЪ рд╕реНрд╡рд┐рдЪ рдХрд░рдиреЗ рдХреА рдХреНрд╖рдорддрд╛ред

**рдЪреЗрди-рдСрдл-рдереЙрдЯ рдЗрдВрдЯреАрдЧреНрд░реЗрд╢рди**: рддрд░реНрдХ рдЪрд░рдгреЛрдВ рдХрд╛ рдкреНрд░рд╛рдХреГрддрд┐рдХ рд╕рдорд╛рд╡реЗрд╢ рдЬреЛ рдЬрдЯрд┐рд▓ рдХрд╛рд░реНрдпреЛрдВ рдореЗрдВ рдкрд╛рд░рджрд░реНрд╢рд┐рддрд╛ рдФрд░ рд╕рдЯреАрдХрддрд╛ рдореЗрдВ рд╕реБрдзрд╛рд░ рдХрд░рддрд╛ рд╣реИред

### рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░рд▓ рдирд╡рд╛рдЪрд╛рд░

Qwen рдкрд░рд┐рд╡рд╛рд░ рдкреНрд░рджрд░реНрд╢рди рдФрд░ рджрдХреНрд╖рддрд╛ рджреЛрдиреЛрдВ рдХреЗ рд▓рд┐рдП рдбрд┐рдЬрд╝рд╛рдЗрди рдХрд┐рдП рдЧрдП рдХрдИ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░рд▓ рдЕрдиреБрдХреВрд▓рди рдХреЛ рд╢рд╛рдорд┐рд▓ рдХрд░рддрд╛ рд╣реИ:

**рд╕реНрдХреЗрд▓реЗрдмрд▓ рдбрд┐рдЬрд╝рд╛рдЗрди**: рдореЙрдбрд▓ рдЖрдХрд╛рд░реЛрдВ рдореЗрдВ рд╕реБрд╕рдВрдЧрдд рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░ рдЬреЛ рдЖрд╕рд╛рди рд╕реНрдХреЗрд▓рд┐рдВрдЧ рдФрд░ рддреБрд▓рдирд╛ рдХреЛ рд╕рдХреНрд╖рдо рдХрд░рддрд╛ рд╣реИред

**рдорд▓реНрдЯреАрдореЙрдбрд▓ рдЗрдВрдЯреАрдЧреНрд░реЗрд╢рди**: рдПрдХреАрдХреГрдд рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░ рдХреЗ рднреАрддрд░ рдЯреЗрдХреНрд╕реНрдЯ, рд╡рд┐рдЬрд╝рди рдФрд░ рдСрдбрд┐рдпреЛ рдкреНрд░реЛрд╕реЗрд╕рд┐рдВрдЧ рдХреНрд╖рдорддрд╛рдУрдВ рдХрд╛ рд╕рд╣рдЬ рдПрдХреАрдХрд░рдгред

**рдкрд░рд┐рдирд┐рдпреЛрдЬрди рдЕрдиреБрдХреВрд▓рди**: рд╡рд┐рднрд┐рдиреНрди рд╣рд╛рд░реНрдбрд╡реЗрдпрд░ рдХреЙрдиреНрдлрд╝рд┐рдЧрд░реЗрд╢рди рдХреЗ рд▓рд┐рдП рдХрдИ рдХреНрд╡рд╛рдВрдЯрд╛рдЗрдЬрд╝реЗрд╢рди рд╡рд┐рдХрд▓реНрдк рдФрд░ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рдкреНрд░рд╛рд░реВрдкред

## рдореЙрдбрд▓ рдЖрдХрд╛рд░ рдФрд░ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рд╡рд┐рдХрд▓реНрдк

рдЖрдзреБрдирд┐рдХ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рд╡рд╛рддрд╛рд╡рд░рдг Qwen рдореЙрдбрд▓ рдХреА рд╡рд┐рднрд┐рдиреНрди рдЧрдгрдирд╛ рдЖрд╡рд╢реНрдпрдХрддрд╛рдУрдВ рдореЗрдВ рд▓рдЪреАрд▓рд╛рдкрди рд╕реЗ рд▓рд╛рднрд╛рдиреНрд╡рд┐рдд рд╣реЛрддреЗ рд╣реИрдВ:

### рдЫреЛрдЯреЗ рдореЙрдбрд▓ (0.5B-3B)

Qwen рдЫреЛрдЯреЗ рдореЙрдбрд▓ рдкреНрд░рджрд╛рди рдХрд░рддрд╛ рд╣реИ рдЬреЛ рдПрдЬ рдкрд░рд┐рдирд┐рдпреЛрдЬрди, рдореЛрдмрд╛рдЗрд▓ рдЕрдиреБрдкреНрд░рдпреЛрдЧреЛрдВ рдФрд░ рд╕рдВрд╕рд╛рдзрди-рд╕реАрдорд┐рдд рд╡рд╛рддрд╛рд╡рд░рдг рдХреЗ рд▓рд┐рдП рдЙрдкрдпреБрдХреНрдд рд╣реИрдВ, рдЬрдмрдХрд┐ рдкреНрд░рднрд╛рд╡рд╢рд╛рд▓реА рдХреНрд╖рдорддрд╛рдПрдВ рдмрдирд╛рдП рд░рдЦрддреЗ рд╣реИрдВред

### рдордзреНрдпрдо рдореЙрдбрд▓ (7B-32B)

рдордзреНрдп-рд╢реНрд░реЗрдгреА рдХреЗ рдореЙрдбрд▓ рдкреЗрд╢реЗрд╡рд░ рдЕрдиреБрдкреНрд░рдпреЛрдЧреЛрдВ рдХреЗ рд▓рд┐рдП рдЙрдиреНрдирдд рдХреНрд╖рдорддрд╛рдПрдВ рдкреНрд░рджрд╛рди рдХрд░рддреЗ рд╣реИрдВ, рдкреНрд░рджрд░реНрд╢рди рдФрд░ рдЧрдгрдирд╛ рдЖрд╡рд╢реНрдпрдХрддрд╛рдУрдВ рдХреЗ рдмреАрдЪ рдЙрддреНрдХреГрд╖реНрдЯ рд╕рдВрддреБрд▓рди рдкреНрд░рджрд╛рди рдХрд░рддреЗ рд╣реИрдВред

### рдмрдбрд╝реЗ рдореЙрдбрд▓ (72B+)

рдкреВрд░реНрдг-рд╕реНрдХреЗрд▓ рдореЙрдбрд▓ рдЕрддреНрдпрд╛рдзреБрдирд┐рдХ рдкреНрд░рджрд░реНрд╢рди рдкреНрд░рджрд╛рди рдХрд░рддреЗ рд╣реИрдВ, рдЬреЛ рдорд╛рдВрдЧ рд╡рд╛рд▓реЗ рдЕрдиреБрдкреНрд░рдпреЛрдЧреЛрдВ, рдЕрдиреБрд╕рдВрдзрд╛рди рдФрд░ рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЬрд╝ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рдХреЗ рд▓рд┐рдП рдЕрдзрд┐рдХрддрдо рдХреНрд╖рдорддрд╛ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реЛрддреА рд╣реИред

## Qwen рдореЙрдбрд▓ рдкрд░рд┐рд╡рд╛рд░ рдХреЗ рд▓рд╛рдн

### рдУрдкрди-рд╕реЛрд░реНрд╕ рд╕реБрд▓рднрддрд╛

Qwen рдореЙрдбрд▓ рдкреВрд░реНрдг рдкрд╛рд░рджрд░реНрд╢рд┐рддрд╛ рдФрд░ рдЕрдиреБрдХреВрд▓рди рдХреНрд╖рдорддрд╛рдПрдВ рдкреНрд░рджрд╛рди рдХрд░рддреЗ рд╣реИрдВ, рдЬрд┐рд╕рд╕реЗ рд╕рдВрдЧрдардиреЛрдВ рдХреЛ рдЕрдкрдиреЗ рд╡рд┐рд╢рд┐рд╖реНрдЯ рдЖрд╡рд╢реНрдпрдХрддрд╛рдУрдВ рдХреЗ рд▓рд┐рдП рдореЙрдбрд▓ рдХреЛ рд╕рдордЭрдиреЗ, рд╕рдВрд╢реЛрдзрд┐рдд рдХрд░рдиреЗ рдФрд░ рдЕрдиреБрдХреВрд▓рд┐рдд рдХрд░рдиреЗ рдореЗрдВ рд╕рдХреНрд╖рдо рдмрдирд╛рдпрд╛ рдЬрд╛ рд╕рдХреЗ, рдмрд┐рдирд╛ рд╡рд┐рдХреНрд░реЗрддрд╛ рд▓реЙрдХ-рдЗрди рдХреЗред

### рдкрд░рд┐рдирд┐рдпреЛрдЬрди рд▓рдЪреАрд▓рд╛рдкрди

рдореЙрдбрд▓ рдЖрдХрд╛рд░реЛрдВ рдХреА рд╢реНрд░реГрдВрдЦрд▓рд╛ рд╡рд┐рднрд┐рдиреНрди рд╣рд╛рд░реНрдбрд╡реЗрдпрд░ рдХреЙрдиреНрдлрд╝рд┐рдЧрд░реЗрд╢рди рдореЗрдВ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рдХреЛ рд╕рдХреНрд╖рдо рдмрдирд╛рддреА рд╣реИ, рдореЛрдмрд╛рдЗрд▓ рдЙрдкрдХрд░рдгреЛрдВ рд╕реЗ рд▓реЗрдХрд░ рдЙрдЪреНрдЪ-рд╕реНрддрд░реАрдп рд╕рд░реНрд╡рд░реЛрдВ рддрдХ, рд╕рдВрдЧрдардиреЛрдВ рдХреЛ рдЙрдирдХреЗ AI рдЗрдВрдлреНрд░рд╛рд╕реНрдЯреНрд░рдХреНрдЪрд░ рд╡рд┐рдХрд▓реНрдкреЛрдВ рдореЗрдВ рд▓рдЪреАрд▓рд╛рдкрди рдкреНрд░рджрд╛рди рдХрд░рддреА рд╣реИред

### рдмрд╣реБрднрд╛рд╖реА рдЙрддреНрдХреГрд╖реНрдЯрддрд╛

Qwen рдореЙрдбрд▓ рдмрд╣реБрднрд╛рд╖реА рд╕рдордЭ рдФрд░ рдкреАрдврд╝реА рдореЗрдВ рдЙрддреНрдХреГрд╖реНрдЯ рдкреНрд░рджрд░реНрд╢рди рдХрд░рддреЗ рд╣реИрдВ, рджрд░реНрдЬрдиреЛрдВ рднрд╛рд╖рд╛рдУрдВ рдХрд╛ рд╕рдорд░реНрдерди рдХрд░рддреЗ рд╣реИрдВ, рд╡рд┐рд╢реЗрд╖ рд░реВрдк рд╕реЗ рдЕрдВрдЧреНрд░реЗрдЬреА рдФрд░ рдЪреАрдиреА рдореЗрдВ рдордЬрдмреВрдд, рдЬреЛ рдЙрдиреНрд╣реЗрдВ рд╡реИрд╢реНрд╡рд┐рдХ рдЕрдиреБрдкреНрд░рдпреЛрдЧреЛрдВ рдХреЗ рд▓рд┐рдП рдЙрдкрдпреБрдХреНрдд рдмрдирд╛рддрд╛ рд╣реИред

### рдкреНрд░рддрд┐рд╕реНрдкрд░реНрдзреА рдкреНрд░рджрд░реНрд╢рди

Qwen рдореЙрдбрд▓ рд▓рдЧрд╛рддрд╛рд░ рдмреЗрдВрдЪрдорд╛рд░реНрдХ рдкрд░ рдкреНрд░рддрд┐рд╕реНрдкрд░реНрдзреА рдкрд░рд┐рдгрд╛рдо рдкреНрд░рд╛рдкреНрдд рдХрд░рддреЗ рд╣реИрдВ, рдЬрдмрдХрд┐ рдУрдкрди-рд╕реЛрд░реНрд╕ рд╕реБрд▓рднрддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддреЗ рд╣реИрдВ, рдпрд╣ рдкреНрд░рджрд░реНрд╢рд┐рдд рдХрд░рддреЗ рд╣реБрдП рдХрд┐ рдУрдкрди рдореЙрдбрд▓ рдорд╛рд▓рд┐рдХрд╛рдирд╛ рд╡рд┐рдХрд▓реНрдкреЛрдВ рд╕реЗ рдореЗрд▓ рдЦрд╛ рд╕рдХрддреЗ рд╣реИрдВред

### рд╡рд┐рд╢реЗрд╖ рдХреНрд╖рдорддрд╛рдПрдВ

Qwen-Coder рдФрд░ Qwen-Math рдЬреИрд╕реЗ рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рд╡реЗрд░рд┐рдПрдВрдЯреНрд╕ рд╕рд╛рдорд╛рдиреНрдп рднрд╛рд╖рд╛ рд╕рдордЭ рдХреНрд╖рдорддрд╛рдУрдВ рдХреЛ рдмрдирд╛рдП рд░рдЦрддреЗ рд╣реБрдП рд╡рд┐рд╢реЗрд╖ рд╡рд┐рд╢реЗрд╖рдЬреНрдЮрддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддреЗ рд╣реИрдВред

## рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рдЙрджрд╛рд╣рд░рдг рдФрд░ рдЙрдкрдпреЛрдЧ рдХреЗ рдорд╛рдорд▓реЗ

рддрдХрдиреАрдХреА рд╡рд┐рд╡рд░рдг рдореЗрдВ рдЬрд╛рдиреЗ рд╕реЗ рдкрд╣рд▓реЗ, рдЖрдЗрдП рдХреБрдЫ рдареЛрд╕ рдЙрджрд╛рд╣рд░рдгреЛрдВ рдХрд╛ рдкрддрд╛ рд▓рдЧрд╛рдПрдВ рдХрд┐ Qwen рдореЙрдбрд▓ рдХреНрдпрд╛ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ:

### рдЧрдгрд┐рддреАрдп рддрд░реНрдХ рдЙрджрд╛рд╣рд░рдг

Qwen-Math рдЪрд░рдг-рджрд░-рдЪрд░рдг рдЧрдгрд┐рддреАрдп рд╕рдорд╕реНрдпрд╛ рд╕рдорд╛рдзрд╛рди рдореЗрдВ рдЙрддреНрдХреГрд╖реНрдЯ рд╣реИред рдЙрджрд╛рд╣рд░рдг рдХреЗ рд▓рд┐рдП, рдЬрдм рдЬрдЯрд┐рд▓ рдХреИрд▓рдХреБрд▓рд╕ рд╕рдорд╕реНрдпрд╛ рдХреЛ рд╣рд▓ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдХрд╣рд╛ рдЧрдпрд╛:

```
User: Find the derivative of f(x) = x┬│ + 2x┬▓ - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x┬│ + 2x┬▓ - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xтБ┐) = n┬╖xтБ┐тБ╗┬╣

Breaking down each term:
- d/dx(x┬│) = 3x┬▓
- d/dx(2x┬▓) = 2 ┬╖ 2x┬╣ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x┬▓ + 4x - 5
```

### рдмрд╣реБрднрд╛рд╖реА рд╕рдорд░реНрдерди рдЙрджрд╛рд╣рд░рдг

Qwen рдореЙрдбрд▓ рд╡рд┐рднрд┐рдиреНрди рднрд╛рд╖рд╛рдУрдВ рдореЗрдВ рдордЬрдмреВрдд рдмрд╣реБрднрд╛рд╖реА рдХреНрд╖рдорддрд╛рдУрдВ рдХрд╛ рдкреНрд░рджрд░реНрд╢рди рдХрд░рддреЗ рд╣реИрдВ:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**ф╕нцЦЗ:**
ф║║х╖ецЩ║шГ╜я╝ИAIя╝ЙцШпцМЗцЬ║хЩия╝ИчЙ╣хИлцШпшобчоЧцЬ║ч│╗ч╗Яя╝Йхп╣ф║║ч▒╗цЩ║шГ╜ш┐ЗчиЛчЪДцибцЛЯуАВш┐Щф║Ыш┐ЗчиЛхМЕцЛмхнжф╣ауАБцОичРЖуАБшзгхЖ│щЧощвШуАБцДЯчЯехТМшпншиАчРЖшзгуАВф║║х╖ецЩ║шГ╜ф╜┐цЬ║хЩишГ╜хдЯцЙзшбМщАЪх╕╕щЬАшжБф║║ч▒╗шодчЯешГ╜хКЫчЪДф╗╗хКбуАВ
```

### рдорд▓реНрдЯреАрдореЙрдбрд▓ рдХреНрд╖рдорддрд╛рдПрдВ рдЙрджрд╛рд╣рд░рдг

Qwen-VL рдЯреЗрдХреНрд╕реНрдЯ рдФрд░ рдЗрдореЗрдЬ рджреЛрдиреЛрдВ рдХреЛ рдПрдХ рд╕рд╛рде рдкреНрд░реЛрд╕реЗрд╕ рдХрд░ рд╕рдХрддрд╛ рд╣реИ:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### рдХреЛрдб рдЬрдирд░реЗрд╢рди рдЙрджрд╛рд╣рд░рдг

Qwen-Coder рдХрдИ рдкреНрд░реЛрдЧреНрд░рд╛рдорд┐рдВрдЧ рднрд╛рд╖рд╛рдУрдВ рдореЗрдВ рдХреЛрдб рдЙрддреНрдкрдиреНрди рдХрд░рдиреЗ рдФрд░ рд╕рдордЭрд╛рдиреЗ рдореЗрдВ рдЙрддреНрдХреГрд╖реНрдЯ рд╣реИ:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```

### рдПрдЬ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рдЙрджрд╛рд╣рд░рдг

Qwen рдореЙрдбрд▓ рд╡рд┐рднрд┐рдиреНрди рдПрдЬ рдЙрдкрдХрд░рдгреЛрдВ рдкрд░ рдЕрдиреБрдХреВрд▓рд┐рдд рдХреЙрдиреНрдлрд╝рд┐рдЧрд░реЗрд╢рди рдХреЗ рд╕рд╛рде рдкрд░рд┐рдирд┐рдпреЛрдЬрд┐рдд рдХрд┐рдП рдЬрд╛ рд╕рдХрддреЗ рд╣реИрдВ:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Qwen рдкрд░рд┐рд╡рд╛рд░ рдХрд╛ рд╡рд┐рдХрд╛рд╕

### Qwen 1.0 рдФрд░ 1.5: рдлрд╛рдЙрдВрдбреЗрд╢рди рдореЙрдбрд▓

рдкреНрд░рд╛рд░рдВрднрд┐рдХ Qwen рдореЙрдбрд▓ рд╡реНрдпрд╛рдкрдХ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдФрд░ рдУрдкрди-рд╕реЛрд░реНрд╕ рд╕реБрд▓рднрддрд╛ рдХреЗ рдореВрд▓ рд╕рд┐рджреНрдзрд╛рдВрддреЛрдВ рдХреЛ рд╕реНрдерд╛рдкрд┐рдд рдХрд░рддреЗ рд╣реИрдВ:

- **Qwen-7B (7B рдкреИрд░рд╛рдореАрдЯрд░)**: рдЪреАрдиреА рдФрд░ рдЕрдВрдЧреНрд░реЗрдЬреА рднрд╛рд╖рд╛ рд╕рдордЭ рдкрд░ рдзреНрдпрд╛рди рдХреЗрдВрджреНрд░рд┐рдд рдХрд░рддреЗ рд╣реБрдП рдкреНрд░рд╛рд░рдВрднрд┐рдХ рд░рд┐рд▓реАрдЬрд╝
- **Qwen-14B (14B рдкреИрд░рд╛рдореАрдЯрд░)**: рдЙрдиреНрдирдд рдХреНрд╖рдорддрд╛рдПрдВ рдмреЗрд╣рддрд░ рддрд░реНрдХ рдФрд░ рдЬреНрдЮрд╛рди рдХреЗ рд╕рд╛рде
- **Qwen-72B (72B рдкреИрд░рд╛рдореАрдЯрд░)**: рдмрдбрд╝реЗ рдкреИрдорд╛рдиреЗ рдХрд╛ рдореЙрдбрд▓ рдЕрддреНрдпрд╛рдзреБрдирд┐рдХ рдкреНрд░рджрд░реНрд╢рди рдкреНрд░рджрд╛рди рдХрд░рддрд╛ рд╣реИ
- **Qwen1.5 рд╕реАрд░реАрдЬ**: рдмреЗрд╣рддрд░ рд▓рдВрдмреЗ-рдХреЙрдиреНрдЯреЗрдХреНрд╕реНрдЯ рд╣реИрдВрдбрд▓рд┐рдВрдЧ рдХреЗ рд╕рд╛рде рдХрдИ рдЖрдХрд╛рд░реЛрдВ (0.5B рд╕реЗ 110B) рддрдХ рд╡рд┐рд╕реНрддрд╛рд░рд┐рдд

### Qwen2 рдкрд░рд┐рд╡рд╛рд░: рдорд▓реНрдЯреАрдореЙрдбрд▓ рд╡рд┐рд╕реНрддрд╛рд░

Qwen2 рд╢реНрд░реГрдВрдЦрд▓рд╛ рднрд╛рд╖рд╛ рдФрд░ рдорд▓реНрдЯреАрдореЙрдбрд▓ рдХреНрд╖рдорддрд╛рдУрдВ рдореЗрдВ рдорд╣рддреНрд╡рдкреВрд░реНрдг рдкреНрд░рдЧрддрд┐ рдХреЛ рдЪрд┐рд╣реНрдирд┐рдд рдХрд░рддреА рд╣реИ:

- **Qwen2-0.5B рд╕реЗ 72B**: рд╡рд┐рднрд┐рдиреНрди рдкрд░рд┐рдирд┐рдпреЛрдЬрди рдЖрд╡рд╢реНрдпрдХрддрд╛рдУрдВ рдХреЗ рд▓рд┐рдП рд╡реНрдпрд╛рдкрдХ рднрд╛рд╖рд╛ рдореЙрдбрд▓ рдХреА рд╢реНрд░реГрдВрдЦрд▓рд╛
- **Qwen2-57B-A14B (MoE)**: рдХреБрд╢рд▓ рдкреИрд░рд╛рдореАрдЯрд░ рдЙрдкрдпреЛрдЧ рдХреЗ рд▓рд┐рдП рдорд┐рд╢реНрд░рдг-рдСрдл-рдПрдХреНрд╕рдкрд░реНрдЯреНрд╕ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░
- **Qwen2-VL**: рдЗрдореЗрдЬ рд╕рдордЭрдиреЗ рдХреЗ рд▓рд┐рдП рдЙрдиреНрдирдд рд╡рд┐рдЬрд╝рди-рднрд╛рд╖рд╛ рдХреНрд╖рдорддрд╛рдПрдВ
- **Qwen2-Audio**: рдСрдбрд┐рдпреЛ рдкреНрд░реЛрд╕реЗрд╕рд┐рдВрдЧ рдФрд░ рд╕рдордЭрдиреЗ рдХреА рдХреНрд╖рдорддрд╛рдПрдВ
- **Qwen2-Math**: рд╡рд┐рд╢реЗрд╖ рдЧрдгрд┐рддреАрдп рддрд░реНрдХ рдФрд░ рд╕рдорд╕реНрдпрд╛ рд╕рдорд╛рдзрд╛рди

### Qwen2.5 рдкрд░рд┐рд╡рд╛рд░: рдЙрдиреНрдирдд рдкреНрд░рджрд░реНрд╢рди

Qwen2.5 рд╢реНрд░реГрдВрдЦрд▓рд╛ рд╕рднреА рдЖрдпрд╛рдореЛрдВ рдореЗрдВ рдорд╣рддреНрд╡рдкреВрд░реНрдг рд╕реБрдзрд╛рд░ рд▓рд╛рддреА рд╣реИ:

- **рд╡рд┐рд╕реНрддрд╛рд░рд┐рдд рдкреНрд░рд╢рд┐рдХреНрд╖рдг**: 18 рдЯреНрд░рд┐рд▓рд┐рдпрди рдЯреЛрдХрди рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдбреЗрдЯрд╛ рдмреЗрд╣рддрд░ рдХреНрд╖рдорддрд╛рдУрдВ рдХреЗ рд▓рд┐рдП
- **рд╡рд┐рд╕реНрддрд╛рд░рд┐рдд рдХреЙрдиреНрдЯреЗрдХреНрд╕реНрдЯ**: 128K рдЯреЛрдХрди рдХреЙрдиреНрдЯреЗрдХреНрд╕реНрдЯ рд▓рдВрдмрд╛рдИ, Turbo рд╡реЗрд░рд┐рдПрдВрдЯ 1M рдЯреЛрдХрди рдХрд╛ рд╕рдорд░реНрдерди рдХрд░рддрд╛ рд╣реИ
- **рдмреЗрд╣рддрд░ рд╡рд┐рд╢реЗрд╖рдЬреНрдЮрддрд╛**: рдЙрдиреНрдирдд Qwen2.5-Coder рдФрд░ Qwen2.5-Math рд╡реЗрд░рд┐рдПрдВрдЯреНрд╕
- **рдмреЗрд╣рддрд░ рдмрд╣реБрднрд╛рд╖реА рд╕рдорд░реНрдерди**: 27+ рднрд╛рд╖рд╛рдУрдВ рдореЗрдВ рдмреЗрд╣рддрд░ рдкреНрд░рджрд░реНрд╢рди

### Qwen3 рдкрд░рд┐рд╡рд╛рд░: рдЙрдиреНрдирдд рддрд░реНрдХ

рдирд╡реАрдирддрдо рдкреАрдврд╝реА рддрд░реНрдХ рдФрд░ рд╕реЛрдЪ рдХреНрд╖рдорддрд╛рдУрдВ рдХреА рд╕реАрдорд╛рдУрдВ рдХреЛ рдЖрдЧреЗ рдмрдврд╝рд╛рддреА рд╣реИ:

- **Qwen3-235B-A22B**: 235B рдХреБрд▓ рдкреИрд░рд╛рдореАрдЯрд░ рдХреЗ рд╕рд╛рде рдкреНрд░рдореБрдЦ рдорд┐рд╢реНрд░рдг-рдСрдл-рдПрдХреНрд╕рдкрд░реНрдЯреНрд╕ рдореЙрдбрд▓
- **Qwen3-30B-A3B**: рд╕рдХреНрд░рд┐рдп рдкреИрд░рд╛рдореАрдЯрд░ рдкреНрд░рддрд┐ рдордЬрдмреВрдд рдкреНрд░рджрд░реНрд╢рди рдХреЗ рд╕рд╛рде рдХреБрд╢рд▓ MoE рдореЙрдбрд▓
- **рдбреЗрдВрд╕ рдореЙрдбрд▓**: рд╡рд┐рднрд┐рдиреНрди рдкрд░рд┐рдирд┐рдпреЛрдЬрди рдкрд░рд┐рджреГрд╢реНрдпреЛрдВ рдХреЗ рд▓рд┐рдП Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B
- **рдерд┐рдВрдХрд┐рдВрдЧ рдореЛрдб**: рддреНрд╡рд░рд┐рдд рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛рдУрдВ рдФрд░ рдЧрд╣рди рд╕реЛрдЪ рдХрд╛ рд╕рдорд░реНрдерди рдХрд░рдиреЗ рд╡рд╛рд▓рд╛ рд╣рд╛рдЗрдмреНрд░рд┐рдб рддрд░реНрдХ рджреГрд╖реНрдЯрд┐рдХреЛрдг
- **рдмрд╣реБрднрд╛рд╖реА рдЙрддреНрдХреГрд╖реНрдЯрддрд╛**: 119 рднрд╛рд╖рд╛рдУрдВ рдФрд░ рдмреЛрд▓рд┐рдпреЛрдВ рдХрд╛ рд╕рдорд░реНрдерди
- **рдЙрдиреНрдирдд рдкреНрд░рд╢рд┐рдХреНрд╖рдг**: 36 рдЯреНрд░рд┐рд▓рд┐рдпрди рдЯреЛрдХрди рд╡рд┐рд╡рд┐рдз, рдЙрдЪреНрдЪ-рдЧреБрдгрд╡рддреНрддрд╛ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдбреЗрдЯрд╛

## Qwen рдореЙрдбрд▓ рдХреЗ рдЕрдиреБрдкреНрд░рдпреЛрдЧ

### рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЬрд╝ рдЕрдиреБрдкреНрд░рдпреЛрдЧ

рд╕рдВрдЧрдарди рджрд╕реНрддрд╛рд╡реЗрдЬрд╝ рд╡рд┐рд╢реНрд▓реЗрд╖рдг, рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛ рд╕реНрд╡рдЪрд╛рд▓рди, рдХреЛрдб рдЬрдирд░реЗрд╢рди рд╕рд╣рд╛рдпрддрд╛, рдФрд░ рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рдмреБрджреНрдзрд┐рдорддреНрддрд╛ рдЕрдиреБрдкреНрд░рдпреЛрдЧреЛрдВ рдХреЗ рд▓рд┐рдП Qwen рдореЙрдбрд▓ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддреЗ рд╣реИрдВред рдУрдкрди-рд╕реЛрд░реНрд╕ рдкреНрд░рдХреГрддрд┐ рд╡рд┐рд╢рд┐рд╖реНрдЯ рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рдЖрд╡рд╢реНрдпрдХрддрд╛рдУрдВ рдХреЗ рд▓рд┐рдП рдЕрдиреБрдХреВрд▓рди рдХреЛ рд╕рдХреНрд╖рдо рдХрд░рддреА рд╣реИ, рдЬрдмрдХрд┐ рдбреЗрдЯрд╛ рдЧреЛрдкрдиреАрдпрддрд╛ рдФрд░ рдирд┐рдпрдВрддреНрд░рдг рдмрдирд╛рдП рд░рдЦрддреА рд╣реИред

### рдореЛрдмрд╛рдЗрд▓ рдФрд░ рдПрдЬ рдХрдВрдкреНрдпреВрдЯрд┐рдВрдЧ

рдореЛрдмрд╛рдЗрд▓ рдЕрдиреБрдкреНрд░рдпреЛрдЧ рд╡рд╛рд╕реНрддрд╡рд┐рдХ рд╕рдордп рдЕрдиреБрд╡рд╛рдж, рдмреБрджреНрдзрд┐рдорд╛рди рд╕рд╣рд╛рдпрдХ, рд╕рд╛рдордЧреНрд░реА рдирд┐рд░реНрдорд╛рдг, рдФрд░ рд╡реНрдпрдХреНрддрд┐рдЧрдд рдЕрдиреБрд╢рдВрд╕рд╛рдУрдВ рдХреЗ рд▓рд┐рдП Qwen рдореЙрдбрд▓ рдХрд╛ рд▓рд╛рдн рдЙрдард╛рддреЗ рд╣реИрдВред рдореЙрдбрд▓ рдЖрдХрд╛рд░реЛрдВ рдХреА рд╢реНрд░реГрдВрдЦрд▓рд╛ рдореЛрдмрд╛рдЗрд▓ рдЙрдкрдХрд░рдгреЛрдВ рд╕реЗ рд▓реЗрдХрд░ рдПрдЬ рд╕рд░реНрд╡рд░реЛрдВ рддрдХ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рдХреЛ рд╕рдХреНрд╖рдо рдХрд░рддреА рд╣реИред

### рд╢реИрдХреНрд╖рд┐рдХ рдкреНрд░реМрджреНрдпреЛрдЧрд┐рдХреА

рд╢реИрдХреНрд╖рд┐рдХ рдкреНрд▓реЗрдЯрдлрд╝реЙрд░реНрдо рд╡реНрдпрдХреНрддрд┐рдЧрдд рдЯреНрдпреВрдЯрд░рд┐рдВрдЧ, рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд╕рд╛рдордЧреНрд░реА рдирд┐рд░реНрдорд╛рдг, рднрд╛рд╖рд╛ рд╕реАрдЦрдиреЗ рдХреА рд╕рд╣рд╛рдпрддрд╛, рдФрд░ рдЗрдВрдЯрд░реИрдХреНрдЯрд┐рд╡ рд╢реИрдХреНрд╖рд┐рдХ рдЕрдиреБрднрд╡реЛрдВ рдХреЗ рд▓рд┐рдП Qwen рдореЙрдбрд▓ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддреЗ рд╣реИрдВред Qwen-Math рдЬреИрд╕реЗ рд╡рд┐рд╢реЗрд╖ рдореЙрдбрд▓ рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рд╡рд┐рд╢реЗрд╖рдЬреНрдЮрддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддреЗ рд╣реИрдВред

### рд╡реИрд╢реНрд╡рд┐рдХ рдЕрдиреБрдкреНрд░рдпреЛрдЧ

рдЕрдВрддрд░реНрд░рд╛рд╖реНрдЯреНрд░реАрдп рдЕрдиреБрдкреНрд░рдпреЛрдЧ Qwen рдореЙрдбрд▓ рдХреА рдордЬрдмреВрдд рдмрд╣реБрднрд╛рд╖реА рдХреНрд╖рдорддрд╛рдУрдВ рд╕реЗ рд▓рд╛рднрд╛рдиреНрд╡рд┐рдд рд╣реЛрддреЗ рд╣реИрдВ, рдЬреЛ рд╡рд┐рднрд┐рдиреНрди рднрд╛рд╖рд╛рдУрдВ рдФрд░ рд╕рд╛рдВрд╕реНрдХреГрддрд┐рдХ рд╕рдВрджрд░реНрднреЛрдВ рдореЗрдВ рд╕реБрд╕рдВрдЧрдд AI рдЕрдиреБрднрд╡ рд╕рдХреНрд╖рдо рдХрд░рддреЗ рд╣реИрдВред

## рдЪреБрдиреМрддрд┐рдпрд╛рдВ рдФрд░ рд╕реАрдорд╛рдПрдВ

### рдЧрдгрдирд╛ рдЖрд╡рд╢реНрдпрдХрддрд╛рдПрдВ

рд╣рд╛рд▓рд╛рдВрдХрд┐ Qwen рд╡рд┐рднрд┐рдиреНрди рдЖрдХрд╛рд░реЛрдВ рдореЗрдВ рдореЙрдбрд▓ рдкреНрд░рджрд╛рди рдХрд░рддрд╛ рд╣реИ, рдмрдбрд╝реЗ рд╡реЗрд░рд┐рдПрдВрдЯреНрд╕ рдЕрднреА рднреА рдЗрд╖реНрдЯрддрдо рдкреНрд░рджрд░реНрд╢рди рдХреЗ рд▓рд┐рдП рдорд╣рддреНрд╡рдкреВрд░реНрдг рдЧрдгрдирд╛ рд╕рдВрд╕рд╛рдзрдиреЛрдВ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реЛрддреА рд╣реИ, рдЬреЛ рдХреБрдЫ рд╕рдВрдЧрдардиреЛрдВ рдХреЗ рд▓рд┐рдП рдкрд░рд┐рдирд┐рдпреЛрдЬрди рд╡рд┐рдХрд▓реНрдкреЛрдВ рдХреЛ рд╕реАрдорд┐рдд рдХрд░ рд╕рдХрддрд╛ рд╣реИред

### рд╡рд┐рд╢реЗрд╖ рдбреЛрдореЗрди рдкреНрд░рджрд░реНрд╢рди

рд╣рд╛рд▓рд╛рдВрдХрд┐ Qwen рдореЙрдбрд▓ рд╕рд╛рдорд╛рдиреНрдп рдбреЛрдореЗрди рдореЗрдВ рдЕрдЪреНрдЫрд╛ рдкреНрд░рджрд░реНрд╢рди рдХрд░рддреЗ рд╣реИрдВ, рдЕрддреНрдпрдзрд┐рдХ рд╡рд┐рд╢реЗрд╖ рдЕрдиреБрдкреНрд░рдпреЛрдЧ рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рдлрд╛рдЗрди-рдЯреНрдпреВрдирд┐рдВрдЧ рдпрд╛ рд╡рд┐рд╢реЗрд╖ рдореЙрдбрд▓ рд╕реЗ рд▓рд╛рдн рдЙрдард╛ рд╕рдХрддреЗ рд╣реИрдВред

### рдореЙрдбрд▓ рдЪрдпрди рдХреА рдЬрдЯрд┐рд▓рддрд╛

рдЙрдкрд▓рдмреНрдз рдореЙрдбрд▓реЛрдВ рдФрд░ рд╡реЗрд░рд┐рдПрдВрдЯреНрд╕ рдХреА рд╡рд┐рд╕реНрддреГрдд рд╢реНрд░реГрдВрдЦрд▓рд╛ рдирдП рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛рдУрдВ рдХреЗ рд▓рд┐рдП рдЪрдпрди рдХреЛ рдЪреБрдиреМрддреАрдкреВрд░реНрдг рдмрдирд╛ рд╕рдХрддреА рд╣реИред

### рднрд╛рд╖рд╛ рдЕрд╕рдВрддреБрд▓рди

рд╣рд╛рд▓рд╛рдВрдХрд┐ рдХрдИ рднрд╛рд╖рд╛рдУрдВ рдХрд╛ рд╕рдорд░реНрдерди рдХрд░рддреЗ рд╣реИрдВ, рдкреНрд░рджрд░реНрд╢рди рд╡рд┐рднрд┐рдиреНрди рднрд╛рд╖рд╛рдУрдВ рдореЗрдВ рднрд┐рдиреНрди рд╣реЛ рд╕рдХрддрд╛ рд╣реИ, рдЕрдВрдЧреНрд░реЗрдЬреА рдФрд░ рдЪреАрдиреА рдореЗрдВ рд╕рдмрд╕реЗ рдордЬрдмреВрдд рдХреНрд╖рдорддрд╛рдУрдВ рдХреЗ рд╕рд╛рдеред

## Qwen рдореЙрдбрд▓ рдкрд░рд┐рд╡рд╛рд░ рдХрд╛ рднрд╡рд┐рд╖реНрдп

Qwen рдореЙрдбрд▓ рдкрд░рд┐рд╡рд╛рд░ рд▓реЛрдХрддрд╛рдВрддреНрд░рд┐рдХ, рдЙрдЪреНрдЪ-рдЧреБрдгрд╡рддреНрддрд╛ AI рдХреА рдУрд░ рдЪрд▓ рд░рд╣реЗ рд╡рд┐рдХрд╛рд╕ рдХрд╛ рдкреНрд░рддрд┐рдирд┐рдзрд┐рддреНрд╡ рдХрд░рддрд╛ рд╣реИред рднрд╡рд┐рд╖реНрдп рдХреЗ рд╡рд┐рдХрд╛рд╕ рдореЗрдВ рдЙрдиреНрдирдд рджрдХреНрд╖рддрд╛ рдЕрдиреБрдХреВрд▓рди, рд╡рд┐рд╕реНрддрд╛рд░рд┐рдд рдорд▓реНрдЯреАрдореЙрдбрд▓ рдХреНрд╖рдорддрд╛рдПрдВ, рдмреЗрд╣рддрд░ рддрд░реНрдХ рддрдВрддреНрд░, рдФрд░ рд╡рд┐рднрд┐рдиреНрди рдкрд░рд┐рдирд┐рдпреЛрдЬрди рдкрд░рд┐рджреГрд╢реНрдпреЛрдВ рдореЗрдВ рдмреЗрд╣рддрд░ рдПрдХреАрдХрд░рдг рд╢рд╛рдорд┐рд▓ рд╣реИрдВред

рдЬреИрд╕реЗ-рдЬреИрд╕реЗ рддрдХрдиреАрдХ рд╡рд┐рдХрд╕рд┐рдд рд╣реЛрддреА рд░рд╣рддреА рд╣реИ, рд╣рдо рдЙрдореНрдореАрдж рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ рдХрд┐ Qwen
Qwen рдореЙрдбрд▓реНрд╕ рдХрд╛ рдЙрдкрдпреЛрдЧ рд╢реБрд░реВ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП Hugging Face Transformers рд▓рд╛рдЗрдмреНрд░реЗрд░реА рдХрд╛ рдЙрдкрдпреЛрдЧ рдХреИрд╕реЗ рдХрд░реЗрдВ:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Qwen2.5 рдореЙрдбрд▓реНрд╕ рдХрд╛ рдЙрдкрдпреЛрдЧ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### рд╡рд┐рд╢реЗрд╖ рдореЙрдбрд▓ рдЙрдкрдпреЛрдЧ

**Qwen-Coder рдХреЗ рд╕рд╛рде рдХреЛрдб рдЬрдирд░реЗрд╢рди:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**рдЧрдгрд┐рддреАрдп рд╕рдорд╕реНрдпрд╛рдУрдВ рдХрд╛ рд╕рдорд╛рдзрд╛рди:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = x┬│ + 2x┬▓ - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**рд╡рд┐рдЬрд╝рди-рд▓реИрдВрдЧреНрд╡реЗрдЬ рдЯрд╛рд╕реНрдХреНрд╕:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### рдерд┐рдВрдХрд┐рдВрдЧ рдореЛрдб (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### ЁЯУ▒ рдореЛрдмрд╛рдЗрд▓ рдФрд░ рдПрдЬ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### API рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдЙрджрд╛рд╣рд░рдг

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## рдкреНрд░рджрд░реНрд╢рди рдмреЗрдВрдЪрдорд╛рд░реНрдХ рдФрд░ рдЙрдкрд▓рдмреНрдзрд┐рдпрд╛рдВ

Qwen рдореЙрдбрд▓ рдкрд░рд┐рд╡рд╛рд░ рдиреЗ рд╡рд┐рднрд┐рдиреНрди рдмреЗрдВрдЪрдорд╛рд░реНрдХреНрд╕ рдкрд░ рдЙрддреНрдХреГрд╖реНрдЯ рдкреНрд░рджрд░реНрд╢рди рд╣рд╛рд╕рд┐рд▓ рдХрд┐рдпрд╛ рд╣реИ, рд╕рд╛рде рд╣реА рдУрдкрди-рд╕реЛрд░реНрд╕ рдПрдХреНрд╕реЗрд╕рд┐рдмрд┐рд▓рд┐рдЯреА рдмрдирд╛рдП рд░рдЦреА рд╣реИ:

### рдкреНрд░рдореБрдЦ рдкреНрд░рджрд░реНрд╢рди рд╣рд╛рдЗрд▓рд╛рдЗрдЯреНрд╕

**рддрд░реНрдХ рдХреНрд╖рдорддрд╛ рдореЗрдВ рдЙрддреНрдХреГрд╖реНрдЯрддрд╛:**
- Qwen3-235B-A22B рдиреЗ рдХреЛрдбрд┐рдВрдЧ, рдЧрдгрд┐рдд, рдФрд░ рд╕рд╛рдорд╛рдиреНрдп рдХреНрд╖рдорддрд╛рдУрдВ рдХреЗ рдмреЗрдВрдЪрдорд╛рд░реНрдХ рдореВрд▓реНрдпрд╛рдВрдХрди рдореЗрдВ DeepSeek-R1, o1, o3-mini, Grok-3, рдФрд░ Gemini-2.5-Pro рдЬреИрд╕реЗ рд╢реАрд░реНрд╖ рд╕реНрддрд░реАрдп рдореЙрдбрд▓реНрд╕ рдХреЗ рд╕рд╛рде рдкреНрд░рддрд┐рд╕реНрдкрд░реНрдзрд╛рддреНрдордХ рдкрд░рд┐рдгрд╛рдо рдкреНрд░рд╛рдкреНрдд рдХрд┐рдП рд╣реИрдВред
- Qwen3-30B-A3B рдиреЗ QwQ-32B рдХреЛ 10 рдЧреБрдирд╛ рд╕рдХреНрд░рд┐рдп рдкреИрд░рд╛рдореАрдЯрд░реНрд╕ рдХреЗ рд╕рд╛рде рдкреАрдЫреЗ рдЫреЛрдбрд╝ рджрд┐рдпрд╛ рд╣реИред
- Qwen3-4B, Qwen2.5-72B-Instruct рдХреЗ рдкреНрд░рджрд░реНрд╢рди рдХреЗ рдмрд░рд╛рдмрд░ рд╣реИред

**рдХреНрд╖рдорддрд╛ рдореЗрдВ рд╕реБрдзрд╛рд░:**
- Qwen3-MoE рдмреЗрд╕ рдореЙрдбрд▓реНрд╕ рдиреЗ Qwen2.5 рдбреЗрдВрд╕ рдмреЗрд╕ рдореЙрдбрд▓реНрд╕ рдХреЗ рд╕рдорд╛рди рдкреНрд░рджрд░реНрд╢рди рдкреНрд░рд╛рдкреНрдд рдХрд┐рдпрд╛ рд╣реИ, рдЬрдмрдХрд┐ рдХреЗрд╡рд▓ 10% рд╕рдХреНрд░рд┐рдп рдкреИрд░рд╛рдореАрдЯрд░реНрд╕ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рд╣реИред
- рдбреЗрдВрд╕ рдореЙрдбрд▓реНрд╕ рдХреА рддреБрд▓рдирд╛ рдореЗрдВ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдФрд░ рдЗрдиреНрдлреНрд░реЗрдВрд╕ рдореЗрдВ рдорд╣рддреНрд╡рдкреВрд░реНрдг рд▓рд╛рдЧрдд рдмрдЪрддред

**рдмрд╣реБрднрд╛рд╖реА рдХреНрд╖рдорддрд╛рдПрдВ:**
- Qwen3 рдореЙрдбрд▓реНрд╕ 119 рднрд╛рд╖рд╛рдУрдВ рдФрд░ рдмреЛрд▓рд┐рдпреЛрдВ рдХрд╛ рд╕рдорд░реНрдерди рдХрд░рддреЗ рд╣реИрдВред
- рд╡рд┐рд╡рд┐рдз рднрд╛рд╖рд╛рдИ рдФрд░ рд╕рд╛рдВрд╕реНрдХреГрддрд┐рдХ рд╕рдВрджрд░реНрднреЛрдВ рдореЗрдВ рдордЬрдмреВрдд рдкреНрд░рджрд░реНрд╢рдиред

**рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдкреИрдорд╛рдирд╛:**
- Qwen3 рдиреЗ рд▓рдЧрднрдЧ 36 рдЯреНрд░рд┐рд▓рд┐рдпрди рдЯреЛрдХрдиреНрд╕ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рд╣реИ, рдЬреЛ 119 рднрд╛рд╖рд╛рдУрдВ рдФрд░ рдмреЛрд▓рд┐рдпреЛрдВ рдХреЛ рдХрд╡рд░ рдХрд░рддрд╛ рд╣реИ, рдЬрдмрдХрд┐ Qwen2.5 рдиреЗ 18 рдЯреНрд░рд┐рд▓рд┐рдпрди рдЯреЛрдХрдиреНрд╕ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдерд╛ред

### рдореЙрдбрд▓ рддреБрд▓рдирд╛ рдореИрдЯреНрд░рд┐рдХреНрд╕

| рдореЙрдбрд▓ рд╕реАрд░реАрдЬ | рдкреИрд░рд╛рдореАрдЯрд░реНрд╕ рд░реЗрдВрдЬ | рдХреЙрдиреНрдЯреЗрдХреНрд╕реНрдЯ рд▓рдВрдмрд╛рдИ | рдкреНрд░рдореБрдЦ рддрд╛рдХрддреЗрдВ | рд╕рд░реНрд╡реЛрддреНрддрдо рдЙрдкрдпреЛрдЧ рдХреЗ рдорд╛рдорд▓реЗ |
|------------|-----------------|------------------|---------------|--------------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | рд╕рдВрддреБрд▓рд┐рдд рдкреНрд░рджрд░реНрд╢рди, рдмрд╣реБрднрд╛рд╖реА | рд╕рд╛рдорд╛рдиреНрдп рдПрдкреНрд▓рд┐рдХреЗрд╢рди, рдкреНрд░реЛрдбрдХреНрд╢рди рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | рдХреЛрдб рдЬрдирд░реЗрд╢рди, рдкреНрд░реЛрдЧреНрд░рд╛рдорд┐рдВрдЧ | рд╕реЙрдлрд╝реНрдЯрд╡реЗрдпрд░ рд╡рд┐рдХрд╛рд╕, рдХреЛрдбрд┐рдВрдЧ рд╕рд╣рд╛рдпрддрд╛ |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | рдЧрдгрд┐рддреАрдп рддрд░реНрдХ | рд╢реИрдХреНрд╖рд┐рдХ рдкреНрд▓реЗрдЯрдлреЙрд░реНрдо, STEM рдПрдкреНрд▓рд┐рдХреЗрд╢рди |
| **Qwen2.5-VL** | рд╡рд┐рднрд┐рдиреНрди | рдкрд░рд┐рд╡рд░реНрддрдирд╢реАрд▓ | рд╡рд┐рдЬрд╝рди-рд▓реИрдВрдЧреНрд╡реЗрдЬ рд╕рдордЭ | рдорд▓реНрдЯреАрдореЙрдбрд▓ рдПрдкреНрд▓рд┐рдХреЗрд╢рди, рдЗрдореЗрдЬ рдПрдирд╛рд▓рд┐рд╕рд┐рд╕ |
| **Qwen3** | 0.6B-235B | рдкрд░рд┐рд╡рд░реНрддрдирд╢реАрд▓ | рдЙрдиреНрдирдд рддрд░реНрдХ, рдерд┐рдВрдХрд┐рдВрдЧ рдореЛрдб | рдЬрдЯрд┐рд▓ рддрд░реНрдХ, рд╢реЛрдз рдПрдкреНрд▓рд┐рдХреЗрд╢рди |
| **Qwen3 MoE** | 30B-235B рдХреБрд▓ | рдкрд░рд┐рд╡рд░реНрддрдирд╢реАрд▓ | рдХреБрд╢рд▓ рдмрдбрд╝реЗ рдкреИрдорд╛рдиреЗ рдкрд░ рдкреНрд░рджрд░реНрд╢рди | рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЬ рдПрдкреНрд▓рд┐рдХреЗрд╢рди, рдЙрдЪреНрдЪ рдкреНрд░рджрд░реНрд╢рди рдЖрд╡рд╢реНрдпрдХрддрд╛рдПрдВ |

## рдореЙрдбрд▓ рдЪрдпрди рдЧрд╛рдЗрдб

### рд╕рд╛рдорд╛рдиреНрдп рдПрдкреНрд▓рд┐рдХреЗрд╢рди рдХреЗ рд▓рд┐рдП
- **Qwen2.5-0.5B/1.5B**: рдореЛрдмрд╛рдЗрд▓ рдРрдкреНрд╕, рдПрдЬ рдбрд┐рд╡рд╛рдЗрд╕, рд░рд┐рдпрд▓-рдЯрд╛рдЗрдо рдПрдкреНрд▓рд┐рдХреЗрд╢рди
- **Qwen2.5-3B/7B**: рд╕рд╛рдорд╛рдиреНрдп рдЪреИрдЯрдмреЙрдЯреНрд╕, рдХрдВрдЯреЗрдВрдЯ рдЬрдирд░реЗрд╢рди, Q&A рд╕рд┐рд╕реНрдЯрдореНрд╕

### рдЧрдгрд┐рддреАрдп рдФрд░ рддрд░реНрдХ рдХрд╛рд░реНрдпреЛрдВ рдХреЗ рд▓рд┐рдП
- **Qwen2.5-Math**: рдЧрдгрд┐рддреАрдп рд╕рдорд╕реНрдпрд╛ рд╕рдорд╛рдзрд╛рди рдФрд░ STEM рд╢рд┐рдХреНрд╖рд╛
- **Qwen3 рдерд┐рдВрдХрд┐рдВрдЧ рдореЛрдб рдХреЗ рд╕рд╛рде**: рдЬрдЯрд┐рд▓ рддрд░реНрдХ рдЬреЛ рдЪрд░рдг-рджрд░-рдЪрд░рдг рд╡рд┐рд╢реНрд▓реЗрд╖рдг рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реИ

### рдкреНрд░реЛрдЧреНрд░рд╛рдорд┐рдВрдЧ рдФрд░ рд╡рд┐рдХрд╛рд╕ рдХреЗ рд▓рд┐рдП
- **Qwen2.5-Coder**: рдХреЛрдб рдЬрдирд░реЗрд╢рди, рдбрд┐рдмрдЧрд┐рдВрдЧ, рдкреНрд░реЛрдЧреНрд░рд╛рдорд┐рдВрдЧ рд╕рд╣рд╛рдпрддрд╛
- **Qwen3**: рдЙрдиреНрдирдд рдкреНрд░реЛрдЧреНрд░рд╛рдорд┐рдВрдЧ рдХрд╛рд░реНрдп рддрд░реНрдХ рдХреНрд╖рдорддрд╛рдУрдВ рдХреЗ рд╕рд╛рде

### рдорд▓реНрдЯреАрдореЙрдбрд▓ рдПрдкреНрд▓рд┐рдХреЗрд╢рди рдХреЗ рд▓рд┐рдП
- **Qwen2.5-VL**: рдЗрдореЗрдЬ рд╕рдордЭ, рд╡рд┐рдЬрд╝реБрдЕрд▓ рдкреНрд░рд╢реНрди рдЙрддреНрддрд░
- **Qwen-Audio**: рдСрдбрд┐рдпреЛ рдкреНрд░реЛрд╕реЗрд╕рд┐рдВрдЧ рдФрд░ рд╕реНрдкреАрдЪ рд╕рдордЭ

### рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЬ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдХреЗ рд▓рд┐рдП
- **Qwen2.5-32B/72B**: рдЙрдЪреНрдЪ рдкреНрд░рджрд░реНрд╢рди рднрд╛рд╖рд╛ рд╕рдордЭ
- **Qwen3-235B-A22B**: рдорд╛рдВрдЧ рд╡рд╛рд▓реЗ рдПрдкреНрд▓рд┐рдХреЗрд╢рди рдХреЗ рд▓рд┐рдП рдЕрдзрд┐рдХрддрдо рдХреНрд╖рдорддрд╛

## рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдкреНрд▓реЗрдЯрдлреЙрд░реНрдореНрд╕ рдФрд░ рдПрдХреНрд╕реЗрд╕рд┐рдмрд┐рд▓рд┐рдЯреА
### рдХреНрд▓рд╛рдЙрдб рдкреНрд▓реЗрдЯрдлреЙрд░реНрдореНрд╕
- **Hugging Face Hub**: рд╡реНрдпрд╛рдкрдХ рдореЙрдбрд▓ рд░рд┐рдкреЙрдЬрд┐рдЯрд░реА рдФрд░ рд╕рдореБрджрд╛рдп рд╕рдорд░реНрдерди
- **ModelScope**: рдЕрд▓реАрдмрд╛рдмрд╛ рдХрд╛ рдореЙрдбрд▓ рдкреНрд▓реЗрдЯрдлреЙрд░реНрдо рдЕрдиреБрдХреВрд▓рди рдЯреВрд▓реНрд╕ рдХреЗ рд╕рд╛рде
- **рд╡рд┐рднрд┐рдиреНрди рдХреНрд▓рд╛рдЙрдб рдкреНрд░рджрд╛рддрд╛**: рдорд╛рдирдХ ML рдкреНрд▓реЗрдЯрдлреЙрд░реНрдореНрд╕ рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ рд╕рдорд░реНрдерди

### рд▓реЛрдХрд▓ рдбреЗрд╡рд▓рдкрдореЗрдВрдЯ рдлреНрд░реЗрдорд╡рд░реНрдХреНрд╕
- **Transformers**: рдЖрд╕рд╛рди рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдХреЗ рд▓рд┐рдП рдорд╛рдирдХ Hugging Face рдЗрдВрдЯреАрдЧреНрд░реЗрд╢рди
- **vLLM**: рдкреНрд░реЛрдбрдХреНрд╢рди рдПрдирд╡рд╛рдпрд░рдирдореЗрдВрдЯреНрд╕ рдХреЗ рд▓рд┐рдП рдЙрдЪреНрдЪ рдкреНрд░рджрд░реНрд╢рди рд╕рд░реНрд╡рд┐рдВрдЧ
- **Ollama**: рд▓реЛрдХрд▓ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдФрд░ рдкреНрд░рдмрдВрдзрди рдХреЗ рд▓рд┐рдП рд╕рд░рд▓ рд╕рдорд╛рдзрд╛рди
- **ONNX Runtime**: рд╡рд┐рднрд┐рдиреНрди рд╣рд╛рд░реНрдбрд╡реЗрдпрд░ рдХреЗ рд▓рд┐рдП рдХреНрд░реЙрд╕-рдкреНрд▓реЗрдЯрдлреЙрд░реНрдо рдЕрдиреБрдХреВрд▓рди
- **llama.cpp**: рд╡рд┐рд╡рд┐рдз рдкреНрд▓реЗрдЯрдлреЙрд░реНрдореНрд╕ рдХреЗ рд▓рд┐рдП рдХреБрд╢рд▓ C++ рдЗрдореНрдкреНрд▓реАрдореЗрдВрдЯреЗрд╢рди

### рд▓рд░реНрдирд┐рдВрдЧ рд╕рдВрд╕рд╛рдзрди
- **Qwen рдбреЙрдХреНрдпреВрдореЗрдВрдЯреЗрд╢рди**: рдЖрдзрд┐рдХрд╛рд░рд┐рдХ рдбреЙрдХреНрдпреВрдореЗрдВрдЯреЗрд╢рди рдФрд░ рдореЙрдбрд▓ рдХрд╛рд░реНрдбреНрд╕
- **Hugging Face Model Hub**: рдЗрдВрдЯрд░реИрдХреНрдЯрд┐рд╡ рдбреЗрдореЛ рдФрд░ рд╕рдореБрджрд╛рдп рдЙрджрд╛рд╣рд░рдг
- **рд░рд┐рд╕рд░реНрдЪ рдкреЗрдкрд░реНрд╕**: рдЧрд╣рди рд╕рдордЭ рдХреЗ рд▓рд┐рдП arxiv рдкрд░ рддрдХрдиреАрдХреА рдкреЗрдкрд░реНрд╕
- **рд╕рдореБрджрд╛рдп рдлреЛрд░рдореНрд╕**: рд╕рдХреНрд░рд┐рдп рд╕рдореБрджрд╛рдп рд╕рдорд░реНрдерди рдФрд░ рдЪрд░реНрдЪрд╛рдПрдВ

### Qwen рдореЙрдбрд▓реНрд╕ рдХреЗ рд╕рд╛рде рд╢реБрд░реБрдЖрдд

#### рдбреЗрд╡рд▓рдкрдореЗрдВрдЯ рдкреНрд▓реЗрдЯрдлреЙрд░реНрдореНрд╕
1. **Hugging Face Transformers**: рдорд╛рдирдХ Python рдЗрдВрдЯреАрдЧреНрд░реЗрд╢рди рдХреЗ рд╕рд╛рде рд╢реБрд░реБрдЖрдд рдХрд░реЗрдВ
2. **ModelScope**: рдЕрд▓реАрдмрд╛рдмрд╛ рдХреЗ рдЕрдиреБрдХреВрд▓рд┐рдд рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдЯреВрд▓реНрд╕ рдХрд╛ рдЕрдиреНрд╡реЗрд╖рдг рдХрд░реЗрдВ
3. **рд▓реЛрдХрд▓ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ**: Ollama рдпрд╛ рдбрд╛рдпрд░реЗрдХреНрдЯ Transformers рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ рд▓реЛрдХрд▓ рдЯреЗрд╕реНрдЯрд┐рдВрдЧ рдХреЗ рд▓рд┐рдП

#### рд▓рд░реНрдирд┐рдВрдЧ рдкрде
1. **рдХреЛрд░ рдХреЙрдиреНрд╕реЗрдкреНрдЯреНрд╕ рдХреЛ рд╕рдордЭреЗрдВ**: Qwen рдкрд░рд┐рд╡рд╛рд░ рдХреА рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░ рдФрд░ рдХреНрд╖рдорддрд╛рдУрдВ рдХрд╛ рдЕрдзреНрдпрдпрди рдХрд░реЗрдВ
2. **рд╡реЗрд░рд┐рдПрдВрдЯреНрд╕ рдХреЗ рд╕рд╛рде рдкреНрд░рдпреЛрдЧ рдХрд░реЗрдВ**: рдкреНрд░рджрд░реНрд╢рди рд╕рдордЭрдиреЗ рдХреЗ рд▓рд┐рдП рд╡рд┐рднрд┐рдиреНрди рдореЙрдбрд▓ рд╕рд╛рдЗрдЬ рдЖрдЬрд╝рдорд╛рдПрдВ
3. **рдЗрдореНрдкреНрд▓реАрдореЗрдВрдЯреЗрд╢рди рдХрд╛ рдЕрднреНрдпрд╛рд╕ рдХрд░реЗрдВ**: рдбреЗрд╡рд▓рдкрдореЗрдВрдЯ рдПрдирд╡рд╛рдпрд░рдореЗрдВрдЯреНрд╕ рдореЗрдВ рдореЙрдбрд▓реНрд╕ рдХреЛ рдбрд┐рдкреНрд▓реЙрдп рдХрд░реЗрдВ
4. **рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдХреЛ рдЕрдиреБрдХреВрд▓рд┐рдд рдХрд░реЗрдВ**: рдкреНрд░реЛрдбрдХреНрд╢рди рдЙрдкрдпреЛрдЧ рдорд╛рдорд▓реЛрдВ рдХреЗ рд▓рд┐рдП рдлрд╛рдЗрди-рдЯреНрдпреВрди рдХрд░реЗрдВ

#### рд╕рд░реНрд╡реЛрддреНрддрдо рдкреНрд░рдерд╛рдПрдВ
- **рдЫреЛрдЯреЗ рд╕реЗ рд╢реБрд░реВ рдХрд░реЗрдВ**: рдЫреЛрдЯреЗ рдореЙрдбрд▓реНрд╕ (1.5B-7B) рдХреЗ рд╕рд╛рде рдкреНрд░рд╛рд░рдВрднрд┐рдХ рд╡рд┐рдХрд╛рд╕ рдХрд░реЗрдВ
- **рдЪреИрдЯ рдЯреЗрдореНрдкрд▓реЗрдЯреНрд╕ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ**: рдЗрд╖реНрдЯрддрдо рдкрд░рд┐рдгрд╛рдореЛрдВ рдХреЗ рд▓рд┐рдП рдЙрдЪрд┐рдд рдлреЙрд░реНрдореЗрдЯрд┐рдВрдЧ рд▓рд╛рдЧреВ рдХрд░реЗрдВ
- **рд╕рдВрд╕рд╛рдзрдиреЛрдВ рдХреА рдирд┐рдЧрд░рд╛рдиреА рдХрд░реЗрдВ**: рдореЗрдореЛрд░реА рдЙрдкрдпреЛрдЧ рдФрд░ рдЗрдиреНрдлреНрд░реЗрдВрд╕ рд╕реНрдкреАрдб рдХреЛ рдЯреНрд░реИрдХ рдХрд░реЗрдВ
- **рд╡рд┐рд╢реЗрд╖реАрдХрд░рдг рдкрд░ рд╡рд┐рдЪрд╛рд░ рдХрд░реЗрдВ**: рдЙрдкрдпреБрдХреНрдд рд╣реЛрдиреЗ рдкрд░ рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рд╡реЗрд░рд┐рдПрдВрдЯреНрд╕ рдЪреБрдиреЗрдВ

## рдЙрдиреНрдирдд рдЙрдкрдпреЛрдЧ рдкреИрдЯрд░реНрди

### рдлрд╛рдЗрди-рдЯреНрдпреВрдирд┐рдВрдЧ рдЙрджрд╛рд╣рд░рдг

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### рд╡рд┐рд╢реЗрд╖ рдкреНрд░реЙрдореНрдкреНрдЯ рдЗрдВрдЬреАрдирд┐рдпрд░рд┐рдВрдЧ

**рдЬрдЯрд┐рд▓ рддрд░реНрдХ рдХрд╛рд░реНрдпреЛрдВ рдХреЗ рд▓рд┐рдП:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**рдХреЙрдиреНрдЯреЗрдХреНрд╕реНрдЯ рдХреЗ рд╕рд╛рде рдХреЛрдб рдЬрдирд░реЗрд╢рди рдХреЗ рд▓рд┐рдП:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### рдмрд╣реБрднрд╛рд╖реА рдПрдкреНрд▓рд┐рдХреЗрд╢рди

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (ф╕нцЦЗ)",
        "es": "Spanish (Espa├▒ol)",
        "fr": "French (Fran├зais)",
        "de": "German (Deutsch)",
        "ja": "Japanese (цЧецЬмшкЮ)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### ЁЯФз рдкреНрд░реЛрдбрдХреНрд╢рди рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдкреИрдЯрд░реНрдиреНрд╕

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## рдкреНрд░рджрд░реНрд╢рди рдЕрдиреБрдХреВрд▓рди рд░рдгрдиреАрддрд┐рдпрд╛рдВ

### рдореЗрдореЛрд░реА рдЕрдиреБрдХреВрд▓рди

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### рдЗрдиреНрдлреНрд░реЗрдВрд╕ рдЕрдиреБрдХреВрд▓рди

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## рд╕рд░реНрд╡реЛрддреНрддрдо рдкреНрд░рдерд╛рдПрдВ рдФрд░ рджрд┐рд╢рд╛рдирд┐рд░реНрджреЗрд╢

### рд╕реБрд░рдХреНрд╖рд╛ рдФрд░ рдЧреЛрдкрдиреАрдпрддрд╛

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### рдирд┐рдЧрд░рд╛рдиреА рдФрд░ рдореВрд▓реНрдпрд╛рдВрдХрди

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## рдирд┐рд╖реНрдХрд░реНрд╖

Qwen рдореЙрдбрд▓ рдкрд░рд┐рд╡рд╛рд░ AI рддрдХрдиреАрдХ рдХреЛ рд▓реЛрдХрддрд╛рдВрддреНрд░рд┐рдХ рдмрдирд╛рдиреЗ рдХреЗ рд▓рд┐рдП рдПрдХ рд╡реНрдпрд╛рдкрдХ рджреГрд╖реНрдЯрд┐рдХреЛрдг рдХрд╛ рдкреНрд░рддрд┐рдирд┐рдзрд┐рддреНрд╡ рдХрд░рддрд╛ рд╣реИ, рд╕рд╛рде рд╣реА рд╡рд┐рд╡рд┐рдз рдПрдкреНрд▓рд┐рдХреЗрд╢рди рдореЗрдВ рдкреНрд░рддрд┐рд╕реНрдкрд░реНрдзрд╛рддреНрдордХ рдкреНрд░рджрд░реНрд╢рди рдмрдирд╛рдП рд░рдЦрддрд╛ рд╣реИред рдУрдкрди-рд╕реЛрд░реНрд╕ рдПрдХреНрд╕реЗрд╕рд┐рдмрд┐рд▓рд┐рдЯреА, рдмрд╣реБрднрд╛рд╖реА рдХреНрд╖рдорддрд╛рдУрдВ, рдФрд░ рд▓рдЪреАрд▓реЗ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рд╡рд┐рдХрд▓реНрдкреЛрдВ рдХреЗ рдкреНрд░рддрд┐ рдЕрдкрдиреА рдкреНрд░рддрд┐рдмрджреНрдзрддрд╛ рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ, Qwen рд╕рдВрдЧрдардиреЛрдВ рдФрд░ рдбреЗрд╡рд▓рдкрд░реНрд╕ рдХреЛ рдЙрдирдХреЗ рд╕рдВрд╕рд╛рдзрдиреЛрдВ рдпрд╛ рд╡рд┐рд╢рд┐рд╖реНрдЯ рдЖрд╡рд╢реНрдпрдХрддрд╛рдУрдВ рдХреА рдкрд░рд╡рд╛рд╣ рдХрд┐рдП рдмрд┐рдирд╛ рд╢рдХреНрддрд┐рд╢рд╛рд▓реА AI рдХреНрд╖рдорддрд╛рдУрдВ рдХрд╛ рд▓рд╛рдн рдЙрдард╛рдиреЗ рдореЗрдВ рд╕рдХреНрд╖рдо рдмрдирд╛рддрд╛ рд╣реИред

### рдореБрдЦреНрдп рдмрд╛рддреЗрдВ

**рдУрдкрди рд╕реЛрд░реНрд╕ рдЙрддреНрдХреГрд╖реНрдЯрддрд╛**: Qwen рдпрд╣ рджрд┐рдЦрд╛рддрд╛ рд╣реИ рдХрд┐ рдУрдкрди-рд╕реЛрд░реНрд╕ рдореЙрдбрд▓реНрд╕ рдорд╛рд▓рд┐рдХрд╛рдирд╛ рд╡рд┐рдХрд▓реНрдкреЛрдВ рдХреЗ рд╕рд╛рде рдкреНрд░рддрд┐рд╕реНрдкрд░реНрдзрд╛рддреНрдордХ рдкреНрд░рджрд░реНрд╢рди рдкреНрд░рд╛рдкреНрдд рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ, рд╕рд╛рде рд╣реА рдкрд╛рд░рджрд░реНрд╢рд┐рддрд╛, рдХрд╕реНрдЯрдорд╛рдЗрдЬреЗрд╢рди, рдФрд░ рдирд┐рдпрдВрддреНрд░рдг рдкреНрд░рджрд╛рди рдХрд░рддреЗ рд╣реИрдВред

**рд╕реНрдХреЗрд▓реЗрдмрд▓ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░**: 0.5B рд╕реЗ 235B рдкреИрд░рд╛рдореАрдЯрд░реНрд╕ рддрдХ рдХреА рд░реЗрдВрдЬ, рдореЛрдмрд╛рдЗрд▓ рдбрд┐рд╡рд╛рдЗрд╕ рд╕реЗ рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЬ рдХреНрд▓рд╕реНрдЯрд░реНрд╕ рддрдХ рдХреЗ рдкреВрд░реЗ рд╕реНрдкреЗрдХреНрдЯреНрд░рдо рдореЗрдВ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдХреЛ рд╕рдХреНрд╖рдо рдмрдирд╛рддреА рд╣реИред

**рд╡рд┐рд╢реЗрд╖реАрдХреГрдд рдХреНрд╖рдорддрд╛рдПрдВ**: рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рд╡реЗрд░рд┐рдПрдВрдЯреНрд╕ рдЬреИрд╕реЗ Qwen-Coder, Qwen-Math, рдФрд░ Qwen-VL рд╕рд╛рдорд╛рдиреНрдп рднрд╛рд╖рд╛ рд╕рдордЭ рдмрдирд╛рдП рд░рдЦрддреЗ рд╣реБрдП рд╡рд┐рд╢реЗрд╖ рд╡рд┐рд╢реЗрд╖рдЬреНрдЮрддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддреЗ рд╣реИрдВред

**рд╡реИрд╢реНрд╡рд┐рдХ рдПрдХреНрд╕реЗрд╕рд┐рдмрд┐рд▓рд┐рдЯреА**: 119+ рднрд╛рд╖рд╛рдУрдВ рдореЗрдВ рдордЬрдмреВрдд рдмрд╣реБрднрд╛рд╖реА рд╕рдорд░реНрдерди Qwen рдХреЛ рдЕрдВрддрд░рд░рд╛рд╖реНрдЯреНрд░реАрдп рдПрдкреНрд▓рд┐рдХреЗрд╢рди рдФрд░ рд╡рд┐рд╡рд┐рдз рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдЖрдзрд╛рд░реЛрдВ рдХреЗ рд▓рд┐рдП рдЙрдкрдпреБрдХреНрдд рдмрдирд╛рддрд╛ рд╣реИред

**рдирд┐рд░рдВрддрд░ рдирд╡рд╛рдЪрд╛рд░**: Qwen 1.0 рд╕реЗ Qwen3 рддрдХ рдХрд╛ рд╡рд┐рдХрд╛рд╕ рдХреНрд╖рдорддрд╛рдУрдВ, рджрдХреНрд╖рддрд╛, рдФрд░ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рд╡рд┐рдХрд▓реНрдкреЛрдВ рдореЗрдВ рдирд┐рд░рдВрддрд░ рд╕реБрдзрд╛рд░ рджрд┐рдЦрд╛рддрд╛ рд╣реИред

### рднрд╡рд┐рд╖реНрдп рдХреА рджреГрд╖реНрдЯрд┐

рдЬреИрд╕реЗ-рдЬреИрд╕реЗ Qwen рдкрд░рд┐рд╡рд╛рд░ рд╡рд┐рдХрд╕рд┐рдд рд╣реЛрддрд╛ рд░рд╣реЗрдЧрд╛, рд╣рдо рдЙрдореНрдореАрдж рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ:

- **рджрдХреНрд╖рддрд╛ рдореЗрдВ рд╕реБрдзрд╛рд░**: рдмреЗрд╣рддрд░ рдкреНрд░рджрд░реНрд╢рди-рдкреНрд░рддрд┐-рдкреИрд░рд╛рдореАрдЯрд░ рдЕрдиреБрдкрд╛рдд рдХреЗ рд▓рд┐рдП рдирд┐рд░рдВрддрд░ рдЕрдиреБрдХреВрд▓рди
- **рд╡рд┐рд╕реНрддрд╛рд░рд┐рдд рдорд▓реНрдЯреАрдореЙрдбрд▓ рдХреНрд╖рдорддрд╛рдПрдВ**: рдЕрдзрд┐рдХ рдкрд░рд┐рд╖реНрдХреГрдд рд╡рд┐рдЬрд╝рди, рдСрдбрд┐рдпреЛ, рдФрд░ рдЯреЗрдХреНрд╕реНрдЯ рдкреНрд░реЛрд╕реЗрд╕рд┐рдВрдЧ рдХрд╛ рдПрдХреАрдХрд░рдг
- **рдмреЗрд╣рддрд░ рддрд░реНрдХ**: рдЙрдиреНрдирдд рдерд┐рдВрдХрд┐рдВрдЧ рдореИрдХреЗрдирд┐рдЬреНрдо рдФрд░ рдорд▓реНрдЯреА-рд╕реНрдЯреЗрдк рд╕рдорд╕реНрдпрд╛ рд╕рдорд╛рдзрд╛рди рдХреНрд╖рдорддрд╛рдПрдВ
- **рдмреЗрд╣рддрд░ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдЯреВрд▓реНрд╕**: рд╡рд┐рд╡рд┐рдз рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдкрд░рд┐рджреГрд╢реНрдпреЛрдВ рдХреЗ рд▓рд┐рдП рдЙрдиреНрдирдд рдлреНрд░реЗрдорд╡рд░реНрдХреНрд╕ рдФрд░ рдЕрдиреБрдХреВрд▓рди рдЯреВрд▓реНрд╕
- **рд╕рдореБрджрд╛рдп рдХрд╛ рд╡рд┐рдХрд╛рд╕**: рдЯреВрд▓реНрд╕, рдПрдкреНрд▓рд┐рдХреЗрд╢рди, рдФрд░ рд╕рдореБрджрд╛рдп рдпреЛрдЧрджрд╛рдиреЛрдВ рдХрд╛ рд╡рд┐рд╕реНрддрд╛рд░рд┐рдд рдЗрдХреЛрд╕рд┐рд╕реНрдЯрдо

### рдЕрдЧрд▓реЗ рдХрджрдо

рдЪрд╛рд╣реЗ рдЖрдк рдЪреИрдЯрдмреЙрдЯ рдмрдирд╛ рд░рд╣реЗ рд╣реЛрдВ, рд╢реИрдХреНрд╖рд┐рдХ рдЯреВрд▓реНрд╕ рд╡рд┐рдХрд╕рд┐рдд рдХрд░ рд░рд╣реЗ рд╣реЛрдВ, рдХреЛрдбрд┐рдВрдЧ рдЕрд╕рд┐рд╕реНрдЯреЗрдВрдЯреНрд╕ рдмрдирд╛ рд░рд╣реЗ рд╣реЛрдВ, рдпрд╛ рдмрд╣реБрднрд╛рд╖реА рдПрдкреНрд▓рд┐рдХреЗрд╢рди рдкрд░ рдХрд╛рдо рдХрд░ рд░рд╣реЗ рд╣реЛрдВ, Qwen рдкрд░рд┐рд╡рд╛рд░ рдордЬрдмреВрдд рд╕рдореБрджрд╛рдп рд╕рдорд░реНрдерди рдФрд░ рд╡реНрдпрд╛рдкрдХ рдбреЙрдХреНрдпреВрдореЗрдВрдЯреЗрд╢рди рдХреЗ рд╕рд╛рде рд╕реНрдХреЗрд▓реЗрдмрд▓ рд╕рдорд╛рдзрд╛рди рдкреНрд░рджрд╛рди рдХрд░рддрд╛ рд╣реИред

рдирд╡реАрдирддрдо рдЕрдкрдбреЗрдЯ, рдореЙрдбрд▓ рд░рд┐рд▓реАрдЬрд╝, рдФрд░ рд╡рд┐рд╕реНрддреГрдд рддрдХрдиреАрдХреА рдбреЙрдХреНрдпреВрдореЗрдВрдЯреЗрд╢рди рдХреЗ рд▓рд┐рдП, Hugging Face рдкрд░ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ Qwen рд░рд┐рдкреЙрдЬрд┐рдЯрд░реАрдЬрд╝ рдкрд░ рдЬрд╛рдПрдВ рдФрд░ рд╕рдХреНрд░рд┐рдп рд╕рдореБрджрд╛рдп рдЪрд░реНрдЪрд╛рдУрдВ рдФрд░ рдЙрджрд╛рд╣рд░рдгреЛрдВ рдХрд╛ рдЕрдиреНрд╡реЗрд╖рдг рдХрд░реЗрдВред

AI рд╡рд┐рдХрд╛рд╕ рдХрд╛ рднрд╡рд┐рд╖реНрдп рд╕реБрд▓рдн, рдкрд╛рд░рджрд░реНрд╢реА, рдФрд░ рд╢рдХреНрддрд┐рд╢рд╛рд▓реА рдЯреВрд▓реНрд╕ рдореЗрдВ рдирд┐рд╣рд┐рдд рд╣реИ рдЬреЛ рд╕рднреА рдХреНрд╖реЗрддреНрд░реЛрдВ рдФрд░ рдкреИрдорд╛рдиреЛрдВ рдореЗрдВ рдирд╡рд╛рдЪрд╛рд░ рдХреЛ рд╕рдХреНрд╖рдо рдмрдирд╛рддреЗ рд╣реИрдВред Qwen рдкрд░рд┐рд╡рд╛рд░ рдЗрд╕ рджреГрд╖реНрдЯрд┐ рдХрд╛ рдЙрджрд╛рд╣рд░рдг рдкреНрд░рд╕реНрддреБрдд рдХрд░рддрд╛ рд╣реИ, рд╕рдВрдЧрдардиреЛрдВ рдФрд░ рдбреЗрд╡рд▓рдкрд░реНрд╕ рдХреЛ рдЕрдЧрд▓реА рдкреАрдврд╝реА рдХреЗ AI-рд╕рдВрдЪрд╛рд▓рд┐рдд рдПрдкреНрд▓рд┐рдХреЗрд╢рди рдмрдирд╛рдиреЗ рдХреЗ рд▓рд┐рдП рдЖрдзрд╛рд░ рдкреНрд░рджрд╛рди рдХрд░рддрд╛ рд╣реИред

## рдЕрддрд┐рд░рд┐рдХреНрдд рд╕рдВрд╕рд╛рдзрди

- **рдЖрдзрд┐рдХрд╛рд░рд┐рдХ рдбреЙрдХреНрдпреВрдореЗрдВрдЯреЗрд╢рди**: [Qwen Documentation](https://qwen.readthedocs.io/)
- **рдореЙрдбрд▓ рд╣рдм**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **рддрдХрдиреАрдХреА рдкреЗрдкрд░реНрд╕**: [Qwen Research Publications](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **рд╕рдореБрджрд╛рдп**: [GitHub Discussions and Issues](https://github.com/QwenLM/)
- **ModelScope рдкреНрд▓реЗрдЯрдлреЙрд░реНрдо**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## рд▓рд░реНрдирд┐рдВрдЧ рдЖрдЙрдЯрдХрдореНрд╕

рдЗрд╕ рдореЙрдбреНрдпреВрд▓ рдХреЛ рдкреВрд░рд╛ рдХрд░рдиреЗ рдХреЗ рдмрд╛рдж, рдЖрдк рд╕рдХреНрд╖рдо рд╣реЛрдВрдЧреЗ:

1. Qwen рдореЙрдбрд▓ рдкрд░рд┐рд╡рд╛рд░ рдХреА рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░рд▓ рд╡рд┐рд╢реЗрд╖рддрд╛рдУрдВ рдФрд░ рдЗрд╕рдХреЗ рдУрдкрди-рд╕реЛрд░реНрд╕ рджреГрд╖реНрдЯрд┐рдХреЛрдг рдХреЛ рд╕рдордЭрд╛рдиреЗ рдореЗрдВ
2. рд╡рд┐рд╢рд┐рд╖реНрдЯ рдПрдкреНрд▓рд┐рдХреЗрд╢рди рдЖрд╡рд╢реНрдпрдХрддрд╛рдУрдВ рдФрд░ рд╕рдВрд╕рд╛рдзрди рд╕реАрдорд╛рдУрдВ рдХреЗ рдЖрдзрд╛рд░ рдкрд░ рдЙрдкрдпреБрдХреНрдд Qwen рд╡реЗрд░рд┐рдПрдВрдЯ рдХрд╛ рдЪрдпрди рдХрд░рдиреЗ рдореЗрдВ
3. рд╡рд┐рднрд┐рдиреНрди рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдкрд░рд┐рджреГрд╢реНрдпреЛрдВ рдореЗрдВ Qwen рдореЙрдбрд▓реНрд╕ рдХреЛ рдЕрдиреБрдХреВрд▓рд┐рдд рдХреЙрдиреНрдлрд╝рд┐рдЧрд░реЗрд╢рди рдХреЗ рд╕рд╛рде рд▓рд╛рдЧреВ рдХрд░рдиреЗ рдореЗрдВ
4. Qwen рдореЙрдбрд▓ рдкреНрд░рджрд░реНрд╢рди рдХреЛ рд╕реБрдзрд╛рд░рдиреЗ рдХреЗ рд▓рд┐рдП рдХреНрд╡рд╛рдВрдЯрд╛рдЗрдЬреЗрд╢рди рдФрд░ рдЕрдиреБрдХреВрд▓рди рддрдХрдиреАрдХреЛрдВ рдХреЛ рд▓рд╛рдЧреВ рдХрд░рдиреЗ рдореЗрдВ
5. Qwen рдкрд░рд┐рд╡рд╛рд░ рдХреЗ рднреАрддрд░ рдореЙрдбрд▓ рд╕рд╛рдЗрдЬ, рдкреНрд░рджрд░реНрд╢рди, рдФрд░ рдХреНрд╖рдорддрд╛рдУрдВ рдХреЗ рдмреАрдЪ рдЯреНрд░реЗрдб-рдСрдлреНрд╕ рдХрд╛ рдореВрд▓реНрдпрд╛рдВрдХрди рдХрд░рдиреЗ рдореЗрдВ

## рдЖрдЧреЗ рдХреНрдпрд╛

- [03: Gemma Family Fundamentals](03.GemmaFamily.md)

---

**рдЕрд╕реНрд╡реАрдХрд░рдг**:  
рдпрд╣ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝ AI рдЕрдиреБрд╡рд╛рдж рд╕реЗрд╡рд╛ [Co-op Translator](https://github.com/Azure/co-op-translator) рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдХреЗ рдЕрдиреБрд╡рд╛рджрд┐рдд рдХрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИред рдЬрдмрдХрд┐ рд╣рдо рд╕рдЯреАрдХрддрд╛ рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░рдиреЗ рдХрд╛ рдкреНрд░рдпрд╛рд╕ рдХрд░рддреЗ рд╣реИрдВ, рдХреГрдкрдпрд╛ рдзреНрдпрд╛рди рджреЗрдВ рдХрд┐ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рдЕрдиреБрд╡рд╛рдж рдореЗрдВ рддреНрд░реБрдЯрд┐рдпрд╛рдВ рдпрд╛ рдЕрд╢реБрджреНрдзрд┐рдпрд╛рдВ рд╣реЛ рд╕рдХрддреА рд╣реИрдВред рдореВрд▓ рднрд╛рд╖рд╛ рдореЗрдВ рдЙрдкрд▓рдмреНрдз рдореВрд▓ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝ рдХреЛ рдкреНрд░рд╛рдорд╛рдгрд┐рдХ рд╕реНрд░реЛрдд рдорд╛рдирд╛ рдЬрд╛рдирд╛ рдЪрд╛рд╣рд┐рдПред рдорд╣рддреНрд╡рдкреВрд░реНрдг рдЬрд╛рдирдХрд╛рд░реА рдХреЗ рд▓рд┐рдП, рдкреЗрд╢реЗрд╡рд░ рдорд╛рдирд╡ рдЕрдиреБрд╡рд╛рдж рдХреА рд╕рд┐рдлрд╛рд░рд┐рд╢ рдХреА рдЬрд╛рддреА рд╣реИред рдЗрд╕ рдЕрдиреБрд╡рд╛рдж рдХреЗ рдЙрдкрдпреЛрдЧ рд╕реЗ рдЙрддреНрдкрдиреНрди рдХрд┐рд╕реА рднреА рдЧрд▓рддрдлрд╣рдореА рдпрд╛ рдЧрд▓рдд рд╡реНрдпрд╛рдЦреНрдпрд╛ рдХреЗ рд▓рд┐рдП рд╣рдо рдЬрд┐рдореНрдореЗрджрд╛рд░ рдирд╣реАрдВ рд╣реИрдВред