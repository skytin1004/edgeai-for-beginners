{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0382ebeb",
   "metadata": {},
   "source": [
    "# ç¯„ä¾‹ 04ï¼šChainlit ç¶²é æ‡‰ç”¨ç¨‹å¼\n",
    "\n",
    "æ­¤ç­†è¨˜æœ¬å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Microsoft Foundry Local èˆ‡ OpenAI SDKï¼Œå»ºç«‹ä¸¦ç†è§£ä¸€å€‹ç¾ä»£åŒ–çš„ Chainlit ç¶²é æ‡‰ç”¨ç¨‹å¼ï¼Œç”¨æ–¼å°è©±å¼äººå·¥æ™ºèƒ½ã€‚\n",
    "\n",
    "## æ¦‚è¿°\n",
    "\n",
    "Chainlit æ˜¯ä¸€å€‹å¼·å¤§çš„æ¡†æ¶ï¼Œç”¨æ–¼æ§‹å»ºå…·æœ‰ç¾ä»£åŒ–ç¶²é ä»‹é¢çš„å°è©±å¼äººå·¥æ™ºèƒ½æ‡‰ç”¨ç¨‹å¼ã€‚æœ¬ç¯„ä¾‹å±•ç¤ºï¼š\n",
    "\n",
    "- ğŸŒ **ç¾ä»£åŒ–ç¶²é ä»‹é¢**ï¼šå°ˆæ¥­çš„èŠå¤©ä»‹é¢ï¼Œæ”¯æ´å³æ™‚æ›´æ–°\n",
    "- ğŸ”„ **ä¸²æµå›æ‡‰**ï¼šå³æ™‚è¨Šæ¯ä¸²æµï¼Œæå‡ä½¿ç”¨è€…é«”é©—\n",
    "- ğŸ¯ **OpenAI SDK æ•´åˆ**ï¼šèˆ‡ Foundry Local çš„æ­£ç¢º API å®¢æˆ¶ç«¯è¨­ç½®\n",
    "- ğŸ›¡ï¸ **éŒ¯èª¤è™•ç†**ï¼šå„ªé›…çš„å‚™æ´æ©Ÿåˆ¶åŠä½¿ç”¨è€…å‹å¥½çš„éŒ¯èª¤è¨Šæ¯\n",
    "- âš™ï¸ **ç”Ÿç”¢å°±ç·’**ï¼šä¼æ¥­ç´šé…ç½®åŠéƒ¨ç½²æ¨¡å¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b131d",
   "metadata": {},
   "source": [
    "## å…ˆæ±ºæ¢ä»¶åŠè¨­ç½®\n",
    "\n",
    "åœ¨åŸ·è¡Œæ­¤ç­†è¨˜æœ¬ä¹‹å‰ï¼Œè«‹ç¢ºä¿æ‚¨å·²å®‰è£æ‰€éœ€çš„å¥—ä»¶ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b547d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: chainlit in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: openai in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (1.109.0)\n",
      "Requirement already satisfied: foundry-local-sdk in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: aiofiles<25.0.0,>=23.1.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (24.1.0)\n",
      "Requirement already satisfied: asyncer<0.1.0,>=0.0.8 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.0.8)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (8.3.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.6.7)\n",
      "Requirement already satisfied: fastapi<0.117,>=0.116.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.116.2)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (1.2.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.27.2)\n",
      "Requirement already satisfied: lazify<0.5.0,>=0.4.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.4.0)\n",
      "Requirement already satisfied: literalai==0.1.201 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.1.201)\n",
      "Requirement already satisfied: mcp<2.0.0,>=1.11.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (1.14.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (1.6.0)\n",
      "Requirement already satisfied: packaging>=23.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (24.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.10.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (2.10.1)\n",
      "Requirement already satisfied: pydantic<3,>=2.7.2 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (2.11.9)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (2.10.1)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (1.1.1)\n",
      "Requirement already satisfied: python-multipart<1.0.0,>=0.0.18 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.0.20)\n",
      "Requirement already satisfied: python-socketio<6.0.0,>=5.11.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (5.13.0)\n",
      "Requirement already satisfied: starlette>=0.47.2 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.48.0)\n",
      "Requirement already satisfied: syncer<3.0.0,>=2.0.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (2.0.3)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (2.2.1)\n",
      "Requirement already satisfied: uvicorn>=0.35.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.37.0)\n",
      "Requirement already satisfied: watchfiles<1.0.0,>=0.20.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from chainlit) (0.24.0)\n",
      "Requirement already satisfied: chevron>=0.14.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from literalai==0.1.201->chainlit) (0.14.0)\n",
      "Requirement already satisfied: traceloop-sdk>=0.33.12 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from click<9.0.0,>=8.1.3->chainlit) (0.4.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->chainlit) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->chainlit) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from httpx>=0.23.0->chainlit) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from httpx>=0.23.0->chainlit) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from httpcore==1.*->httpx>=0.23.0->chainlit) (0.14.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from mcp<2.0.0,>=1.11.0->chainlit) (0.4.1)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from mcp<2.0.0,>=1.11.0->chainlit) (4.23.0)\n",
      "Requirement already satisfied: pywin32>=310 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from mcp<2.0.0,>=1.11.0->chainlit) (311)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from mcp<2.0.0,>=1.11.0->chainlit) (3.0.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from pydantic<3,>=2.7.2->chainlit) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from pydantic<3,>=2.7.2->chainlit) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from pydantic<3,>=2.7.2->chainlit) (0.4.1)\n",
      "Requirement already satisfied: bidict>=0.21.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from python-socketio<6.0.0,>=5.11.0->chainlit) (0.23.1)\n",
      "Requirement already satisfied: python-engineio>=4.11.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from python-socketio<6.0.0,>=5.11.0->chainlit) (4.12.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->chainlit) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->chainlit) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->chainlit) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->chainlit) (0.20.0)\n",
      "Requirement already satisfied: simple-websocket>=0.10.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from python-engineio>=4.11.0->python-socketio<6.0.0,>=5.11.0->chainlit) (1.1.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.11.11 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (3.12.15)\n",
      "Requirement already satisfied: cuid<0.5,>=0.4 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.4)\n",
      "Requirement already satisfied: deprecated<2.0.0,>=1.2.14 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.2.18)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.5 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (3.1.6)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.28.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.28.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.28.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-alephalpha==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-anthropic==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-bedrock==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-chromadb==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-cohere==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-crewai==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-google-generativeai==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-groq==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-haystack==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-lancedb==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-langchain==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-llamaindex==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-logging>=0.57b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-marqo==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-mcp==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-milvus==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-mistralai==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-ollama==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-openai==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-openai-agents==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-pinecone==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-qdrant==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-redis>=0.50b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-replicate==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-requests>=0.50b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-sagemaker==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-sqlalchemy>=0.50b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-threading>=0.50b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-together==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-transformers==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-urllib3>=0.50b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-vertexai==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-watsonx==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-weaviate==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-writer==0.47.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.47.3)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.28.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.13 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.4.13)\n",
      "Requirement already satisfied: posthog<4,>3.0.2 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (3.25.0)\n",
      "Requirement already satisfied: tenacity<10.0,>=8.2.3 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (9.1.2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation>=0.50b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-instrumentation-alephalpha==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions>=0.50b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-instrumentation-alephalpha==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: anthropic>=0.17.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-instrumentation-bedrock==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.68.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-instrumentation-bedrock==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.20.0)\n",
      "Requirement already satisfied: inflection<0.6.0,>=0.5.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-instrumentation-llamaindex==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.5.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp<2.0.0,>=1.34.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-instrumentation-mcp==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.37.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->chainlit) (1.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.11->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.11->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.11->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.11->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.11->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.11->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.20.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from deprecated<2.0.0,>=1.2.14->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.17.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from jinja2<4.0.0,>=3.1.5->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (2.1.5)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.75.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.37.0)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-proto==1.37.0->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (6.32.1)\n",
      "Requirement already satisfied: requests~=2.7 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (2.32.3)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.58b0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from opentelemetry-instrumentation-requests>=0.50b0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.58b0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from posthog<4,>3.0.2->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from posthog<4,>3.0.2->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from posthog<4,>3.0.2->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from posthog<4,>3.0.2->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (2.9.0)\n",
      "Requirement already satisfied: wsproto in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio<6.0.0,>=5.11.0->chainlit) (1.2.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from anthropic>=0.17.0->opentelemetry-instrumentation-bedrock==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.16)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (3.20.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.28.0->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (2.2.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from tokenizers>=0.13.0->opentelemetry-instrumentation-bedrock==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (0.25.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->opentelemetry-instrumentation-bedrock==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->opentelemetry-instrumentation-bedrock==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\leestott\\appdata\\local\\miniforge\\envs\\pydev\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->opentelemetry-instrumentation-bedrock==0.47.3->traceloop-sdk>=0.33.12->literalai==0.1.201->chainlit) (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install chainlit openai foundry-local-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc08221b",
   "metadata": {},
   "source": [
    "## ç†è§£ Chainlit æ‡‰ç”¨ç¨‹å¼\n",
    "\n",
    "è®“æˆ‘å€‘ä¾†çœ‹çœ‹æˆ‘å€‘çš„ Chainlit æ‡‰ç”¨ç¨‹å¼çš„çµæ§‹å’Œä¸»è¦çµ„æˆéƒ¨åˆ†ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6569e528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Foundry Local SDK is available\n",
      "ğŸ“¦ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import chainlit as cl\n",
    "import os\n",
    "import sys\n",
    "from typing import Optional, Dict, Any\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import asyncio\n",
    "\n",
    "# Check for Foundry Local SDK\n",
    "try:\n",
    "    from foundry_local import FoundryLocalManager\n",
    "    FOUNDRY_SDK_AVAILABLE = True\n",
    "    print(\"âœ… Foundry Local SDK is available\")\n",
    "except ImportError:\n",
    "    FOUNDRY_SDK_AVAILABLE = False\n",
    "    print(\"âš ï¸ Foundry Local SDK not available, will use manual configuration\")\n",
    "\n",
    "print(\"ğŸ“¦ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d20de",
   "metadata": {},
   "source": [
    "## å®¢æˆ¶ç«¯é…ç½®é¡åˆ¥\n",
    "\n",
    "æ­¤é¡åˆ¥è² è²¬è™•ç†å¸¶æœ‰ Foundry Local é›†æˆçš„ OpenAI å®¢æˆ¶ç«¯è¨­ç½®ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "638523d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Initializing Foundry Local SDK with model: phi-4-mini...\n",
      "2025-09-23 21:43:21 - HTTP Request: GET http://127.0.0.1:51211/foundry/list \"HTTP/1.1 200 OK\"\n",
      "2025-09-23 21:43:21 - HTTP Request: GET http://127.0.0.1:51211/openai/models \"HTTP/1.1 200 OK\"\n",
      "2025-09-23 21:43:21 - HTTP Request: GET http://127.0.0.1:51211/openai/load/Phi-4-mini-instruct-cuda-gpu?ttl=600&ep= \"HTTP/1.1 200 OK\"\n",
      "âœ… Foundry Local SDK initialized at http://127.0.0.1:51211/v1\n",
      "\n",
      "ğŸ“Š **Client Initialization Result:**\n",
      "   Status: success\n",
      "   Method: foundry_sdk\n",
      "   Base_Url: http://127.0.0.1:51211/v1\n",
      "   Model: phi-4-mini\n"
     ]
    }
   ],
   "source": [
    "class FoundryClientManager:\n",
    "    \"\"\"Manages OpenAI client setup for Foundry Local integration.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"phi-4-mini\"):\n",
    "        self.model_name = model_name\n",
    "        self.client = None\n",
    "        self.async_client = None\n",
    "        self.base_url = None\n",
    "        self.api_key = None\n",
    "        \n",
    "    def _get_fallback_config(self) -> tuple[str, str]:\n",
    "        \"\"\"Get fallback configuration from environment variables.\"\"\"\n",
    "        base_url = os.getenv(\"BASE_URL\", \"http://localhost:8000\")\n",
    "        api_key = os.getenv(\"API_KEY\", \"\")\n",
    "        return base_url, api_key\n",
    "    \n",
    "    def initialize_clients(self) -> Dict[str, Any]:\n",
    "        \"\"\"Initialize both sync and async OpenAI clients.\"\"\"\n",
    "        if FOUNDRY_SDK_AVAILABLE:\n",
    "            try:\n",
    "                print(f\"ğŸ”„ Initializing Foundry Local SDK with model: {self.model_name}...\")\n",
    "                manager = FoundryLocalManager(self.model_name)\n",
    "                \n",
    "                self.base_url = manager.endpoint\n",
    "                self.api_key = manager.api_key\n",
    "                \n",
    "                # Create both sync and async clients\n",
    "                self.client = OpenAI(\n",
    "                    base_url=self.base_url,\n",
    "                    api_key=self.api_key\n",
    "                )\n",
    "                \n",
    "                self.async_client = AsyncOpenAI(\n",
    "                    base_url=self.base_url,\n",
    "                    api_key=self.api_key\n",
    "                )\n",
    "                \n",
    "                print(f\"âœ… Foundry Local SDK initialized at {self.base_url}\")\n",
    "                return {\n",
    "                    \"status\": \"success\",\n",
    "                    \"method\": \"foundry_sdk\",\n",
    "                    \"base_url\": self.base_url,\n",
    "                    \"model\": self.model_name\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Foundry SDK failed ({e}), falling back to manual configuration\")\n",
    "        \n",
    "        # Fallback to manual configuration\n",
    "        self.base_url, self.api_key = self._get_fallback_config()\n",
    "        \n",
    "        self.client = OpenAI(\n",
    "            base_url=f\"{self.base_url}/v1\",\n",
    "            api_key=self.api_key\n",
    "        )\n",
    "        \n",
    "        self.async_client = AsyncOpenAI(\n",
    "            base_url=f\"{self.base_url}/v1\",\n",
    "            api_key=self.api_key\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ”§ Manual configuration at {self.base_url}/v1\")\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"method\": \"manual\",\n",
    "            \"base_url\": f\"{self.base_url}/v1\",\n",
    "            \"model\": self.model_name\n",
    "        }\n",
    "    \n",
    "    async def test_connection(self) -> Dict[str, Any]:\n",
    "        \"\"\"Test the connection to Foundry Local service.\"\"\"\n",
    "        try:\n",
    "            # Try to list available models\n",
    "            models = await self.async_client.models.list()\n",
    "            available_models = [model.id for model in models.data]\n",
    "            \n",
    "            # Test with a simple completion\n",
    "            response = await self.async_client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Hello, are you working?\"}],\n",
    "                max_tokens=50\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"healthy\",\n",
    "                \"available_models\": available_models,\n",
    "                \"current_model\": self.model_name,\n",
    "                \"test_response\": response.choices[0].message.content,\n",
    "                \"base_url\": self.base_url\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "                \"base_url\": self.base_url\n",
    "            }\n",
    "\n",
    "# Create a client manager instance\n",
    "client_manager = FoundryClientManager(\"phi-4-mini\")\n",
    "init_result = client_manager.initialize_clients()\n",
    "\n",
    "print(f\"\\nğŸ“Š **Client Initialization Result:**\")\n",
    "for key, value in init_result.items():\n",
    "    print(f\"   {key.title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05013074",
   "metadata": {},
   "source": [
    "## é€£ç·šæ¸¬è©¦\n",
    "\n",
    "è®“æˆ‘å€‘æ¸¬è©¦èˆ‡ Foundry Local æœå‹™çš„é€£ç·šï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96614f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” **Testing Foundry Local Connection**\n",
      "==================================================\n",
      "2025-09-23 21:43:24 - HTTP Request: GET http://127.0.0.1:51211/v1/models \"HTTP/1.1 200 OK\"\n",
      "2025-09-23 21:43:24 - HTTP Request: POST http://127.0.0.1:51211/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-09-23 21:43:24 - HTTP Request: POST http://127.0.0.1:51211/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "âŒ **Connection Status:** Error\n",
      "ğŸ”— **Base URL:** http://127.0.0.1:51211/v1\n",
      "âš ï¸ **Error:** Error code: 400\n",
      "\n",
      "ğŸ”§ **Troubleshooting:**\n",
      "1. Check if Foundry Local is running: foundry service status\n",
      "2. Start a model: foundry model run phi-4-mini\n",
      "3. Verify the endpoint URL and port\n"
     ]
    }
   ],
   "source": [
    "# Test the connection asynchronously\n",
    "async def test_service_connection():\n",
    "    \"\"\"Test connection to Foundry Local service.\"\"\"\n",
    "    print(\"ğŸ” **Testing Foundry Local Connection**\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    health_check = await client_manager.test_connection()\n",
    "    \n",
    "    if health_check[\"status\"] == \"healthy\":\n",
    "        print(\"âœ… **Connection Status:** Healthy\")\n",
    "        print(f\"ğŸ”— **Base URL:** {health_check['base_url']}\")\n",
    "        print(f\"ğŸ¤– **Current Model:** {health_check['current_model']}\")\n",
    "        print(f\"ğŸ’¬ **Test Response:** {health_check['test_response']}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ **Available Models ({len(health_check['available_models'])}):**\")\n",
    "        for i, model in enumerate(health_check['available_models'], 1):\n",
    "            current = \" (current)\" if model == health_check['current_model'] else \"\"\n",
    "            print(f\"   {i}. {model}{current}\")\n",
    "    else:\n",
    "        print(\"âŒ **Connection Status:** Error\")\n",
    "        print(f\"ğŸ”— **Base URL:** {health_check['base_url']}\")\n",
    "        print(f\"âš ï¸ **Error:** {health_check['error']}\")\n",
    "        print(\"\\nğŸ”§ **Troubleshooting:**\")\n",
    "        print(\"1. Check if Foundry Local is running: foundry service status\")\n",
    "        print(\"2. Start a model: foundry model run phi-4-mini\")\n",
    "        print(\"3. Verify the endpoint URL and port\")\n",
    "    \n",
    "    return health_check\n",
    "\n",
    "# Run the connection test\n",
    "connection_result = await test_service_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be87081",
   "metadata": {},
   "source": [
    "## Chainlit æ‡‰ç”¨ç¨‹å¼çµæ§‹\n",
    "\n",
    "ç¾åœ¨è®“æˆ‘å€‘ä¾†çœ‹çœ‹ Chainlit æ‡‰ç”¨ç¨‹å¼çš„ä¸»è¦çµ„æˆéƒ¨åˆ†ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fe8c2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ **Chainlit Application Code:**\n",
      "============================================================\n",
      "#!/usr/bin/env python3\n",
      "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
      "# Licensed under the MIT License.\n",
      "\n",
      "import os\n",
      "import chainlit as cl\n",
      "from openai import OpenAI\n",
      "\n",
      "try:\n",
      "    from foundry_local import FoundryLocalManager\n",
      "    FOUNDRY_SDK_AVAILABLE = True\n",
      "except ImportError:\n",
      "    FOUNDRY_SDK_AVAILABLE = False\n",
      "\n",
      "# Global variables for client and model\n",
      "client = None\n",
      "model_name = None\n",
      "\n",
      "\n",
      "async def initialize_client():\n",
      "    \"\"\"Initialize OpenAI client with Foundry Local or fallback configuration.\"\"\"\n",
      "    global client, model_name\n",
      "    \n",
      "    alias = os.environ.get(\"MODEL\", \"phi-4-mini\")\n",
      "    \n",
      "    if FOUNDRY_SDK_AVAILABLE:\n",
      "        try:\n",
      "            # Use FoundryLocalManager for proper service management\n",
      "            manager = FoundryLocalManager(alias)\n",
      "            model_info = manager.get_model_info(alias)\n",
      "            \n",
      "            # Configure OpenAI client to use local Foundry service\n",
      "            client = OpenAI(\n",
      "                base_url=manager.endpoint,\n",
      "                api_key=manager.api_key or \"not-required\"  # Ensure API key is not None\n",
      "            )\n",
      "            model_name = model_info.id if model_info else alias\n",
      "            print(f\"Initialized Foundry Local with model: {model_name}\")\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            print(f\"Warning: Could not use Foundry SDK ({e}), falling back to manual configuration\")\n",
      "    \n",
      "    # Fallback to manual configuration\n",
      "    base_url = os.environ.get(\"BASE_URL\", \"http://localhost:51211\")\n",
      "    api_key = os.environ.get(\"API_KEY\", \"not-required\")\n",
      "    model_name = alias\n",
      "    \n",
      "    client = OpenAI(\n",
      "        base_url=f\"{base_url}/v1\",\n",
      "        api_key=api_key\n",
      "    )\n",
      "    print(f\"Initialized manual configuration with model: {model_name}\")\n",
      "    return True\n",
      "\n",
      "\n",
      "@cl.on_chat_start\n",
      "async def start():\n",
      "    \"\"\"Initialize the chat session.\"\"\"\n",
      "    global client, model_name\n",
      "    \n",
      "    if client is None:\n",
      "        try:\n",
      "            await initialize_client()\n",
      "        except Exception as e:\n",
      "            error_msg = f\"âŒ **Initialization Error**\\n\\nCould not initialize the AI client. Please ensure Foundry Local is running.\\n\\n**Error:** {str(e)}\"\n",
      "            await cl.Message(content=error_msg).send()\n",
      "            return\n",
      "    \n",
      "    welcome_msg = f\"\"\"ğŸ¤– **Welcome to Foundry Local RAG Chat!**\n",
      "    \n",
      "**Model:** {model_name or 'Unknown'}\n",
      "**Powered by:** Microsoft Foundry Local\n",
      "\n",
      "You can ask me anything and I'll respond using the local AI model. The conversation supports:\n",
      "- âœ… Natural language processing\n",
      "- âœ… Code generation and explanation\n",
      "- âœ… Question answering\n",
      "- âœ… Creative writing\n",
      "\n",
      "Try asking me something!\"\"\"\n",
      "    \n",
      "    await cl.Message(content=welcome_msg).send()\n",
      "\n",
      "\n",
      "@cl.on_message\n",
      "async def main(message: cl.Message):\n",
      "    \"\"\"Handle incoming messages and generate responses.\"\"\"\n",
      "    global client, model_name\n",
      "    \n",
      "    if client is None:\n",
      "        await cl.Message(content=\"âŒ Error: Client not initialized. Please restart the application.\").send()\n",
      "        return\n",
      "    \n",
      "    try:\n",
      "        # Show typing indicator\n",
      "        msg = cl.Message(content=\"\")\n",
      "        await msg.send()\n",
      "        \n",
      "        # Create streaming response\n",
      "        stream = client.chat.completions.create(\n",
      "            model=model_name,\n",
      "            messages=[\n",
      "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant powered by Microsoft Foundry Local. Provide clear, accurate, and helpful responses.\"},\n",
      "                {\"role\": \"user\", \"content\": message.content}\n",
      "            ],\n",
      "            max_tokens=500,\n",
      "            temperature=0.7,\n",
      "            stream=True\n",
      "        )\n",
      "        \n",
      "        # Stream the response\n",
      "        for chunk in stream:\n",
      "            if hasattr(chunk, 'choices') and len(chunk.choices) > 0:\n",
      "                delta_content = chunk.choices[0].delta.content\n",
      "                if delta_content is not None:\n",
      "                    await msg.stream_token(delta_content)\n",
      "        \n",
      "        # Finalize the message\n",
      "        await msg.update()\n",
      "        \n",
      "    except Exception as e:\n",
      "        error_msg = f\"âŒ **Error generating response:**\\n\\n{str(e)}\\n\\nğŸ’¡ **Troubleshooting:**\\n1. Ensure Foundry Local is running: `foundry service status`\\n2. Check if model is loaded: `foundry service ps`\\n3. Verify endpoint: `curl http://localhost:51211/v1/models`\"\n",
      "        await cl.Message(content=error_msg).send()\n",
      "\n",
      "\n",
      "# Note: Client initialization happens in @cl.on_chat_start to ensure async context\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the actual Chainlit application file\n",
    "app_file_path = \"../04/app.py\"\n",
    "\n",
    "try:\n",
    "    with open(app_file_path, 'r', encoding='utf-8') as f:\n",
    "        app_content = f.read()\n",
    "    \n",
    "    print(\"ğŸ“„ **Chainlit Application Code:**\")\n",
    "    print(\"=\" * 60)\n",
    "    print(app_content)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ Application file not found at {app_file_path}\")\n",
    "    print(\"Let's create a sample application structure instead:\")\n",
    "    \n",
    "    sample_app = '''\n",
    "# Chainlit Application Structure\n",
    "\n",
    "import chainlit as cl\n",
    "from openai import AsyncOpenAI\n",
    "from foundry_local import FoundryLocalManager\n",
    "\n",
    "# Global client variable\n",
    "client = None\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    \"\"\"Initialize the chat session.\"\"\"\n",
    "    # Setup client and welcome user\n",
    "    \n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    \"\"\"Handle incoming messages with streaming response.\"\"\"\n",
    "    # Process message and stream response\n",
    "    \n",
    "# Error handling and client setup functions...\n",
    "'''\n",
    "    print(sample_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5674f9",
   "metadata": {},
   "source": [
    "## Chainlit ä¸»è¦æ¦‚å¿µ\n",
    "\n",
    "è®“æˆ‘å€‘äº†è§£åœ¨ Chainlit æ‡‰ç”¨ç¨‹å¼ä¸­ä½¿ç”¨çš„ä¸»è¦æ¦‚å¿µï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab85a613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ **Key Chainlit Concepts**\n",
      "==================================================\n",
      "\n",
      "ğŸ”§ **@cl.on_chat_start**\n",
      "   Purpose: Decorator for session initialization\n",
      "   When Called: When a new chat session begins\n",
      "   Typical Use: Setup client, show welcome message, initialize context\n",
      "\n",
      "ğŸ”§ **@cl.on_message**\n",
      "   Purpose: Decorator for message handling\n",
      "   When Called: When user sends a message\n",
      "   Typical Use: Process user input, generate AI response, stream output\n",
      "\n",
      "ğŸ”§ **cl.Message**\n",
      "   Purpose: Message object containing user input\n",
      "   Properties: content, author, timestamp, elements\n",
      "   Typical Use: Access user's message content and metadata\n",
      "\n",
      "ğŸ”§ **cl.make_async**\n",
      "   Purpose: Convert sync functions to async\n",
      "   When Needed: When using sync OpenAI client in async context\n",
      "   Typical Use: Wrap synchronous API calls for Chainlit compatibility\n",
      "\n",
      "ğŸ”§ **Streaming Response**\n",
      "   Purpose: Real-time message updates\n",
      "   Implementation: Create empty message, update content progressively\n",
      "   Typical Use: Better UX for long responses, real-time feedback\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ¯ **Key Chainlit Concepts**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "chainlit_concepts = {\n",
    "    \"@cl.on_chat_start\": {\n",
    "        \"purpose\": \"Decorator for session initialization\",\n",
    "        \"when_called\": \"When a new chat session begins\",\n",
    "        \"typical_use\": \"Setup client, show welcome message, initialize context\"\n",
    "    },\n",
    "    \"@cl.on_message\": {\n",
    "        \"purpose\": \"Decorator for message handling\",\n",
    "        \"when_called\": \"When user sends a message\",\n",
    "        \"typical_use\": \"Process user input, generate AI response, stream output\"\n",
    "    },\n",
    "    \"cl.Message\": {\n",
    "        \"purpose\": \"Message object containing user input\",\n",
    "        \"properties\": \"content, author, timestamp, elements\",\n",
    "        \"typical_use\": \"Access user's message content and metadata\"\n",
    "    },\n",
    "    \"cl.make_async\": {\n",
    "        \"purpose\": \"Convert sync functions to async\",\n",
    "        \"when_needed\": \"When using sync OpenAI client in async context\",\n",
    "        \"typical_use\": \"Wrap synchronous API calls for Chainlit compatibility\"\n",
    "    },\n",
    "    \"Streaming Response\": {\n",
    "        \"purpose\": \"Real-time message updates\",\n",
    "        \"implementation\": \"Create empty message, update content progressively\",\n",
    "        \"typical_use\": \"Better UX for long responses, real-time feedback\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for concept, details in chainlit_concepts.items():\n",
    "    print(f\"\\nğŸ”§ **{concept}**\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"   {key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36bd257",
   "metadata": {},
   "source": [
    "## ä¸²æµå›æ‡‰çš„å¯¦ç¾\n",
    "\n",
    "ä»¥ä¸‹æ˜¯ Chainlit ä¸­ä¸²æµå›æ‡‰çš„å·¥ä½œæ–¹å¼ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f70bd7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒŠ **Streaming Response Pattern**\n",
      "==================================================\n",
      "ğŸ“ **Streaming Implementation:**\n",
      "\n",
      "# 1. Create an empty message to update progressively\n",
      "msg = cl.Message(content=\"\")\n",
      "await msg.send()\n",
      "\n",
      "# 2. Make streaming API call\n",
      "stream = await client.chat.completions.create(\n",
      "    model=model_name,\n",
      "    messages=messages,\n",
      "    stream=True,\n",
      "    max_tokens=1000\n",
      ")\n",
      "\n",
      "# 3. Process each chunk and update the message\n",
      "async for chunk in stream:\n",
      "    if chunk.choices[0].delta.content is not None:\n",
      "        await msg.stream_token(chunk.choices[0].delta.content)\n",
      "\n",
      "# 4. Finalize the message\n",
      "await msg.update()\n",
      "\n",
      "\n",
      "âœ¨ **Benefits of Streaming:**\n",
      "   ğŸš€ **Real-time feedback**: Users see responses as they're generated\n",
      "   âš¡ **Better perceived performance**: Feels faster than waiting for complete response\n",
      "   ğŸ¯ **User engagement**: Keeps users engaged during long responses\n",
      "   ğŸ›‘ **Early termination**: Users can interrupt if response goes off-track\n",
      "   ğŸ’¡ **Professional UX**: Modern chat interface experience\n",
      "\n",
      "ğŸ”§ **Implementation Notes:**\n",
      "   â€¢ Always use AsyncOpenAI for Chainlit applications\n",
      "   â€¢ Handle streaming errors gracefully with try-catch blocks\n",
      "   â€¢ Check for None content in delta chunks\n",
      "   â€¢ Update message when streaming is complete\n",
      "   â€¢ Consider rate limiting for production deployments\n",
      "\n",
      "==================================================\n",
      "ğŸ“ **Streaming Implementation:**\n",
      "\n",
      "# 1. Create an empty message to update progressively\n",
      "msg = cl.Message(content=\"\")\n",
      "await msg.send()\n",
      "\n",
      "# 2. Make streaming API call\n",
      "stream = await client.chat.completions.create(\n",
      "    model=model_name,\n",
      "    messages=messages,\n",
      "    stream=True,\n",
      "    max_tokens=1000\n",
      ")\n",
      "\n",
      "# 3. Process each chunk and update the message\n",
      "async for chunk in stream:\n",
      "    if chunk.choices[0].delta.content is not None:\n",
      "        await msg.stream_token(chunk.choices[0].delta.content)\n",
      "\n",
      "# 4. Finalize the message\n",
      "await msg.update()\n",
      "\n",
      "\n",
      "âœ¨ **Benefits of Streaming:**\n",
      "   ğŸš€ **Real-time feedback**: Users see responses as they're generated\n",
      "   âš¡ **Better perceived performance**: Feels faster than waiting for complete response\n",
      "   ğŸ¯ **User engagement**: Keeps users engaged during long responses\n",
      "   ğŸ›‘ **Early termination**: Users can interrupt if response goes off-track\n",
      "   ğŸ’¡ **Professional UX**: Modern chat interface experience\n",
      "\n",
      "ğŸ”§ **Implementation Notes:**\n",
      "   â€¢ Always use AsyncOpenAI for Chainlit applications\n",
      "   â€¢ Handle streaming errors gracefully with try-catch blocks\n",
      "   â€¢ Check for None content in delta chunks\n",
      "   â€¢ Update message when streaming is complete\n",
      "   â€¢ Consider rate limiting for production deployments\n"
     ]
    }
   ],
   "source": [
    "async def demonstrate_streaming_pattern():\n",
    "    \"\"\"Demonstrate the streaming response pattern used in Chainlit.\"\"\"\n",
    "    print(\"ğŸŒŠ **Streaming Response Pattern**\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # This is how streaming works in the actual Chainlit app\n",
    "    streaming_code = '''\n",
    "# 1. Create an empty message to update progressively\n",
    "msg = cl.Message(content=\"\")\n",
    "await msg.send()\n",
    "\n",
    "# 2. Make streaming API call\n",
    "stream = await client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "# 3. Process each chunk and update the message\n",
    "async for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        await msg.stream_token(chunk.choices[0].delta.content)\n",
    "\n",
    "# 4. Finalize the message\n",
    "await msg.update()\n",
    "'''\n",
    "    \n",
    "    print(\"ğŸ“ **Streaming Implementation:**\")\n",
    "    print(streaming_code)\n",
    "    \n",
    "    print(\"\\nâœ¨ **Benefits of Streaming:**\")\n",
    "    benefits = [\n",
    "        \"ğŸš€ **Real-time feedback**: Users see responses as they're generated\",\n",
    "        \"âš¡ **Better perceived performance**: Feels faster than waiting for complete response\",\n",
    "        \"ğŸ¯ **User engagement**: Keeps users engaged during long responses\",\n",
    "        \"ğŸ›‘ **Early termination**: Users can interrupt if response goes off-track\",\n",
    "        \"ğŸ’¡ **Professional UX**: Modern chat interface experience\"\n",
    "    ]\n",
    "    \n",
    "    for benefit in benefits:\n",
    "        print(f\"   {benefit}\")\n",
    "    \n",
    "    print(\"\\nğŸ”§ **Implementation Notes:**\")\n",
    "    notes = [\n",
    "        \"Always use AsyncOpenAI for Chainlit applications\",\n",
    "        \"Handle streaming errors gracefully with try-catch blocks\",\n",
    "        \"Check for None content in delta chunks\",\n",
    "        \"Update message when streaming is complete\",\n",
    "        \"Consider rate limiting for production deployments\"\n",
    "    ]\n",
    "    \n",
    "    for note in notes:\n",
    "        print(f\"   â€¢ {note}\")\n",
    "\n",
    "await demonstrate_streaming_pattern()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722fd61b",
   "metadata": {},
   "source": [
    "## éŒ¯èª¤è™•ç†æ¨¡å¼\n",
    "\n",
    "ç©©å¥çš„éŒ¯èª¤è™•ç†å°æ–¼ç”Ÿç”¢ç’°å¢ƒä¸­çš„ Chainlit æ‡‰ç”¨è‡³é—œé‡è¦ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a723b71e",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›¡ï¸ **Error Handling Patterns**\n",
      "==================================================\n",
      "\n",
      "âš ï¸ **Client Initialization Failure**\n",
      "   ğŸ” Cause: Foundry Local service not running\n",
      "   ğŸ”§ Handling: Graceful fallback to manual configuration\n",
      "   ğŸ’¬ User Message: 'Service initializing, please wait...'\n",
      "\n",
      "âš ï¸ **Model Not Available**\n",
      "   ğŸ” Cause: Requested model not loaded\n",
      "   ğŸ”§ Handling: Try alternative models or suggest model loading\n",
      "   ğŸ’¬ User Message: 'Model unavailable, trying alternative...'\n",
      "\n",
      "âš ï¸ **Network Connection Error**\n",
      "   ğŸ” Cause: Network issues or service down\n",
      "   ğŸ”§ Handling: Retry with exponential backoff\n",
      "   ğŸ’¬ User Message: 'Connection issue, retrying...'\n",
      "\n",
      "âš ï¸ **Streaming Interruption**\n",
      "   ğŸ” Cause: Stream ends unexpectedly\n",
      "   ğŸ”§ Handling: Complete partial response gracefully\n",
      "   ğŸ’¬ User Message: 'Response completed (partial)'\n",
      "\n",
      "âš ï¸ **Rate Limiting**\n",
      "   ğŸ” Cause: Too many requests\n",
      "   ğŸ”§ Handling: Queue requests or ask user to wait\n",
      "   ğŸ’¬ User Message: 'High traffic, please wait a moment...'\n",
      "\n",
      "ğŸ“‹ **Error Handling Best Practices:**\n",
      "   ğŸ¯ **User-Friendly Messages**: Never show technical errors to users\n",
      "   ğŸ”„ **Automatic Retry**: Implement retry logic for transient failures\n",
      "   ğŸ“Š **Logging**: Log errors for debugging while keeping user experience smooth\n",
      "   ğŸ› ï¸ **Graceful Degradation**: Provide limited functionality when services are down\n",
      "   ğŸ’¡ **Helpful Suggestions**: Guide users on how to resolve issues\n",
      "   âš¡ **Fast Failure**: Fail quickly rather than letting users wait indefinitely\n",
      "\n",
      "ğŸ’» **Example Error Handling Code:**\n",
      "\n",
      "try:\n",
      "    stream = await client.chat.completions.create(\n",
      "        model=model_name,\n",
      "        messages=messages,\n",
      "        stream=True,\n",
      "        max_tokens=1000\n",
      "    )\n",
      "    \n",
      "    async for chunk in stream:\n",
      "        if chunk.choices[0].delta.content is not None:\n",
      "            await msg.stream_token(chunk.choices[0].delta.content)\n",
      "            \n",
      "except Exception as e:\n",
      "    error_msg = \"I encountered an issue. Please try again.\"\n",
      "    await cl.Message(content=error_msg, author=\"System\").send()\n",
      "    # Log the actual error for debugging\n",
      "    print(f\"Error: {e}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def demonstrate_error_handling():\n",
    "    \"\"\"Show error handling patterns for Chainlit applications.\"\"\"\n",
    "    print(\"ğŸ›¡ï¸ **Error Handling Patterns**\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    error_scenarios = {\n",
    "        \"Client Initialization Failure\": {\n",
    "            \"cause\": \"Foundry Local service not running\",\n",
    "            \"handling\": \"Graceful fallback to manual configuration\",\n",
    "            \"user_message\": \"Service initializing, please wait...\"\n",
    "        },\n",
    "        \"Model Not Available\": {\n",
    "            \"cause\": \"Requested model not loaded\",\n",
    "            \"handling\": \"Try alternative models or suggest model loading\",\n",
    "            \"user_message\": \"Model unavailable, trying alternative...\"\n",
    "        },\n",
    "        \"Network Connection Error\": {\n",
    "            \"cause\": \"Network issues or service down\",\n",
    "            \"handling\": \"Retry with exponential backoff\",\n",
    "            \"user_message\": \"Connection issue, retrying...\"\n",
    "        },\n",
    "        \"Streaming Interruption\": {\n",
    "            \"cause\": \"Stream ends unexpectedly\",\n",
    "            \"handling\": \"Complete partial response gracefully\",\n",
    "            \"user_message\": \"Response completed (partial)\"\n",
    "        },\n",
    "        \"Rate Limiting\": {\n",
    "            \"cause\": \"Too many requests\",\n",
    "            \"handling\": \"Queue requests or ask user to wait\",\n",
    "            \"user_message\": \"High traffic, please wait a moment...\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for scenario, details in error_scenarios.items():\n",
    "        print(f\"\\nâš ï¸ **{scenario}**\")\n",
    "        print(f\"   ğŸ” Cause: {details['cause']}\")\n",
    "        print(f\"   ğŸ”§ Handling: {details['handling']}\")\n",
    "        print(f\"   ğŸ’¬ User Message: '{details['user_message']}'\")\n",
    "    \n",
    "    print(\"\\nğŸ“‹ **Error Handling Best Practices:**\")\n",
    "    best_practices = [\n",
    "        \"ğŸ¯ **User-Friendly Messages**: Never show technical errors to users\",\n",
    "        \"ğŸ”„ **Automatic Retry**: Implement retry logic for transient failures\",\n",
    "        \"ğŸ“Š **Logging**: Log errors for debugging while keeping user experience smooth\",\n",
    "        \"ğŸ› ï¸ **Graceful Degradation**: Provide limited functionality when services are down\",\n",
    "        \"ğŸ’¡ **Helpful Suggestions**: Guide users on how to resolve issues\",\n",
    "        \"âš¡ **Fast Failure**: Fail quickly rather than letting users wait indefinitely\"\n",
    "    ]\n",
    "    \n",
    "    for practice in best_practices:\n",
    "        print(f\"   {practice}\")\n",
    "    \n",
    "    # Show example error handling code\n",
    "    print(\"\\nğŸ’» **Example Error Handling Code:**\")\n",
    "    error_code = '''\n",
    "try:\n",
    "    stream = await client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    async for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            await msg.stream_token(chunk.choices[0].delta.content)\n",
    "            \n",
    "except Exception as e:\n",
    "    error_msg = \"I encountered an issue. Please try again.\"\n",
    "    await cl.Message(content=error_msg, author=\"System\").send()\n",
    "    # Log the actual error for debugging\n",
    "    print(f\"Error: {e}\")\n",
    "'''\n",
    "    print(error_code)\n",
    "\n",
    "await demonstrate_error_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6269dfef",
   "metadata": {},
   "source": [
    "## é‹è¡Œ Chainlit æ‡‰ç”¨ç¨‹å¼\n",
    "\n",
    "ä»¥ä¸‹æ˜¯é‹è¡Œå’Œéƒ¨ç½² Chainlit æ‡‰ç”¨ç¨‹å¼çš„æ–¹æ³•ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60fa850b",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ **Running Chainlit Application**\n",
      "==================================================\n",
      "\n",
      "ğŸ”§ **Development Mode**\n",
      "   Command: chainlit run app.py --watch\n",
      "   Description: Auto-reload on file changes\n",
      "   Port: 8000 (default)\n",
      "   Use_Case: Local development and testing\n",
      "\n",
      "ğŸ”§ **Production Mode**\n",
      "   Command: chainlit run app.py --host 0.0.0.0 --port 8080\n",
      "   Description: Production deployment\n",
      "   Port: 8080 (configurable)\n",
      "   Use_Case: Server deployment\n",
      "\n",
      "ğŸ”§ **Custom Configuration**\n",
      "   Command: chainlit run app.py -h 0.0.0.0 -p 3000 --no-cache\n",
      "   Description: Custom host, port, and caching options\n",
      "   Port: 3000 (custom)\n",
      "   Use_Case: Specific deployment requirements\n",
      "\n",
      "ğŸŒ **Access Points:**\n",
      "   ğŸ“± **Local Development**: http://localhost:8000\n",
      "   ğŸŒ **Network Access**: http://YOUR_IP:8000\n",
      "   ğŸ”— **Production**: https://your-domain.com\n",
      "   ğŸ“Š **Health Check**: Add /health endpoint for monitoring\n",
      "\n",
      "âš™ï¸ **Environment Variables for Production:**\n",
      "   BASE_URL=http://localhost:8000 (Foundry Local endpoint)\n",
      "   API_KEY=your-api-key (if required)\n",
      "   MODEL_NAME=phi-4-mini (default model)\n",
      "   MAX_TOKENS=1000 (response length limit)\n",
      "   CHAINLIT_HOST=0.0.0.0 (production host)\n",
      "   CHAINLIT_PORT=8080 (production port)\n",
      "\n",
      "==================================================\n",
      "\n",
      "ğŸ”§ **Development Mode**\n",
      "   Command: chainlit run app.py --watch\n",
      "   Description: Auto-reload on file changes\n",
      "   Port: 8000 (default)\n",
      "   Use_Case: Local development and testing\n",
      "\n",
      "ğŸ”§ **Production Mode**\n",
      "   Command: chainlit run app.py --host 0.0.0.0 --port 8080\n",
      "   Description: Production deployment\n",
      "   Port: 8080 (configurable)\n",
      "   Use_Case: Server deployment\n",
      "\n",
      "ğŸ”§ **Custom Configuration**\n",
      "   Command: chainlit run app.py -h 0.0.0.0 -p 3000 --no-cache\n",
      "   Description: Custom host, port, and caching options\n",
      "   Port: 3000 (custom)\n",
      "   Use_Case: Specific deployment requirements\n",
      "\n",
      "ğŸŒ **Access Points:**\n",
      "   ğŸ“± **Local Development**: http://localhost:8000\n",
      "   ğŸŒ **Network Access**: http://YOUR_IP:8000\n",
      "   ğŸ”— **Production**: https://your-domain.com\n",
      "   ğŸ“Š **Health Check**: Add /health endpoint for monitoring\n",
      "\n",
      "âš™ï¸ **Environment Variables for Production:**\n",
      "   BASE_URL=http://localhost:8000 (Foundry Local endpoint)\n",
      "   API_KEY=your-api-key (if required)\n",
      "   MODEL_NAME=phi-4-mini (default model)\n",
      "   MAX_TOKENS=1000 (response length limit)\n",
      "   CHAINLIT_HOST=0.0.0.0 (production host)\n",
      "   CHAINLIT_PORT=8080 (production port)\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ **Running Chainlit Application**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "running_instructions = {\n",
    "    \"Development Mode\": {\n",
    "        \"command\": \"chainlit run app.py --watch\",\n",
    "        \"description\": \"Auto-reload on file changes\",\n",
    "        \"port\": \"8000 (default)\",\n",
    "        \"use_case\": \"Local development and testing\"\n",
    "    },\n",
    "    \"Production Mode\": {\n",
    "        \"command\": \"chainlit run app.py --host 0.0.0.0 --port 8080\",\n",
    "        \"description\": \"Production deployment\",\n",
    "        \"port\": \"8080 (configurable)\",\n",
    "        \"use_case\": \"Server deployment\"\n",
    "    },\n",
    "    \"Custom Configuration\": {\n",
    "        \"command\": \"chainlit run app.py -h 0.0.0.0 -p 3000 --no-cache\",\n",
    "        \"description\": \"Custom host, port, and caching options\",\n",
    "        \"port\": \"3000 (custom)\",\n",
    "        \"use_case\": \"Specific deployment requirements\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for mode, config in running_instructions.items():\n",
    "    print(f\"\\nğŸ”§ **{mode}**\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"   {key.title()}: {value}\")\n",
    "\n",
    "print(\"\\nğŸŒ **Access Points:**\")\n",
    "access_info = [\n",
    "    \"ğŸ“± **Local Development**: http://localhost:8000\",\n",
    "    \"ğŸŒ **Network Access**: http://YOUR_IP:8000\",\n",
    "    \"ğŸ”— **Production**: https://your-domain.com\",\n",
    "    \"ğŸ“Š **Health Check**: Add /health endpoint for monitoring\"\n",
    "]\n",
    "\n",
    "for info in access_info:\n",
    "    print(f\"   {info}\")\n",
    "\n",
    "print(\"\\nâš™ï¸ **Environment Variables for Production:**\")\n",
    "env_vars = {\n",
    "    \"BASE_URL\": \"http://localhost:8000 (Foundry Local endpoint)\",\n",
    "    \"API_KEY\": \"your-api-key (if required)\",\n",
    "    \"MODEL_NAME\": \"phi-4-mini (default model)\",\n",
    "    \"MAX_TOKENS\": \"1000 (response length limit)\",\n",
    "    \"CHAINLIT_HOST\": \"0.0.0.0 (production host)\",\n",
    "    \"CHAINLIT_PORT\": \"8080 (production port)\"\n",
    "}\n",
    "\n",
    "for var, desc in env_vars.items():\n",
    "    print(f\"   {var}={desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928e0dc1",
   "metadata": {},
   "source": [
    "## è‡ªè¨‚é¸é …\n",
    "\n",
    "Chainlit æä¾›å¤šç¨®è‡ªè¨‚é¸é …ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e0ecc0c",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¨ **Chainlit Customization Options**\n",
      "==================================================\n",
      "\n",
      "ğŸ”§ **UI Customization**\n",
      "   ğŸ“„ Config: .chainlit/config.toml\n",
      "   âš™ï¸ Options:\n",
      "      â€¢ Custom theme colors and fonts\n",
      "      â€¢ Company logo and branding\n",
      "      â€¢ Custom CSS styling\n",
      "      â€¢ Dark/light mode preferences\n",
      "\n",
      "ğŸ”§ **Chat Features**\n",
      "   ğŸ“„ Config: app.py (programmatic)\n",
      "   âš™ï¸ Options:\n",
      "      â€¢ File upload support\n",
      "      â€¢ Image and media handling\n",
      "      â€¢ Custom message elements\n",
      "      â€¢ Action buttons and quick replies\n",
      "\n",
      "ğŸ”§ **Authentication**\n",
      "   ğŸ“„ Config: auth.py + config.toml\n",
      "   âš™ï¸ Options:\n",
      "      â€¢ OAuth integration (Google, GitHub)\n",
      "      â€¢ LDAP/Active Directory\n",
      "      â€¢ Custom authentication providers\n",
      "      â€¢ Role-based access control\n",
      "\n",
      "ğŸ”§ **Deployment**\n",
      "   ğŸ“„ Config: docker-compose.yml / Dockerfile\n",
      "   âš™ï¸ Options:\n",
      "      â€¢ Docker containerization\n",
      "      â€¢ Kubernetes deployment\n",
      "      â€¢ Cloud platform integration\n",
      "      â€¢ Reverse proxy configuration\n",
      "\n",
      "ğŸ“ **Sample .chainlit/config.toml:**\n",
      "\n",
      "[project]\n",
      "name = \"Foundry Local Chat\"\n",
      "author = \"Your Organization\"\n",
      "description = \"AI Chat powered by Foundry Local\"\n",
      "\n",
      "[UI]\n",
      "name = \"Foundry AI Assistant\"\n",
      "show_readme_as_default = true\n",
      "show_cloud_icon = false\n",
      "\n",
      "[theme]\n",
      "primary_color = \"#0078d4\"\n",
      "background_color = \"#ffffff\"\n",
      "text_color = \"#323130\"\n",
      "\n",
      "[features]\n",
      "allow_unsafe_html = false\n",
      "max_message_size = 4096\n",
      "max_file_size_mb = 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ¨ **Chainlit Customization Options**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "customization_areas = {\n",
    "    \"UI Customization\": {\n",
    "        \"config_file\": \".chainlit/config.toml\",\n",
    "        \"options\": [\n",
    "            \"Custom theme colors and fonts\",\n",
    "            \"Company logo and branding\",\n",
    "            \"Custom CSS styling\",\n",
    "            \"Dark/light mode preferences\"\n",
    "        ]\n",
    "    },\n",
    "    \"Chat Features\": {\n",
    "        \"config_file\": \"app.py (programmatic)\",\n",
    "        \"options\": [\n",
    "            \"File upload support\",\n",
    "            \"Image and media handling\",\n",
    "            \"Custom message elements\",\n",
    "            \"Action buttons and quick replies\"\n",
    "        ]\n",
    "    },\n",
    "    \"Authentication\": {\n",
    "        \"config_file\": \"auth.py + config.toml\",\n",
    "        \"options\": [\n",
    "            \"OAuth integration (Google, GitHub)\",\n",
    "            \"LDAP/Active Directory\",\n",
    "            \"Custom authentication providers\",\n",
    "            \"Role-based access control\"\n",
    "        ]\n",
    "    },\n",
    "    \"Deployment\": {\n",
    "        \"config_file\": \"docker-compose.yml / Dockerfile\",\n",
    "        \"options\": [\n",
    "            \"Docker containerization\",\n",
    "            \"Kubernetes deployment\",\n",
    "            \"Cloud platform integration\",\n",
    "            \"Reverse proxy configuration\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for area, details in customization_areas.items():\n",
    "    print(f\"\\nğŸ”§ **{area}**\")\n",
    "    print(f\"   ğŸ“„ Config: {details['config_file']}\")\n",
    "    print(f\"   âš™ï¸ Options:\")\n",
    "    for option in details['options']:\n",
    "        print(f\"      â€¢ {option}\")\n",
    "\n",
    "# Show sample configuration\n",
    "print(\"\\nğŸ“ **Sample .chainlit/config.toml:**\")\n",
    "sample_config = '''\n",
    "[project]\n",
    "name = \"Foundry Local Chat\"\n",
    "author = \"Your Organization\"\n",
    "description = \"AI Chat powered by Foundry Local\"\n",
    "\n",
    "[UI]\n",
    "name = \"Foundry AI Assistant\"\n",
    "show_readme_as_default = true\n",
    "show_cloud_icon = false\n",
    "\n",
    "[theme]\n",
    "primary_color = \"#0078d4\"\n",
    "background_color = \"#ffffff\"\n",
    "text_color = \"#323130\"\n",
    "\n",
    "[features]\n",
    "allow_unsafe_html = false\n",
    "max_message_size = 4096\n",
    "max_file_size_mb = 10\n",
    "'''\n",
    "print(sample_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33562e8",
   "metadata": {},
   "source": [
    "## é«˜ç´šåŠŸèƒ½\n",
    "\n",
    "æ¢ç´¢é©ç”¨æ–¼ç”Ÿç”¢æ‡‰ç”¨çš„é«˜ç´š Chainlit åŠŸèƒ½ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86670e5d",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ **Advanced Chainlit Features**\n",
      "==================================================\n",
      "\n",
      "ğŸ¯ **Session Management**\n",
      "   ğŸ“ Description: Maintain conversation context across messages\n",
      "   ğŸ”§ Implementation: cl.user_session for storing state\n",
      "   ğŸ’¡ Use Cases:\n",
      "      â€¢ Multi-turn conversations\n",
      "      â€¢ User preferences\n",
      "      â€¢ Context persistence\n",
      "\n",
      "ğŸ¯ **File Uploads**\n",
      "   ğŸ“ Description: Handle document uploads and processing\n",
      "   ğŸ”§ Implementation: @cl.on_file_upload decorator\n",
      "   ğŸ’¡ Use Cases:\n",
      "      â€¢ Document analysis\n",
      "      â€¢ Image processing\n",
      "      â€¢ Data ingestion\n",
      "\n",
      "ğŸ¯ **Action Buttons**\n",
      "   ğŸ“ Description: Interactive buttons for user actions\n",
      "   ğŸ”§ Implementation: cl.Action elements\n",
      "   ğŸ’¡ Use Cases:\n",
      "      â€¢ Quick replies\n",
      "      â€¢ Workflow triggers\n",
      "      â€¢ Menu navigation\n",
      "\n",
      "ğŸ¯ **Data Persistence**\n",
      "   ğŸ“ Description: Store conversation history and user data\n",
      "   ğŸ”§ Implementation: Database integration\n",
      "   ğŸ’¡ Use Cases:\n",
      "      â€¢ Chat history\n",
      "      â€¢ User analytics\n",
      "      â€¢ Feedback collection\n",
      "\n",
      "ğŸ¯ **Multi-modal Support**\n",
      "   ğŸ“ Description: Handle text, images, and other media\n",
      "   ğŸ”§ Implementation: cl.Image, cl.File elements\n",
      "   ğŸ’¡ Use Cases:\n",
      "      â€¢ Visual Q&A\n",
      "      â€¢ Document chat\n",
      "      â€¢ Media analysis\n",
      "\n",
      "ğŸ’» **Session Management Example:**\n",
      "\n",
      "@cl.on_chat_start\n",
      "async def on_chat_start():\n",
      "    # Initialize session state\n",
      "    cl.user_session.set(\"conversation_history\", [])\n",
      "    cl.user_session.set(\"user_preferences\", {\"temperature\": 0.7})\n",
      "\n",
      "@cl.on_message\n",
      "async def on_message(message: cl.Message):\n",
      "    # Get session state\n",
      "    history = cl.user_session.get(\"conversation_history\", [])\n",
      "    preferences = cl.user_session.get(\"user_preferences\", {})\n",
      "    \n",
      "    # Add current message to history\n",
      "    history.append({\"role\": \"user\", \"content\": message.content})\n",
      "    \n",
      "    # Use full conversation context\n",
      "    response = await client.chat.completions.create(\n",
      "        model=\"phi-4-mini\",\n",
      "        messages=history,\n",
      "        temperature=preferences.get(\"temperature\", 0.7)\n",
      "    )\n",
      "    \n",
      "    # Update session with AI response\n",
      "    history.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
      "    cl.user_session.set(\"conversation_history\", history)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ **Advanced Chainlit Features**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "advanced_features = {\n",
    "    \"Session Management\": {\n",
    "        \"description\": \"Maintain conversation context across messages\",\n",
    "        \"implementation\": \"cl.user_session for storing state\",\n",
    "        \"use_cases\": [\"Multi-turn conversations\", \"User preferences\", \"Context persistence\"]\n",
    "    },\n",
    "    \"File Uploads\": {\n",
    "        \"description\": \"Handle document uploads and processing\",\n",
    "        \"implementation\": \"@cl.on_file_upload decorator\",\n",
    "        \"use_cases\": [\"Document analysis\", \"Image processing\", \"Data ingestion\"]\n",
    "    },\n",
    "    \"Action Buttons\": {\n",
    "        \"description\": \"Interactive buttons for user actions\",\n",
    "        \"implementation\": \"cl.Action elements\",\n",
    "        \"use_cases\": [\"Quick replies\", \"Workflow triggers\", \"Menu navigation\"]\n",
    "    },\n",
    "    \"Data Persistence\": {\n",
    "        \"description\": \"Store conversation history and user data\",\n",
    "        \"implementation\": \"Database integration\",\n",
    "        \"use_cases\": [\"Chat history\", \"User analytics\", \"Feedback collection\"]\n",
    "    },\n",
    "    \"Multi-modal Support\": {\n",
    "        \"description\": \"Handle text, images, and other media\",\n",
    "        \"implementation\": \"cl.Image, cl.File elements\",\n",
    "        \"use_cases\": [\"Visual Q&A\", \"Document chat\", \"Media analysis\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "for feature, details in advanced_features.items():\n",
    "    print(f\"\\nğŸ¯ **{feature}**\")\n",
    "    print(f\"   ğŸ“ Description: {details['description']}\")\n",
    "    print(f\"   ğŸ”§ Implementation: {details['implementation']}\")\n",
    "    print(f\"   ğŸ’¡ Use Cases:\")\n",
    "    for use_case in details['use_cases']:\n",
    "        print(f\"      â€¢ {use_case}\")\n",
    "\n",
    "# Show example code for session management\n",
    "print(\"\\nğŸ’» **Session Management Example:**\")\n",
    "session_code = '''\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    # Initialize session state\n",
    "    cl.user_session.set(\"conversation_history\", [])\n",
    "    cl.user_session.set(\"user_preferences\", {\"temperature\": 0.7})\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    # Get session state\n",
    "    history = cl.user_session.get(\"conversation_history\", [])\n",
    "    preferences = cl.user_session.get(\"user_preferences\", {})\n",
    "    \n",
    "    # Add current message to history\n",
    "    history.append({\"role\": \"user\", \"content\": message.content})\n",
    "    \n",
    "    # Use full conversation context\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"phi-4-mini\",\n",
    "        messages=history,\n",
    "        temperature=preferences.get(\"temperature\", 0.7)\n",
    "    )\n",
    "    \n",
    "    # Update session with AI response\n",
    "    history.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "    cl.user_session.set(\"conversation_history\", history)\n",
    "'''\n",
    "print(session_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be192a",
   "metadata": {},
   "source": [
    "## ç”Ÿç”¢éƒ¨ç½²æ¸…å–®\n",
    "\n",
    "éƒ¨ç½² Chainlit æ‡‰ç”¨ç¨‹å¼è‡³ç”Ÿç”¢ç’°å¢ƒæ™‚éœ€è¦æ³¨æ„çš„é‡è¦äº‹é …ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b18750c",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… **Production Deployment Checklist**\n",
      "==================================================\n",
      "\n",
      "ğŸ”’ Security\n",
      "   â˜ Enable authentication and authorization\n",
      "   â˜ Use HTTPS with proper SSL certificates\n",
      "   â˜ Implement rate limiting and request validation\n",
      "   â˜ Sanitize user inputs and prevent injection attacks\n",
      "   â˜ Set up proper CORS policies\n",
      "   â˜ Use environment variables for sensitive configuration\n",
      "\n",
      "âš¡ Performance\n",
      "   â˜ Configure connection pooling for database\n",
      "   â˜ Implement caching for frequent responses\n",
      "   â˜ Set up load balancing for multiple instances\n",
      "   â˜ Monitor memory usage and optimize where needed\n",
      "   â˜ Configure appropriate timeout values\n",
      "   â˜ Use CDN for static assets\n",
      "\n",
      "ğŸ“Š Monitoring\n",
      "   â˜ Set up application logging and monitoring\n",
      "   â˜ Configure health checks and uptime monitoring\n",
      "   â˜ Track user engagement and conversation metrics\n",
      "   â˜ Monitor API response times and error rates\n",
      "   â˜ Set up alerting for critical issues\n",
      "   â˜ Implement user feedback collection\n",
      "\n",
      "ğŸ› ï¸ Maintenance\n",
      "   â˜ Regular backups of conversation data\n",
      "   â˜ Automated deployment pipelines\n",
      "   â˜ Version control for configuration changes\n",
      "   â˜ Documentation for troubleshooting\n",
      "   â˜ Capacity planning and scaling procedures\n",
      "   â˜ Update procedures for dependencies\n",
      "\n",
      "ğŸŒ Infrastructure\n",
      "   â˜ Container orchestration (Docker/Kubernetes)\n",
      "   â˜ Reverse proxy configuration (nginx/Apache)\n",
      "   â˜ Database setup and optimization\n",
      "   â˜ Network security and firewall rules\n",
      "   â˜ Backup and disaster recovery plans\n",
      "   â˜ Multi-region deployment for redundancy\n",
      "\n",
      "ğŸš€ **Quick Production Setup Commands:**\n",
      "\n",
      "ğŸ’¡ # Build Docker image\n",
      "   docker build -t chainlit-app .\n",
      "\n",
      "\n",
      "ğŸ’¡ # Run with production settings\n",
      "   docker run -d -p 8080:8080 -e NODE_ENV=production chainlit-app\n",
      "\n",
      "\n",
      "ğŸ’¡ # Health check\n",
      "   curl http://localhost:8080/health\n",
      "\n",
      "\n",
      "ğŸ’¡ # Monitor logs\n",
      "   docker logs -f chainlit-app\n"
     ]
    }
   ],
   "source": [
    "print(\"âœ… **Production Deployment Checklist**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "deployment_checklist = {\n",
    "    \"ğŸ”’ Security\": [\n",
    "        \"Enable authentication and authorization\",\n",
    "        \"Use HTTPS with proper SSL certificates\",\n",
    "        \"Implement rate limiting and request validation\",\n",
    "        \"Sanitize user inputs and prevent injection attacks\",\n",
    "        \"Set up proper CORS policies\",\n",
    "        \"Use environment variables for sensitive configuration\"\n",
    "    ],\n",
    "    \"âš¡ Performance\": [\n",
    "        \"Configure connection pooling for database\",\n",
    "        \"Implement caching for frequent responses\",\n",
    "        \"Set up load balancing for multiple instances\",\n",
    "        \"Monitor memory usage and optimize where needed\",\n",
    "        \"Configure appropriate timeout values\",\n",
    "        \"Use CDN for static assets\"\n",
    "    ],\n",
    "    \"ğŸ“Š Monitoring\": [\n",
    "        \"Set up application logging and monitoring\",\n",
    "        \"Configure health checks and uptime monitoring\",\n",
    "        \"Track user engagement and conversation metrics\",\n",
    "        \"Monitor API response times and error rates\",\n",
    "        \"Set up alerting for critical issues\",\n",
    "        \"Implement user feedback collection\"\n",
    "    ],\n",
    "    \"ğŸ› ï¸ Maintenance\": [\n",
    "        \"Regular backups of conversation data\",\n",
    "        \"Automated deployment pipelines\",\n",
    "        \"Version control for configuration changes\",\n",
    "        \"Documentation for troubleshooting\",\n",
    "        \"Capacity planning and scaling procedures\",\n",
    "        \"Update procedures for dependencies\"\n",
    "    ],\n",
    "    \"ğŸŒ Infrastructure\": [\n",
    "        \"Container orchestration (Docker/Kubernetes)\",\n",
    "        \"Reverse proxy configuration (nginx/Apache)\",\n",
    "        \"Database setup and optimization\",\n",
    "        \"Network security and firewall rules\",\n",
    "        \"Backup and disaster recovery plans\",\n",
    "        \"Multi-region deployment for redundancy\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in deployment_checklist.items():\n",
    "    print(f\"\\n{category}\")\n",
    "    for item in items:\n",
    "        print(f\"   â˜ {item}\")\n",
    "\n",
    "print(\"\\nğŸš€ **Quick Production Setup Commands:**\")\n",
    "commands = [\n",
    "    \"# Build Docker image\",\n",
    "    \"docker build -t chainlit-app .\",\n",
    "    \"\",\n",
    "    \"# Run with production settings\",\n",
    "    \"docker run -d -p 8080:8080 -e NODE_ENV=production chainlit-app\",\n",
    "    \"\",\n",
    "    \"# Health check\",\n",
    "    \"curl http://localhost:8080/health\",\n",
    "    \"\",\n",
    "    \"# Monitor logs\",\n",
    "    \"docker logs -f chainlit-app\"\n",
    "]\n",
    "\n",
    "for cmd in commands:\n",
    "    if cmd.startswith(\"#\"):\n",
    "        print(f\"\\nğŸ’¡ {cmd}\")\n",
    "    elif cmd:\n",
    "        print(f\"   {cmd}\")\n",
    "    else:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab2130",
   "metadata": {},
   "source": [
    "## æ‘˜è¦åŠæœ€ä½³å¯¦è¸\n",
    "\n",
    "é€™ä»½ç­†è¨˜æœ¬æ¶µè“‹äº†å®Œæ•´çš„ Chainlit æ‡‰ç”¨ç¨‹å¼é–‹ç™¼æµç¨‹ï¼š\n",
    "\n",
    "### âœ… æ¶µè“‹çš„ä¸»è¦çµ„ä»¶\n",
    "\n",
    "1. **ğŸ”§ å®¢æˆ¶ç«¯è¨­ç½®**ï¼šFoundry Local SDK æ•´åˆåŠå‚™æ´é…ç½®\n",
    "2. **ğŸŒŠ æµå¼å›æ‡‰**ï¼šå³æ™‚æ¶ˆæ¯æ›´æ–°ä»¥æå‡ç”¨æˆ¶é«”é©—\n",
    "3. **ğŸ›¡ï¸ éŒ¯èª¤è™•ç†**ï¼šå„ªé›…çš„æ•…éšœè™•ç†åŠå‹å¥½çš„ç”¨æˆ¶æç¤º\n",
    "4. **âš™ï¸ é…ç½®**ï¼šåŸºæ–¼ç’°å¢ƒçš„è¨­ç½®åŠè‡ªå®šç¾©é¸é …\n",
    "5. **ğŸš€ éƒ¨ç½²**ï¼šç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²æ¨¡å¼åŠæœ€ä½³å¯¦è¸\n",
    "\n",
    "### ğŸ¯ Chainlit æ‡‰ç”¨ç¨‹å¼æ¶æ§‹\n",
    "\n",
    "```\n",
    "User Browser â†â†’ Chainlit UI â†â†’ Python Backend â†â†’ Foundry Local â†â†’ AI Model\n",
    "      â†“              â†“              â†“              â†“            â†“\n",
    "   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU\n",
    "```\n",
    "\n",
    "### ğŸ’¡ æœ€ä½³å¯¦è¸æ‘˜è¦\n",
    "\n",
    "- **ğŸ”„ å§‹çµ‚ä½¿ç”¨ Async**ï¼šChainlit éœ€è¦ä½¿ç”¨ç•°æ­¥å‡½æ•¸ä»¥é¿å…é˜»å¡æ“ä½œ\n",
    "- **ğŸŒŠ å¯¦ç¾æµå¼å›æ‡‰**ï¼šæ¯”ç­‰å¾…å®Œæ•´å›æ‡‰æ›´èƒ½æä¾›è‰¯å¥½çš„ç”¨æˆ¶é«”é©—\n",
    "- **ğŸ›¡ï¸ å„ªé›…è™•ç†éŒ¯èª¤**ï¼šé¿å…å‘çµ‚ç«¯ç”¨æˆ¶å±•ç¤ºæŠ€è¡“æ€§éŒ¯èª¤\n",
    "- **ğŸ“Š ç›£æ§æ€§èƒ½**ï¼šè¿½è¹¤å›æ‡‰æ™‚é–“åŠç”¨æˆ¶åƒèˆ‡åº¦æŒ‡æ¨™\n",
    "- **ğŸ”’ é è¨­å®‰å…¨**ï¼šå¾ä¸€é–‹å§‹å°±å¯¦æ–½èº«ä»½é©—è­‰åŠè¼¸å…¥é©—è­‰\n",
    "- **âš¡ ç‚ºæ“´å±•æ€§å„ªåŒ–**ï¼šå¾ç¬¬ä¸€å¤©èµ·å°±è¨­è¨ˆæ”¯æŒå¤šç”¨æˆ¶ä¸¦ç™¼æ“ä½œ\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥\n",
    "\n",
    "- **ğŸ“± å¤šæ¨¡æ…‹æ”¯æŒ**ï¼šæ·»åŠ åœ–ç‰‡åŠæ–‡ä»¶è™•ç†åŠŸèƒ½\n",
    "- **ğŸ¤– ä»£ç†æ•´åˆ**ï¼šé€£æ¥å¤šä»£ç†ç³»çµ±ä»¥æ”¯æŒè¤‡é›œå·¥ä½œæµç¨‹\n",
    "- **ğŸ“Š åˆ†æå„€è¡¨æ¿**ï¼šæ§‹å»ºç®¡ç†ç•Œé¢ä»¥é€²è¡Œç›£æ§åŠç®¡ç†\n",
    "- **ğŸ”§ è‡ªå®šç¾©æ’ä»¶**ï¼šé–‹ç™¼è‡ªå®šç¾©çš„ Chainlit å…ƒä»¶åŠæ•´åˆ\n",
    "- **ğŸŒ API æ•´åˆ**ï¼šé€£æ¥å¤–éƒ¨æœå‹™åŠæ•¸æ“šåº«\n",
    "\n",
    "é€™å€‹ Chainlit æ‡‰ç”¨ç¨‹å¼å±•ç¤ºäº†å¦‚ä½•æ§‹å»ºç”Ÿç”¢ç’°å¢ƒæº–å‚™å°±ç·’çš„å°è©±å¼ AI ä»‹é¢ï¼Œåˆ©ç”¨ Microsoft Foundry Local çš„æœ¬åœ° AI æ¨¡å‹ï¼ŒåŒæ™‚æä¾›ç¾ä»£åŒ–åŠéŸ¿æ‡‰è¿…é€Ÿçš„ç”¨æˆ¶é«”é©—ã€‚\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "coopTranslator": {
   "original_hash": "eed20b9ecdd7cd5f88db77bfda326883",
   "translation_date": "2025-09-24T10:08:54+00:00",
   "source_file": "Module08/samples/04/chainlit_app.ipynb",
   "language_code": "hk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}