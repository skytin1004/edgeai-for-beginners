<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "76b134be93f45283a2df8f8a93717d06",
  "translation_date": "2025-10-17T10:26:28+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "et"
}
-->
# 1. peatÃ¼kk: EdgeAI pÃµhialused

EdgeAI esindab paradigmat, kus tehisintellekti rakendamine toimub otse servaseadmetes, mitte ainult pilvepÃµhise tÃ¶Ã¶tlemise kaudu. Oluline on mÃµista, kuidas EdgeAI vÃµimaldab kohalikku tehisintellekti tÃ¶Ã¶tlemist piiratud ressurssidega seadmetes, sÃ¤ilitades samal ajal mÃµistliku jÃµudluse ja lahendades privaatsuse, latentsuse ja vÃµrguÃ¼henduseta tÃ¶Ã¶tamisega seotud vÃ¤ljakutseid.

## Sissejuhatus

Selles ÃµppetÃ¼kis uurime EdgeAI-d ja selle pÃµhikontseptsioone. KÃ¤sitleme traditsioonilist tehisintellekti arvutusparadigmat, servaarvutuse vÃ¤ljakutseid, EdgeAI-d vÃµimaldavaid vÃµtmetehnoloogiaid ja praktilisi rakendusi erinevates tÃ¶Ã¶stusharudes.

## Ã•pieesmÃ¤rgid

Selle ÃµppetÃ¼ki lÃµpuks suudad:

- MÃµista erinevust traditsioonilise pilvepÃµhise tehisintellekti ja EdgeAI lÃ¤henemisviiside vahel.
- Tuvastada vÃµtmetehnoloogiad, mis vÃµimaldavad tehisintellekti tÃ¶Ã¶tlemist servaseadmetes.
- Tunnustada EdgeAI rakenduste eeliseid ja piiranguid.
- Rakendada EdgeAI teadmisi reaalses maailmas ja kasutusjuhtumites.

## Traditsioonilise tehisintellekti arvutusparadigma mÃµistmine

Traditsiooniliselt tuginevad generatiivse tehisintellekti rakendused suure jÃµudlusega arvutustaristule, et kÃ¤itada suuri keelemudeleid (LLM) tÃµhusalt. Organisatsioonid paigutavad tavaliselt need mudelid GPU klastritesse pilvekeskkonnas, kasutades nende vÃµimalusi API-liideste kaudu.

See tsentraliseeritud mudel tÃ¶Ã¶tab hÃ¤sti paljude rakenduste puhul, kuid sellel on servaarvutuse stsenaariumides kaasasÃ¼ndinud piirangud. Traditsiooniline lÃ¤henemine hÃµlmab kasutaja pÃ¤ringute saatmist kaugserveritesse, nende tÃ¶Ã¶tlemist vÃµimsa riistvara abil ja tulemuste tagastamist interneti kaudu. Kuigi see meetod pakub juurdepÃ¤Ã¤su tipptasemel mudelitele, tekitab see sÃµltuvusi internetiÃ¼hendusest, toob kaasa latentsuse probleeme ja tÃµstatab privaatsuse kÃ¼simusi, kui tundlikke andmeid tuleb edastada vÃ¤listele serveritele.

Traditsioonilise tehisintellekti arvutusparadigma puhul tuleb mÃµista mÃµningaid pÃµhikontseptsioone, nimelt:

- **â˜ï¸ PilvepÃµhine tÃ¶Ã¶tlemine**: Tehisintellekti mudelid tÃ¶Ã¶tavad vÃµimsal serveritaristul, millel on suured arvutusressursid.
- **ğŸ”Œ API-pÃµhine juurdepÃ¤Ã¤s**: Rakendused kasutavad tehisintellekti vÃµimalusi kaug-API-kÃµnede kaudu, mitte kohalikult tÃ¶Ã¶tlemiselt.
- **ğŸ›ï¸ Tsentraliseeritud mudelite haldamine**: Mudelid hoitakse ja uuendatakse tsentraalselt, tagades jÃ¤rjepidevuse, kuid nÃµudes vÃµrguÃ¼hendust.
- **ğŸ“ˆ Ressursside skaleeritavus**: Pilvetaristu saab dÃ¼naamiliselt kohanduda erinevate arvutusnÃµudmistega.

## Servaarvutuse vÃ¤ljakutsed

Servaseadmed, nagu sÃ¼learvutid, mobiiltelefonid ja asjade interneti (IoT) seadmed, nÃ¤iteks Raspberry Pi ja NVIDIA Orin Nano, esindavad unikaalseid arvutuslikke piiranguid. Nendel seadmetel on tavaliselt piiratud tÃ¶Ã¶tlemisvÃµimsus, mÃ¤lu ja energiavarud vÃµrreldes andmekeskuste taristuga.

Traditsiooniliste LLM-ide kÃ¤itamine sellistel seadmetel on ajalooliselt olnud keeruline nende riistvaraliste piirangute tÃµttu. Kuid servatehisintellekti tÃ¶Ã¶tlemise vajadus on muutunud Ã¼ha olulisemaks erinevates olukordades. MÃµelge olukordadele, kus internetiÃ¼hendus on ebausaldusvÃ¤Ã¤rne vÃµi puudub, nÃ¤iteks kaugetes tÃ¶Ã¶stuskohtades, transiidis olevates sÃµidukites vÃµi piirkondades, kus vÃµrguÃ¼hendus on kehv. Lisaks vÃµivad rakendused, mis nÃµuavad kÃµrgeid turvastandardeid, nagu meditsiiniseadmed, finantssÃ¼steemid vÃµi valitsuse rakendused, vajada tundlike andmete kohalikku tÃ¶Ã¶tlemist, et sÃ¤ilitada privaatsus ja vastavus nÃµuetele.

### Servaarvutuse pÃµhilised piirangud

Servaarvutuse keskkonnad seisavad silmitsi mitmete pÃµhiliste piirangutega, mida traditsioonilised pilvepÃµhised tehisintellekti lahendused ei kohta:

- **Piiratud tÃ¶Ã¶tlemisvÃµimsus**: Servaseadmetel on tavaliselt vÃ¤hem CPU-tuumasid ja madalamad taktsagedused vÃµrreldes serveriklassi riistvaraga.
- **MÃ¤lu piirangud**: Saadaval olev RAM ja salvestusmaht on servaseadmetes oluliselt vÃ¤iksemad.
- **Energiapiirangud**: Akutoitel seadmed peavad tasakaalustama jÃµudlust ja energiatarbimist pikema tÃ¶Ã¶aja tagamiseks.
- **Termiline haldamine**: Kompaktne vormifaktor piirab jahutusvÃµimalusi, mÃµjutades pidevat jÃµudlust koormuse all.

## Mis on EdgeAI?

### Kontseptsioon: EdgeAI mÃ¤Ã¤ratlus

EdgeAI viitab tehisintellekti algoritmide paigutamisele ja kÃ¤itamisele otse servaseadmetesâ€”fÃ¼Ã¼silises riistvaras, mis asub vÃµrgu "servas", lÃ¤hedal andmete genereerimisele ja kogumisele. Need seadmed hÃµlmavad nutitelefone, IoT-andureid, nutikaameraid, autonoomseid sÃµidukeid, kantavaid seadmeid ja tÃ¶Ã¶stusseadmeid. Erinevalt traditsioonilistest tehisintellekti sÃ¼steemidest, mis tuginevad tÃ¶Ã¶tlemiseks pilveserveritele, toob EdgeAI intelligentsuse otse andmeallikale.

EdgeAI keskmes on tehisintellekti tÃ¶Ã¶tlemise detsentraliseerimine, viies selle tsentraliseeritud andmekeskustest eemale ja jaotades selle Ã¼le ulatusliku seadmete vÃµrgu, mis moodustab meie digitaalse Ã¶kosÃ¼steemi. See esindab fundamentaalset arhitektuurilist muutust tehisintellekti sÃ¼steemide kujundamisel ja rakendamisel.

EdgeAI vÃµtmekontseptuaalsed sambad hÃµlmavad:

- **LÃ¤hedane tÃ¶Ã¶tlemine**: Arvutused toimuvad fÃ¼Ã¼siliselt lÃ¤hedal andmete pÃ¤ritolule.
- **Detsentraliseeritud intelligentsus**: OtsustusvÃµime jaotatakse mitme seadme vahel.
- **Andmete suverÃ¤Ã¤nsus**: Informatsioon jÃ¤Ã¤b kohaliku kontrolli alla, sageli ei lahku seadmest.
- **Autonoomne toimimine**: Seadmed suudavad toimida intelligentselt ilma pideva Ã¼henduvuseta.
- **Sisseehitatud tehisintellekt**: Intelligentsus muutub igapÃ¤evaste seadmete lahutamatuks osaks.

### EdgeAI arhitektuuri visualiseerimine

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TRADITIONAL AI ARCHITECTURE                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Data Transfer  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   API Response   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Edge Devices â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Cloud Servers â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ End Users â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            EDGE AI ARCHITECTURE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Direct Response  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Edge Devices with Embedded AI         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ End Users â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â”‚ Sensors â”‚â”€>â”‚ SLM Inference â”‚â”€>â”‚ Local Action â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI esindab paradigmat, kus tehisintellekti rakendamine toimub otse servaseadmetes, mitte ainult pilvepÃµhise tÃ¶Ã¶tlemise kaudu. See lÃ¤henemine vÃµimaldab tehisintellekti mudeleid kÃ¤itada kohapeal seadmetes, millel on piiratud arvutusressursid, pakkudes reaalajas jÃ¤reldusvÃµimekust ilma pideva internetiÃ¼henduseta.

EdgeAI hÃµlmab mitmesuguseid tehnoloogiaid ja tehnikaid, mis on mÃµeldud tehisintellekti mudelite tÃµhusamaks muutmiseks ja nende sobivaks paigutamiseks piiratud ressurssidega seadmetesse. EesmÃ¤rk on sÃ¤ilitada mÃµistlik jÃµudlus, vÃ¤hendades oluliselt tehisintellekti mudelite arvutus- ja mÃ¤lunÃµudeid.

Vaatame pÃµhilisi lÃ¤henemisviise, mis vÃµimaldavad EdgeAI rakendusi erinevat tÃ¼Ã¼pi seadmetes ja kasutusjuhtumites.

### EdgeAI pÃµhialused

EdgeAI tugineb mitmele pÃµhimÃµttele, mis eristavad seda traditsioonilisest pilvepÃµhisest tehisintellektist:

- **Kohalik tÃ¶Ã¶tlemine**: Tehisintellekti jÃ¤reldused tehakse otse servaseadmes ilma vÃ¤lise Ã¼henduvuseta.
- **Ressursside optimeerimine**: Mudelid on optimeeritud spetsiaalselt sihtseadmete riistvaraliste piirangute jaoks.
- **Reaalajas jÃµudlus**: TÃ¶Ã¶tlemine toimub minimaalse latentsusega ajakriitiliste rakenduste jaoks.
- **Privaatsus disainis**: Tundlikud andmed jÃ¤Ã¤vad seadmesse, suurendades turvalisust ja vastavust.

## EdgeAI-d vÃµimaldavad vÃµtmetehnoloogiad

### Mudelite kvantiseerimine

Ãœks olulisemaid tehnikaid EdgeAI-s on mudelite kvantiseerimine. See protsess hÃµlmab mudeli parameetrite tÃ¤psuse vÃ¤hendamist, tavaliselt 32-bitistest ujukomaarvudest 8-bitisteks tÃ¤isarvudeks vÃµi isegi madalama tÃ¤psusega formaatideks. Kuigi see tÃ¤psuse vÃ¤hendamine vÃµib tunduda murettekitav, on uuringud nÃ¤idanud, et paljud tehisintellekti mudelid suudavad sÃ¤ilitada oma jÃµudluse isegi oluliselt vÃ¤hendatud tÃ¤psusega.

Kvantiseerimine tÃ¶Ã¶tab, kaardistades ujukomaarvude vahemiku vÃ¤iksemale diskreetsete vÃ¤Ã¤rtuste kogumile. NÃ¤iteks 32 bitti iga parameetri esindamiseks kasutamise asemel vÃµib kvantiseerimine kasutada ainult 8 bitti, mis toob kaasa 4-kordse mÃ¤lunÃµuete vÃ¤henemise ja sageli kiiremad jÃ¤reldusajad.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

Erinevad kvantiseerimistehnikad hÃµlmavad:

- **JÃ¤reltreeningu kvantiseerimine (PTQ)**: Rakendatakse pÃ¤rast mudeli treenimist ilma uuesti treenimist vajamata.
- **Kvantiseerimisest teadlik treenimine (QAT)**: HÃµlmab kvantiseerimise mÃµju treenimise ajal parema tÃ¤psuse saavutamiseks.
- **DÃ¼naamiline kvantiseerimine**: Kvantiseerib kaalud int8-ks, kuid arvutab aktivatsioonid dÃ¼naamiliselt.
- **Staatiline kvantiseerimine**: Eelnevalt arvutab kÃµik kvantiseerimisparameetrid nii kaaludele kui aktivatsioonidele.

EdgeAI rakenduste puhul sÃµltub sobiva kvantiseerimisstrateegia valik konkreetse mudeli arhitektuurist, jÃµudlusnÃµuetest ja sihtseadme riistvaralistest vÃµimalustest.

### Mudelite tihendamine ja optimeerimine

Lisaks kvantiseerimisele aitavad erinevad tihendustehnikad vÃ¤hendada mudeli suurust ja arvutusnÃµudeid. Need hÃµlmavad:

**PÃ¼gamine**: See tehnika eemaldab neuralvÃµrkudest mittevajalikud Ã¼hendused vÃµi neuronid. Identifitseerides ja kÃµrvaldades parameetrid, mis mudeli jÃµudlusele vÃ¤he kaasa aitavad, vÃµib pÃ¼gamine oluliselt vÃ¤hendada mudeli suurust, sÃ¤ilitades samal ajal tÃ¤psuse.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Teadmiste destilleerimine**: See lÃ¤henemine hÃµlmab vÃ¤iksema "Ãµpilasmudeli" treenimist, et jÃ¤ljendada suurema "Ãµpetajamudeli" kÃ¤itumist. Ã•pilasmudel Ãµpib Ãµpetaja vÃ¤ljundeid ligikaudselt jÃ¤ljendama, saavutades sageli sarnase jÃµudluse oluliselt vÃ¤iksema parameetrite arvuga.

**Mudeli arhitektuuri optimeerimine**: Teadlased on vÃ¤lja tÃ¶Ã¶tanud spetsiaalsed arhitektuurid, mis on mÃµeldud spetsiaalselt servaseadmetes kasutamiseks, nagu MobileNets, EfficientNets ja muud kerged arhitektuurid, mis tasakaalustavad jÃµudlust ja arvutustÃµhusust.

### VÃ¤ikesed keelemudelid (SLM)

EdgeAI-s on esile kerkinud trend vÃ¤ikeste keelemudelite (SLM) arendamisel. Need mudelid on algusest peale loodud kompaktseks ja tÃµhusaks, pakkudes samal ajal olulisi loomuliku keele vÃµimalusi. SLM-id saavutavad selle lÃ¤bi hoolikate arhitektuurivalikute, tÃµhusate treenimistehnikate ja keskendunud treenimise konkreetsetele domeenidele vÃµi Ã¼lesannetele.

Erinevalt traditsioonilistest lÃ¤henemistest, mis hÃµlmavad suurte mudelite tihendamist, treenitakse SLM-e sageli vÃ¤iksemate andmekogumite ja optimeeritud arhitektuuridega, mis on spetsiaalselt mÃµeldud servaseadmetes kasutamiseks. See lÃ¤henemine vÃµib tulemuseks anda mudelid, mis on mitte ainult vÃ¤iksemad, vaid ka tÃµhusamad konkreetsete kasutusjuhtumite jaoks.

## Riistvarakiirendus EdgeAI jaoks

Kaasaegsed servaseadmed sisaldavad Ã¼ha enam spetsiaalset riistvara, mis on mÃµeldud tehisintellekti tÃ¶Ã¶koormuste kiirendamiseks:

### NeuraaltÃ¶Ã¶tlusÃ¼ksused (NPU-d)

NPU-d on spetsiaalsed protsessorid, mis on mÃµeldud spetsiaalselt neuralvÃµrkude arvutuste jaoks. Need kiibid suudavad tehisintellekti jÃ¤reldusÃ¼lesandeid tÃ¤ita palju tÃµhusamalt kui traditsioonilised CPU-d, sageli madalama energiatarbimisega. Paljud kaasaegsed nutitelefonid, sÃ¼learvutid ja IoT-seadmed sisaldavad nÃ¼Ã¼d NPU-sid, et vÃµimaldada seadmesisest tehisintellekti tÃ¶Ã¶tlemist.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

Seadmed NPU-dega hÃµlmavad:

- **Apple**: A-seeria ja M-seeria kiibid Neural Engine'iga
- **Qualcomm**: Snapdragon protsessorid Hexagon DSP/NPU-ga
- **Samsung**: Exynos protsessorid NPU-ga
- **Intel**: Movidius VPU-d ja Habana Labs kiirendid
- **Microsoft**: Windows Copilot+ arvutid NPU-dega

### ğŸ® GPU kiirendus

Kuigi servaseadmetes ei pruugi olla andmekeskustes leiduvaid vÃµimsaid GPU-sid, sisaldavad paljud siiski integreeritud vÃµi eraldiseisvaid GPU-sid, mis suudavad tehisintellekti tÃ¶Ã¶koormusi kiirendada. Kaasaegsed mobiilsed GPU-d ja integreeritud graafikaprotsessorid vÃµivad pakkuda mÃ¤rkimisvÃ¤Ã¤rseid jÃµudluse parandusi tehisintellekti jÃ¤reldusÃ¼lesannete jaoks.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### CPU optimeerimine

Isegi ainult CPU-ga seadmed vÃµivad EdgeAI-st kasu saada optimeeritud rakenduste kaudu. Kaasaegsed CPU-d sisaldavad spetsiaalseid juhiseid tehisintellekti tÃ¶Ã¶koormuste jaoks ning tarkvararaamistikud on vÃ¤lja tÃ¶Ã¶tatud, et maksimeerida CPU jÃµudlust tehisintellekti jÃ¤relduste jaoks.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

EdgeAI-ga tÃ¶Ã¶tavate tarkvarainseneride jaoks on kriitiline mÃµista, kuidas kasutada neid riistvarakiirenduse vÃµimalusi, et optimeerida jÃ¤relduste jÃµudlust ja energiatÃµhusust sihtseadmetes.

## EdgeAI eelised

### Privaatsus ja turvalisus

Ãœks EdgeAI suurimaid eeliseid on paranenud privaatsus ja turvalisus. TÃ¶Ã¶tlemine toimub kohapeal seadmes, mistÃµttu tundlik teave ei lahku kunagi kasutaja kontrolli alt. See on eriti oluline rakenduste puhul, mis kÃ¤sitlevad isikuandmeid, meditsiinilist teavet vÃµi konfidentsiaalset Ã¤rialast teavet.

### VÃ¤hendatud latentsus

EdgeAI kÃµrvaldab vajaduse saata andmeid kaugserveritesse tÃ¶Ã¶tlemiseks, vÃ¤hendades oluliselt latentsust. See on kriitiline reaalajas rakenduste jaoks, nagu autonoomsed sÃµidukid, tÃ¶Ã¶stusautomaatika vÃµi interaktiivsed rakendused, kus on vaja koheseid vastuseid.

### VÃµrguÃ¼henduseta vÃµimekus

EdgeAI vÃµimaldab tehisintellekti funktsionaalsust isegi siis, kui internetiÃ¼hendus puudub. See on vÃ¤Ã¤rtuslik rakenduste jaoks kaugetes asukohtades, reisimise ajal vÃµi olukordades, kus vÃµrgu usaldusvÃ¤Ã¤rsus on probleem.

### KulutÃµhusus

VÃ¤hendades sÃµltuvust pilvepÃµhistest tehisintellekti teenustest, vÃµib EdgeAI aidata vÃ¤hendada tegevuskulusid, eriti rakenduste puhul, millel on suur kasutusmaht. Organisatsioonid saavad vÃ¤ltida pidevaid API-kulusid ja vÃ¤hendada ribalaiuse nÃµudeid.

### Skaleeritavus

EdgeAI jaotab arvutuskoormuse servaseadmete vahel, mitte ei tsentraliseeri seda andmekeskustes. See vÃµib aidata vÃ¤hendada taristukulusid ja parandada kogu sÃ¼steemi skaleeritavust.

## EdgeAI rakendused

### Nutiseadmed ja IoT

EdgeAI toetab paljusid nutiseadmete funktsioone, alates hÃ¤Ã¤lassistentidest, mis suudavad kÃ¤ske kohapeal tÃ¶Ã¶del
- [02: EdgeAI Rakendused](02.RealWorldCaseStudies.md)

---

**LahtiÃ¼tlus**:  
See dokument on tÃµlgitud AI tÃµlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi pÃ¼Ã¼ame tagada tÃ¤psust, palume arvestada, et automaatsed tÃµlked vÃµivad sisaldada vigu vÃµi ebatÃ¤psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtÃµlget. Me ei vastuta arusaamatuste vÃµi valesti tÃµlgenduste eest, mis vÃµivad tekkida selle tÃµlke kasutamise tÃµttu.