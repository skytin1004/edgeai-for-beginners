<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a35d3b47e6ae98ad9b3e89fb73917e90",
  "translation_date": "2025-10-11T11:11:34+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "et"
}
-->
# Section 1: EdgeAI PÃµhitÃµed

EdgeAI esindab paradigmat, mis toob tehisintellekti vÃµimekuse otse servaseadmetesse, mitte ei tugine ainult pilvepÃµhisele tÃ¶Ã¶tlemisele. Oluline on mÃµista, kuidas EdgeAI vÃµimaldab kohalikku AI tÃ¶Ã¶tlemist piiratud ressurssidega seadmetel, sÃ¤ilitades samal ajal mÃµistliku jÃµudluse ja lahendades privaatsuse, latentsuse ja vÃµrguÃ¼henduseta tÃ¶Ã¶tamisega seotud vÃ¤ljakutseid.

## Sissejuhatus

Selles ÃµppetÃ¼kis uurime EdgeAI-d ja selle pÃµhimÃµisteid. KÃ¤sitleme traditsioonilist AI arvutusparadigmat, servaarvutuse vÃ¤ljakutseid, EdgeAI-d vÃµimaldavaid vÃµtmetehnoloogiaid ja praktilisi rakendusi erinevates tÃ¶Ã¶stusharudes.

## Ã•ppeeesmÃ¤rgid

Selle ÃµppetÃ¼ki lÃµpuks suudad:

- MÃµista traditsioonilise pilvepÃµhise AI ja EdgeAI lÃ¤henemisviiside erinevust.
- Tuvastada vÃµtmetehnoloogiad, mis vÃµimaldavad AI tÃ¶Ã¶tlemist servaseadmetel.
- Tunnustada EdgeAI rakenduste eeliseid ja piiranguid.
- Rakendada EdgeAI teadmisi reaalses maailmas ja kasutusjuhtumites.

## Traditsioonilise AI arvutusparadigma mÃµistmine

Traditsiooniliselt tuginevad generatiivse AI rakendused suure jÃµudlusega arvutustaristule, et kÃ¤itada suuri keelemudeleid (LLM) tÃµhusalt. Organisatsioonid paigutavad need mudelid tavaliselt GPU klastritesse pilvekeskkondades, kasutades nende vÃµimekust API-liideste kaudu.

See tsentraliseeritud mudel tÃ¶Ã¶tab hÃ¤sti paljude rakenduste puhul, kuid sellel on servaarvutuse stsenaariumides kaasasÃ¼ndinud piirangud. Traditsiooniline lÃ¤henemine hÃµlmab kasutaja pÃ¤ringute saatmist kaugserveritesse, nende tÃ¶Ã¶tlemist vÃµimsa riistvara abil ja tulemuste tagastamist interneti kaudu. Kuigi see meetod pakub juurdepÃ¤Ã¤su tipptasemel mudelitele, tekitab see sÃµltuvusi internetiÃ¼hendusest, suurendab latentsust ja tÃµstatab privaatsuse kÃ¼simusi, kui tundlikke andmeid tuleb edastada vÃ¤listele serveritele.

Traditsioonilise AI arvutusparadigma puhul tuleb mÃµista mÃµningaid pÃµhikontseptsioone, nimelt:

- **â˜ï¸ PilvepÃµhine tÃ¶Ã¶tlemine**: AI mudelid tÃ¶Ã¶tavad vÃµimsal serveritaristul, millel on suured arvutusressursid.
- **ğŸ”Œ API-pÃµhine juurdepÃ¤Ã¤s**: Rakendused kasutavad AI vÃµimekust kaug-API-kÃµnede kaudu, mitte kohalikult.
- **ğŸ›ï¸ Tsentraliseeritud mudelihaldus**: Mudeleid hallatakse ja uuendatakse tsentraalselt, tagades jÃ¤rjepidevuse, kuid nÃµudes vÃµrguÃ¼hendust.
- **ğŸ“ˆ Ressursside skaleeritavus**: Pilvetaristu saab dÃ¼naamiliselt skaleerida, et toime tulla erinevate arvutusnÃµudmistega.

## Servaarvutuse vÃ¤ljakutsed

Servaseadmed, nagu sÃ¼learvutid, mobiiltelefonid ja asjade interneti (IoT) seadmed, nÃ¤iteks Raspberry Pi ja NVIDIA Orin Nano, esitavad ainulaadseid arvutuspiiranguid. Neil seadmetel on tavaliselt piiratud tÃ¶Ã¶tlemisvÃµimsus, mÃ¤lu ja energiavarud vÃµrreldes andmekeskuste taristuga.

Traditsiooniliste LLM-ide kÃ¤itamine sellistel seadmetel on ajalooliselt olnud keeruline nende riistvarapiirangute tÃµttu. Kuid serva AI tÃ¶Ã¶tlemise vajadus on muutunud Ã¼ha olulisemaks erinevates olukordades. MÃµelge olukordadele, kus internetiÃ¼hendus on ebausaldusvÃ¤Ã¤rne vÃµi puudub, nÃ¤iteks kaugemates tÃ¶Ã¶stuskohtades, transiidis olevates sÃµidukites vÃµi piirkondades, kus vÃµrguÃ¼hendus on kehv. Lisaks vÃµivad rakendused, mis nÃµuavad kÃµrgeid turvastandardeid, nagu meditsiiniseadmed, finantssÃ¼steemid vÃµi valitsuse rakendused, vajada tundlike andmete kohalikku tÃ¶Ã¶tlemist privaatsuse ja vastavusnÃµuete sÃ¤ilitamiseks.

### Servaarvutuse pÃµhipiirangud

Servaarvutuskeskkonnad seisavad silmitsi mitmete pÃµhiliste piirangutega, mida traditsioonilised pilvepÃµhised AI lahendused ei kohta:

- **Piiratud tÃ¶Ã¶tlemisvÃµimsus**: Servaseadmetel on tavaliselt vÃ¤hem CPU tuumasid ja madalamad taktsagedused vÃµrreldes serveriklassi riistvaraga.
- **MÃ¤lu piirangud**: Saadaval olev RAM ja salvestusmaht on servaseadmetel oluliselt vÃ¤iksemad.
- **Energiapiirangud**: Aku toitel tÃ¶Ã¶tavad seadmed peavad tasakaalustama jÃµudlust ja energiatarbimist pikema tÃ¶Ã¶aja tagamiseks.
- **Termohaldus**: Kompaktne vormifaktor piirab jahutusvÃµimalusi, mÃµjutades pidevat jÃµudlust koormuse all.

## Mis on EdgeAI?

### Kontseptsioon: EdgeAI mÃ¤Ã¤ratlus

EdgeAI viitab tehisintellekti algoritmide juurutamisele ja kÃ¤itamisele otse servaseadmetelâ€”fÃ¼Ã¼silisel riistvaral, mis asub vÃµrgu "servas", lÃ¤hedal andmete genereerimisele ja kogumisele. Need seadmed hÃµlmavad nutitelefone, IoT sensoreid, nutikaameraid, autonoomseid sÃµidukeid, kantavaid seadmeid ja tÃ¶Ã¶stusseadmeid. Erinevalt traditsioonilistest AI sÃ¼steemidest, mis tuginevad tÃ¶Ã¶tlemiseks pilveserveritele, toob EdgeAI intelligentsuse otse andmeallikasse.

EdgeAI keskmes on AI tÃ¶Ã¶tlemise detsentraliseerimine, viies selle tsentraliseeritud andmekeskustest eemale ja jaotades selle Ã¼le digitaalse Ã¶kosÃ¼steemi ulatusliku seadmete vÃµrgu. See esindab fundamentaalset arhitektuurilist muutust AI sÃ¼steemide kujundamisel ja juurutamisel.

EdgeAI vÃµtmekontseptuaalsed sambad hÃµlmavad:

- **LÃ¤hedustÃ¶Ã¶tlemine**: Arvutus toimub fÃ¼Ã¼siliselt lÃ¤hedal andmete pÃ¤ritolule.
- **Detsentraliseeritud intelligentsus**: OtsustusvÃµime jaotatakse mitme seadme vahel.
- **AndmesuverÃ¤Ã¤nsus**: Informatsioon jÃ¤Ã¤b kohaliku kontrolli alla, sageli ei lahku seadmest.
- **Autonoomne toimimine**: Seadmed suudavad toimida intelligentselt ilma pideva Ã¼henduvuseta.
- **Sisseehitatud AI**: Intelligentsus muutub igapÃ¤evaste seadmete lahutamatuks osaks.

### EdgeAI arhitektuuri visualiseerimine

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TRADITIONAL AI ARCHITECTURE                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Data Transfer  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   API Response   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Edge Devices â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Cloud Servers â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ End Users â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            EDGE AI ARCHITECTURE                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Direct Response   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Edge Devices with Embedded AI        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ End Users â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â”‚ Sensors â”‚â”€>â”‚ SLM Inference â”‚â”€>â”‚ Local Action â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI esindab paradigmat, mis toob tehisintellekti vÃµimekuse otse servaseadmetesse, mitte ei tugine ainult pilvepÃµhisele tÃ¶Ã¶tlemisele. See lÃ¤henemine vÃµimaldab AI mudeleid kÃ¤itada kohapeal seadmetel, millel on piiratud arvutusressursid, pakkudes reaalajas jÃ¤reldusvÃµimekust ilma pideva internetiÃ¼henduseta.

EdgeAI hÃµlmab mitmesuguseid tehnoloogiaid ja tehnikaid, mis on mÃµeldud AI mudelite tÃµhusamaks muutmiseks ja nende juurutamiseks piiratud ressurssidega seadmetel. EesmÃ¤rk on sÃ¤ilitada mÃµistlik jÃµudlus, vÃ¤hendades oluliselt AI mudelite arvutus- ja mÃ¤lunÃµudeid.

Vaatame pÃµhilisi lÃ¤henemisviise, mis vÃµimaldavad EdgeAI rakendusi erinevatel seadmetÃ¼Ã¼pidel ja kasutusjuhtudel.

### EdgeAI pÃµhialused

EdgeAI tugineb mitmele pÃµhimÃµttele, mis eristavad seda traditsioonilisest pilvepÃµhisest AI-st:

- **Kohalik tÃ¶Ã¶tlemine**: AI jÃ¤reldus toimub otse servaseadmel, ilma et oleks vaja vÃ¤list Ã¼henduvust.
- **Ressursside optimeerimine**: Mudelid on optimeeritud spetsiaalselt sihtseadmete riistvarapiirangute jaoks.
- **Reaalajas jÃµudlus**: TÃ¶Ã¶tlemine toimub minimaalse latentsusega ajakriitiliste rakenduste jaoks.
- **Privaatsus disainis**: Tundlikud andmed jÃ¤Ã¤vad seadmesse, suurendades turvalisust ja vastavust.

## EdgeAI-d vÃµimaldavad vÃµtmetehnoloogiad

### Mudeli kvantiseerimine

Ãœks olulisemaid tehnikaid EdgeAI-s on mudeli kvantiseerimine. See protsess hÃµlmab mudeli parameetrite tÃ¤psuse vÃ¤hendamist, tavaliselt 32-bitistest ujukomaarvudest 8-bitisteks tÃ¤isarvudeks vÃµi isegi madalama tÃ¤psusega formaatideks. Kuigi tÃ¤psuse vÃ¤hendamine vÃµib tunduda murettekitav, on uuringud nÃ¤idanud, et paljud AI mudelid suudavad sÃ¤ilitada oma jÃµudluse isegi oluliselt vÃ¤hendatud tÃ¤psusega.

Kvantiseerimine tÃ¶Ã¶tab, kaardistades ujukomaarvude vahemiku vÃ¤iksemale diskreetsete vÃ¤Ã¤rtuste kogumile. NÃ¤iteks 32 bitti kasutamise asemel iga parameetri esindamiseks vÃµib kvantiseerimine kasutada ainult 8 bitti, mis toob kaasa 4-kordse mÃ¤lunÃµuete vÃ¤henemise ja sageli kiiremad jÃ¤reldusajad.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

Erinevad kvantiseerimistehnikad hÃµlmavad:

- **Post-treeningu kvantiseerimine (PTQ)**: Rakendatakse pÃ¤rast mudeli treenimist, ilma et oleks vaja uuesti treenimist.
- **Kvantiseerimisest teadlik treenimine (QAT)**: HÃµlmab kvantiseerimise mÃµju treenimise ajal, et saavutada parem tÃ¤psus.
- **DÃ¼naamiline kvantiseerimine**: Kvantiseerib kaalud int8-ks, kuid arvutab aktivatsioonid dÃ¼naamiliselt.
- **Staatiline kvantiseerimine**: Eelnevalt arvutab kÃµik kvantiseerimisparameetrid nii kaaludele kui aktivatsioonidele.

EdgeAI juurutuste puhul sÃµltub sobiva kvantiseerimisstrateegia valik konkreetse mudeli arhitektuurist, jÃµudlusnÃµuetest ja sihtseadme riistvaravÃµimekusest.

### Mudeli tihendamine ja optimeerimine

Lisaks kvantiseerimisele aitavad mitmesugused tihendustehnikad vÃ¤hendada mudeli suurust ja arvutusnÃµudeid. Nende hulka kuuluvad:

**PÃ¼gamine**: See tehnika eemaldab neuralvÃµrkudest mittevajalikud Ã¼hendused vÃµi neuronid. Identifitseerides ja kÃµrvaldades parameetrid, mis mudeli jÃµudlusele vÃ¤he kaasa aitavad, vÃµib pÃ¼gamine oluliselt vÃ¤hendada mudeli suurust, sÃ¤ilitades samal ajal tÃ¤psuse.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Teadmiste destilleerimine**: See lÃ¤henemine hÃµlmab vÃ¤iksema "Ãµpilasmudeli" treenimist, et jÃ¤ljendada suurema "Ãµpetajamudeli" kÃ¤itumist. Ã•pilasmudel Ãµpib Ãµpetaja vÃ¤ljundeid ligikaudselt jÃ¤ljendama, saavutades sageli sarnase jÃµudluse oluliselt vÃ¤iksema parameetrite arvuga.

**Mudeli arhitektuuri optimeerimine**: Teadlased on vÃ¤lja tÃ¶Ã¶tanud spetsiaalsed arhitektuurid, mis on mÃµeldud spetsiaalselt serva juurutamiseks, nagu MobileNets, EfficientNets ja muud kerged arhitektuurid, mis tasakaalustavad jÃµudlust ja arvutustÃµhusust.

### VÃ¤ikesed keelemudelid (SLM)

EdgeAI-s on esile kerkinud trend vÃ¤ikeste keelemudelite (SLM) arendamisel. Need mudelid on algusest peale loodud kompaktseks ja tÃµhusaks, pakkudes samal ajal tÃ¤henduslikke loomuliku keele vÃµimekusi. SLM-id saavutavad selle lÃ¤bi hoolikate arhitektuurivalikute, tÃµhusate treenimistehnikate ja keskendunud treenimise konkreetsetele domeenidele vÃµi Ã¼lesannetele.

Erinevalt traditsioonilistest lÃ¤henemistest, mis hÃµlmavad suurte mudelite tihendamist, treenitakse SLM-e sageli vÃ¤iksemate andmekogumite ja optimeeritud arhitektuuridega, mis on spetsiaalselt serva juurutamiseks mÃµeldud. See lÃ¤henemine vÃµib tuua kaasa mudeleid, mis on mitte ainult vÃ¤iksemad, vaid ka tÃµhusamad konkreetsete kasutusjuhtumite jaoks.

## Riistvarakiirendus EdgeAI jaoks

Kaasaegsed servaseadmed sisaldavad Ã¼ha enam spetsiaalset riistvara, mis on mÃµeldud AI tÃ¶Ã¶koormuste kiirendamiseks:

### NeuraaltÃ¶Ã¶tlusÃ¼ksused (NPU-d)

NPU-d on spetsiaalsed protsessorid, mis on mÃµeldud spetsiaalselt neuralvÃµrkude arvutuste jaoks. Need kiibid suudavad AI jÃ¤reldusÃ¼lesandeid tÃ¤ita palju tÃµhusamalt kui traditsioonilised CPU-d, sageli madalama energiatarbimisega. Paljud kaasaegsed nutitelefonid, sÃ¼learvutid ja IoT seadmed sisaldavad nÃ¼Ã¼d NPU-sid, et vÃµimaldada seadmesisest AI tÃ¶Ã¶tlemist.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

Seadmed NPU-dega hÃµlmavad:

- **Apple**: A-seeria ja M-seeria kiibid Neural Engine'iga
- **Qualcomm**: Snapdragon protsessorid Hexagon DSP/NPU-ga
- **Samsung**: Exynos protsessorid NPU-ga
- **Intel**: Movidius VPU-d ja Habana Labs kiirendid
- **Microsoft**: Windows Copilot+ PC-d NPU-dega

### ğŸ® GPU kiirendus

Kuigi servaseadmetel ei pruugi olla andmekeskustes leiduvate vÃµimsate GPU-dega vÃµrreldavat jÃµudlust, sisaldavad paljud siiski integreeritud vÃµi eraldiseisvaid GPU-sid, mis suudavad AI tÃ¶Ã¶koormusi kiirendada. Kaasaegsed mobiilsed GPU-d ja integreeritud graafikaprotsessorid vÃµivad pakkuda AI jÃ¤reldusÃ¼lesannete jaoks mÃ¤rkimisvÃ¤Ã¤rset jÃµudluse paranemist.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### CPU optimeerimine

Isegi ainult CPU-ga seadmed vÃµivad EdgeAI-st kasu saada lÃ¤bi optimeeritud rakenduste. Kaasaegsed CPU-d sisaldavad spetsiaalseid juhiseid AI tÃ¶Ã¶koormuste jaoks ning tarkvararaamistikud on vÃ¤lja tÃ¶Ã¶tatud, et maksimeerida CPU jÃµudlust AI jÃ¤relduste jaoks.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

EdgeAI-ga tÃ¶Ã¶tavate tarkvarainseneride jaoks on kriitiline mÃµista, kuidas kasutada neid riistvarakiirenduse vÃµimalusi, et optimeerida jÃ¤relduste jÃµudlust ja energiatÃµhusust sihtseadmetel.

## EdgeAI eelised

### Privaatsus ja turvalisus

Ãœks EdgeAI suurimaid eeliseid on paranenud privaatsus ja turvalisus. TÃ¶Ã¶tledes andmeid kohapeal seadmes, ei lahku tundlik teave kunagi kasutaja kontrolli alt. See on eriti oluline rakenduste puhul, mis kÃ¤sitlevad isikuandmeid, meditsiinilist teavet vÃµi konfidentsiaalseid Ã¤rilisi andmeid.

### VÃ¤hendatud latentsus

EdgeAI kÃµrvaldab vajaduse saata andmeid kaugserveritesse tÃ¶Ã¶tlemiseks, vÃ¤hendades oluliselt latentsust. See on kriitiline reaalajas rakenduste jaoks, nagu autonoomsed sÃµidukid, tÃ¶Ã¶stusautomaatika vÃµi interaktiivsed rakendused, kus on vaja koheseid vastuseid.

### VÃµrguÃ¼henduseta vÃµimekus

EdgeAI vÃµimaldab AI funktsionaalsust isegi siis, kui internetiÃ¼hendus puudub. See on vÃ¤Ã¤rtuslik rakenduste jaoks kaugemates asukohtades, reisimise ajal vÃµi olukordades, kus vÃµrgu usaldusvÃ¤Ã¤rsus on probleem.

### KulutÃµhusus

VÃ¤hendades sÃµltuvust pilvepÃµhistest AI teenustest, vÃµib EdgeAI aidata vÃ¤hendada tegevuskulusid, eriti rakenduste puhul, millel on suur kasutusmaht. Organisatsioonid saavad vÃ¤ltida pidevaid API kulusid ja vÃ¤hendada ribalaiuse nÃµudeid.

### Skaleeritavus

EdgeAI jaotab arvutuskoormuse servaseadmete vahel, mitte ei tsentraliseeri seda andmekeskustes. See vÃµib aidata vÃ¤hendada taristukulusid ja parandada kogu sÃ¼steemi skaleeritavust.

## EdgeAI rakendused

### Nutiseadmed ja IoT

EdgeAI toetab paljusid nutiseadmete funktsioone, alates hÃ¤Ã¤lassistentidest, mis suudavad kÃ¤ske kohapeal tÃ¶Ã¶delda, kuni nutikaamerateni, mis suudavad tuvastada objekte ja inimesi ilma videot pilve saatmata. IoT seadmed kasutavad EdgeAI-d ennustava hoolduse, keskkonnaseire ja automatiseeritud otsuste tegemiseks.

### Mobiilirakendused

Nutitelefonid ja tahvelarvutid kasutavad EdgeAI-d mitmesuguste funktsioonide jaoks, sealhulgas fototÃ¶Ã¶tlus, reaalajas tÃµlkimine, liitreaalsus ja isikupÃ¤rastatud soov
- [02: EdgeAI Rakendused](02.RealWorldCaseStudies.md)

---

**LahtiÃ¼tlus**:  
See dokument on tÃµlgitud AI tÃµlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi pÃ¼Ã¼ame tagada tÃ¤psust, palume arvestada, et automaatsed tÃµlked vÃµivad sisaldada vigu vÃµi ebatÃ¤psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtÃµlget. Me ei vastuta selle tÃµlke kasutamisest tulenevate arusaamatuste vÃµi valesti tÃµlgenduste eest.