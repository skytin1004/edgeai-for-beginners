<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "be25052ac4c842765e7f6f7eb4d7dcc5",
  "translation_date": "2025-10-20T10:07:40+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "et"
}
-->
# 1. peatÃ¼kk: EdgeAI pÃµhialused

EdgeAI esindab tehisintellekti juurutamise paradigmat, mis toob AI vÃµimekuse otse Ã¤Ã¤reseadmetesse, mitte ei tugine ainult pilvepÃµhisele tÃ¶Ã¶tlemisele. Oluline on mÃµista, kuidas EdgeAI vÃµimaldab kohalikku AI tÃ¶Ã¶tlemist piiratud ressurssidega seadmetel, sÃ¤ilitades samal ajal mÃµistliku jÃµudluse ja lahendades vÃ¤ljakutseid nagu privaatsus, latentsus ja vÃµrguÃ¼henduseta toimimine.

## Sissejuhatus

Selles ÃµppetÃ¼kis uurime EdgeAI-d ja selle pÃµhikontseptsioone. KÃ¤sitleme traditsioonilist AI arvutusparadigmat, Ã¤Ã¤retÃ¶Ã¶tluse vÃ¤ljakutseid, EdgeAI-d vÃµimaldavaid vÃµtmetehnoloogiaid ja praktilisi rakendusi erinevates tÃ¶Ã¶stusharudes.

## Ã•ppe-eesmÃ¤rgid

Selle ÃµppetÃ¼ki lÃµpuks oskate:

- MÃµista traditsioonilise pilvepÃµhise AI ja EdgeAI lÃ¤henemisviiside erinevusi.
- Tuvastada vÃµtmetehnoloogiad, mis vÃµimaldavad AI tÃ¶Ã¶tlemist Ã¤Ã¤reseadmetes.
- Tunnustada EdgeAI rakenduste eeliseid ja piiranguid.
- Rakendada EdgeAI teadmisi reaalsetes olukordades ja kasutusjuhtumites.

## Traditsioonilise AI arvutusparadigma mÃµistmine

Traditsiooniliselt tuginevad generatiivse tehisintellekti rakendused suure jÃµudlusega arvutustaristule, et kÃ¤itada tÃµhusalt suuri keelemudeleid (LLM). Organisatsioonid juurutavad tavaliselt need mudelid GPU klastritele pilvekeskkonnas, kasutades nende vÃµimekust API-liideste kaudu.

See tsentraliseeritud mudel toimib hÃ¤sti paljude rakenduste puhul, kuid sellel on Ã¤Ã¤retÃ¶Ã¶tluse stsenaariumides omased piirangud. Traditsiooniline lÃ¤henemine hÃµlmab kasutaja pÃ¤ringute saatmist kaugserveritesse, nende tÃ¶Ã¶tlemist vÃµimsa riistvara abil ja tulemuste tagastamist interneti kaudu. Kuigi see meetod vÃµimaldab juurdepÃ¤Ã¤su tipptasemel mudelitele, tekitab see sÃµltuvuse internetiÃ¼hendusest, suurendab latentsust ja tÃµstatab privaatsuse kÃ¼simusi, kui tundlikke andmeid tuleb edastada vÃ¤listele serveritele.

Traditsioonilise AI arvutusparadigma puhul tuleb mÃµista mÃµningaid pÃµhikontseptsioone, nimelt:

- **â˜ï¸ PilvepÃµhine tÃ¶Ã¶tlemine**: AI mudelid tÃ¶Ã¶tavad vÃµimsal serveritaristul, millel on suured arvutusressursid.
- **ğŸ”Œ API-pÃµhine juurdepÃ¤Ã¤s**: Rakendused kasutavad AI vÃµimekust kaug-API-kÃµnede kaudu, mitte kohalikult.
- **ğŸ›ï¸ Tsentraliseeritud mudelihaldus**: Mudeleid hallatakse ja uuendatakse tsentraalselt, tagades jÃ¤rjepidevuse, kuid nÃµudes vÃµrguÃ¼hendust.
- **ğŸ“ˆ Ressursside skaleeritavus**: Pilvetaristu saab dÃ¼naamiliselt skaleerida, et toime tulla erinevate arvutusnÃµudmistega.

## Ã„Ã¤retÃ¶Ã¶tluse vÃ¤ljakutsed

Ã„Ã¤reseadmed, nagu sÃ¼learvutid, mobiiltelefonid ja asjade interneti (IoT) seadmed, nÃ¤iteks Raspberry Pi ja NVIDIA Orin Nano, esitlevad unikaalseid arvutuspiiranguid. Need seadmed omavad tavaliselt piiratud tÃ¶Ã¶tlemisvÃµimsust, mÃ¤lu ja energiavarusid vÃµrreldes andmekeskuste taristuga.

Traditsiooniliste LLM-ide kÃ¤itamine sellistel seadmetel on ajalooliselt olnud keeruline nende riistvarapiirangute tÃµttu. Kuid vajadus Ã¤Ã¤re-AI tÃ¶Ã¶tlemise jÃ¤rele on muutunud Ã¼ha olulisemaks erinevates olukordades. MÃµelge olukordadele, kus internetiÃ¼hendus on ebausaldusvÃ¤Ã¤rne vÃµi puudub, nÃ¤iteks kaugemates tÃ¶Ã¶stuspiirkondades, transiidis olevates sÃµidukites vÃµi kehva vÃµrguÃ¼hendusega piirkondades. Lisaks vÃµivad rakendused, mis nÃµuavad kÃµrgeid turvastandardeid, nagu meditsiiniseadmed, finantssÃ¼steemid vÃµi valitsuse rakendused, vajada tundlike andmete kohalikku tÃ¶Ã¶tlemist privaatsuse ja vastavusnÃµuete sÃ¤ilitamiseks.

### Ã„Ã¤retÃ¶Ã¶tluse pÃµhilised piirangud

Ã„Ã¤retÃ¶Ã¶tluskeskkonnad seisavad silmitsi mitmete pÃµhiliste piirangutega, mida traditsioonilised pilvepÃµhised AI lahendused ei kohta:

- **Piiratud tÃ¶Ã¶tlemisvÃµimsus**: Ã„Ã¤reseadmetel on tavaliselt vÃ¤hem CPU tuumasid ja madalamad taktsagedused vÃµrreldes serveriklassi riistvaraga.
- **MÃ¤lupiirangud**: Saadaval olev RAM ja salvestusmaht on Ã¤Ã¤reseadmetes oluliselt vÃ¤iksemad.
- **Energiapiirangud**: Aku toitel tÃ¶Ã¶tavad seadmed peavad tasakaalustama jÃµudlust ja energiatarbimist pikema tÃ¶Ã¶aja tagamiseks.
- **Termohaldus**: Kompaktsed vormid piiravad jahutusvÃµimalusi, mÃµjutades koormuse all pÃ¼sivat jÃµudlust.

## Mis on EdgeAI?

### Kontseptsioon: EdgeAI mÃ¤Ã¤ratlus

EdgeAI viitab tehisintellekti algoritmide juurutamisele ja kÃ¤itamisele otse Ã¤Ã¤reseadmetesâ€”fÃ¼Ã¼silises riistvaras, mis asub vÃµrgu "Ã¤Ã¤res", lÃ¤hedal andmete genereerimisele ja kogumisele. Nende seadmete hulka kuuluvad nutitelefonid, IoT sensorid, nutikad kaamerad, autonoomsed sÃµidukid, kantavad seadmed ja tÃ¶Ã¶stusseadmed. Erinevalt traditsioonilistest AI sÃ¼steemidest, mis tuginevad tÃ¶Ã¶tlemiseks pilveserveritele, toob EdgeAI intelligentsuse otse andmeallikani.

EdgeAI pÃµhineb AI tÃ¶Ã¶tlemise detsentraliseerimisel, viies selle tsentraliseeritud andmekeskustest eemale ja jaotades selle laialdase digitaalse Ã¶kosÃ¼steemi seadmete vahel. See esindab fundamentaalset arhitektuurilist muutust AI sÃ¼steemide kujundamises ja juurutamises.

EdgeAI peamised kontseptuaalsed tugisambad hÃµlmavad:

- **LÃ¤hedus tÃ¶Ã¶tlemisele**: Arvutused toimuvad fÃ¼Ã¼siliselt lÃ¤hedal andmete pÃ¤ritolule.
- **Detsentraliseeritud intelligentsus**: OtsustusvÃµime jaotatakse mitme seadme vahel.
- **Andmete suverÃ¤Ã¤nsus**: Informatsioon jÃ¤Ã¤b kohaliku kontrolli alla, sageli ei lahku see seadmest.
- **Autonoomne toimimine**: Seadmed suudavad toimida intelligentselt ilma pideva Ã¼henduvuseta.
- **Sisseehitatud AI**: Intelligentsus muutub igapÃ¤evaste seadmete lahutamatuks osaks.

### EdgeAI arhitektuuri visualiseerimine

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TRADITIONAL AI ARCHITECTURE                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Data Transfer  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   API Response   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Edge Devices â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Cloud Servers â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ End Users â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            EDGE AI ARCHITECTURE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Direct Response  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Edge Devices with Embedded AI         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ End Users â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â”‚ Sensors â”‚â”€>â”‚ SLM Inference â”‚â”€>â”‚ Local Action â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI esindab tehisintellekti juurutamise paradigmat, mis toob AI vÃµimekuse otse Ã¤Ã¤reseadmetesse, mitte ei tugine ainult pilvepÃµhisele tÃ¶Ã¶tlemisele. See lÃ¤henemine vÃµimaldab AI mudeleid kÃ¤itada kohapeal piiratud arvutusressurssidega seadmetel, pakkudes reaalajas jÃ¤reldusvÃµimekust ilma pideva internetiÃ¼henduseta.

EdgeAI hÃµlmab mitmesuguseid tehnoloogiaid ja tehnikaid, mis on mÃµeldud AI mudelite tÃµhusamaks muutmiseks ja nende juurutamiseks piiratud ressurssidega seadmetel. EesmÃ¤rk on sÃ¤ilitada mÃµistlik jÃµudlus, vÃ¤hendades oluliselt AI mudelite arvutus- ja mÃ¤lunÃµudeid.

Vaatame peamisi lÃ¤henemisviise, mis vÃµimaldavad EdgeAI rakendusi erinevat tÃ¼Ã¼pi seadmetes ja kasutusjuhtumites.

### EdgeAI pÃµhialused

EdgeAI tugineb mitmele pÃµhimÃµttele, mis eristavad seda traditsioonilisest pilvepÃµhisest AI-st:

- **Kohalik tÃ¶Ã¶tlemine**: AI jÃ¤reldused tehakse otse Ã¤Ã¤reseadmes, ilma et oleks vaja vÃ¤list Ã¼henduvust.
- **Ressursside optimeerimine**: Mudeleid optimeeritakse spetsiaalselt sihtseadmete riistvarapiirangute jaoks.
- **Reaalajas jÃµudlus**: TÃ¶Ã¶tlemine toimub minimaalse latentsusega ajakriitiliste rakenduste jaoks.
- **Privaatsus disainis**: Tundlikud andmed jÃ¤Ã¤vad seadmesse, suurendades turvalisust ja vastavust.

## EdgeAI-d vÃµimaldavad vÃµtmetehnoloogiad

### Mudelite kvantiseerimine

Ãœks olulisemaid tehnikaid EdgeAI-s on mudelite kvantiseerimine. See protsess hÃµlmab mudeli parameetrite tÃ¤psuse vÃ¤hendamist, tavaliselt 32-bitistest ujukomaarvudest 8-bitisteks tÃ¤isarvudeks vÃµi isegi madalama tÃ¤psusega formaatideks. Kuigi see tÃ¤psuse vÃ¤hendamine vÃµib tunduda murettekitav, on uuringud nÃ¤idanud, et paljud AI mudelid suudavad sÃ¤ilitada oma jÃµudluse isegi oluliselt vÃ¤hendatud tÃ¤psusega.

Kvantiseerimine toimib, kaardistades ujukomaarvude vahemiku vÃ¤iksemaks diskreetsete vÃ¤Ã¤rtuste kogumiks. NÃ¤iteks 32 bitti kasutamise asemel iga parameetri esindamiseks vÃµib kvantiseerimine kasutada ainult 8 bitti, mis toob kaasa 4-kordse mÃ¤lunÃµuete vÃ¤henemise ja sageli kiiremaks jÃ¤relduste tegemiseks.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

Erinevad kvantiseerimistehnikad hÃµlmavad:

- **Post-treeningu kvantiseerimine (PTQ)**: Rakendatakse pÃ¤rast mudeli treenimist, ilma et oleks vaja uuesti treenida.
- **Kvantiseerimisega teadlik treenimine (QAT)**: Kaasatakse kvantiseerimise mÃµjud treenimise ajal parema tÃ¤psuse saavutamiseks.
- **DÃ¼naamiline kvantiseerimine**: Kvantiseerib kaalu int8-le, kuid arvutab aktivatsioonid dÃ¼naamiliselt.
- **Staatiline kvantiseerimine**: Eelnevalt arvutab kÃµik kvantiseerimisparameetrid nii kaaludele kui ka aktivatsioonidele.

EdgeAI juurutuste puhul sÃµltub sobiva kvantiseerimisstrateegia valik konkreetse mudeli arhitektuurist, jÃµudlusnÃµuetest ja sihtseadme riistvaravÃµimekusest.

### Mudelite tihendamine ja optimeerimine

Lisaks kvantiseerimisele aitavad mitmesugused tihendustehnikad vÃ¤hendada mudeli suurust ja arvutusnÃµudeid. Nende hulka kuuluvad:

**PÃ¼gamine**: See tehnika eemaldab neuralvÃµrkudest mittevajalikud Ã¼hendused vÃµi neuronid. Tuues vÃ¤lja ja kÃµrvaldades parameetrid, mis mudeli jÃµudlusele vÃ¤he kaasa aitavad, vÃµib pÃ¼gamine oluliselt vÃ¤hendada mudeli suurust, sÃ¤ilitades samal ajal tÃ¤psuse.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Teadmiste destilleerimine**: See lÃ¤henemine hÃµlmab vÃ¤iksema "Ãµpilasmudeli" treenimist, et jÃ¤ljendada suurema "Ãµpetajamudeli" kÃ¤itumist. Ã•pilasmudel Ãµpib Ãµpetaja vÃ¤ljundeid jÃ¤ljendama, saavutades sageli sarnase jÃµudluse oluliselt vÃ¤iksema parameetrite arvuga.

**Mudeli arhitektuuri optimeerimine**: Teadlased on vÃ¤lja tÃ¶Ã¶tanud spetsiaalsed arhitektuurid, mis on mÃµeldud spetsiaalselt Ã¤Ã¤rejuurutamiseks, nagu MobileNets, EfficientNets ja muud kerged arhitektuurid, mis tasakaalustavad jÃµudlust ja arvutustÃµhusust.

### VÃ¤ikesed keelemudelid (SLM)

EdgeAI-s on esile kerkinud trend vÃ¤ikeste keelemudelite (SLM) arendamisel. Need mudelid on algusest peale loodud kompaktseks ja tÃµhusaks, pakkudes samal ajal olulisi loomuliku keele vÃµimekusi. SLM-id saavutavad selle lÃ¤bi hoolikate arhitektuurivalikute, tÃµhusate treenimistehnikate ja keskendunud treenimise konkreetsetele domeenidele vÃµi Ã¼lesannetele.

Erinevalt traditsioonilistest lÃ¤henemistest, mis hÃµlmavad suurte mudelite tihendamist, treenitakse SLM-e sageli vÃ¤iksemate andmekogumite ja optimeeritud arhitektuuridega, mis on spetsiaalselt mÃµeldud Ã¤Ã¤rejuurutamiseks. See lÃ¤henemine vÃµib anda mudeleid, mis on mitte ainult vÃ¤iksemad, vaid ka tÃµhusamad konkreetsete kasutusjuhtumite jaoks.

## Riistvara kiirendus EdgeAI jaoks

Kaasaegsed Ã¤Ã¤reseadmed sisaldavad Ã¼ha enam spetsiaalset riistvara, mis on mÃµeldud AI tÃ¶Ã¶koormuste kiirendamiseks:

### NeuraaltÃ¶Ã¶tlusÃ¼ksused (NPUs)

NPUs on spetsiaalsed protsessorid, mis on mÃµeldud spetsiaalselt neuralvÃµrkude arvutuste jaoks. Need kiibid suudavad AI jÃ¤reldusÃ¼lesandeid tÃ¤ita palju tÃµhusamalt kui traditsioonilised CPU-d, sageli madalama energiatarbimisega. Paljud kaasaegsed nutitelefonid, sÃ¼learvutid ja IoT-seadmed sisaldavad nÃ¼Ã¼d NPUsid, et vÃµimaldada seadmesisest AI tÃ¶Ã¶tlemist.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

Seadmed, millel on NPUs, hÃµlmavad:

- **Apple**: A-seeria ja M-seeria kiibid Neural Engine'iga
- **Qualcomm**: Snapdragon protsessorid Hexagon DSP/NPU-ga
- **Samsung**: Exynos protsessorid NPU-ga
- **Intel**: Movidius VPU-d ja Habana Labs kiirendid
- **Microsoft**: Windows Copilot+ arvutid NPUsidega

### ğŸ® GPU kiirendus

Kuigi Ã¤Ã¤reseadmetel ei pruugi olla andmekeskustes leiduvaid vÃµimsaid GPU-sid, sisaldavad paljud siiski integreeritud vÃµi eraldiseisvaid GPU-sid, mis suudavad kiirendada AI tÃ¶Ã¶koormusi. Kaasaegsed mobiilsed GPU-d ja integreeritud graafikaprotsessorid vÃµivad pakkuda AI jÃ¤reldusÃ¼lesannete jaoks mÃ¤rkimisvÃ¤Ã¤rset jÃµudluse paranemist.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### CPU optimeerimine

Isegi ainult CPU-ga seadmed vÃµivad EdgeAI-st kasu saada optimeeritud rakenduste kaudu. Kaasaegsed CPU-d sisaldavad spetsiaalseid juhiseid AI tÃ¶Ã¶koormuste jaoks ning tarkvararaamistikud on vÃ¤lja tÃ¶Ã¶tatud, et maksimeerida CPU jÃµudlust AI jÃ¤relduste tegemisel.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

EdgeAI-ga tÃ¶Ã¶tavatele tarkvarainseneridele on kriitiline mÃµista, kuidas kasutada neid riistvara kiirendusvÃµimalusi, et optimeerida jÃ¤relduste jÃµudlust ja energiatÃµhusust sihtseadmetes.

## EdgeAI eelised

### Privaatsus ja turvalisus

Ãœks EdgeAI suurimaid eeliseid on parem privaatsus ja turvalisus. TÃ¶Ã¶tledes andmeid kohapeal seadmes, ei lahku tundlik teave kunagi kasutaja kontrolli alt. See on eriti oluline rakenduste puhul, mis tÃ¶Ã¶tlevad isikuandmeid, meditsiinilist teavet vÃµi konfidentsiaalset Ã¤rialast teavet.

### VÃ¤henenud latentsus

EdgeAI kÃµrvaldab vajaduse saata andmeid kaugserveritesse tÃ¶Ã¶tlemiseks, vÃ¤hendades oluliselt latentsust. See on kriitiline reaalajas rakenduste jaoks, nagu autonoomsed sÃµidukid, tÃ¶Ã¶stusautomaatika vÃµi interaktiivsed rakendused, kus on vaja koheseid vastuseid.

### VÃµrguÃ¼henduseta vÃµimekus

EdgeAI vÃµimaldab AI funktsionaalsust isegi siis, kui internetiÃ¼hendus puudub. See on vÃ¤Ã¤rtuslik rakenduste jaoks kaugemates piirkondades, reisimise ajal vÃµi olukordades, kus vÃµrgu usaldusvÃ¤Ã¤rsus on probleem.

### KulutÃµhusus

VÃ¤hendades sÃµltuvust pilvepÃµhistest AI teenustest, vÃµib EdgeAI aidata vÃ¤hendada tegevuskulusid, eriti rakenduste puhul, millel on suur kasutusmaht. Organisatsioonid saavad vÃ¤ltida pidevaid API kulusid ja vÃ¤hendada ribalaiuse nÃµudeid.

### Skaleeritavus

EdgeAI jaotab arvutuskoormuse Ã¤Ã¤reseadmete vahel, mitte ei tsentraliseeri seda andmekeskustes. See vÃµib aidata vÃ¤hendada taristukulusid ja parandada kogu sÃ¼steemi skaleeritavust.

## EdgeAI rakendused

### Nutiseadmed ja IoT

EdgeAI toetab paljusid nutiseadmete funktsioone, alates hÃ¤Ã¤lassistentidest, mis suudavad kÃ¤ske kohapeal tÃ¶Ã¶delda, kuni nutikate kaamerateni, mis suudavad tuvastada objekte ja inimesi ilma videot pilve saatmata. IoT-seadmed kasutavad EdgeAI-d ennustava hoolduse, keskkonnaseire ja automatiseeritud otsuste tegemise jaoks.

### Mobiilirakendused

Nutitelefonid ja tahvelarvutid kasutavad EdgeAI-d mitmesuguste funktsioonide jaoks, sealhulgas fotot
- [02: EdgeAI Rakendused](02.RealWorldCaseStudies.md)

---

**LahtiÃ¼tlus**:  
See dokument on tÃµlgitud AI tÃµlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi pÃ¼Ã¼ame tagada tÃ¤psust, palume arvestada, et automaatsed tÃµlked vÃµivad sisaldada vigu vÃµi ebatÃ¤psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtÃµlget. Me ei vastuta arusaamatuste vÃµi valesti tÃµlgenduste eest, mis vÃµivad tekkida selle tÃµlke kasutamise tÃµttu.